SLURM_JOBID=2333153
SLURM_JOB_NODELIST=ise-cpu-intl-18
Device is set up to cpu
Seed is set to 0
Logging to models/ShieldPPO_run/logs
Using cpu device
Warning: State is not initialized. Returning an unmasked action mask.
Warning: State is not initialized. Returning an unmasked action mask.
Warning: State is not initialized. Returning an unmasked action mask.
Warning: State is not initialized. Returning an unmasked action mask.
Progress update from timestep 100: Updated shield, loss is 0.1683996021747589
Progress update from timestep 200: Updated shield, loss is 0.14156368374824524
Eval num_timesteps=200, episode_reward=8.71 +/- 12.43
Episode length: 13.60 +/- 18.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.6     |
|    mean_reward     | 8.71     |
| time/              |          |
|    total_timesteps | 200      |
---------------------------------
New best mean reward!
Progress update from timestep 300: Updated shield, loss is 0.1535486802458763
Progress update from timestep 400: Updated shield, loss is 0.15503011643886566
Eval num_timesteps=400, episode_reward=18.06 +/- 14.90
Episode length: 26.20 +/- 20.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | 18.1     |
| time/              |          |
|    total_timesteps | 400      |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.82     |
|    ep_rew_mean     | 3.86     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1        |
|    time_elapsed    | 113      |
|    total_timesteps | 400      |
---------------------------------
Progress update from timestep 500: Updated shield, loss is 0.15440476685762405
Progress update from timestep 600: Updated shield, loss is 0.1495145633816719
Eval num_timesteps=600, episode_reward=2.66 +/- 1.42
Episode length: 4.00 +/- 1.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 4            |
|    mean_reward          | 2.66         |
| time/                   |              |
|    total_timesteps      | 600          |
| train/                  |              |
|    approx_kl            | 1.557331e-05 |
|    clip_fraction        | 0.000223     |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0165      |
|    explained_variance   | -0.0535      |
|    learning_rate        | 0.0001       |
|    loss                 | 1.56         |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.000205    |
|    value_loss           | 3.51         |
------------------------------------------
Progress update from timestep 700: Updated shield, loss is 0.15475398302078247
Progress update from timestep 800: Updated shield, loss is 0.15531832724809647
Eval num_timesteps=800, episode_reward=2.58 +/- 1.27
Episode length: 3.80 +/- 1.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3.8      |
|    mean_reward     | 2.58     |
| time/              |          |
|    total_timesteps | 800      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.22     |
|    ep_rew_mean     | 3.5      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 2        |
|    time_elapsed    | 166      |
|    total_timesteps | 800      |
---------------------------------
Progress update from timestep 900: Updated shield, loss is 0.15202108770608902
Progress update from timestep 1000: Updated shield, loss is 0.15556465834379196
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 1000: Saved shield_loss.csv. 
Eval num_timesteps=1000, episode_reward=3.50 +/- 5.22
Episode length: 5.00 +/- 6.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 3.5       |
| time/                   |           |
|    total_timesteps      | 1000      |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | -0.0706   |
|    learning_rate        | 0.0001    |
|    loss                 | 0.584     |
|    n_updates            | 20        |
|    policy_gradient_loss | -2.93e-10 |
|    value_loss           | 2.7       |
---------------------------------------
Progress update from timestep 1100: Updated shield, loss is 0.15402574837207794
Progress update from timestep 1200: Updated shield, loss is 0.1525743156671524
Eval num_timesteps=1200, episode_reward=1.23 +/- 0.41
Episode length: 2.40 +/- 0.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.4      |
|    mean_reward     | 1.23     |
| time/              |          |
|    total_timesteps | 1200     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.93     |
|    ep_rew_mean     | 3.27     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 3        |
|    time_elapsed    | 223      |
|    total_timesteps | 1200     |
---------------------------------
Progress update from timestep 1300: Updated shield, loss is 0.15305303037166595
Progress update from timestep 1400: Updated shield, loss is 0.1551608145236969
Eval num_timesteps=1400, episode_reward=2.99 +/- 1.82
Episode length: 4.40 +/- 2.06
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 4.4      |
|    mean_reward          | 2.99     |
| time/                   |          |
|    total_timesteps      | 1400     |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | -0.0232  |
|    learning_rate        | 0.0001   |
|    loss                 | 1.21     |
|    n_updates            | 30       |
|    policy_gradient_loss | 3.26e-10 |
|    value_loss           | 2.36     |
--------------------------------------
Progress update from timestep 1500: Updated shield, loss is 0.15350864827632904
Progress update from timestep 1600: Updated shield, loss is 0.15668004751205444
Eval num_timesteps=1600, episode_reward=1.47 +/- 1.38
Episode length: 2.60 +/- 1.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.6      |
|    mean_reward     | 1.47     |
| time/              |          |
|    total_timesteps | 1600     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.94     |
|    ep_rew_mean     | 3.25     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 4        |
|    time_elapsed    | 276      |
|    total_timesteps | 1600     |
---------------------------------
Progress update from timestep 1700: Updated shield, loss is 0.1516437530517578
Progress update from timestep 1800: Updated shield, loss is 0.14994274079799652
Eval num_timesteps=1800, episode_reward=3.71 +/- 1.79
Episode length: 5.20 +/- 2.04
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5.2      |
|    mean_reward          | 3.71     |
| time/                   |          |
|    total_timesteps      | 1800     |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.00691  |
|    learning_rate        | 0.0001   |
|    loss                 | 0.853    |
|    n_updates            | 40       |
|    policy_gradient_loss | 2.21e-09 |
|    value_loss           | 2.55     |
--------------------------------------
Progress update from timestep 1900: Updated shield, loss is 0.14454138278961182
Progress update from timestep 2000: Updated shield, loss is 0.1564287394285202
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 2000: Saved shield_loss.csv. 
Eval num_timesteps=2000, episode_reward=2.97 +/- 2.53
Episode length: 4.40 +/- 2.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.4      |
|    mean_reward     | 2.97     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.18     |
|    ep_rew_mean     | 3.4      |
| time/              |          |
|    fps             | 6        |
|    iterations      | 5        |
|    time_elapsed    | 331      |
|    total_timesteps | 2000     |
---------------------------------
Progress update from timestep 2100: Updated shield, loss is 0.15490618348121643
Progress update from timestep 2200: Updated shield, loss is 0.1559412181377411
Eval num_timesteps=2200, episode_reward=0.91 +/- 0.51
Episode length: 2.00 +/- 0.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 2         |
|    mean_reward          | 0.91      |
| time/                   |           |
|    total_timesteps      | 2200      |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.00191   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.02      |
|    n_updates            | 50        |
|    policy_gradient_loss | -9.98e-11 |
|    value_loss           | 2.4       |
---------------------------------------
Progress update from timestep 2300: Updated shield, loss is 0.14800332486629486
Progress update from timestep 2400: Updated shield, loss is 0.16380180418491364
Eval num_timesteps=2400, episode_reward=3.37 +/- 1.45
Episode length: 4.80 +/- 1.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.8      |
|    mean_reward     | 3.37     |
| time/              |          |
|    total_timesteps | 2400     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.37     |
|    ep_rew_mean     | 2.78     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 6        |
|    time_elapsed    | 385      |
|    total_timesteps | 2400     |
---------------------------------
Progress update from timestep 2500: Updated shield, loss is 0.15732169151306152
Progress update from timestep 2600: Updated shield, loss is 0.1483306586742401
Eval num_timesteps=2600, episode_reward=2.14 +/- 1.38
Episode length: 3.40 +/- 1.62
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 3.4      |
|    mean_reward          | 2.14     |
| time/                   |          |
|    total_timesteps      | 2600     |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0108   |
|    learning_rate        | 0.0001   |
|    loss                 | 0.848    |
|    n_updates            | 60       |
|    policy_gradient_loss | 2.56e-10 |
|    value_loss           | 1.6      |
--------------------------------------
Progress update from timestep 2700: Updated shield, loss is 0.1344940960407257
Progress update from timestep 2800: Updated shield, loss is 0.15450623631477356
Eval num_timesteps=2800, episode_reward=2.50 +/- 1.73
Episode length: 3.80 +/- 2.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3.8      |
|    mean_reward     | 2.5      |
| time/              |          |
|    total_timesteps | 2800     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.06     |
|    ep_rew_mean     | 3.34     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 7        |
|    time_elapsed    | 439      |
|    total_timesteps | 2800     |
---------------------------------
Progress update from timestep 2900: Updated shield, loss is 0.15644513070583344
Progress update from timestep 3000: Updated shield, loss is 0.1419481635093689
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 3000: Saved shield_loss.csv. 
Eval num_timesteps=3000, episode_reward=2.36 +/- 1.72
Episode length: 3.60 +/- 1.96
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 3.6      |
|    mean_reward          | 2.36     |
| time/                   |          |
|    total_timesteps      | 3000     |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0934   |
|    learning_rate        | 0.0001   |
|    loss                 | 1.73     |
|    n_updates            | 70       |
|    policy_gradient_loss | 2.35e-09 |
|    value_loss           | 2.4      |
--------------------------------------
Progress update from timestep 3100: Updated shield, loss is 0.15483038127422333
Progress update from timestep 3200: Updated shield, loss is 0.15167810022830963
Eval num_timesteps=3200, episode_reward=2.99 +/- 2.86
Episode length: 4.40 +/- 3.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.4      |
|    mean_reward     | 2.99     |
| time/              |          |
|    total_timesteps | 3200     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.55     |
|    ep_rew_mean     | 2.97     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 8        |
|    time_elapsed    | 494      |
|    total_timesteps | 3200     |
---------------------------------
Progress update from timestep 3300: Updated shield, loss is 0.15421520173549652
Progress update from timestep 3400: Updated shield, loss is 0.14887507259845734
Eval num_timesteps=3400, episode_reward=4.22 +/- 4.88
Episode length: 5.80 +/- 5.64
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5.8      |
|    mean_reward          | 4.22     |
| time/                   |          |
|    total_timesteps      | 3400     |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0873   |
|    learning_rate        | 0.0001   |
|    loss                 | 0.431    |
|    n_updates            | 80       |
|    policy_gradient_loss | 1.66e-12 |
|    value_loss           | 1.76     |
--------------------------------------
Progress update from timestep 3500: Updated shield, loss is 0.13965992629528046
Progress update from timestep 3600: Updated shield, loss is 0.14626029133796692
Eval num_timesteps=3600, episode_reward=1.47 +/- 0.46
Episode length: 2.60 +/- 0.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.6      |
|    mean_reward     | 1.47     |
| time/              |          |
|    total_timesteps | 3600     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.06     |
|    ep_rew_mean     | 3.38     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 9        |
|    time_elapsed    | 552      |
|    total_timesteps | 3600     |
---------------------------------
Progress update from timestep 3700: Updated shield, loss is 0.15627701580524445
Progress update from timestep 3800: Updated shield, loss is 0.13477955758571625
Eval num_timesteps=3800, episode_reward=3.20 +/- 3.70
Episode length: 4.60 +/- 4.27
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 4.6      |
|    mean_reward          | 3.2      |
| time/                   |          |
|    total_timesteps      | 3800     |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.00528  |
|    learning_rate        | 0.0001   |
|    loss                 | 2.01     |
|    n_updates            | 90       |
|    policy_gradient_loss | 2.06e-09 |
|    value_loss           | 3.49     |
--------------------------------------
Progress update from timestep 3900: Updated shield, loss is 0.14149218797683716
Progress update from timestep 4000: Updated shield, loss is 0.15517856180667877
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 4000: Saved shield_loss.csv. 
Eval num_timesteps=4000, episode_reward=3.36 +/- 2.05
Episode length: 4.80 +/- 2.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.8      |
|    mean_reward     | 3.36     |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.75     |
|    ep_rew_mean     | 3.14     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 10       |
|    time_elapsed    | 608      |
|    total_timesteps | 4000     |
---------------------------------
Progress update from timestep 4100: Updated shield, loss is 0.15067653357982635
Progress update from timestep 4200: Updated shield, loss is 0.1486220508813858
Eval num_timesteps=4200, episode_reward=8.03 +/- 6.05
Episode length: 10.20 +/- 6.94
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 10.2     |
|    mean_reward          | 8.03     |
| time/                   |          |
|    total_timesteps      | 4200     |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.00164  |
|    learning_rate        | 0.0001   |
|    loss                 | 0.82     |
|    n_updates            | 100      |
|    policy_gradient_loss | 1.42e-09 |
|    value_loss           | 2.45     |
--------------------------------------
Progress update from timestep 4300: Updated shield, loss is 0.15440469980239868
Progress update from timestep 4400: Updated shield, loss is 0.15239831805229187
Eval num_timesteps=4400, episode_reward=8.04 +/- 6.38
Episode length: 10.20 +/- 7.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10.2     |
|    mean_reward     | 8.04     |
| time/              |          |
|    total_timesteps | 4400     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.56     |
|    ep_rew_mean     | 2.92     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 11       |
|    time_elapsed    | 677      |
|    total_timesteps | 4400     |
---------------------------------
Progress update from timestep 4500: Updated shield, loss is 0.14386899769306183
Progress update from timestep 4600: Updated shield, loss is 0.14660240709781647
Eval num_timesteps=4600, episode_reward=2.99 +/- 2.61
Episode length: 4.40 +/- 3.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 4.4       |
|    mean_reward          | 2.99      |
| time/                   |           |
|    total_timesteps      | 4600      |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.0398    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.12      |
|    n_updates            | 110       |
|    policy_gradient_loss | -2.59e-09 |
|    value_loss           | 2.2       |
---------------------------------------
Progress update from timestep 4700: Updated shield, loss is 0.1503271609544754
Progress update from timestep 4800: Updated shield, loss is 0.14752621948719025
Eval num_timesteps=4800, episode_reward=4.05 +/- 2.54
Episode length: 5.60 +/- 2.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5.6      |
|    mean_reward     | 4.05     |
| time/              |          |
|    total_timesteps | 4800     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.59     |
|    ep_rew_mean     | 2.95     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 12       |
|    time_elapsed    | 733      |
|    total_timesteps | 4800     |
---------------------------------
Progress update from timestep 4900: Updated shield, loss is 0.14603228867053986
Progress update from timestep 5000: Updated shield, loss is 0.15042106807231903
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 5000: Saved shield_loss.csv. 
Eval num_timesteps=5000, episode_reward=1.10 +/- 0.59
Episode length: 2.20 +/- 0.75
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 2.2      |
|    mean_reward          | 1.1      |
| time/                   |          |
|    total_timesteps      | 5000     |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0716   |
|    learning_rate        | 0.0001   |
|    loss                 | 1.05     |
|    n_updates            | 120      |
|    policy_gradient_loss | 2.32e-09 |
|    value_loss           | 1.71     |
--------------------------------------
Progress update from timestep 5100: Updated shield, loss is 0.13667747378349304
Progress update from timestep 5200: Updated shield, loss is 0.14420410990715027
Eval num_timesteps=5200, episode_reward=3.36 +/- 1.52
Episode length: 4.80 +/- 1.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.8      |
|    mean_reward     | 3.36     |
| time/              |          |
|    total_timesteps | 5200     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.6      |
|    ep_rew_mean     | 2.99     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 13       |
|    time_elapsed    | 786      |
|    total_timesteps | 5200     |
---------------------------------
Progress update from timestep 5300: Updated shield, loss is 0.1523626446723938
Progress update from timestep 5400: Updated shield, loss is 0.1459110826253891
Eval num_timesteps=5400, episode_reward=3.35 +/- 2.01
Episode length: 4.80 +/- 2.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 4.8       |
|    mean_reward          | 3.35      |
| time/                   |           |
|    total_timesteps      | 5400      |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.0312    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.22      |
|    n_updates            | 130       |
|    policy_gradient_loss | -8.05e-10 |
|    value_loss           | 2.06      |
---------------------------------------
Progress update from timestep 5500: Updated shield, loss is 0.14837655425071716
Progress update from timestep 5600: Updated shield, loss is 0.1538999378681183
Eval num_timesteps=5600, episode_reward=1.64 +/- 0.64
Episode length: 2.80 +/- 0.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.8      |
|    mean_reward     | 1.64     |
| time/              |          |
|    total_timesteps | 5600     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.63     |
|    ep_rew_mean     | 3.03     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 14       |
|    time_elapsed    | 839      |
|    total_timesteps | 5600     |
---------------------------------
Progress update from timestep 5700: Updated shield, loss is 0.14543601870536804
Progress update from timestep 5800: Updated shield, loss is 0.15600918233394623
Eval num_timesteps=5800, episode_reward=3.53 +/- 3.08
Episode length: 5.00 +/- 3.52
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5        |
|    mean_reward          | 3.53     |
| time/                   |          |
|    total_timesteps      | 5800     |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0539   |
|    learning_rate        | 0.0001   |
|    loss                 | 0.928    |
|    n_updates            | 140      |
|    policy_gradient_loss | 9.31e-10 |
|    value_loss           | 2.49     |
--------------------------------------
Progress update from timestep 5900: Updated shield, loss is 0.14615923166275024
Progress update from timestep 6000: Updated shield, loss is 0.1503526270389557
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 6000: Saved shield_loss.csv. 
Eval num_timesteps=6000, episode_reward=1.44 +/- 0.88
Episode length: 2.60 +/- 1.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.6      |
|    mean_reward     | 1.44     |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.23     |
|    ep_rew_mean     | 3.48     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 15       |
|    time_elapsed    | 893      |
|    total_timesteps | 6000     |
---------------------------------
Progress update from timestep 6100: Updated shield, loss is 0.15221241116523743
Progress update from timestep 6200: Updated shield, loss is 0.14009426534175873
Eval num_timesteps=6200, episode_reward=4.58 +/- 1.40
Episode length: 6.20 +/- 1.60
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 6.2      |
|    mean_reward          | 4.58     |
| time/                   |          |
|    total_timesteps      | 6200     |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.00713  |
|    learning_rate        | 0.0001   |
|    loss                 | 1.02     |
|    n_updates            | 150      |
|    policy_gradient_loss | 1.26e-10 |
|    value_loss           | 2.13     |
--------------------------------------
Progress update from timestep 6300: Updated shield, loss is 0.15650023519992828
Progress update from timestep 6400: Updated shield, loss is 0.14890459179878235
Eval num_timesteps=6400, episode_reward=6.67 +/- 6.62
Episode length: 8.60 +/- 7.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8.6      |
|    mean_reward     | 6.67     |
| time/              |          |
|    total_timesteps | 6400     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.91     |
|    ep_rew_mean     | 3.21     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 16       |
|    time_elapsed    | 955      |
|    total_timesteps | 6400     |
---------------------------------
Progress update from timestep 6500: Updated shield, loss is 0.15114136040210724
Progress update from timestep 6600: Updated shield, loss is 0.1634475737810135
Eval num_timesteps=6600, episode_reward=2.70 +/- 1.59
Episode length: 4.00 +/- 1.90
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 4        |
|    mean_reward          | 2.7      |
| time/                   |          |
|    total_timesteps      | 6600     |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.144    |
|    learning_rate        | 0.0001   |
|    loss                 | 0.654    |
|    n_updates            | 160      |
|    policy_gradient_loss | 1.18e-09 |
|    value_loss           | 1.59     |
--------------------------------------
Progress update from timestep 6700: Updated shield, loss is 0.1559837907552719
Progress update from timestep 6800: Updated shield, loss is 0.14366160333156586
Eval num_timesteps=6800, episode_reward=2.14 +/- 0.90
Episode length: 3.40 +/- 1.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3.4      |
|    mean_reward     | 2.14     |
| time/              |          |
|    total_timesteps | 6800     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.81     |
|    ep_rew_mean     | 3.12     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 17       |
|    time_elapsed    | 1008     |
|    total_timesteps | 6800     |
---------------------------------
Progress update from timestep 6900: Updated shield, loss is 0.1399286985397339
Progress update from timestep 7000: Updated shield, loss is 0.15171806514263153
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 7000: Saved shield_loss.csv. 
Eval num_timesteps=7000, episode_reward=2.99 +/- 2.95
Episode length: 4.40 +/- 3.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 4.4       |
|    mean_reward          | 2.99      |
| time/                   |           |
|    total_timesteps      | 7000      |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.066     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.602     |
|    n_updates            | 170       |
|    policy_gradient_loss | -1.49e-09 |
|    value_loss           | 1.69      |
---------------------------------------
Progress update from timestep 7100: Updated shield, loss is 0.14634482562541962
Progress update from timestep 7200: Updated shield, loss is 0.14083512127399445
Eval num_timesteps=7200, episode_reward=2.52 +/- 1.91
Episode length: 3.80 +/- 2.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3.8      |
|    mean_reward     | 2.52     |
| time/              |          |
|    total_timesteps | 7200     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.85     |
|    ep_rew_mean     | 3.15     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 18       |
|    time_elapsed    | 1066     |
|    total_timesteps | 7200     |
---------------------------------
Progress update from timestep 7300: Updated shield, loss is 0.13745060563087463
Progress update from timestep 7400: Updated shield, loss is 0.1405578851699829
Eval num_timesteps=7400, episode_reward=3.89 +/- 3.17
Episode length: 5.40 +/- 3.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5.4       |
|    mean_reward          | 3.89      |
| time/                   |           |
|    total_timesteps      | 7400      |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.0544    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.1       |
|    n_updates            | 180       |
|    policy_gradient_loss | -1.62e-09 |
|    value_loss           | 2.29      |
---------------------------------------
Progress update from timestep 7500: Updated shield, loss is 0.14105448126792908
Progress update from timestep 7600: Updated shield, loss is 0.14420180022716522
Eval num_timesteps=7600, episode_reward=1.47 +/- 1.09
Episode length: 2.60 +/- 1.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.6      |
|    mean_reward     | 1.47     |
| time/              |          |
|    total_timesteps | 7600     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.06     |
|    ep_rew_mean     | 3.32     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 19       |
|    time_elapsed    | 1120     |
|    total_timesteps | 7600     |
---------------------------------
Progress update from timestep 7700: Updated shield, loss is 0.15212170779705048
Progress update from timestep 7800: Updated shield, loss is 0.14202362298965454
Eval num_timesteps=7800, episode_reward=2.83 +/- 0.66
Episode length: 4.20 +/- 0.75
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 4.2      |
|    mean_reward          | 2.83     |
| time/                   |          |
|    total_timesteps      | 7800     |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.138    |
|    learning_rate        | 0.0001   |
|    loss                 | 0.967    |
|    n_updates            | 190      |
|    policy_gradient_loss | 1.88e-09 |
|    value_loss           | 2.16     |
--------------------------------------
Progress update from timestep 7900: Updated shield, loss is 0.15893511474132538
Progress update from timestep 8000: Updated shield, loss is 0.15503843128681183
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 8000: Saved shield_loss.csv. 
Eval num_timesteps=8000, episode_reward=1.80 +/- 1.20
Episode length: 3.00 +/- 1.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3        |
|    mean_reward     | 1.8      |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.69     |
|    ep_rew_mean     | 3.06     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 20       |
|    time_elapsed    | 1174     |
|    total_timesteps | 8000     |
---------------------------------
Progress update from timestep 8100: Updated shield, loss is 0.1464315503835678
Progress update from timestep 8200: Updated shield, loss is 0.1499572992324829
Eval num_timesteps=8200, episode_reward=11.67 +/- 16.13
Episode length: 14.20 +/- 18.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 14.2      |
|    mean_reward          | 11.7      |
| time/                   |           |
|    total_timesteps      | 8200      |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.133     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.64      |
|    n_updates            | 200       |
|    policy_gradient_loss | -5.22e-10 |
|    value_loss           | 2.19      |
---------------------------------------
Progress update from timestep 8300: Updated shield, loss is 0.14835382997989655
Progress update from timestep 8400: Updated shield, loss is 0.14730659127235413
Eval num_timesteps=8400, episode_reward=6.83 +/- 10.50
Episode length: 8.80 +/- 12.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8.8      |
|    mean_reward     | 6.83     |
| time/              |          |
|    total_timesteps | 8400     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.18     |
|    ep_rew_mean     | 3.48     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 21       |
|    time_elapsed    | 1260     |
|    total_timesteps | 8400     |
---------------------------------
Progress update from timestep 8500: Updated shield, loss is 0.1465575397014618
Progress update from timestep 8600: Updated shield, loss is 0.15909433364868164
Eval num_timesteps=8600, episode_reward=4.57 +/- 4.54
Episode length: 6.20 +/- 5.23
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 6.2      |
|    mean_reward          | 4.57     |
| time/                   |          |
|    total_timesteps      | 8600     |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.141    |
|    learning_rate        | 0.0001   |
|    loss                 | 0.863    |
|    n_updates            | 210      |
|    policy_gradient_loss | 1.2e-09  |
|    value_loss           | 2.13     |
--------------------------------------
Progress update from timestep 8700: Updated shield, loss is 0.13650672137737274
Progress update from timestep 8800: Updated shield, loss is 0.1423197239637375
Eval num_timesteps=8800, episode_reward=8.38 +/- 9.83
Episode length: 10.60 +/- 11.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10.6     |
|    mean_reward     | 8.38     |
| time/              |          |
|    total_timesteps | 8800     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.77     |
|    ep_rew_mean     | 3.15     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 22       |
|    time_elapsed    | 1327     |
|    total_timesteps | 8800     |
---------------------------------
Progress update from timestep 8900: Updated shield, loss is 0.14664103090763092
Progress update from timestep 9000: Updated shield, loss is 0.1463254690170288
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 9000: Saved shield_loss.csv. 
Eval num_timesteps=9000, episode_reward=1.94 +/- 1.38
Episode length: 3.20 +/- 1.60
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 3.2      |
|    mean_reward          | 1.94     |
| time/                   |          |
|    total_timesteps      | 9000     |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0737   |
|    learning_rate        | 0.0001   |
|    loss                 | 1.37     |
|    n_updates            | 220      |
|    policy_gradient_loss | 1.4e-09  |
|    value_loss           | 1.99     |
--------------------------------------
Progress update from timestep 9100: Updated shield, loss is 0.1587359756231308
Progress update from timestep 9200: Updated shield, loss is 0.1556377410888672
Eval num_timesteps=9200, episode_reward=2.46 +/- 1.85
Episode length: 3.80 +/- 2.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3.8      |
|    mean_reward     | 2.46     |
| time/              |          |
|    total_timesteps | 9200     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.12     |
|    ep_rew_mean     | 2.63     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 23       |
|    time_elapsed    | 1381     |
|    total_timesteps | 9200     |
---------------------------------
Progress update from timestep 9300: Updated shield, loss is 0.1486516147851944
Progress update from timestep 9400: Updated shield, loss is 0.1375717669725418
Eval num_timesteps=9400, episode_reward=6.48 +/- 8.42
Episode length: 8.40 +/- 9.71
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 8.4       |
|    mean_reward          | 6.48      |
| time/                   |           |
|    total_timesteps      | 9400      |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.0863    |
|    learning_rate        | 0.0001    |
|    loss                 | 0.727     |
|    n_updates            | 230       |
|    policy_gradient_loss | -6.52e-10 |
|    value_loss           | 1.57      |
---------------------------------------
Progress update from timestep 9500: Updated shield, loss is 0.15129904448986053
Progress update from timestep 9600: Updated shield, loss is 0.14223910868167877
Eval num_timesteps=9600, episode_reward=1.96 +/- 1.00
Episode length: 3.20 +/- 1.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3.2      |
|    mean_reward     | 1.96     |
| time/              |          |
|    total_timesteps | 9600     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.48     |
|    ep_rew_mean     | 2.92     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 24       |
|    time_elapsed    | 1442     |
|    total_timesteps | 9600     |
---------------------------------
Progress update from timestep 9700: Updated shield, loss is 0.1472335159778595
Progress update from timestep 9800: Updated shield, loss is 0.1374790519475937
Eval num_timesteps=9800, episode_reward=1.97 +/- 1.49
Episode length: 3.20 +/- 1.72
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 3.2       |
|    mean_reward          | 1.97      |
| time/                   |           |
|    total_timesteps      | 9800      |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.0235    |
|    learning_rate        | 0.0001    |
|    loss                 | 0.689     |
|    n_updates            | 240       |
|    policy_gradient_loss | -1.93e-09 |
|    value_loss           | 2.2       |
---------------------------------------
Progress update from timestep 9900: Updated shield, loss is 0.15775495767593384
Progress update from timestep 10000: Updated shield, loss is 0.14292609691619873
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 10000: Saved shield_loss.csv. 
Eval num_timesteps=10000, episode_reward=2.16 +/- 1.85
Episode length: 3.40 +/- 2.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3.4      |
|    mean_reward     | 2.16     |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.56     |
|    ep_rew_mean     | 2.96     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 25       |
|    time_elapsed    | 1494     |
|    total_timesteps | 10000    |
---------------------------------
Progress update from timestep 10100: Updated shield, loss is 0.14632943272590637
Progress update from timestep 10200: Updated shield, loss is 0.14489033818244934
Eval num_timesteps=10200, episode_reward=1.95 +/- 0.67
Episode length: 3.20 +/- 0.75
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 3.2      |
|    mean_reward          | 1.95     |
| time/                   |          |
|    total_timesteps      | 10200    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0682   |
|    learning_rate        | 0.0001   |
|    loss                 | 0.547    |
|    n_updates            | 250      |
|    policy_gradient_loss | 3.58e-09 |
|    value_loss           | 2.43     |
--------------------------------------
Progress update from timestep 10300: Updated shield, loss is 0.15169118344783783
Progress update from timestep 10400: Updated shield, loss is 0.14945285022258759
Eval num_timesteps=10400, episode_reward=2.13 +/- 1.65
Episode length: 3.40 +/- 1.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3.4      |
|    mean_reward     | 2.13     |
| time/              |          |
|    total_timesteps | 10400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.25     |
|    ep_rew_mean     | 3.5      |
| time/              |          |
|    fps             | 6        |
|    iterations      | 26       |
|    time_elapsed    | 1548     |
|    total_timesteps | 10400    |
---------------------------------
Progress update from timestep 10500: Updated shield, loss is 0.1495065987110138
Progress update from timestep 10600: Updated shield, loss is 0.14064428210258484
Eval num_timesteps=10600, episode_reward=1.62 +/- 0.65
Episode length: 2.80 +/- 0.75
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 2.8      |
|    mean_reward          | 1.62     |
| time/                   |          |
|    total_timesteps      | 10600    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.115    |
|    learning_rate        | 0.0001   |
|    loss                 | 2.46     |
|    n_updates            | 260      |
|    policy_gradient_loss | 8.65e-10 |
|    value_loss           | 2.74     |
--------------------------------------
Progress update from timestep 10700: Updated shield, loss is 0.1402076780796051
Progress update from timestep 10800: Updated shield, loss is 0.15025408565998077
Eval num_timesteps=10800, episode_reward=2.32 +/- 2.81
Episode length: 3.60 +/- 3.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3.6      |
|    mean_reward     | 2.32     |
| time/              |          |
|    total_timesteps | 10800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.59     |
|    ep_rew_mean     | 2.97     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 27       |
|    time_elapsed    | 1603     |
|    total_timesteps | 10800    |
---------------------------------
Progress update from timestep 10900: Updated shield, loss is 0.14352448284626007
Progress update from timestep 11000: Updated shield, loss is 0.14446750283241272
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 11000: Saved shield_loss.csv. 
Eval num_timesteps=11000, episode_reward=3.87 +/- 0.89
Episode length: 5.40 +/- 1.02
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5.4      |
|    mean_reward          | 3.87     |
| time/                   |          |
|    total_timesteps      | 11000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0761   |
|    learning_rate        | 0.0001   |
|    loss                 | 1.1      |
|    n_updates            | 270      |
|    policy_gradient_loss | 3.73e-10 |
|    value_loss           | 1.64     |
--------------------------------------
Progress update from timestep 11100: Updated shield, loss is 0.149186909198761
Progress update from timestep 11200: Updated shield, loss is 0.15747980773448944
Eval num_timesteps=11200, episode_reward=4.91 +/- 3.32
Episode length: 6.60 +/- 3.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6.6      |
|    mean_reward     | 4.91     |
| time/              |          |
|    total_timesteps | 11200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.05     |
|    ep_rew_mean     | 3.32     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 28       |
|    time_elapsed    | 1660     |
|    total_timesteps | 11200    |
---------------------------------
Progress update from timestep 11300: Updated shield, loss is 0.15028810501098633
Progress update from timestep 11400: Updated shield, loss is 0.15857671201229095
Eval num_timesteps=11400, episode_reward=1.63 +/- 0.86
Episode length: 2.80 +/- 0.98
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 2.8      |
|    mean_reward          | 1.63     |
| time/                   |          |
|    total_timesteps      | 11400    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0742   |
|    learning_rate        | 0.0001   |
|    loss                 | 1.18     |
|    n_updates            | 280      |
|    policy_gradient_loss | 5.08e-09 |
|    value_loss           | 1.99     |
--------------------------------------
Progress update from timestep 11500: Updated shield, loss is 0.14693458378314972
Progress update from timestep 11600: Updated shield, loss is 0.1537269651889801
Eval num_timesteps=11600, episode_reward=2.83 +/- 1.98
Episode length: 4.20 +/- 2.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.2      |
|    mean_reward     | 2.83     |
| time/              |          |
|    total_timesteps | 11600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.52     |
|    ep_rew_mean     | 2.94     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 29       |
|    time_elapsed    | 1712     |
|    total_timesteps | 11600    |
---------------------------------
Progress update from timestep 11700: Updated shield, loss is 0.1512099653482437
Progress update from timestep 11800: Updated shield, loss is 0.14953653514385223
Eval num_timesteps=11800, episode_reward=3.01 +/- 2.25
Episode length: 4.40 +/- 2.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 4.4       |
|    mean_reward          | 3.01      |
| time/                   |           |
|    total_timesteps      | 11800     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.142     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.462     |
|    n_updates            | 290       |
|    policy_gradient_loss | -1.65e-09 |
|    value_loss           | 1.44      |
---------------------------------------
Progress update from timestep 11900: Updated shield, loss is 0.14194515347480774
Progress update from timestep 12000: Updated shield, loss is 0.13790729641914368
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 12000: Saved shield_loss.csv. 
Eval num_timesteps=12000, episode_reward=2.80 +/- 2.98
Episode length: 4.20 +/- 3.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.2      |
|    mean_reward     | 2.8      |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.56     |
|    ep_rew_mean     | 2.99     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 30       |
|    time_elapsed    | 1768     |
|    total_timesteps | 12000    |
---------------------------------
Progress update from timestep 12100: Updated shield, loss is 0.14728137850761414
Progress update from timestep 12200: Updated shield, loss is 0.15779253840446472
Eval num_timesteps=12200, episode_reward=2.46 +/- 1.28
Episode length: 3.80 +/- 1.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 3.8       |
|    mean_reward          | 2.46      |
| time/                   |           |
|    total_timesteps      | 12200     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.0429    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.09      |
|    n_updates            | 300       |
|    policy_gradient_loss | -1.49e-09 |
|    value_loss           | 2.44      |
---------------------------------------
Progress update from timestep 12300: Updated shield, loss is 0.13569003343582153
Progress update from timestep 12400: Updated shield, loss is 0.150130033493042
Eval num_timesteps=12400, episode_reward=2.34 +/- 1.23
Episode length: 3.60 +/- 1.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3.6      |
|    mean_reward     | 2.34     |
| time/              |          |
|    total_timesteps | 12400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.42     |
|    ep_rew_mean     | 2.8      |
| time/              |          |
|    fps             | 6        |
|    iterations      | 31       |
|    time_elapsed    | 1818     |
|    total_timesteps | 12400    |
---------------------------------
Progress update from timestep 12500: Updated shield, loss is 0.13860294222831726
Progress update from timestep 12600: Updated shield, loss is 0.14433932304382324
Eval num_timesteps=12600, episode_reward=5.96 +/- 6.33
Episode length: 7.80 +/- 7.30
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 7.8      |
|    mean_reward          | 5.96     |
| time/                   |          |
|    total_timesteps      | 12600    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0436   |
|    learning_rate        | 0.0001   |
|    loss                 | 1.19     |
|    n_updates            | 310      |
|    policy_gradient_loss | 1.41e-09 |
|    value_loss           | 2.01     |
--------------------------------------
Progress update from timestep 12700: Updated shield, loss is 0.1528339833021164
Progress update from timestep 12800: Updated shield, loss is 0.14641956984996796
Eval num_timesteps=12800, episode_reward=2.71 +/- 2.46
Episode length: 4.00 +/- 2.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 2.71     |
| time/              |          |
|    total_timesteps | 12800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.43     |
|    ep_rew_mean     | 3.6      |
| time/              |          |
|    fps             | 6        |
|    iterations      | 32       |
|    time_elapsed    | 1882     |
|    total_timesteps | 12800    |
---------------------------------
Progress update from timestep 12900: Updated shield, loss is 0.15024910867214203
Progress update from timestep 13000: Updated shield, loss is 0.14564338326454163
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 13000: Saved shield_loss.csv. 
Eval num_timesteps=13000, episode_reward=2.31 +/- 1.18
Episode length: 3.60 +/- 1.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 3.6       |
|    mean_reward          | 2.31      |
| time/                   |           |
|    total_timesteps      | 13000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.0448    |
|    learning_rate        | 0.0001    |
|    loss                 | 0.988     |
|    n_updates            | 320       |
|    policy_gradient_loss | -3.92e-09 |
|    value_loss           | 2.39      |
---------------------------------------
Progress update from timestep 13100: Updated shield, loss is 0.1545802652835846
Progress update from timestep 13200: Updated shield, loss is 0.14339745044708252
Eval num_timesteps=13200, episode_reward=3.36 +/- 2.20
Episode length: 4.80 +/- 2.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.8      |
|    mean_reward     | 3.36     |
| time/              |          |
|    total_timesteps | 13200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.8      |
|    ep_rew_mean     | 3.16     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 33       |
|    time_elapsed    | 1936     |
|    total_timesteps | 13200    |
---------------------------------
Progress update from timestep 13300: Updated shield, loss is 0.13971498608589172
Progress update from timestep 13400: Updated shield, loss is 0.14679761230945587
Eval num_timesteps=13400, episode_reward=3.00 +/- 2.18
Episode length: 4.40 +/- 2.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 4.4       |
|    mean_reward          | 3         |
| time/                   |           |
|    total_timesteps      | 13400     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.0454    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.38      |
|    n_updates            | 330       |
|    policy_gradient_loss | -6.72e-10 |
|    value_loss           | 2.68      |
---------------------------------------
Progress update from timestep 13500: Updated shield, loss is 0.13505467772483826
Progress update from timestep 13600: Updated shield, loss is 0.1354781836271286
Eval num_timesteps=13600, episode_reward=3.50 +/- 2.84
Episode length: 5.00 +/- 3.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 3.5      |
| time/              |          |
|    total_timesteps | 13600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.3      |
|    ep_rew_mean     | 3.53     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 34       |
|    time_elapsed    | 1991     |
|    total_timesteps | 13600    |
---------------------------------
Progress update from timestep 13700: Updated shield, loss is 0.1469574272632599
Progress update from timestep 13800: Updated shield, loss is 0.14422288537025452
Eval num_timesteps=13800, episode_reward=2.68 +/- 3.13
Episode length: 4.00 +/- 3.63
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 4        |
|    mean_reward          | 2.68     |
| time/                   |          |
|    total_timesteps      | 13800    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.184    |
|    learning_rate        | 0.0001   |
|    loss                 | 1.54     |
|    n_updates            | 340      |
|    policy_gradient_loss | 1.96e-09 |
|    value_loss           | 2.37     |
--------------------------------------
Progress update from timestep 13900: Updated shield, loss is 0.1545010358095169
Progress update from timestep 14000: Updated shield, loss is 0.1342516392469406
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 14000: Saved shield_loss.csv. 
Eval num_timesteps=14000, episode_reward=3.13 +/- 2.10
Episode length: 4.60 +/- 2.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.6      |
|    mean_reward     | 3.13     |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.4      |
|    ep_rew_mean     | 2.79     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 35       |
|    time_elapsed    | 2047     |
|    total_timesteps | 14000    |
---------------------------------
Progress update from timestep 14100: Updated shield, loss is 0.1343405544757843
Progress update from timestep 14200: Updated shield, loss is 0.1441040337085724
Eval num_timesteps=14200, episode_reward=2.12 +/- 2.43
Episode length: 3.40 +/- 2.80
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 3.4      |
|    mean_reward          | 2.12     |
| time/                   |          |
|    total_timesteps      | 14200    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0711   |
|    learning_rate        | 0.0001   |
|    loss                 | 0.904    |
|    n_updates            | 350      |
|    policy_gradient_loss | 0        |
|    value_loss           | 1.58     |
--------------------------------------
Progress update from timestep 14300: Updated shield, loss is 0.13559506833553314
Progress update from timestep 14400: Updated shield, loss is 0.13128425180912018
Eval num_timesteps=14400, episode_reward=3.52 +/- 1.88
Episode length: 5.00 +/- 2.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 3.52     |
| time/              |          |
|    total_timesteps | 14400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.61     |
|    ep_rew_mean     | 2.96     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 36       |
|    time_elapsed    | 2100     |
|    total_timesteps | 14400    |
---------------------------------
Progress update from timestep 14500: Updated shield, loss is 0.13615231215953827
Progress update from timestep 14600: Updated shield, loss is 0.1596478372812271
Eval num_timesteps=14600, episode_reward=1.58 +/- 1.00
Episode length: 2.80 +/- 1.17
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 2.8      |
|    mean_reward          | 1.58     |
| time/                   |          |
|    total_timesteps      | 14600    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | -0.00691 |
|    learning_rate        | 0.0001   |
|    loss                 | 1.08     |
|    n_updates            | 360      |
|    policy_gradient_loss | 3.25e-09 |
|    value_loss           | 2.09     |
--------------------------------------
Progress update from timestep 14700: Updated shield, loss is 0.1513977199792862
Progress update from timestep 14800: Updated shield, loss is 0.12834537029266357
Eval num_timesteps=14800, episode_reward=4.12 +/- 5.69
Episode length: 5.60 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5.6      |
|    mean_reward     | 4.12     |
| time/              |          |
|    total_timesteps | 14800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.91     |
|    ep_rew_mean     | 3.25     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 37       |
|    time_elapsed    | 2156     |
|    total_timesteps | 14800    |
---------------------------------
Progress update from timestep 14900: Updated shield, loss is 0.13406327366828918
Progress update from timestep 15000: Updated shield, loss is 0.1547677367925644
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 15000: Saved shield_loss.csv. 
Eval num_timesteps=15000, episode_reward=1.42 +/- 0.69
Episode length: 2.60 +/- 0.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 2.6       |
|    mean_reward          | 1.42      |
| time/                   |           |
|    total_timesteps      | 15000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.0467    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.46      |
|    n_updates            | 370       |
|    policy_gradient_loss | -5.72e-10 |
|    value_loss           | 2.38      |
---------------------------------------
Progress update from timestep 15100: Updated shield, loss is 0.14539168775081635
Progress update from timestep 15200: Updated shield, loss is 0.1519518792629242
Eval num_timesteps=15200, episode_reward=5.46 +/- 7.07
Episode length: 7.20 +/- 8.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7.2      |
|    mean_reward     | 5.46     |
| time/              |          |
|    total_timesteps | 15200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.49     |
|    ep_rew_mean     | 2.9      |
| time/              |          |
|    fps             | 6        |
|    iterations      | 38       |
|    time_elapsed    | 2214     |
|    total_timesteps | 15200    |
---------------------------------
Progress update from timestep 15300: Updated shield, loss is 0.15521666407585144
Progress update from timestep 15400: Updated shield, loss is 0.1313854157924652
Eval num_timesteps=15400, episode_reward=5.07 +/- 3.82
Episode length: 6.80 +/- 4.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 6.8       |
|    mean_reward          | 5.07      |
| time/                   |           |
|    total_timesteps      | 15400     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.00118   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.82      |
|    n_updates            | 380       |
|    policy_gradient_loss | -1.77e-09 |
|    value_loss           | 2.44      |
---------------------------------------
Progress update from timestep 15500: Updated shield, loss is 0.1517854928970337
Progress update from timestep 15600: Updated shield, loss is 0.1527603417634964
Eval num_timesteps=15600, episode_reward=2.12 +/- 1.71
Episode length: 3.40 +/- 1.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3.4      |
|    mean_reward     | 2.12     |
| time/              |          |
|    total_timesteps | 15600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.49     |
|    ep_rew_mean     | 2.93     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 39       |
|    time_elapsed    | 2270     |
|    total_timesteps | 15600    |
---------------------------------
Progress update from timestep 15700: Updated shield, loss is 0.13655902445316315
Progress update from timestep 15800: Updated shield, loss is 0.14270813763141632
Eval num_timesteps=15800, episode_reward=8.36 +/- 10.26
Episode length: 10.60 +/- 11.83
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 10.6     |
|    mean_reward          | 8.36     |
| time/                   |          |
|    total_timesteps      | 15800    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0835   |
|    learning_rate        | 0.0001   |
|    loss                 | 0.709    |
|    n_updates            | 390      |
|    policy_gradient_loss | 2.69e-09 |
|    value_loss           | 1.89     |
--------------------------------------
Progress update from timestep 15900: Updated shield, loss is 0.14841708540916443
Progress update from timestep 16000: Updated shield, loss is 0.14763891696929932
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 16000: Saved shield_loss.csv. 
Eval num_timesteps=16000, episode_reward=3.53 +/- 3.37
Episode length: 5.00 +/- 3.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 3.53     |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.89     |
|    ep_rew_mean     | 3.23     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 40       |
|    time_elapsed    | 2341     |
|    total_timesteps | 16000    |
---------------------------------
Progress update from timestep 16100: Updated shield, loss is 0.14358066022396088
Progress update from timestep 16200: Updated shield, loss is 0.13758203387260437
Eval num_timesteps=16200, episode_reward=3.01 +/- 2.88
Episode length: 4.40 +/- 3.32
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 4.4      |
|    mean_reward          | 3.01     |
| time/                   |          |
|    total_timesteps      | 16200    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0348   |
|    learning_rate        | 0.0001   |
|    loss                 | 1.29     |
|    n_updates            | 400      |
|    policy_gradient_loss | 3.03e-09 |
|    value_loss           | 2.69     |
--------------------------------------
Progress update from timestep 16300: Updated shield, loss is 0.1488035023212433
Progress update from timestep 16400: Updated shield, loss is 0.14090055227279663
Eval num_timesteps=16400, episode_reward=2.98 +/- 2.25
Episode length: 4.40 +/- 2.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.4      |
|    mean_reward     | 2.98     |
| time/              |          |
|    total_timesteps | 16400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.14     |
|    ep_rew_mean     | 3.4      |
| time/              |          |
|    fps             | 6        |
|    iterations      | 41       |
|    time_elapsed    | 2396     |
|    total_timesteps | 16400    |
---------------------------------
Progress update from timestep 16500: Updated shield, loss is 0.14696906507015228
Progress update from timestep 16600: Updated shield, loss is 0.16093654930591583
Eval num_timesteps=16600, episode_reward=2.14 +/- 1.18
Episode length: 3.40 +/- 1.36
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 3.4      |
|    mean_reward          | 2.14     |
| time/                   |          |
|    total_timesteps      | 16600    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.05     |
|    learning_rate        | 0.0001   |
|    loss                 | 0.722    |
|    n_updates            | 410      |
|    policy_gradient_loss | 1.4e-09  |
|    value_loss           | 2.64     |
--------------------------------------
Progress update from timestep 16700: Updated shield, loss is 0.1376076638698578
Progress update from timestep 16800: Updated shield, loss is 0.1421363651752472
Eval num_timesteps=16800, episode_reward=1.85 +/- 1.20
Episode length: 3.00 +/- 1.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3        |
|    mean_reward     | 1.85     |
| time/              |          |
|    total_timesteps | 16800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.83     |
|    ep_rew_mean     | 3.13     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 42       |
|    time_elapsed    | 2448     |
|    total_timesteps | 16800    |
---------------------------------
Progress update from timestep 16900: Updated shield, loss is 0.1387794017791748
Progress update from timestep 17000: Updated shield, loss is 0.14465872943401337
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 17000: Saved shield_loss.csv. 
Eval num_timesteps=17000, episode_reward=6.13 +/- 10.84
Episode length: 8.00 +/- 12.51
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 8        |
|    mean_reward          | 6.13     |
| time/                   |          |
|    total_timesteps      | 17000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0374   |
|    learning_rate        | 0.0001   |
|    loss                 | 1.17     |
|    n_updates            | 420      |
|    policy_gradient_loss | 2.04e-09 |
|    value_loss           | 1.84     |
--------------------------------------
Progress update from timestep 17100: Updated shield, loss is 0.1399974822998047
Progress update from timestep 17200: Updated shield, loss is 0.13969415426254272
Eval num_timesteps=17200, episode_reward=1.43 +/- 0.87
Episode length: 2.60 +/- 1.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.6      |
|    mean_reward     | 1.43     |
| time/              |          |
|    total_timesteps | 17200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.54     |
|    ep_rew_mean     | 2.88     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 43       |
|    time_elapsed    | 2510     |
|    total_timesteps | 17200    |
---------------------------------
Progress update from timestep 17300: Updated shield, loss is 0.14115992188453674
Progress update from timestep 17400: Updated shield, loss is 0.14567187428474426
Eval num_timesteps=17400, episode_reward=2.13 +/- 1.62
Episode length: 3.40 +/- 1.85
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 3.4      |
|    mean_reward          | 2.13     |
| time/                   |          |
|    total_timesteps      | 17400    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0926   |
|    learning_rate        | 0.0001   |
|    loss                 | 0.928    |
|    n_updates            | 430      |
|    policy_gradient_loss | 2.33e-10 |
|    value_loss           | 2.1      |
--------------------------------------
Progress update from timestep 17500: Updated shield, loss is 0.15118549764156342
Progress update from timestep 17600: Updated shield, loss is 0.12712454795837402
Eval num_timesteps=17600, episode_reward=2.33 +/- 1.84
Episode length: 3.60 +/- 2.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3.6      |
|    mean_reward     | 2.33     |
| time/              |          |
|    total_timesteps | 17600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.68     |
|    ep_rew_mean     | 3.03     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 44       |
|    time_elapsed    | 2563     |
|    total_timesteps | 17600    |
---------------------------------
Progress update from timestep 17700: Updated shield, loss is 0.13808608055114746
Progress update from timestep 17800: Updated shield, loss is 0.14181536436080933
Eval num_timesteps=17800, episode_reward=4.43 +/- 2.80
Episode length: 6.00 +/- 3.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 6         |
|    mean_reward          | 4.43      |
| time/                   |           |
|    total_timesteps      | 17800     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.0585    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.57      |
|    n_updates            | 440       |
|    policy_gradient_loss | -9.31e-11 |
|    value_loss           | 2.45      |
---------------------------------------
Progress update from timestep 17900: Updated shield, loss is 0.13842545449733734
Progress update from timestep 18000: Updated shield, loss is 0.14648020267486572
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 18000: Saved shield_loss.csv. 
Eval num_timesteps=18000, episode_reward=3.34 +/- 2.82
Episode length: 4.80 +/- 3.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.8      |
|    mean_reward     | 3.34     |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.92     |
|    ep_rew_mean     | 3.2      |
| time/              |          |
|    fps             | 6        |
|    iterations      | 45       |
|    time_elapsed    | 2618     |
|    total_timesteps | 18000    |
---------------------------------
Progress update from timestep 18100: Updated shield, loss is 0.15826648473739624
Progress update from timestep 18200: Updated shield, loss is 0.13706697523593903
Eval num_timesteps=18200, episode_reward=4.20 +/- 3.71
Episode length: 5.80 +/- 4.31
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5.8      |
|    mean_reward          | 4.2      |
| time/                   |          |
|    total_timesteps      | 18200    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0555   |
|    learning_rate        | 0.0001   |
|    loss                 | 1.78     |
|    n_updates            | 450      |
|    policy_gradient_loss | 3.21e-09 |
|    value_loss           | 2.22     |
--------------------------------------
Progress update from timestep 18300: Updated shield, loss is 0.1369752287864685
Progress update from timestep 18400: Updated shield, loss is 0.13583862781524658
Eval num_timesteps=18400, episode_reward=4.97 +/- 7.63
Episode length: 6.60 +/- 8.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6.6      |
|    mean_reward     | 4.97     |
| time/              |          |
|    total_timesteps | 18400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.85     |
|    ep_rew_mean     | 3.15     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 46       |
|    time_elapsed    | 2681     |
|    total_timesteps | 18400    |
---------------------------------
Progress update from timestep 18500: Updated shield, loss is 0.14712511003017426
Progress update from timestep 18600: Updated shield, loss is 0.13078907132148743
Eval num_timesteps=18600, episode_reward=5.41 +/- 3.88
Episode length: 7.20 +/- 4.45
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 7.2      |
|    mean_reward          | 5.41     |
| time/                   |          |
|    total_timesteps      | 18600    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.101    |
|    learning_rate        | 0.0001   |
|    loss                 | 1.06     |
|    n_updates            | 460      |
|    policy_gradient_loss | 2.79e-09 |
|    value_loss           | 2.21     |
--------------------------------------
Progress update from timestep 18700: Updated shield, loss is 0.14626213908195496
Progress update from timestep 18800: Updated shield, loss is 0.13982847332954407
Eval num_timesteps=18800, episode_reward=4.04 +/- 2.36
Episode length: 5.60 +/- 2.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5.6      |
|    mean_reward     | 4.04     |
| time/              |          |
|    total_timesteps | 18800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.98     |
|    ep_rew_mean     | 3.24     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 47       |
|    time_elapsed    | 2736     |
|    total_timesteps | 18800    |
---------------------------------
Progress update from timestep 18900: Updated shield, loss is 0.14496785402297974
Progress update from timestep 19000: Updated shield, loss is 0.15033289790153503
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 19000: Saved shield_loss.csv. 
Eval num_timesteps=19000, episode_reward=4.22 +/- 2.37
Episode length: 5.80 +/- 2.71
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5.8      |
|    mean_reward          | 4.22     |
| time/                   |          |
|    total_timesteps      | 19000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.105    |
|    learning_rate        | 0.0001   |
|    loss                 | 1.13     |
|    n_updates            | 470      |
|    policy_gradient_loss | 3.13e-09 |
|    value_loss           | 2.14     |
--------------------------------------
Progress update from timestep 19100: Updated shield, loss is 0.1434057354927063
Progress update from timestep 19200: Updated shield, loss is 0.13499858975410461
Eval num_timesteps=19200, episode_reward=3.69 +/- 3.21
Episode length: 5.20 +/- 3.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5.2      |
|    mean_reward     | 3.69     |
| time/              |          |
|    total_timesteps | 19200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.7      |
|    ep_rew_mean     | 3.02     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 48       |
|    time_elapsed    | 2792     |
|    total_timesteps | 19200    |
---------------------------------
Progress update from timestep 19300: Updated shield, loss is 0.1297256052494049
Progress update from timestep 19400: Updated shield, loss is 0.1351439207792282
Eval num_timesteps=19400, episode_reward=1.09 +/- 0.35
Episode length: 2.20 +/- 0.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 2.2       |
|    mean_reward          | 1.09      |
| time/                   |           |
|    total_timesteps      | 19400     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.146     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.835     |
|    n_updates            | 480       |
|    policy_gradient_loss | -6.92e-10 |
|    value_loss           | 1.75      |
---------------------------------------
Progress update from timestep 19500: Updated shield, loss is 0.14937013387680054
Progress update from timestep 19600: Updated shield, loss is 0.13588947057724
Eval num_timesteps=19600, episode_reward=6.64 +/- 8.03
Episode length: 8.60 +/- 9.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8.6      |
|    mean_reward     | 6.64     |
| time/              |          |
|    total_timesteps | 19600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.78     |
|    ep_rew_mean     | 3.09     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 49       |
|    time_elapsed    | 2853     |
|    total_timesteps | 19600    |
---------------------------------
Progress update from timestep 19700: Updated shield, loss is 0.14021293818950653
Progress update from timestep 19800: Updated shield, loss is 0.14686937630176544
Eval num_timesteps=19800, episode_reward=4.90 +/- 3.51
Episode length: 6.60 +/- 4.03
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 6.6      |
|    mean_reward          | 4.9      |
| time/                   |          |
|    total_timesteps      | 19800    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.144    |
|    learning_rate        | 0.0001   |
|    loss                 | 0.493    |
|    n_updates            | 490      |
|    policy_gradient_loss | 9.03e-10 |
|    value_loss           | 1.49     |
--------------------------------------
Progress update from timestep 19900: Updated shield, loss is 0.13342970609664917
Progress update from timestep 20000: Updated shield, loss is 0.14280179142951965
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 20000: Saved shield_loss.csv. 
Eval num_timesteps=20000, episode_reward=4.03 +/- 2.37
Episode length: 5.60 +/- 2.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5.6      |
|    mean_reward     | 4.03     |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.77     |
|    ep_rew_mean     | 3.13     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 50       |
|    time_elapsed    | 2909     |
|    total_timesteps | 20000    |
---------------------------------
Progress update from timestep 20100: Updated shield, loss is 0.1318465918302536
Progress update from timestep 20200: Updated shield, loss is 0.14355117082595825
Eval num_timesteps=20200, episode_reward=5.60 +/- 5.50
Episode length: 7.40 +/- 6.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 7.4       |
|    mean_reward          | 5.6       |
| time/                   |           |
|    total_timesteps      | 20200     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.0496    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.17      |
|    n_updates            | 500       |
|    policy_gradient_loss | -3.07e-09 |
|    value_loss           | 2.12      |
---------------------------------------
Progress update from timestep 20300: Updated shield, loss is 0.15142662823200226
Progress update from timestep 20400: Updated shield, loss is 0.14053313434123993
Eval num_timesteps=20400, episode_reward=4.74 +/- 6.36
Episode length: 6.40 +/- 7.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6.4      |
|    mean_reward     | 4.74     |
| time/              |          |
|    total_timesteps | 20400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.18     |
|    ep_rew_mean     | 2.65     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 51       |
|    time_elapsed    | 2972     |
|    total_timesteps | 20400    |
---------------------------------
Progress update from timestep 20500: Updated shield, loss is 0.1382012665271759
Progress update from timestep 20600: Updated shield, loss is 0.13448739051818848
Eval num_timesteps=20600, episode_reward=1.60 +/- 0.64
Episode length: 2.80 +/- 0.75
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 2.8      |
|    mean_reward          | 1.6      |
| time/                   |          |
|    total_timesteps      | 20600    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0862   |
|    learning_rate        | 0.0001   |
|    loss                 | 0.445    |
|    n_updates            | 510      |
|    policy_gradient_loss | 1.82e-09 |
|    value_loss           | 1.67     |
--------------------------------------
Progress update from timestep 20700: Updated shield, loss is 0.14046241343021393
Progress update from timestep 20800: Updated shield, loss is 0.13203637301921844
Eval num_timesteps=20800, episode_reward=5.06 +/- 5.23
Episode length: 6.80 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6.8      |
|    mean_reward     | 5.06     |
| time/              |          |
|    total_timesteps | 20800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.6      |
|    ep_rew_mean     | 3.78     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 52       |
|    time_elapsed    | 3026     |
|    total_timesteps | 20800    |
---------------------------------
Progress update from timestep 20900: Updated shield, loss is 0.13978002965450287
Progress update from timestep 21000: Updated shield, loss is 0.15498077869415283
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 21000: Saved shield_loss.csv. 
Eval num_timesteps=21000, episode_reward=2.71 +/- 1.83
Episode length: 4.00 +/- 2.10
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 4        |
|    mean_reward          | 2.71     |
| time/                   |          |
|    total_timesteps      | 21000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.216    |
|    learning_rate        | 0.0001   |
|    loss                 | 1.25     |
|    n_updates            | 520      |
|    policy_gradient_loss | 5.85e-09 |
|    value_loss           | 2.65     |
--------------------------------------
Progress update from timestep 21100: Updated shield, loss is 0.14524579048156738
Progress update from timestep 21200: Updated shield, loss is 0.13340230286121368
Eval num_timesteps=21200, episode_reward=3.17 +/- 1.83
Episode length: 4.60 +/- 2.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.6      |
|    mean_reward     | 3.17     |
| time/              |          |
|    total_timesteps | 21200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.78     |
|    ep_rew_mean     | 3.13     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 53       |
|    time_elapsed    | 3080     |
|    total_timesteps | 21200    |
---------------------------------
Progress update from timestep 21300: Updated shield, loss is 0.13470976054668427
Progress update from timestep 21400: Updated shield, loss is 0.14069868624210358
Eval num_timesteps=21400, episode_reward=3.35 +/- 1.58
Episode length: 4.80 +/- 1.83
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 4.8      |
|    mean_reward          | 3.35     |
| time/                   |          |
|    total_timesteps      | 21400    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.073    |
|    learning_rate        | 0.0001   |
|    loss                 | 0.795    |
|    n_updates            | 530      |
|    policy_gradient_loss | -2.2e-10 |
|    value_loss           | 2.48     |
--------------------------------------
Progress update from timestep 21500: Updated shield, loss is 0.14821606874465942
Progress update from timestep 21600: Updated shield, loss is 0.13935014605522156
Eval num_timesteps=21600, episode_reward=2.82 +/- 2.48
Episode length: 4.20 +/- 2.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.2      |
|    mean_reward     | 2.82     |
| time/              |          |
|    total_timesteps | 21600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.91     |
|    ep_rew_mean     | 2.42     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 54       |
|    time_elapsed    | 3132     |
|    total_timesteps | 21600    |
---------------------------------
Progress update from timestep 21700: Updated shield, loss is 0.13880608975887299
Progress update from timestep 21800: Updated shield, loss is 0.14326494932174683
Eval num_timesteps=21800, episode_reward=7.69 +/- 8.07
Episode length: 9.80 +/- 9.30
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 9.8      |
|    mean_reward          | 7.69     |
| time/                   |          |
|    total_timesteps      | 21800    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0343   |
|    learning_rate        | 0.0001   |
|    loss                 | 0.846    |
|    n_updates            | 540      |
|    policy_gradient_loss | 2.51e-09 |
|    value_loss           | 1.86     |
--------------------------------------
Progress update from timestep 21900: Updated shield, loss is 0.14719629287719727
Progress update from timestep 22000: Updated shield, loss is 0.13555708527565002
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 22000: Saved shield_loss.csv. 
Eval num_timesteps=22000, episode_reward=3.03 +/- 1.46
Episode length: 4.40 +/- 1.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.4      |
|    mean_reward     | 3.03     |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.91     |
|    ep_rew_mean     | 3.18     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 55       |
|    time_elapsed    | 3192     |
|    total_timesteps | 22000    |
---------------------------------
Progress update from timestep 22100: Updated shield, loss is 0.13480018079280853
Progress update from timestep 22200: Updated shield, loss is 0.13480429351329803
Eval num_timesteps=22200, episode_reward=2.32 +/- 1.47
Episode length: 3.60 +/- 1.74
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 3.6      |
|    mean_reward          | 2.32     |
| time/                   |          |
|    total_timesteps      | 22200    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.132    |
|    learning_rate        | 0.0001   |
|    loss                 | 0.906    |
|    n_updates            | 550      |
|    policy_gradient_loss | 2.06e-09 |
|    value_loss           | 1.61     |
--------------------------------------
Progress update from timestep 22300: Updated shield, loss is 0.1385136842727661
Progress update from timestep 22400: Updated shield, loss is 0.14329400658607483
Eval num_timesteps=22400, episode_reward=2.48 +/- 1.94
Episode length: 3.80 +/- 2.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3.8      |
|    mean_reward     | 2.48     |
| time/              |          |
|    total_timesteps | 22400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.38     |
|    ep_rew_mean     | 2.87     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 56       |
|    time_elapsed    | 3246     |
|    total_timesteps | 22400    |
---------------------------------
Progress update from timestep 22500: Updated shield, loss is 0.14631055295467377
Progress update from timestep 22600: Updated shield, loss is 0.1461600363254547
Eval num_timesteps=22600, episode_reward=7.17 +/- 9.15
Episode length: 9.20 +/- 10.53
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 9.2      |
|    mean_reward          | 7.17     |
| time/                   |          |
|    total_timesteps      | 22600    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0972   |
|    learning_rate        | 0.0001   |
|    loss                 | 1.11     |
|    n_updates            | 560      |
|    policy_gradient_loss | 1.2e-09  |
|    value_loss           | 1.75     |
--------------------------------------
Progress update from timestep 22700: Updated shield, loss is 0.14055339992046356
Progress update from timestep 22800: Updated shield, loss is 0.13784544169902802
Eval num_timesteps=22800, episode_reward=2.45 +/- 2.73
Episode length: 3.80 +/- 3.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3.8      |
|    mean_reward     | 2.45     |
| time/              |          |
|    total_timesteps | 22800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.98     |
|    ep_rew_mean     | 3.28     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 57       |
|    time_elapsed    | 3311     |
|    total_timesteps | 22800    |
---------------------------------
Progress update from timestep 22900: Updated shield, loss is 0.14908142387866974
Progress update from timestep 23000: Updated shield, loss is 0.1432204395532608
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 23000: Saved shield_loss.csv. 
Eval num_timesteps=23000, episode_reward=3.75 +/- 3.00
Episode length: 5.20 +/- 3.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5.2       |
|    mean_reward          | 3.75      |
| time/                   |           |
|    total_timesteps      | 23000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.0057    |
|    learning_rate        | 0.0001    |
|    loss                 | 0.936     |
|    n_updates            | 570       |
|    policy_gradient_loss | -5.26e-10 |
|    value_loss           | 2.49      |
---------------------------------------
Progress update from timestep 23100: Updated shield, loss is 0.1523350030183792
Progress update from timestep 23200: Updated shield, loss is 0.137077197432518
Eval num_timesteps=23200, episode_reward=4.61 +/- 2.89
Episode length: 6.20 +/- 3.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6.2      |
|    mean_reward     | 4.61     |
| time/              |          |
|    total_timesteps | 23200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.52     |
|    ep_rew_mean     | 3.65     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 58       |
|    time_elapsed    | 3368     |
|    total_timesteps | 23200    |
---------------------------------
Progress update from timestep 23300: Updated shield, loss is 0.14193612337112427
Progress update from timestep 23400: Updated shield, loss is 0.14174477756023407
Eval num_timesteps=23400, episode_reward=3.33 +/- 1.48
Episode length: 4.80 +/- 1.72
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 4.8      |
|    mean_reward          | 3.33     |
| time/                   |          |
|    total_timesteps      | 23400    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.121    |
|    learning_rate        | 0.0001   |
|    loss                 | 0.862    |
|    n_updates            | 580      |
|    policy_gradient_loss | 3.46e-10 |
|    value_loss           | 2.96     |
--------------------------------------
Progress update from timestep 23500: Updated shield, loss is 0.14829792082309723
Progress update from timestep 23600: Updated shield, loss is 0.14171922206878662
Eval num_timesteps=23600, episode_reward=2.85 +/- 1.43
Episode length: 4.20 +/- 1.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.2      |
|    mean_reward     | 2.85     |
| time/              |          |
|    total_timesteps | 23600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.77     |
|    ep_rew_mean     | 3.16     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 59       |
|    time_elapsed    | 3421     |
|    total_timesteps | 23600    |
---------------------------------
Progress update from timestep 23700: Updated shield, loss is 0.1456204205751419
Progress update from timestep 23800: Updated shield, loss is 0.15781280398368835
Eval num_timesteps=23800, episode_reward=2.68 +/- 1.36
Episode length: 4.00 +/- 1.55
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 4        |
|    mean_reward          | 2.68     |
| time/                   |          |
|    total_timesteps      | 23800    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.018    |
|    learning_rate        | 0.0001   |
|    loss                 | 0.87     |
|    n_updates            | 590      |
|    policy_gradient_loss | 3.82e-09 |
|    value_loss           | 2.06     |
--------------------------------------
Progress update from timestep 23900: Updated shield, loss is 0.13997220993041992
Progress update from timestep 24000: Updated shield, loss is 0.13291381299495697
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 24000: Saved shield_loss.csv. 
Eval num_timesteps=24000, episode_reward=1.45 +/- 0.85
Episode length: 2.60 +/- 1.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.6      |
|    mean_reward     | 1.45     |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.01     |
|    ep_rew_mean     | 3.28     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 60       |
|    time_elapsed    | 3473     |
|    total_timesteps | 24000    |
---------------------------------
Progress update from timestep 24100: Updated shield, loss is 0.13944053649902344
Progress update from timestep 24200: Updated shield, loss is 0.1492285430431366
Eval num_timesteps=24200, episode_reward=4.75 +/- 1.16
Episode length: 6.40 +/- 1.36
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 6.4      |
|    mean_reward          | 4.75     |
| time/                   |          |
|    total_timesteps      | 24200    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.158    |
|    learning_rate        | 0.0001   |
|    loss                 | 0.897    |
|    n_updates            | 600      |
|    policy_gradient_loss | 1.74e-09 |
|    value_loss           | 1.72     |
--------------------------------------
Progress update from timestep 24300: Updated shield, loss is 0.1338319033384323
Progress update from timestep 24400: Updated shield, loss is 0.13300257921218872
Eval num_timesteps=24400, episode_reward=4.01 +/- 5.86
Episode length: 5.60 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5.6      |
|    mean_reward     | 4.01     |
| time/              |          |
|    total_timesteps | 24400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.88     |
|    ep_rew_mean     | 3.12     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 61       |
|    time_elapsed    | 3533     |
|    total_timesteps | 24400    |
---------------------------------
Progress update from timestep 24500: Updated shield, loss is 0.1424008011817932
Progress update from timestep 24600: Updated shield, loss is 0.1371815949678421
Eval num_timesteps=24600, episode_reward=2.30 +/- 2.19
Episode length: 3.60 +/- 2.58
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 3.6      |
|    mean_reward          | 2.3      |
| time/                   |          |
|    total_timesteps      | 24600    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.133    |
|    learning_rate        | 0.0001   |
|    loss                 | 1.06     |
|    n_updates            | 610      |
|    policy_gradient_loss | 4.39e-10 |
|    value_loss           | 2.21     |
--------------------------------------
Progress update from timestep 24700: Updated shield, loss is 0.13400480151176453
Progress update from timestep 24800: Updated shield, loss is 0.1448560208082199
Eval num_timesteps=24800, episode_reward=2.84 +/- 1.33
Episode length: 4.20 +/- 1.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.2      |
|    mean_reward     | 2.84     |
| time/              |          |
|    total_timesteps | 24800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.66     |
|    ep_rew_mean     | 2.99     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 62       |
|    time_elapsed    | 3586     |
|    total_timesteps | 24800    |
---------------------------------
Progress update from timestep 24900: Updated shield, loss is 0.13918977975845337
Progress update from timestep 25000: Updated shield, loss is 0.13890968263149261
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 25000: Saved shield_loss.csv. 
Eval num_timesteps=25000, episode_reward=2.13 +/- 1.28
Episode length: 3.40 +/- 1.50
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 3.4      |
|    mean_reward          | 2.13     |
| time/                   |          |
|    total_timesteps      | 25000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.102    |
|    learning_rate        | 0.0001   |
|    loss                 | 1.33     |
|    n_updates            | 620      |
|    policy_gradient_loss | 2.79e-09 |
|    value_loss           | 2.3      |
--------------------------------------
Progress update from timestep 25100: Updated shield, loss is 0.13306394219398499
Progress update from timestep 25200: Updated shield, loss is 0.136914923787117
Eval num_timesteps=25200, episode_reward=5.07 +/- 6.29
Episode length: 6.80 +/- 7.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6.8      |
|    mean_reward     | 5.07     |
| time/              |          |
|    total_timesteps | 25200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.73     |
|    ep_rew_mean     | 3.09     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 63       |
|    time_elapsed    | 3643     |
|    total_timesteps | 25200    |
---------------------------------
Progress update from timestep 25300: Updated shield, loss is 0.14024199545383453
Progress update from timestep 25400: Updated shield, loss is 0.14069359004497528
Eval num_timesteps=25400, episode_reward=1.61 +/- 2.28
Episode length: 2.80 +/- 2.64
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 2.8      |
|    mean_reward          | 1.61     |
| time/                   |          |
|    total_timesteps      | 25400    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0903   |
|    learning_rate        | 0.0001   |
|    loss                 | 1.2      |
|    n_updates            | 630      |
|    policy_gradient_loss | 8.78e-10 |
|    value_loss           | 2.2      |
--------------------------------------
Progress update from timestep 25500: Updated shield, loss is 0.1385885328054428
Progress update from timestep 25600: Updated shield, loss is 0.1395081728696823
Eval num_timesteps=25600, episode_reward=2.84 +/- 3.06
Episode length: 4.20 +/- 3.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.2      |
|    mean_reward     | 2.84     |
| time/              |          |
|    total_timesteps | 25600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.69     |
|    ep_rew_mean     | 3.03     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 64       |
|    time_elapsed    | 3697     |
|    total_timesteps | 25600    |
---------------------------------
Progress update from timestep 25700: Updated shield, loss is 0.13538773357868195
Progress update from timestep 25800: Updated shield, loss is 0.12871377170085907
Eval num_timesteps=25800, episode_reward=2.66 +/- 1.59
Episode length: 4.00 +/- 1.90
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 4        |
|    mean_reward          | 2.66     |
| time/                   |          |
|    total_timesteps      | 25800    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0798   |
|    learning_rate        | 0.0001   |
|    loss                 | 0.956    |
|    n_updates            | 640      |
|    policy_gradient_loss | 6.52e-10 |
|    value_loss           | 1.53     |
--------------------------------------
Progress update from timestep 25900: Updated shield, loss is 0.1323956996202469
Progress update from timestep 26000: Updated shield, loss is 0.13027606904506683
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 26000: Saved shield_loss.csv. 
Eval num_timesteps=26000, episode_reward=3.36 +/- 1.28
Episode length: 4.80 +/- 1.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.8      |
|    mean_reward     | 3.36     |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.56     |
|    ep_rew_mean     | 2.93     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 65       |
|    time_elapsed    | 3748     |
|    total_timesteps | 26000    |
---------------------------------
Progress update from timestep 26100: Updated shield, loss is 0.13323427736759186
Progress update from timestep 26200: Updated shield, loss is 0.13277636468410492
Eval num_timesteps=26200, episode_reward=4.60 +/- 3.42
Episode length: 6.20 +/- 3.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 6.2       |
|    mean_reward          | 4.6       |
| time/                   |           |
|    total_timesteps      | 26200     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.105     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.711     |
|    n_updates            | 650       |
|    policy_gradient_loss | -1.18e-09 |
|    value_loss           | 1.48      |
---------------------------------------
Progress update from timestep 26300: Updated shield, loss is 0.13163746893405914
Progress update from timestep 26400: Updated shield, loss is 0.14048568904399872
Eval num_timesteps=26400, episode_reward=0.91 +/- 0.02
Episode length: 2.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2        |
|    mean_reward     | 0.905    |
| time/              |          |
|    total_timesteps | 26400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.78     |
|    ep_rew_mean     | 3.12     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 66       |
|    time_elapsed    | 3800     |
|    total_timesteps | 26400    |
---------------------------------
Progress update from timestep 26500: Updated shield, loss is 0.1325453370809555
Progress update from timestep 26600: Updated shield, loss is 0.14099565148353577
Eval num_timesteps=26600, episode_reward=3.17 +/- 1.58
Episode length: 4.60 +/- 1.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 4.6       |
|    mean_reward          | 3.17      |
| time/                   |           |
|    total_timesteps      | 26600     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.0597    |
|    learning_rate        | 0.0001    |
|    loss                 | 0.906     |
|    n_updates            | 660       |
|    policy_gradient_loss | -9.51e-10 |
|    value_loss           | 1.52      |
---------------------------------------
Progress update from timestep 26700: Updated shield, loss is 0.1318874955177307
Progress update from timestep 26800: Updated shield, loss is 0.1328154057264328
Eval num_timesteps=26800, episode_reward=5.09 +/- 4.23
Episode length: 6.80 +/- 4.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6.8      |
|    mean_reward     | 5.09     |
| time/              |          |
|    total_timesteps | 26800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.94     |
|    ep_rew_mean     | 3.21     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 67       |
|    time_elapsed    | 3857     |
|    total_timesteps | 26800    |
---------------------------------
Progress update from timestep 26900: Updated shield, loss is 0.14386042952537537
Progress update from timestep 27000: Updated shield, loss is 0.1415712684392929
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 27000: Saved shield_loss.csv. 
Eval num_timesteps=27000, episode_reward=4.42 +/- 3.54
Episode length: 6.00 +/- 4.05
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 6        |
|    mean_reward          | 4.42     |
| time/                   |          |
|    total_timesteps      | 27000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.107    |
|    learning_rate        | 0.0001   |
|    loss                 | 1.41     |
|    n_updates            | 670      |
|    policy_gradient_loss | 9.58e-10 |
|    value_loss           | 1.83     |
--------------------------------------
Progress update from timestep 27100: Updated shield, loss is 0.1382613629102707
Progress update from timestep 27200: Updated shield, loss is 0.13518784940242767
Eval num_timesteps=27200, episode_reward=2.65 +/- 1.82
Episode length: 4.00 +/- 2.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 2.65     |
| time/              |          |
|    total_timesteps | 27200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.04     |
|    ep_rew_mean     | 3.33     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 68       |
|    time_elapsed    | 3911     |
|    total_timesteps | 27200    |
---------------------------------
Progress update from timestep 27300: Updated shield, loss is 0.13179774582386017
Progress update from timestep 27400: Updated shield, loss is 0.13195165991783142
Eval num_timesteps=27400, episode_reward=3.55 +/- 1.46
Episode length: 5.00 +/- 1.67
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5        |
|    mean_reward          | 3.55     |
| time/                   |          |
|    total_timesteps      | 27400    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.065    |
|    learning_rate        | 0.0001   |
|    loss                 | 0.754    |
|    n_updates            | 680      |
|    policy_gradient_loss | 1.66e-09 |
|    value_loss           | 1.84     |
--------------------------------------
Progress update from timestep 27500: Updated shield, loss is 0.1382277011871338
Progress update from timestep 27600: Updated shield, loss is 0.14195218682289124
Eval num_timesteps=27600, episode_reward=11.85 +/- 15.75
Episode length: 14.40 +/- 17.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.4     |
|    mean_reward     | 11.9     |
| time/              |          |
|    total_timesteps | 27600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.9      |
|    ep_rew_mean     | 3.97     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 69       |
|    time_elapsed    | 3983     |
|    total_timesteps | 27600    |
---------------------------------
Progress update from timestep 27700: Updated shield, loss is 0.14515647292137146
Progress update from timestep 27800: Updated shield, loss is 0.1331438422203064
Eval num_timesteps=27800, episode_reward=4.76 +/- 4.80
Episode length: 6.40 +/- 5.54
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 6.4      |
|    mean_reward          | 4.76     |
| time/                   |          |
|    total_timesteps      | 27800    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0694   |
|    learning_rate        | 0.0001   |
|    loss                 | 1.01     |
|    n_updates            | 690      |
|    policy_gradient_loss | 2.83e-09 |
|    value_loss           | 2.58     |
--------------------------------------
Progress update from timestep 27900: Updated shield, loss is 0.13469266891479492
Progress update from timestep 28000: Updated shield, loss is 0.13926228880882263
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 28000: Saved shield_loss.csv. 
Eval num_timesteps=28000, episode_reward=5.22 +/- 6.15
Episode length: 7.00 +/- 7.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7        |
|    mean_reward     | 5.22     |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.69     |
|    ep_rew_mean     | 3.8      |
| time/              |          |
|    fps             | 6        |
|    iterations      | 70       |
|    time_elapsed    | 4046     |
|    total_timesteps | 28000    |
---------------------------------
Progress update from timestep 28100: Updated shield, loss is 0.13560813665390015
Progress update from timestep 28200: Updated shield, loss is 0.12920649349689484
Eval num_timesteps=28200, episode_reward=2.67 +/- 1.83
Episode length: 4.00 +/- 2.10
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 4        |
|    mean_reward          | 2.67     |
| time/                   |          |
|    total_timesteps      | 28200    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.152    |
|    learning_rate        | 0.0001   |
|    loss                 | 0.838    |
|    n_updates            | 700      |
|    policy_gradient_loss | -1.3e-09 |
|    value_loss           | 2.08     |
--------------------------------------
Progress update from timestep 28300: Updated shield, loss is 0.13824881613254547
Progress update from timestep 28400: Updated shield, loss is 0.13640064001083374
Eval num_timesteps=28400, episode_reward=1.99 +/- 2.57
Episode length: 3.20 +/- 2.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3.2      |
|    mean_reward     | 1.99     |
| time/              |          |
|    total_timesteps | 28400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.44     |
|    ep_rew_mean     | 3.68     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 71       |
|    time_elapsed    | 4100     |
|    total_timesteps | 28400    |
---------------------------------
Progress update from timestep 28500: Updated shield, loss is 0.13721494376659393
Progress update from timestep 28600: Updated shield, loss is 0.1372157484292984
Eval num_timesteps=28600, episode_reward=3.87 +/- 2.13
Episode length: 5.40 +/- 2.50
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5.4      |
|    mean_reward          | 3.87     |
| time/                   |          |
|    total_timesteps      | 28600    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0732   |
|    learning_rate        | 0.0001   |
|    loss                 | 1.64     |
|    n_updates            | 710      |
|    policy_gradient_loss | 1.13e-09 |
|    value_loss           | 2.63     |
--------------------------------------
Progress update from timestep 28700: Updated shield, loss is 0.14176468551158905
Progress update from timestep 28800: Updated shield, loss is 0.1426914483308792
Eval num_timesteps=28800, episode_reward=2.98 +/- 2.04
Episode length: 4.40 +/- 2.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.4      |
|    mean_reward     | 2.98     |
| time/              |          |
|    total_timesteps | 28800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.49     |
|    ep_rew_mean     | 3.7      |
| time/              |          |
|    fps             | 6        |
|    iterations      | 72       |
|    time_elapsed    | 4156     |
|    total_timesteps | 28800    |
---------------------------------
Progress update from timestep 28900: Updated shield, loss is 0.12740150094032288
Progress update from timestep 29000: Updated shield, loss is 0.14504116773605347
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 29000: Saved shield_loss.csv. 
Eval num_timesteps=29000, episode_reward=3.22 +/- 2.84
Episode length: 4.60 +/- 3.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 4.6       |
|    mean_reward          | 3.22      |
| time/                   |           |
|    total_timesteps      | 29000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.116     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.926     |
|    n_updates            | 720       |
|    policy_gradient_loss | -4.66e-10 |
|    value_loss           | 2.17      |
---------------------------------------
Progress update from timestep 29100: Updated shield, loss is 0.12708263099193573
Progress update from timestep 29200: Updated shield, loss is 0.13081666827201843
Eval num_timesteps=29200, episode_reward=2.22 +/- 1.97
Episode length: 3.40 +/- 2.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3.4      |
|    mean_reward     | 2.22     |
| time/              |          |
|    total_timesteps | 29200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.73     |
|    ep_rew_mean     | 3.88     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 73       |
|    time_elapsed    | 4212     |
|    total_timesteps | 29200    |
---------------------------------
Progress update from timestep 29300: Updated shield, loss is 0.12982651591300964
Progress update from timestep 29400: Updated shield, loss is 0.13654930889606476
Eval num_timesteps=29400, episode_reward=2.46 +/- 1.94
Episode length: 3.80 +/- 2.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 3.8       |
|    mean_reward          | 2.46      |
| time/                   |           |
|    total_timesteps      | 29400     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.0665    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.4       |
|    n_updates            | 730       |
|    policy_gradient_loss | -4.08e-09 |
|    value_loss           | 3.06      |
---------------------------------------
Progress update from timestep 29500: Updated shield, loss is 0.14064651727676392
Progress update from timestep 29600: Updated shield, loss is 0.12637677788734436
Eval num_timesteps=29600, episode_reward=1.94 +/- 1.69
Episode length: 3.20 +/- 1.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3.2      |
|    mean_reward     | 1.94     |
| time/              |          |
|    total_timesteps | 29600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.77     |
|    ep_rew_mean     | 3.1      |
| time/              |          |
|    fps             | 6        |
|    iterations      | 74       |
|    time_elapsed    | 4264     |
|    total_timesteps | 29600    |
---------------------------------
Progress update from timestep 29700: Updated shield, loss is 0.1355619728565216
Progress update from timestep 29800: Updated shield, loss is 0.11809128522872925
Eval num_timesteps=29800, episode_reward=3.88 +/- 2.02
Episode length: 5.40 +/- 2.33
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5.4      |
|    mean_reward          | 3.88     |
| time/                   |          |
|    total_timesteps      | 29800    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.169    |
|    learning_rate        | 0.0001   |
|    loss                 | 0.935    |
|    n_updates            | 740      |
|    policy_gradient_loss | 1.86e-09 |
|    value_loss           | 1.82     |
--------------------------------------
Progress update from timestep 29900: Updated shield, loss is 0.14241570234298706
Progress update from timestep 30000: Updated shield, loss is 0.1390378624200821
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 30000: Saved shield_loss.csv. 
Eval num_timesteps=30000, episode_reward=2.97 +/- 3.34
Episode length: 4.40 +/- 3.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.4      |
|    mean_reward     | 2.97     |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.6      |
|    ep_rew_mean     | 2.98     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 75       |
|    time_elapsed    | 4320     |
|    total_timesteps | 30000    |
---------------------------------
Progress update from timestep 30100: Updated shield, loss is 0.13156548142433167
Progress update from timestep 30200: Updated shield, loss is 0.13568313419818878
Eval num_timesteps=30200, episode_reward=2.54 +/- 2.83
Episode length: 3.80 +/- 3.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 3.8       |
|    mean_reward          | 2.54      |
| time/                   |           |
|    total_timesteps      | 30200     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.108     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.867     |
|    n_updates            | 750       |
|    policy_gradient_loss | -3.46e-10 |
|    value_loss           | 2.52      |
---------------------------------------
Progress update from timestep 30300: Updated shield, loss is 0.14111901819705963
Progress update from timestep 30400: Updated shield, loss is 0.13914763927459717
Eval num_timesteps=30400, episode_reward=4.91 +/- 5.45
Episode length: 6.60 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6.6      |
|    mean_reward     | 4.91     |
| time/              |          |
|    total_timesteps | 30400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 6.29     |
|    ep_rew_mean     | 4.31     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 76       |
|    time_elapsed    | 4380     |
|    total_timesteps | 30400    |
---------------------------------
Progress update from timestep 30500: Updated shield, loss is 0.12466161698102951
Progress update from timestep 30600: Updated shield, loss is 0.1301373988389969
Eval num_timesteps=30600, episode_reward=2.28 +/- 1.20
Episode length: 3.60 +/- 1.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 3.6       |
|    mean_reward          | 2.28      |
| time/                   |           |
|    total_timesteps      | 30600     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.247     |
|    learning_rate        | 0.0001    |
|    loss                 | 2.24      |
|    n_updates            | 760       |
|    policy_gradient_loss | -3.66e-10 |
|    value_loss           | 3.61      |
---------------------------------------
Progress update from timestep 30700: Updated shield, loss is 0.14151202142238617
Progress update from timestep 30800: Updated shield, loss is 0.12254466116428375
Eval num_timesteps=30800, episode_reward=4.91 +/- 3.27
Episode length: 6.60 +/- 3.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6.6      |
|    mean_reward     | 4.91     |
| time/              |          |
|    total_timesteps | 30800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.44     |
|    ep_rew_mean     | 2.91     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 77       |
|    time_elapsed    | 4436     |
|    total_timesteps | 30800    |
---------------------------------
Progress update from timestep 30900: Updated shield, loss is 0.13463549315929413
Progress update from timestep 31000: Updated shield, loss is 0.13020005822181702
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 31000: Saved shield_loss.csv. 
Eval num_timesteps=31000, episode_reward=1.78 +/- 1.42
Episode length: 3.00 +/- 1.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 3         |
|    mean_reward          | 1.78      |
| time/                   |           |
|    total_timesteps      | 31000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.277     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.04      |
|    n_updates            | 770       |
|    policy_gradient_loss | -3.04e-09 |
|    value_loss           | 2.68      |
---------------------------------------
Progress update from timestep 31100: Updated shield, loss is 0.1424991488456726
Progress update from timestep 31200: Updated shield, loss is 0.13679088652133942
Eval num_timesteps=31200, episode_reward=4.74 +/- 3.94
Episode length: 6.40 +/- 4.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6.4      |
|    mean_reward     | 4.74     |
| time/              |          |
|    total_timesteps | 31200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.04     |
|    ep_rew_mean     | 3.33     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 78       |
|    time_elapsed    | 4491     |
|    total_timesteps | 31200    |
---------------------------------
Progress update from timestep 31300: Updated shield, loss is 0.13011860847473145
Progress update from timestep 31400: Updated shield, loss is 0.13196437060832977
Eval num_timesteps=31400, episode_reward=2.18 +/- 1.31
Episode length: 3.40 +/- 1.50
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 3.4      |
|    mean_reward          | 2.18     |
| time/                   |          |
|    total_timesteps      | 31400    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | -0.0192  |
|    learning_rate        | 0.0001   |
|    loss                 | 1.13     |
|    n_updates            | 780      |
|    policy_gradient_loss | 6.72e-10 |
|    value_loss           | 3.11     |
--------------------------------------
Progress update from timestep 31500: Updated shield, loss is 0.12759976089000702
Progress update from timestep 31600: Updated shield, loss is 0.1285867989063263
Eval num_timesteps=31600, episode_reward=1.76 +/- 0.96
Episode length: 3.00 +/- 1.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3        |
|    mean_reward     | 1.76     |
| time/              |          |
|    total_timesteps | 31600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.77     |
|    ep_rew_mean     | 3.1      |
| time/              |          |
|    fps             | 6        |
|    iterations      | 79       |
|    time_elapsed    | 4543     |
|    total_timesteps | 31600    |
---------------------------------
Progress update from timestep 31700: Updated shield, loss is 0.13701674342155457
Progress update from timestep 31800: Updated shield, loss is 0.13660049438476562
Eval num_timesteps=31800, episode_reward=2.34 +/- 1.59
Episode length: 3.60 +/- 1.85
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 3.6      |
|    mean_reward          | 2.34     |
| time/                   |          |
|    total_timesteps      | 31800    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0197   |
|    learning_rate        | 0.0001   |
|    loss                 | 1.08     |
|    n_updates            | 790      |
|    policy_gradient_loss | 6.65e-11 |
|    value_loss           | 2.33     |
--------------------------------------
Progress update from timestep 31900: Updated shield, loss is 0.13281166553497314
Progress update from timestep 32000: Updated shield, loss is 0.1308128982782364
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 32000: Saved shield_loss.csv. 
Eval num_timesteps=32000, episode_reward=2.00 +/- 2.08
Episode length: 3.20 +/- 2.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3.2      |
|    mean_reward     | 2        |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.42     |
|    ep_rew_mean     | 3.6      |
| time/              |          |
|    fps             | 6        |
|    iterations      | 80       |
|    time_elapsed    | 4595     |
|    total_timesteps | 32000    |
---------------------------------
Progress update from timestep 32100: Updated shield, loss is 0.13830940425395966
Progress update from timestep 32200: Updated shield, loss is 0.13343220949172974
Eval num_timesteps=32200, episode_reward=1.24 +/- 0.43
Episode length: 2.40 +/- 0.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 2.4       |
|    mean_reward          | 1.24      |
| time/                   |           |
|    total_timesteps      | 32200     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.0101    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.14      |
|    n_updates            | 800       |
|    policy_gradient_loss | -2.56e-09 |
|    value_loss           | 2.63      |
---------------------------------------
Progress update from timestep 32300: Updated shield, loss is 0.13524623215198517
Progress update from timestep 32400: Updated shield, loss is 0.12836074829101562
Eval num_timesteps=32400, episode_reward=2.14 +/- 2.02
Episode length: 3.40 +/- 2.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3.4      |
|    mean_reward     | 2.14     |
| time/              |          |
|    total_timesteps | 32400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.13     |
|    ep_rew_mean     | 3.27     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 81       |
|    time_elapsed    | 4646     |
|    total_timesteps | 32400    |
---------------------------------
Progress update from timestep 32500: Updated shield, loss is 0.12913095951080322
Progress update from timestep 32600: Updated shield, loss is 0.13336332142353058
Eval num_timesteps=32600, episode_reward=3.69 +/- 2.16
Episode length: 5.20 +/- 2.48
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5.2      |
|    mean_reward          | 3.69     |
| time/                   |          |
|    total_timesteps      | 32600    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.14     |
|    learning_rate        | 0.0001   |
|    loss                 | 1.26     |
|    n_updates            | 810      |
|    policy_gradient_loss | -1.6e-10 |
|    value_loss           | 2        |
--------------------------------------
Progress update from timestep 32700: Updated shield, loss is 0.1392868161201477
Progress update from timestep 32800: Updated shield, loss is 0.1277942955493927
Eval num_timesteps=32800, episode_reward=2.50 +/- 1.69
Episode length: 3.80 +/- 1.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3.8      |
|    mean_reward     | 2.5      |
| time/              |          |
|    total_timesteps | 32800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.95     |
|    ep_rew_mean     | 3.17     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 82       |
|    time_elapsed    | 4698     |
|    total_timesteps | 32800    |
---------------------------------
Progress update from timestep 32900: Updated shield, loss is 0.13184230029582977
Progress update from timestep 33000: Updated shield, loss is 0.12107815593481064
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 33000: Saved shield_loss.csv. 
Eval num_timesteps=33000, episode_reward=6.67 +/- 5.49
Episode length: 8.60 +/- 6.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 8.6       |
|    mean_reward          | 6.67      |
| time/                   |           |
|    total_timesteps      | 33000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.00941   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.37      |
|    n_updates            | 820       |
|    policy_gradient_loss | -4.79e-10 |
|    value_loss           | 2.65      |
---------------------------------------
Progress update from timestep 33100: Updated shield, loss is 0.12912443280220032
Progress update from timestep 33200: Updated shield, loss is 0.13161589205265045
Eval num_timesteps=33200, episode_reward=4.55 +/- 1.41
Episode length: 6.20 +/- 1.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6.2      |
|    mean_reward     | 4.55     |
| time/              |          |
|    total_timesteps | 33200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.54     |
|    ep_rew_mean     | 3.77     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 83       |
|    time_elapsed    | 4764     |
|    total_timesteps | 33200    |
---------------------------------
Progress update from timestep 33300: Updated shield, loss is 0.12250839918851852
Progress update from timestep 33400: Updated shield, loss is 0.13077470660209656
Eval num_timesteps=33400, episode_reward=5.62 +/- 5.43
Episode length: 7.40 +/- 6.25
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 7.4      |
|    mean_reward          | 5.62     |
| time/                   |          |
|    total_timesteps      | 33400    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.137    |
|    learning_rate        | 0.0001   |
|    loss                 | 0.853    |
|    n_updates            | 830      |
|    policy_gradient_loss | 1.18e-09 |
|    value_loss           | 2.07     |
--------------------------------------
Progress update from timestep 33500: Updated shield, loss is 0.13148988783359528
Progress update from timestep 33600: Updated shield, loss is 0.13267113268375397
Eval num_timesteps=33600, episode_reward=9.81 +/- 15.17
Episode length: 12.20 +/- 17.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 12.2     |
|    mean_reward     | 9.81     |
| time/              |          |
|    total_timesteps | 33600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.23     |
|    ep_rew_mean     | 3.54     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 84       |
|    time_elapsed    | 4838     |
|    total_timesteps | 33600    |
---------------------------------
Progress update from timestep 33700: Updated shield, loss is 0.1304156631231308
Progress update from timestep 33800: Updated shield, loss is 0.12010365724563599
Eval num_timesteps=33800, episode_reward=1.47 +/- 1.15
Episode length: 2.60 +/- 1.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 2.6       |
|    mean_reward          | 1.47      |
| time/                   |           |
|    total_timesteps      | 33800     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.0677    |
|    learning_rate        | 0.0001    |
|    loss                 | 0.951     |
|    n_updates            | 840       |
|    policy_gradient_loss | -1.16e-09 |
|    value_loss           | 2.49      |
---------------------------------------
Progress update from timestep 33900: Updated shield, loss is 0.13739436864852905
Progress update from timestep 34000: Updated shield, loss is 0.12371139228343964
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 34000: Saved shield_loss.csv. 
Eval num_timesteps=34000, episode_reward=3.86 +/- 3.26
Episode length: 5.40 +/- 3.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5.4      |
|    mean_reward     | 3.86     |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.21     |
|    ep_rew_mean     | 3.47     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 85       |
|    time_elapsed    | 4894     |
|    total_timesteps | 34000    |
---------------------------------
Progress update from timestep 34100: Updated shield, loss is 0.12906838953495026
Progress update from timestep 34200: Updated shield, loss is 0.11989311128854752
Eval num_timesteps=34200, episode_reward=5.96 +/- 5.65
Episode length: 7.80 +/- 6.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 7.8       |
|    mean_reward          | 5.96      |
| time/                   |           |
|    total_timesteps      | 34200     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.14      |
|    learning_rate        | 0.0001    |
|    loss                 | 1.73      |
|    n_updates            | 850       |
|    policy_gradient_loss | -1.96e-09 |
|    value_loss           | 2.29      |
---------------------------------------
Progress update from timestep 34300: Updated shield, loss is 0.1438581943511963
Progress update from timestep 34400: Updated shield, loss is 0.13498100638389587
Eval num_timesteps=34400, episode_reward=2.12 +/- 2.09
Episode length: 3.40 +/- 2.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3.4      |
|    mean_reward     | 2.12     |
| time/              |          |
|    total_timesteps | 34400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.93     |
|    ep_rew_mean     | 3.26     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 86       |
|    time_elapsed    | 4955     |
|    total_timesteps | 34400    |
---------------------------------
Progress update from timestep 34500: Updated shield, loss is 0.11672813445329666
Progress update from timestep 34600: Updated shield, loss is 0.1295659840106964
Eval num_timesteps=34600, episode_reward=4.94 +/- 4.05
Episode length: 6.60 +/- 4.72
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 6.6       |
|    mean_reward          | 4.94      |
| time/                   |           |
|    total_timesteps      | 34600     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.138     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.66      |
|    n_updates            | 860       |
|    policy_gradient_loss | -4.39e-10 |
|    value_loss           | 2.2       |
---------------------------------------
Progress update from timestep 34700: Updated shield, loss is 0.1256481260061264
Progress update from timestep 34800: Updated shield, loss is 0.12394042313098907
Eval num_timesteps=34800, episode_reward=8.59 +/- 6.84
Episode length: 10.80 +/- 7.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10.8     |
|    mean_reward     | 8.59     |
| time/              |          |
|    total_timesteps | 34800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.79     |
|    ep_rew_mean     | 3.13     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 87       |
|    time_elapsed    | 5017     |
|    total_timesteps | 34800    |
---------------------------------
Progress update from timestep 34900: Updated shield, loss is 0.14195787906646729
Progress update from timestep 35000: Updated shield, loss is 0.13227123022079468
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 35000: Saved shield_loss.csv. 
Eval num_timesteps=35000, episode_reward=3.35 +/- 1.28
Episode length: 4.80 +/- 1.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 4.8       |
|    mean_reward          | 3.35      |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.114     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.871     |
|    n_updates            | 870       |
|    policy_gradient_loss | -8.85e-10 |
|    value_loss           | 2.62      |
---------------------------------------
Progress update from timestep 35100: Updated shield, loss is 0.13454429805278778
Progress update from timestep 35200: Updated shield, loss is 0.1266486793756485
Eval num_timesteps=35200, episode_reward=4.39 +/- 2.63
Episode length: 6.00 +/- 3.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6        |
|    mean_reward     | 4.39     |
| time/              |          |
|    total_timesteps | 35200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.11     |
|    ep_rew_mean     | 3.42     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 88       |
|    time_elapsed    | 5073     |
|    total_timesteps | 35200    |
---------------------------------
Progress update from timestep 35300: Updated shield, loss is 0.12040268629789352
Progress update from timestep 35400: Updated shield, loss is 0.13232767581939697
Eval num_timesteps=35400, episode_reward=6.68 +/- 6.38
Episode length: 8.60 +/- 7.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 8.6       |
|    mean_reward          | 6.68      |
| time/                   |           |
|    total_timesteps      | 35400     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.0919    |
|    learning_rate        | 0.0001    |
|    loss                 | 0.551     |
|    n_updates            | 880       |
|    policy_gradient_loss | -1.14e-09 |
|    value_loss           | 1.95      |
---------------------------------------
Progress update from timestep 35500: Updated shield, loss is 0.12095211446285248
Progress update from timestep 35600: Updated shield, loss is 0.12932102382183075
Eval num_timesteps=35600, episode_reward=1.27 +/- 0.43
Episode length: 2.40 +/- 0.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.4      |
|    mean_reward     | 1.27     |
| time/              |          |
|    total_timesteps | 35600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.21     |
|    ep_rew_mean     | 3.46     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 89       |
|    time_elapsed    | 5131     |
|    total_timesteps | 35600    |
---------------------------------
Progress update from timestep 35700: Updated shield, loss is 0.13600672781467438
Progress update from timestep 35800: Updated shield, loss is 0.11890009790658951
Eval num_timesteps=35800, episode_reward=3.20 +/- 1.76
Episode length: 4.60 +/- 2.06
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 4.6       |
|    mean_reward          | 3.2       |
| time/                   |           |
|    total_timesteps      | 35800     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.124     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.693     |
|    n_updates            | 890       |
|    policy_gradient_loss | -2.11e-09 |
|    value_loss           | 1.79      |
---------------------------------------
Progress update from timestep 35900: Updated shield, loss is 0.1297958940267563
Progress update from timestep 36000: Updated shield, loss is 0.12921294569969177
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 36000: Saved shield_loss.csv. 
Eval num_timesteps=36000, episode_reward=3.16 +/- 2.47
Episode length: 4.60 +/- 2.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.6      |
|    mean_reward     | 3.16     |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.73     |
|    ep_rew_mean     | 3.09     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 90       |
|    time_elapsed    | 5186     |
|    total_timesteps | 36000    |
---------------------------------
Progress update from timestep 36100: Updated shield, loss is 0.13993290066719055
Progress update from timestep 36200: Updated shield, loss is 0.13315758109092712
Eval num_timesteps=36200, episode_reward=5.96 +/- 6.50
Episode length: 7.80 +/- 7.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 7.8       |
|    mean_reward          | 5.96      |
| time/                   |           |
|    total_timesteps      | 36200     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.0207    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.17      |
|    n_updates            | 900       |
|    policy_gradient_loss | -1.09e-09 |
|    value_loss           | 2.88      |
---------------------------------------
Progress update from timestep 36300: Updated shield, loss is 0.12826156616210938
Progress update from timestep 36400: Updated shield, loss is 0.12375541031360626
Eval num_timesteps=36400, episode_reward=2.51 +/- 2.38
Episode length: 3.80 +/- 2.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3.8      |
|    mean_reward     | 2.51     |
| time/              |          |
|    total_timesteps | 36400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.98     |
|    ep_rew_mean     | 3.32     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 91       |
|    time_elapsed    | 5245     |
|    total_timesteps | 36400    |
---------------------------------
Progress update from timestep 36500: Updated shield, loss is 0.1330452710390091
Progress update from timestep 36600: Updated shield, loss is 0.14077523350715637
Eval num_timesteps=36600, episode_reward=3.02 +/- 2.99
Episode length: 4.40 +/- 3.38
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 4.4      |
|    mean_reward          | 3.02     |
| time/                   |          |
|    total_timesteps      | 36600    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0823   |
|    learning_rate        | 0.0001   |
|    loss                 | 0.603    |
|    n_updates            | 910      |
|    policy_gradient_loss | 4.66e-10 |
|    value_loss           | 1.69     |
--------------------------------------
Progress update from timestep 36700: Updated shield, loss is 0.13677985966205597
Progress update from timestep 36800: Updated shield, loss is 0.139034241437912
Eval num_timesteps=36800, episode_reward=1.13 +/- 0.83
Episode length: 2.20 +/- 0.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.2      |
|    mean_reward     | 1.13     |
| time/              |          |
|    total_timesteps | 36800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.24     |
|    ep_rew_mean     | 2.68     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 92       |
|    time_elapsed    | 5298     |
|    total_timesteps | 36800    |
---------------------------------
Progress update from timestep 36900: Updated shield, loss is 0.13428059220314026
Progress update from timestep 37000: Updated shield, loss is 0.13840295374393463
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 37000: Saved shield_loss.csv. 
Eval num_timesteps=37000, episode_reward=3.52 +/- 1.47
Episode length: 5.00 +/- 1.67
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5        |
|    mean_reward          | 3.52     |
| time/                   |          |
|    total_timesteps      | 37000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | -0.0168  |
|    learning_rate        | 0.0001   |
|    loss                 | 0.808    |
|    n_updates            | 920      |
|    policy_gradient_loss | 1.49e-09 |
|    value_loss           | 2.01     |
--------------------------------------
Progress update from timestep 37100: Updated shield, loss is 0.1222313717007637
Progress update from timestep 37200: Updated shield, loss is 0.12092709541320801
Eval num_timesteps=37200, episode_reward=3.03 +/- 1.80
Episode length: 4.40 +/- 2.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.4      |
|    mean_reward     | 3.03     |
| time/              |          |
|    total_timesteps | 37200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.59     |
|    ep_rew_mean     | 2.98     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 93       |
|    time_elapsed    | 5350     |
|    total_timesteps | 37200    |
---------------------------------
Progress update from timestep 37300: Updated shield, loss is 0.12196579575538635
Progress update from timestep 37400: Updated shield, loss is 0.1374543011188507
Eval num_timesteps=37400, episode_reward=2.13 +/- 1.50
Episode length: 3.40 +/- 1.74
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 3.4      |
|    mean_reward          | 2.13     |
| time/                   |          |
|    total_timesteps      | 37400    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.168    |
|    learning_rate        | 0.0001   |
|    loss                 | 0.667    |
|    n_updates            | 930      |
|    policy_gradient_loss | 2.61e-09 |
|    value_loss           | 2        |
--------------------------------------
Progress update from timestep 37500: Updated shield, loss is 0.1300085186958313
Progress update from timestep 37600: Updated shield, loss is 0.13255435228347778
Eval num_timesteps=37600, episode_reward=3.21 +/- 1.56
Episode length: 4.60 +/- 1.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.6      |
|    mean_reward     | 3.21     |
| time/              |          |
|    total_timesteps | 37600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5        |
|    ep_rew_mean     | 3.3      |
| time/              |          |
|    fps             | 6        |
|    iterations      | 94       |
|    time_elapsed    | 5402     |
|    total_timesteps | 37600    |
---------------------------------
Progress update from timestep 37700: Updated shield, loss is 0.12683305144309998
Progress update from timestep 37800: Updated shield, loss is 0.1312602460384369
Eval num_timesteps=37800, episode_reward=1.25 +/- 0.41
Episode length: 2.40 +/- 0.49
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 2.4      |
|    mean_reward          | 1.25     |
| time/                   |          |
|    total_timesteps      | 37800    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0583   |
|    learning_rate        | 0.0001   |
|    loss                 | 1.44     |
|    n_updates            | 940      |
|    policy_gradient_loss | 1.09e-09 |
|    value_loss           | 2.58     |
--------------------------------------
Progress update from timestep 37900: Updated shield, loss is 0.12638670206069946
Progress update from timestep 38000: Updated shield, loss is 0.12910163402557373
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 38000: Saved shield_loss.csv. 
Eval num_timesteps=38000, episode_reward=2.82 +/- 2.76
Episode length: 4.20 +/- 3.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.2      |
|    mean_reward     | 2.82     |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.43     |
|    ep_rew_mean     | 2.9      |
| time/              |          |
|    fps             | 6        |
|    iterations      | 95       |
|    time_elapsed    | 5454     |
|    total_timesteps | 38000    |
---------------------------------
Progress update from timestep 38100: Updated shield, loss is 0.12716300785541534
Progress update from timestep 38200: Updated shield, loss is 0.11581332236528397
Eval num_timesteps=38200, episode_reward=2.11 +/- 0.90
Episode length: 3.40 +/- 1.02
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 3.4      |
|    mean_reward          | 2.11     |
| time/                   |          |
|    total_timesteps      | 38200    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0926   |
|    learning_rate        | 0.0001   |
|    loss                 | 1.51     |
|    n_updates            | 950      |
|    policy_gradient_loss | 6.59e-10 |
|    value_loss           | 2.25     |
--------------------------------------
Progress update from timestep 38300: Updated shield, loss is 0.1291237622499466
Progress update from timestep 38400: Updated shield, loss is 0.12937089800834656
Eval num_timesteps=38400, episode_reward=1.25 +/- 0.43
Episode length: 2.40 +/- 0.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.4      |
|    mean_reward     | 1.25     |
| time/              |          |
|    total_timesteps | 38400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.6      |
|    ep_rew_mean     | 2.98     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 96       |
|    time_elapsed    | 5505     |
|    total_timesteps | 38400    |
---------------------------------
Progress update from timestep 38500: Updated shield, loss is 0.12291168421506882
Progress update from timestep 38600: Updated shield, loss is 0.1300695389509201
Eval num_timesteps=38600, episode_reward=5.61 +/- 4.42
Episode length: 7.40 +/- 5.08
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 7.4      |
|    mean_reward          | 5.61     |
| time/                   |          |
|    total_timesteps      | 38600    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0728   |
|    learning_rate        | 0.0001   |
|    loss                 | 2.22     |
|    n_updates            | 960      |
|    policy_gradient_loss | -2.1e-09 |
|    value_loss           | 2.42     |
--------------------------------------
Progress update from timestep 38700: Updated shield, loss is 0.12354835122823715
Progress update from timestep 38800: Updated shield, loss is 0.1262855976819992
Eval num_timesteps=38800, episode_reward=2.13 +/- 1.14
Episode length: 3.40 +/- 1.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3.4      |
|    mean_reward     | 2.13     |
| time/              |          |
|    total_timesteps | 38800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.54     |
|    ep_rew_mean     | 2.92     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 97       |
|    time_elapsed    | 5562     |
|    total_timesteps | 38800    |
---------------------------------
Progress update from timestep 38900: Updated shield, loss is 0.12342696636915207
Progress update from timestep 39000: Updated shield, loss is 0.12082285434007645
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 39000: Saved shield_loss.csv. 
Eval num_timesteps=39000, episode_reward=2.67 +/- 1.10
Episode length: 4.00 +/- 1.26
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 4        |
|    mean_reward          | 2.67     |
| time/                   |          |
|    total_timesteps      | 39000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.129    |
|    learning_rate        | 0.0001   |
|    loss                 | 1.03     |
|    n_updates            | 970      |
|    policy_gradient_loss | 2.71e-09 |
|    value_loss           | 1.99     |
--------------------------------------
Progress update from timestep 39100: Updated shield, loss is 0.12238335609436035
Progress update from timestep 39200: Updated shield, loss is 0.11649586260318756
Eval num_timesteps=39200, episode_reward=3.52 +/- 2.35
Episode length: 5.00 +/- 2.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 3.52     |
| time/              |          |
|    total_timesteps | 39200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.81     |
|    ep_rew_mean     | 3.17     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 98       |
|    time_elapsed    | 5616     |
|    total_timesteps | 39200    |
---------------------------------
Progress update from timestep 39300: Updated shield, loss is 0.11485964804887772
Progress update from timestep 39400: Updated shield, loss is 0.12616616487503052
Eval num_timesteps=39400, episode_reward=1.64 +/- 1.47
Episode length: 2.80 +/- 1.72
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 2.8       |
|    mean_reward          | 1.64      |
| time/                   |           |
|    total_timesteps      | 39400     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.1       |
|    learning_rate        | 0.0001    |
|    loss                 | 0.541     |
|    n_updates            | 980       |
|    policy_gradient_loss | -3.46e-10 |
|    value_loss           | 1.87      |
---------------------------------------
Progress update from timestep 39500: Updated shield, loss is 0.1289694905281067
Progress update from timestep 39600: Updated shield, loss is 0.12176714092493057
Eval num_timesteps=39600, episode_reward=3.87 +/- 4.64
Episode length: 5.40 +/- 5.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5.4      |
|    mean_reward     | 3.87     |
| time/              |          |
|    total_timesteps | 39600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.22     |
|    ep_rew_mean     | 2.71     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 99       |
|    time_elapsed    | 5672     |
|    total_timesteps | 39600    |
---------------------------------
Progress update from timestep 39700: Updated shield, loss is 0.11363818496465683
Progress update from timestep 39800: Updated shield, loss is 0.11741555482149124
Eval num_timesteps=39800, episode_reward=1.80 +/- 0.54
Episode length: 3.00 +/- 0.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 3         |
|    mean_reward          | 1.8       |
| time/                   |           |
|    total_timesteps      | 39800     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.117     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.02      |
|    n_updates            | 990       |
|    policy_gradient_loss | -3.92e-09 |
|    value_loss           | 1.54      |
---------------------------------------
Progress update from timestep 39900: Updated shield, loss is 0.13012567162513733
Progress update from timestep 40000: Updated shield, loss is 0.1154484748840332
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 40000: Saved shield_loss.csv. 
Eval num_timesteps=40000, episode_reward=3.53 +/- 1.27
Episode length: 5.00 +/- 1.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 3.53     |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.09     |
|    ep_rew_mean     | 2.6      |
| time/              |          |
|    fps             | 6        |
|    iterations      | 100      |
|    time_elapsed    | 5725     |
|    total_timesteps | 40000    |
---------------------------------
Progress update from timestep 40100: Updated shield, loss is 0.12454070150852203
Progress update from timestep 40200: Updated shield, loss is 0.12226153165102005
Eval num_timesteps=40200, episode_reward=1.47 +/- 1.08
Episode length: 2.60 +/- 1.20
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 2.6      |
|    mean_reward          | 1.47     |
| time/                   |          |
|    total_timesteps      | 40200    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.121    |
|    learning_rate        | 0.0001   |
|    loss                 | 1.36     |
|    n_updates            | 1000     |
|    policy_gradient_loss | 1.13e-09 |
|    value_loss           | 2.93     |
--------------------------------------
Progress update from timestep 40300: Updated shield, loss is 0.12743191421031952
Progress update from timestep 40400: Updated shield, loss is 0.12901295721530914
Eval num_timesteps=40400, episode_reward=1.64 +/- 1.84
Episode length: 2.80 +/- 2.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.8      |
|    mean_reward     | 1.64     |
| time/              |          |
|    total_timesteps | 40400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.38     |
|    ep_rew_mean     | 3.59     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 101      |
|    time_elapsed    | 5775     |
|    total_timesteps | 40400    |
---------------------------------
Progress update from timestep 40500: Updated shield, loss is 0.12666377425193787
Progress update from timestep 40600: Updated shield, loss is 0.11046203225851059
Eval num_timesteps=40600, episode_reward=1.49 +/- 1.14
Episode length: 2.60 +/- 1.36
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 2.6      |
|    mean_reward          | 1.49     |
| time/                   |          |
|    total_timesteps      | 40600    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0974   |
|    learning_rate        | 0.0001   |
|    loss                 | 1.05     |
|    n_updates            | 1010     |
|    policy_gradient_loss | 4.86e-10 |
|    value_loss           | 2.23     |
--------------------------------------
Progress update from timestep 40700: Updated shield, loss is 0.1274101287126541
Progress update from timestep 40800: Updated shield, loss is 0.13551679253578186
Eval num_timesteps=40800, episode_reward=5.25 +/- 2.65
Episode length: 7.00 +/- 3.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7        |
|    mean_reward     | 5.25     |
| time/              |          |
|    total_timesteps | 40800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.72     |
|    ep_rew_mean     | 3.13     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 102      |
|    time_elapsed    | 5831     |
|    total_timesteps | 40800    |
---------------------------------
Progress update from timestep 40900: Updated shield, loss is 0.11998142302036285
Progress update from timestep 41000: Updated shield, loss is 0.12587621808052063
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 41000: Saved shield_loss.csv. 
Eval num_timesteps=41000, episode_reward=0.76 +/- 0.34
Episode length: 1.80 +/- 0.40
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 1.8      |
|    mean_reward          | 0.764    |
| time/                   |          |
|    total_timesteps      | 41000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0993   |
|    learning_rate        | 0.0001   |
|    loss                 | 0.611    |
|    n_updates            | 1020     |
|    policy_gradient_loss | 1.26e-09 |
|    value_loss           | 1.75     |
--------------------------------------
Progress update from timestep 41100: Updated shield, loss is 0.12050460278987885
Progress update from timestep 41200: Updated shield, loss is 0.13843853771686554
Eval num_timesteps=41200, episode_reward=3.35 +/- 2.59
Episode length: 4.80 +/- 2.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.8      |
|    mean_reward     | 3.35     |
| time/              |          |
|    total_timesteps | 41200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.01     |
|    ep_rew_mean     | 2.56     |
| time/              |          |
|    fps             | 7        |
|    iterations      | 103      |
|    time_elapsed    | 5882     |
|    total_timesteps | 41200    |
---------------------------------
Progress update from timestep 41300: Updated shield, loss is 0.13038429617881775
Progress update from timestep 41400: Updated shield, loss is 0.1340814083814621
Eval num_timesteps=41400, episode_reward=4.72 +/- 5.63
Episode length: 6.40 +/- 6.47
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 6.4      |
|    mean_reward          | 4.72     |
| time/                   |          |
|    total_timesteps      | 41400    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.105    |
|    learning_rate        | 0.0001   |
|    loss                 | 1.4      |
|    n_updates            | 1030     |
|    policy_gradient_loss | 1.14e-09 |
|    value_loss           | 1.93     |
--------------------------------------
Progress update from timestep 41500: Updated shield, loss is 0.11956354975700378
Progress update from timestep 41600: Updated shield, loss is 0.13975010812282562
Eval num_timesteps=41600, episode_reward=2.67 +/- 2.53
Episode length: 4.00 +/- 2.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 2.67     |
| time/              |          |
|    total_timesteps | 41600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.88     |
|    ep_rew_mean     | 3.2      |
| time/              |          |
|    fps             | 7        |
|    iterations      | 104      |
|    time_elapsed    | 5940     |
|    total_timesteps | 41600    |
---------------------------------
Progress update from timestep 41700: Updated shield, loss is 0.11933533102273941
Progress update from timestep 41800: Updated shield, loss is 0.11392661184072495
Eval num_timesteps=41800, episode_reward=1.61 +/- 0.67
Episode length: 2.80 +/- 0.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 2.8       |
|    mean_reward          | 1.61      |
| time/                   |           |
|    total_timesteps      | 41800     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.061     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.837     |
|    n_updates            | 1040      |
|    policy_gradient_loss | -7.24e-09 |
|    value_loss           | 2.49      |
---------------------------------------
Progress update from timestep 41900: Updated shield, loss is 0.1176399439573288
Progress update from timestep 42000: Updated shield, loss is 0.127865731716156
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 42000: Saved shield_loss.csv. 
Eval num_timesteps=42000, episode_reward=2.31 +/- 1.18
Episode length: 3.60 +/- 1.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3.6      |
|    mean_reward     | 2.31     |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.75     |
|    ep_rew_mean     | 3.9      |
| time/              |          |
|    fps             | 7        |
|    iterations      | 105      |
|    time_elapsed    | 5991     |
|    total_timesteps | 42000    |
---------------------------------
Progress update from timestep 42100: Updated shield, loss is 0.10963957756757736
Progress update from timestep 42200: Updated shield, loss is 0.1295665055513382
Eval num_timesteps=42200, episode_reward=3.55 +/- 2.91
Episode length: 5.00 +/- 3.41
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5        |
|    mean_reward          | 3.55     |
| time/                   |          |
|    total_timesteps      | 42200    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.134    |
|    learning_rate        | 0.0001   |
|    loss                 | 1.03     |
|    n_updates            | 1050     |
|    policy_gradient_loss | 1.06e-09 |
|    value_loss           | 2.81     |
--------------------------------------
Progress update from timestep 42300: Updated shield, loss is 0.13224035501480103
Progress update from timestep 42400: Updated shield, loss is 0.12716439366340637
Eval num_timesteps=42400, episode_reward=0.91 +/- 0.08
Episode length: 2.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2        |
|    mean_reward     | 0.905    |
| time/              |          |
|    total_timesteps | 42400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.35     |
|    ep_rew_mean     | 3.6      |
| time/              |          |
|    fps             | 7        |
|    iterations      | 106      |
|    time_elapsed    | 6043     |
|    total_timesteps | 42400    |
---------------------------------
Progress update from timestep 42500: Updated shield, loss is 0.12569062411785126
Progress update from timestep 42600: Updated shield, loss is 0.12980416417121887
Eval num_timesteps=42600, episode_reward=6.53 +/- 10.70
Episode length: 8.40 +/- 12.32
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 8.4      |
|    mean_reward          | 6.53     |
| time/                   |          |
|    total_timesteps      | 42600    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0496   |
|    learning_rate        | 0.0001   |
|    loss                 | 1.54     |
|    n_updates            | 1060     |
|    policy_gradient_loss | 1.58e-09 |
|    value_loss           | 2.49     |
--------------------------------------
Progress update from timestep 42700: Updated shield, loss is 0.12985630333423615
Progress update from timestep 42800: Updated shield, loss is 0.11974126845598221
Eval num_timesteps=42800, episode_reward=3.20 +/- 1.78
Episode length: 4.60 +/- 2.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.6      |
|    mean_reward     | 3.2      |
| time/              |          |
|    total_timesteps | 42800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.08     |
|    ep_rew_mean     | 3.31     |
| time/              |          |
|    fps             | 7        |
|    iterations      | 107      |
|    time_elapsed    | 6106     |
|    total_timesteps | 42800    |
---------------------------------
Progress update from timestep 42900: Updated shield, loss is 0.13084295392036438
Progress update from timestep 43000: Updated shield, loss is 0.11751707643270493
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 43000: Saved shield_loss.csv. 
Eval num_timesteps=43000, episode_reward=2.30 +/- 1.18
Episode length: 3.60 +/- 1.36
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 3.6      |
|    mean_reward          | 2.3      |
| time/                   |          |
|    total_timesteps      | 43000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.193    |
|    learning_rate        | 0.0001   |
|    loss                 | 1.59     |
|    n_updates            | 1070     |
|    policy_gradient_loss | 1.45e-09 |
|    value_loss           | 2.62     |
--------------------------------------
Progress update from timestep 43100: Updated shield, loss is 0.1266578882932663
Progress update from timestep 43200: Updated shield, loss is 0.12665078043937683
Eval num_timesteps=43200, episode_reward=4.40 +/- 2.07
Episode length: 6.00 +/- 2.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6        |
|    mean_reward     | 4.4      |
| time/              |          |
|    total_timesteps | 43200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.12     |
|    ep_rew_mean     | 2.62     |
| time/              |          |
|    fps             | 7        |
|    iterations      | 108      |
|    time_elapsed    | 6160     |
|    total_timesteps | 43200    |
---------------------------------
Progress update from timestep 43300: Updated shield, loss is 0.12333494424819946
Progress update from timestep 43400: Updated shield, loss is 0.1251114010810852
Eval num_timesteps=43400, episode_reward=1.14 +/- 0.67
Episode length: 2.20 +/- 0.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 2.2       |
|    mean_reward          | 1.14      |
| time/                   |           |
|    total_timesteps      | 43400     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.175     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.55      |
|    n_updates            | 1080      |
|    policy_gradient_loss | -3.53e-10 |
|    value_loss           | 2.02      |
---------------------------------------
Progress update from timestep 43500: Updated shield, loss is 0.11954785138368607
Progress update from timestep 43600: Updated shield, loss is 0.1276831477880478
Eval num_timesteps=43600, episode_reward=1.93 +/- 1.28
Episode length: 3.20 +/- 1.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3.2      |
|    mean_reward     | 1.93     |
| time/              |          |
|    total_timesteps | 43600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.26     |
|    ep_rew_mean     | 3.47     |
| time/              |          |
|    fps             | 7        |
|    iterations      | 109      |
|    time_elapsed    | 6210     |
|    total_timesteps | 43600    |
---------------------------------
Progress update from timestep 43700: Updated shield, loss is 0.10885448753833771
Progress update from timestep 43800: Updated shield, loss is 0.1329747438430786
Eval num_timesteps=43800, episode_reward=1.82 +/- 1.78
Episode length: 3.00 +/- 2.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 3         |
|    mean_reward          | 1.82      |
| time/                   |           |
|    total_timesteps      | 43800     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.22      |
|    learning_rate        | 0.0001    |
|    loss                 | 1.01      |
|    n_updates            | 1090      |
|    policy_gradient_loss | -2.81e-09 |
|    value_loss           | 2.57      |
---------------------------------------
Progress update from timestep 43900: Updated shield, loss is 0.13767468929290771
Progress update from timestep 44000: Updated shield, loss is 0.12451537698507309
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 44000: Saved shield_loss.csv. 
Eval num_timesteps=44000, episode_reward=10.12 +/- 5.86
Episode length: 12.60 +/- 6.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 12.6     |
|    mean_reward     | 10.1     |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.7      |
|    ep_rew_mean     | 3.02     |
| time/              |          |
|    fps             | 7        |
|    iterations      | 110      |
|    time_elapsed    | 6270     |
|    total_timesteps | 44000    |
---------------------------------
Progress update from timestep 44100: Updated shield, loss is 0.11507800966501236
Progress update from timestep 44200: Updated shield, loss is 0.10922704637050629
Eval num_timesteps=44200, episode_reward=2.83 +/- 1.71
Episode length: 4.20 +/- 1.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 4.2       |
|    mean_reward          | 2.83      |
| time/                   |           |
|    total_timesteps      | 44200     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.0341    |
|    learning_rate        | 0.0001    |
|    loss                 | 0.877     |
|    n_updates            | 1100      |
|    policy_gradient_loss | -1.86e-10 |
|    value_loss           | 2.28      |
---------------------------------------
Progress update from timestep 44300: Updated shield, loss is 0.11951952427625656
Progress update from timestep 44400: Updated shield, loss is 0.1160469502210617
Eval num_timesteps=44400, episode_reward=4.75 +/- 3.96
Episode length: 6.40 +/- 4.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6.4      |
|    mean_reward     | 4.75     |
| time/              |          |
|    total_timesteps | 44400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.33     |
|    ep_rew_mean     | 2.73     |
| time/              |          |
|    fps             | 7        |
|    iterations      | 111      |
|    time_elapsed    | 6324     |
|    total_timesteps | 44400    |
---------------------------------
Progress update from timestep 44500: Updated shield, loss is 0.11298762261867523
Progress update from timestep 44600: Updated shield, loss is 0.1184723973274231
Eval num_timesteps=44600, episode_reward=2.80 +/- 3.08
Episode length: 4.20 +/- 3.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 4.2       |
|    mean_reward          | 2.8       |
| time/                   |           |
|    total_timesteps      | 44600     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.13      |
|    learning_rate        | 0.0001    |
|    loss                 | 0.401     |
|    n_updates            | 1110      |
|    policy_gradient_loss | -1.74e-09 |
|    value_loss           | 1.89      |
---------------------------------------
Progress update from timestep 44700: Updated shield, loss is 0.12448418140411377
Progress update from timestep 44800: Updated shield, loss is 0.11661013215780258
Eval num_timesteps=44800, episode_reward=2.16 +/- 1.83
Episode length: 3.40 +/- 2.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3.4      |
|    mean_reward     | 2.16     |
| time/              |          |
|    total_timesteps | 44800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.91     |
|    ep_rew_mean     | 3.21     |
| time/              |          |
|    fps             | 7        |
|    iterations      | 112      |
|    time_elapsed    | 6376     |
|    total_timesteps | 44800    |
---------------------------------
Progress update from timestep 44900: Updated shield, loss is 0.11371460556983948
Progress update from timestep 45000: Updated shield, loss is 0.13045090436935425
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 45000: Saved shield_loss.csv. 
Eval num_timesteps=45000, episode_reward=0.93 +/- 0.06
Episode length: 2.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 2        |
|    mean_reward          | 0.931    |
| time/                   |          |
|    total_timesteps      | 45000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.292    |
|    learning_rate        | 0.0001   |
|    loss                 | 2.06     |
|    n_updates            | 1120     |
|    policy_gradient_loss | 6.79e-10 |
|    value_loss           | 2.78     |
--------------------------------------
Progress update from timestep 45100: Updated shield, loss is 0.11962822079658508
Progress update from timestep 45200: Updated shield, loss is 0.12023206800222397
Eval num_timesteps=45200, episode_reward=1.41 +/- 0.40
Episode length: 2.60 +/- 0.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.6      |
|    mean_reward     | 1.41     |
| time/              |          |
|    total_timesteps | 45200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.68     |
|    ep_rew_mean     | 3.77     |
| time/              |          |
|    fps             | 7        |
|    iterations      | 113      |
|    time_elapsed    | 6425     |
|    total_timesteps | 45200    |
---------------------------------
Progress update from timestep 45300: Updated shield, loss is 0.1193690299987793
Progress update from timestep 45400: Updated shield, loss is 0.1390097439289093
Eval num_timesteps=45400, episode_reward=3.01 +/- 2.60
Episode length: 4.40 +/- 3.01
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 4.4      |
|    mean_reward          | 3.01     |
| time/                   |          |
|    total_timesteps      | 45400    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.132    |
|    learning_rate        | 0.0001   |
|    loss                 | 1.07     |
|    n_updates            | 1130     |
|    policy_gradient_loss | 2.99e-11 |
|    value_loss           | 2.14     |
--------------------------------------
Progress update from timestep 45500: Updated shield, loss is 0.12063641846179962
Progress update from timestep 45600: Updated shield, loss is 0.12868016958236694
Eval num_timesteps=45600, episode_reward=3.01 +/- 1.17
Episode length: 4.40 +/- 1.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.4      |
|    mean_reward     | 3.01     |
| time/              |          |
|    total_timesteps | 45600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.41     |
|    ep_rew_mean     | 2.85     |
| time/              |          |
|    fps             | 7        |
|    iterations      | 114      |
|    time_elapsed    | 6479     |
|    total_timesteps | 45600    |
---------------------------------
Progress update from timestep 45700: Updated shield, loss is 0.11607712507247925
Progress update from timestep 45800: Updated shield, loss is 0.1286553293466568
Eval num_timesteps=45800, episode_reward=2.84 +/- 3.04
Episode length: 4.20 +/- 3.43
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 4.2      |
|    mean_reward          | 2.84     |
| time/                   |          |
|    total_timesteps      | 45800    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | -0.00102 |
|    learning_rate        | 0.0001   |
|    loss                 | 0.845    |
|    n_updates            | 1140     |
|    policy_gradient_loss | 3.03e-09 |
|    value_loss           | 2.55     |
--------------------------------------
Progress update from timestep 45900: Updated shield, loss is 0.11948703974485397
Progress update from timestep 46000: Updated shield, loss is 0.10907699912786484
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 46000: Saved shield_loss.csv. 
Eval num_timesteps=46000, episode_reward=1.77 +/- 0.78
Episode length: 3.00 +/- 0.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3        |
|    mean_reward     | 1.77     |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.86     |
|    ep_rew_mean     | 3.11     |
| time/              |          |
|    fps             | 7        |
|    iterations      | 115      |
|    time_elapsed    | 6531     |
|    total_timesteps | 46000    |
---------------------------------
Progress update from timestep 46100: Updated shield, loss is 0.12290962785482407
Progress update from timestep 46200: Updated shield, loss is 0.12221945822238922
Eval num_timesteps=46200, episode_reward=4.88 +/- 3.67
Episode length: 6.60 +/- 4.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 6.6           |
|    mean_reward          | 4.88          |
| time/                   |               |
|    total_timesteps      | 46200         |
| train/                  |               |
|    approx_kl            | 1.4732459e-05 |
|    clip_fraction        | 0.000223      |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00618      |
|    explained_variance   | 0.0527        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.12          |
|    n_updates            | 1150          |
|    policy_gradient_loss | -0.000232     |
|    value_loss           | 2.3           |
-------------------------------------------
Progress update from timestep 46300: Updated shield, loss is 0.12175977230072021
Progress update from timestep 46400: Updated shield, loss is 0.11679746955633163
Eval num_timesteps=46400, episode_reward=2.80 +/- 2.54
Episode length: 4.20 +/- 2.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.2      |
|    mean_reward     | 2.8      |
| time/              |          |
|    total_timesteps | 46400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.83     |
|    ep_rew_mean     | 3.11     |
| time/              |          |
|    fps             | 7        |
|    iterations      | 116      |
|    time_elapsed    | 6587     |
|    total_timesteps | 46400    |
---------------------------------
Progress update from timestep 46500: Updated shield, loss is 0.11957240849733353
Progress update from timestep 46600: Updated shield, loss is 0.11237125843763351
Eval num_timesteps=46600, episode_reward=3.70 +/- 2.15
Episode length: 5.20 +/- 2.48
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5.2      |
|    mean_reward          | 3.7      |
| time/                   |          |
|    total_timesteps      | 46600    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0324   |
|    learning_rate        | 0.0001   |
|    loss                 | 0.901    |
|    n_updates            | 1160     |
|    policy_gradient_loss | 2.85e-09 |
|    value_loss           | 1.77     |
--------------------------------------
Progress update from timestep 46700: Updated shield, loss is 0.12571999430656433
Progress update from timestep 46800: Updated shield, loss is 0.10911169648170471
Eval num_timesteps=46800, episode_reward=3.51 +/- 2.21
Episode length: 5.00 +/- 2.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 3.51     |
| time/              |          |
|    total_timesteps | 46800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.67     |
|    ep_rew_mean     | 3.02     |
| time/              |          |
|    fps             | 7        |
|    iterations      | 117      |
|    time_elapsed    | 6643     |
|    total_timesteps | 46800    |
---------------------------------
Progress update from timestep 46900: Updated shield, loss is 0.11419429630041122
Progress update from timestep 47000: Updated shield, loss is 0.10596440732479095
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 47000: Saved shield_loss.csv. 
Eval num_timesteps=47000, episode_reward=3.18 +/- 2.00
Episode length: 4.60 +/- 2.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 4.6       |
|    mean_reward          | 3.18      |
| time/                   |           |
|    total_timesteps      | 47000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.076     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.29      |
|    n_updates            | 1170      |
|    policy_gradient_loss | -3.49e-09 |
|    value_loss           | 3.11      |
---------------------------------------
Progress update from timestep 47100: Updated shield, loss is 0.10661977529525757
Progress update from timestep 47200: Updated shield, loss is 0.10284893959760666
Eval num_timesteps=47200, episode_reward=2.13 +/- 1.19
Episode length: 3.40 +/- 1.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3.4      |
|    mean_reward     | 2.13     |
| time/              |          |
|    total_timesteps | 47200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.64     |
|    ep_rew_mean     | 3.03     |
| time/              |          |
|    fps             | 7        |
|    iterations      | 118      |
|    time_elapsed    | 6693     |
|    total_timesteps | 47200    |
---------------------------------
Progress update from timestep 47300: Updated shield, loss is 0.09479399770498276
Progress update from timestep 47400: Updated shield, loss is 0.12283811718225479
Eval num_timesteps=47400, episode_reward=3.66 +/- 1.30
Episode length: 5.20 +/- 1.47
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5.2      |
|    mean_reward          | 3.66     |
| time/                   |          |
|    total_timesteps      | 47400    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0545   |
|    learning_rate        | 0.0001   |
|    loss                 | 0.407    |
|    n_updates            | 1180     |
|    policy_gradient_loss | 1.37e-09 |
|    value_loss           | 2.05     |
--------------------------------------
Progress update from timestep 47500: Updated shield, loss is 0.12291304767131805
Progress update from timestep 47600: Updated shield, loss is 0.12103297561407089
Eval num_timesteps=47600, episode_reward=2.31 +/- 2.82
Episode length: 3.60 +/- 3.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3.6      |
|    mean_reward     | 2.31     |
| time/              |          |
|    total_timesteps | 47600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.48     |
|    ep_rew_mean     | 2.89     |
| time/              |          |
|    fps             | 7        |
|    iterations      | 119      |
|    time_elapsed    | 6747     |
|    total_timesteps | 47600    |
---------------------------------
Progress update from timestep 47700: Updated shield, loss is 0.1250409483909607
Progress update from timestep 47800: Updated shield, loss is 0.12103395164012909
Eval num_timesteps=47800, episode_reward=2.65 +/- 1.64
Episode length: 4.00 +/- 1.90
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 4        |
|    mean_reward          | 2.65     |
| time/                   |          |
|    total_timesteps      | 47800    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.054    |
|    learning_rate        | 0.0001   |
|    loss                 | 0.909    |
|    n_updates            | 1190     |
|    policy_gradient_loss | 1.28e-09 |
|    value_loss           | 1.79     |
--------------------------------------
Progress update from timestep 47900: Updated shield, loss is 0.11655053496360779
Progress update from timestep 48000: Updated shield, loss is 0.12211702764034271
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 48000: Saved shield_loss.csv. 
Eval num_timesteps=48000, episode_reward=3.91 +/- 2.64
Episode length: 5.40 +/- 3.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5.4      |
|    mean_reward     | 3.91     |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.86     |
|    ep_rew_mean     | 3.19     |
| time/              |          |
|    fps             | 7        |
|    iterations      | 120      |
|    time_elapsed    | 6800     |
|    total_timesteps | 48000    |
---------------------------------
Progress update from timestep 48100: Updated shield, loss is 0.1071011871099472
Progress update from timestep 48200: Updated shield, loss is 0.10989963263273239
Eval num_timesteps=48200, episode_reward=1.63 +/- 1.13
Episode length: 2.80 +/- 1.33
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 2.8      |
|    mean_reward          | 1.63     |
| time/                   |          |
|    total_timesteps      | 48200    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0557   |
|    learning_rate        | 0.0001   |
|    loss                 | 1.35     |
|    n_updates            | 1200     |
|    policy_gradient_loss | 1.42e-09 |
|    value_loss           | 1.98     |
--------------------------------------
Progress update from timestep 48300: Updated shield, loss is 0.11712656170129776
Progress update from timestep 48400: Updated shield, loss is 0.11633416265249252
Eval num_timesteps=48400, episode_reward=7.76 +/- 8.25
Episode length: 9.80 +/- 9.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.8      |
|    mean_reward     | 7.76     |
| time/              |          |
|    total_timesteps | 48400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.97     |
|    ep_rew_mean     | 3.26     |
| time/              |          |
|    fps             | 7        |
|    iterations      | 121      |
|    time_elapsed    | 6860     |
|    total_timesteps | 48400    |
---------------------------------
Progress update from timestep 48500: Updated shield, loss is 0.11439907550811768
Progress update from timestep 48600: Updated shield, loss is 0.11872830986976624
Eval num_timesteps=48600, episode_reward=2.32 +/- 1.43
Episode length: 3.60 +/- 1.62
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 3.6      |
|    mean_reward          | 2.32     |
| time/                   |          |
|    total_timesteps      | 48600    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.129    |
|    learning_rate        | 0.0001   |
|    loss                 | 1        |
|    n_updates            | 1210     |
|    policy_gradient_loss | 3.67e-09 |
|    value_loss           | 2.3      |
--------------------------------------
Progress update from timestep 48700: Updated shield, loss is 0.10728190839290619
Progress update from timestep 48800: Updated shield, loss is 0.1187974214553833
Eval num_timesteps=48800, episode_reward=1.93 +/- 0.83
Episode length: 3.20 +/- 0.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3.2      |
|    mean_reward     | 1.93     |
| time/              |          |
|    total_timesteps | 48800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.82     |
|    ep_rew_mean     | 3.1      |
| time/              |          |
|    fps             | 7        |
|    iterations      | 122      |
|    time_elapsed    | 6911     |
|    total_timesteps | 48800    |
---------------------------------
Progress update from timestep 48900: Updated shield, loss is 0.1040620505809784
Progress update from timestep 49000: Updated shield, loss is 0.11445587873458862
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 49000: Saved shield_loss.csv. 
Eval num_timesteps=49000, episode_reward=2.01 +/- 1.51
Episode length: 3.20 +/- 1.72
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 3.2       |
|    mean_reward          | 2.01      |
| time/                   |           |
|    total_timesteps      | 49000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.136     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.863     |
|    n_updates            | 1220      |
|    policy_gradient_loss | -3.33e-11 |
|    value_loss           | 1.98      |
---------------------------------------
Progress update from timestep 49100: Updated shield, loss is 0.1124631017446518
Progress update from timestep 49200: Updated shield, loss is 0.12145047634840012
Eval num_timesteps=49200, episode_reward=1.81 +/- 2.62
Episode length: 3.00 +/- 3.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3        |
|    mean_reward     | 1.81     |
| time/              |          |
|    total_timesteps | 49200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.37     |
|    ep_rew_mean     | 2.75     |
| time/              |          |
|    fps             | 7        |
|    iterations      | 123      |
|    time_elapsed    | 6963     |
|    total_timesteps | 49200    |
---------------------------------
Progress update from timestep 49300: Updated shield, loss is 0.11316847056150436
Progress update from timestep 49400: Updated shield, loss is 0.11459339410066605
Eval num_timesteps=49400, episode_reward=3.51 +/- 3.10
Episode length: 4.80 +/- 3.31
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 4.8      |
|    mean_reward          | 3.51     |
| time/                   |          |
|    total_timesteps      | 49400    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.19     |
|    learning_rate        | 0.0001   |
|    loss                 | 0.492    |
|    n_updates            | 1230     |
|    policy_gradient_loss | 4.03e-09 |
|    value_loss           | 1.4      |
--------------------------------------
Progress update from timestep 49500: Updated shield, loss is 0.10271340608596802
Progress update from timestep 49600: Updated shield, loss is 0.10792762786149979
Eval num_timesteps=49600, episode_reward=4.04 +/- 4.41
Episode length: 5.60 +/- 5.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5.6      |
|    mean_reward     | 4.04     |
| time/              |          |
|    total_timesteps | 49600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.5      |
|    ep_rew_mean     | 2.97     |
| time/              |          |
|    fps             | 7        |
|    iterations      | 124      |
|    time_elapsed    | 7019     |
|    total_timesteps | 49600    |
---------------------------------
Progress update from timestep 49700: Updated shield, loss is 0.12024004757404327
Progress update from timestep 49800: Updated shield, loss is 0.11107853055000305
Eval num_timesteps=49800, episode_reward=3.04 +/- 2.27
Episode length: 4.40 +/- 2.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 4.4           |
|    mean_reward          | 3.04          |
| time/                   |               |
|    total_timesteps      | 49800         |
| train/                  |               |
|    approx_kl            | 5.7054283e-05 |
|    clip_fraction        | 0.00424       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00614      |
|    explained_variance   | 0.0341        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.08          |
|    n_updates            | 1240          |
|    policy_gradient_loss | -0.000272     |
|    value_loss           | 1.84          |
-------------------------------------------
Progress update from timestep 49900: Updated shield, loss is 0.10739275068044662
Progress update from timestep 50000: Updated shield, loss is 0.11387607455253601
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 50000: Saved shield_loss.csv. 
Eval num_timesteps=50000, episode_reward=3.35 +/- 2.40
Episode length: 4.80 +/- 2.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.8      |
|    mean_reward     | 3.35     |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.67     |
|    ep_rew_mean     | 2.98     |
| time/              |          |
|    fps             | 7        |
|    iterations      | 125      |
|    time_elapsed    | 7074     |
|    total_timesteps | 50000    |
---------------------------------
Progress update from timestep 50100: Updated shield, loss is 0.11579678952693939
Progress update from timestep 50200: Updated shield, loss is 0.12061017751693726
Eval num_timesteps=50200, episode_reward=4.05 +/- 4.58
Episode length: 5.40 +/- 4.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5.4       |
|    mean_reward          | 4.05      |
| time/                   |           |
|    total_timesteps      | 50200     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.0338    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.18      |
|    n_updates            | 1250      |
|    policy_gradient_loss | -1.37e-09 |
|    value_loss           | 2.12      |
---------------------------------------
Progress update from timestep 50300: Updated shield, loss is 0.11410613358020782
Progress update from timestep 50400: Updated shield, loss is 0.116924948990345
Eval num_timesteps=50400, episode_reward=1.95 +/- 0.58
Episode length: 3.20 +/- 0.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3.2      |
|    mean_reward     | 1.95     |
| time/              |          |
|    total_timesteps | 50400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.63     |
|    ep_rew_mean     | 3        |
| time/              |          |
|    fps             | 7        |
|    iterations      | 126      |
|    time_elapsed    | 7129     |
|    total_timesteps | 50400    |
---------------------------------
Progress update from timestep 50500: Updated shield, loss is 0.1086590513586998
Progress update from timestep 50600: Updated shield, loss is 0.11043167114257812
Eval num_timesteps=50600, episode_reward=3.85 +/- 4.73
Episode length: 5.40 +/- 5.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5.4       |
|    mean_reward          | 3.85      |
| time/                   |           |
|    total_timesteps      | 50600     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.109     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.945     |
|    n_updates            | 1260      |
|    policy_gradient_loss | -1.22e-09 |
|    value_loss           | 1.74      |
---------------------------------------
Progress update from timestep 50700: Updated shield, loss is 0.10737520456314087
Progress update from timestep 50800: Updated shield, loss is 0.10158444195985794
Eval num_timesteps=50800, episode_reward=2.27 +/- 0.44
Episode length: 3.60 +/- 0.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3.6      |
|    mean_reward     | 2.27     |
| time/              |          |
|    total_timesteps | 50800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.2      |
|    ep_rew_mean     | 2.63     |
| time/              |          |
|    fps             | 7        |
|    iterations      | 127      |
|    time_elapsed    | 7185     |
|    total_timesteps | 50800    |
---------------------------------
Progress update from timestep 50900: Updated shield, loss is 0.10517971217632294
Progress update from timestep 51000: Updated shield, loss is 0.10826995223760605
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 51000: Saved shield_loss.csv. 
Eval num_timesteps=51000, episode_reward=2.30 +/- 2.36
Episode length: 3.60 +/- 2.73
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 3.6           |
|    mean_reward          | 2.3           |
| time/                   |               |
|    total_timesteps      | 51000         |
| train/                  |               |
|    approx_kl            | 1.8009518e-05 |
|    clip_fraction        | 0.00067       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0061       |
|    explained_variance   | 0.139         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.12          |
|    n_updates            | 1270          |
|    policy_gradient_loss | -0.000141     |
|    value_loss           | 1.88          |
-------------------------------------------
Progress update from timestep 51100: Updated shield, loss is 0.11005114018917084
Progress update from timestep 51200: Updated shield, loss is 0.11333252489566803
Eval num_timesteps=51200, episode_reward=1.95 +/- 1.27
Episode length: 3.20 +/- 1.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3.2      |
|    mean_reward     | 1.95     |
| time/              |          |
|    total_timesteps | 51200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.28     |
|    ep_rew_mean     | 2.68     |
| time/              |          |
|    fps             | 7        |
|    iterations      | 128      |
|    time_elapsed    | 7238     |
|    total_timesteps | 51200    |
---------------------------------
Progress update from timestep 51300: Updated shield, loss is 0.12045340985059738
Progress update from timestep 51400: Updated shield, loss is 0.11125678569078445
Eval num_timesteps=51400, episode_reward=11.79 +/- 12.67
Episode length: 16.40 +/- 16.98
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.4          |
|    mean_reward          | 11.8          |
| time/                   |               |
|    total_timesteps      | 51400         |
| train/                  |               |
|    approx_kl            | 0.00035982206 |
|    clip_fraction        | 0.0194        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0442       |
|    explained_variance   | 0.144         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.53          |
|    n_updates            | 1280          |
|    policy_gradient_loss | -0.00218      |
|    value_loss           | 1.85          |
-------------------------------------------
Progress update from timestep 51500: Updated shield, loss is 0.10489854216575623
Progress update from timestep 51600: Updated shield, loss is 0.11333024501800537
Eval num_timesteps=51600, episode_reward=4.09 +/- 2.56
Episode length: 6.40 +/- 3.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6.4      |
|    mean_reward     | 4.09     |
| time/              |          |
|    total_timesteps | 51600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.42     |
|    ep_rew_mean     | 2.86     |
| time/              |          |
|    fps             | 7        |
|    iterations      | 129      |
|    time_elapsed    | 7315     |
|    total_timesteps | 51600    |
---------------------------------
Progress update from timestep 51700: Updated shield, loss is 0.10750563442707062
Progress update from timestep 51800: Updated shield, loss is 0.10500968992710114
Eval num_timesteps=51800, episode_reward=34.25 +/- 0.83
Episode length: 50.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 50            |
|    mean_reward          | 34.2          |
| time/                   |               |
|    total_timesteps      | 51800         |
| train/                  |               |
|    approx_kl            | 0.00014027141 |
|    clip_fraction        | 0.0121        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0348       |
|    explained_variance   | 0.376         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.739         |
|    n_updates            | 1290          |
|    policy_gradient_loss | -0.00125      |
|    value_loss           | 2.29          |
-------------------------------------------
New best mean reward!
Progress update from timestep 51900: Updated shield, loss is 0.11997267603874207
Progress update from timestep 52000: Updated shield, loss is 0.11396004259586334
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 52000: Saved shield_loss.csv. 
Eval num_timesteps=52000, episode_reward=28.55 +/- 12.42
Episode length: 41.20 +/- 17.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | 28.6     |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.39     |
|    ep_rew_mean     | 2.77     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 130      |
|    time_elapsed    | 7448     |
|    total_timesteps | 52000    |
---------------------------------
Progress update from timestep 52100: Updated shield, loss is 0.1159597635269165
Progress update from timestep 52200: Updated shield, loss is 0.1168142780661583
Eval num_timesteps=52200, episode_reward=9.13 +/- 13.61
Episode length: 13.40 +/- 18.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 13.4         |
|    mean_reward          | 9.13         |
| time/                   |              |
|    total_timesteps      | 52200        |
| train/                  |              |
|    approx_kl            | 6.677104e-05 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0254      |
|    explained_variance   | 0.0284       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.666        |
|    n_updates            | 1300         |
|    policy_gradient_loss | -0.000489    |
|    value_loss           | 1.62         |
------------------------------------------
Progress update from timestep 52300: Updated shield, loss is 0.11877190321683884
Progress update from timestep 52400: Updated shield, loss is 0.11556224524974823
Eval num_timesteps=52400, episode_reward=13.04 +/- 12.43
Episode length: 18.60 +/- 16.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 13       |
| time/              |          |
|    total_timesteps | 52400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.81     |
|    ep_rew_mean     | 3.17     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 131      |
|    time_elapsed    | 7537     |
|    total_timesteps | 52400    |
---------------------------------
Progress update from timestep 52500: Updated shield, loss is 0.12083972245454788
Progress update from timestep 52600: Updated shield, loss is 0.11266747862100601
Eval num_timesteps=52600, episode_reward=11.39 +/- 12.70
Episode length: 16.40 +/- 16.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | 11.4         |
| time/                   |              |
|    total_timesteps      | 52600        |
| train/                  |              |
|    approx_kl            | 0.0001428883 |
|    clip_fraction        | 0.00826      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.039       |
|    explained_variance   | 0.0977       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.36         |
|    n_updates            | 1310         |
|    policy_gradient_loss | -0.00135     |
|    value_loss           | 2.26         |
------------------------------------------
Progress update from timestep 52700: Updated shield, loss is 0.10651025921106339
Progress update from timestep 52800: Updated shield, loss is 0.1098029762506485
Eval num_timesteps=52800, episode_reward=11.86 +/- 12.43
Episode length: 17.20 +/- 16.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | 11.9     |
| time/              |          |
|    total_timesteps | 52800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.01     |
|    ep_rew_mean     | 3.32     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 132      |
|    time_elapsed    | 7629     |
|    total_timesteps | 52800    |
---------------------------------
Progress update from timestep 52900: Updated shield, loss is 0.1102014034986496
Progress update from timestep 53000: Updated shield, loss is 0.11965495347976685
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 53000: Saved shield_loss.csv. 
Eval num_timesteps=53000, episode_reward=5.77 +/- 2.66
Episode length: 9.20 +/- 3.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 9.2          |
|    mean_reward          | 5.77         |
| time/                   |              |
|    total_timesteps      | 53000        |
| train/                  |              |
|    approx_kl            | 9.725549e-05 |
|    clip_fraction        | 0.0029       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0167      |
|    explained_variance   | 0.169        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.695        |
|    n_updates            | 1320         |
|    policy_gradient_loss | -0.000843    |
|    value_loss           | 1.39         |
------------------------------------------
Progress update from timestep 53100: Updated shield, loss is 0.12151715159416199
Progress update from timestep 53200: Updated shield, loss is 0.12137270718812943
Eval num_timesteps=53200, episode_reward=18.27 +/- 13.75
Episode length: 26.80 +/- 19.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.8     |
|    mean_reward     | 18.3     |
| time/              |          |
|    total_timesteps | 53200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.62     |
|    ep_rew_mean     | 2.94     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 133      |
|    time_elapsed    | 7712     |
|    total_timesteps | 53200    |
---------------------------------
Progress update from timestep 53300: Updated shield, loss is 0.1258946657180786
Progress update from timestep 53400: Updated shield, loss is 0.11030534654855728
Eval num_timesteps=53400, episode_reward=9.99 +/- 12.98
Episode length: 14.80 +/- 17.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.8          |
|    mean_reward          | 9.99          |
| time/                   |               |
|    total_timesteps      | 53400         |
| train/                  |               |
|    approx_kl            | 0.00014193116 |
|    clip_fraction        | 0.00737       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.012        |
|    explained_variance   | 0.166         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.954         |
|    n_updates            | 1330          |
|    policy_gradient_loss | -0.000714     |
|    value_loss           | 1.35          |
-------------------------------------------
Progress update from timestep 53500: Updated shield, loss is 0.11058837175369263
Progress update from timestep 53600: Updated shield, loss is 0.12184140831232071
Eval num_timesteps=53600, episode_reward=10.01 +/- 13.54
Episode length: 14.60 +/- 18.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 10       |
| time/              |          |
|    total_timesteps | 53600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.08     |
|    ep_rew_mean     | 3.32     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 134      |
|    time_elapsed    | 7801     |
|    total_timesteps | 53600    |
---------------------------------
Progress update from timestep 53700: Updated shield, loss is 0.11275327205657959
Progress update from timestep 53800: Updated shield, loss is 0.10101950168609619
Eval num_timesteps=53800, episode_reward=17.12 +/- 15.13
Episode length: 24.80 +/- 20.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.8          |
|    mean_reward          | 17.1          |
| time/                   |               |
|    total_timesteps      | 53800         |
| train/                  |               |
|    approx_kl            | 1.5998792e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0047       |
|    explained_variance   | 0.099         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.614         |
|    n_updates            | 1340          |
|    policy_gradient_loss | -2.12e-05     |
|    value_loss           | 2.1           |
-------------------------------------------
Progress update from timestep 53900: Updated shield, loss is 0.11164822429418564
Progress update from timestep 54000: Updated shield, loss is 0.10759779065847397
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 54000: Saved shield_loss.csv. 
Eval num_timesteps=54000, episode_reward=17.04 +/- 15.34
Episode length: 24.40 +/- 21.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.41     |
|    ep_rew_mean     | 3.56     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 135      |
|    time_elapsed    | 7894     |
|    total_timesteps | 54000    |
---------------------------------
Progress update from timestep 54100: Updated shield, loss is 0.11497648805379868
Progress update from timestep 54200: Updated shield, loss is 0.11135505884885788
Eval num_timesteps=54200, episode_reward=14.92 +/- 11.19
Episode length: 21.20 +/- 14.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 21.2        |
|    mean_reward          | 14.9        |
| time/                   |             |
|    total_timesteps      | 54200       |
| train/                  |             |
|    approx_kl            | 3.48292e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0339     |
|    explained_variance   | 0.0927      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.776       |
|    n_updates            | 1350        |
|    policy_gradient_loss | -0.000374   |
|    value_loss           | 2.14        |
-----------------------------------------
Progress update from timestep 54300: Updated shield, loss is 0.1042604148387909
Progress update from timestep 54400: Updated shield, loss is 0.10942709445953369
Eval num_timesteps=54400, episode_reward=18.09 +/- 15.38
Episode length: 25.40 +/- 20.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | 18.1     |
| time/              |          |
|    total_timesteps | 54400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.43     |
|    ep_rew_mean     | 2.86     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 136      |
|    time_elapsed    | 7993     |
|    total_timesteps | 54400    |
---------------------------------
Progress update from timestep 54500: Updated shield, loss is 0.10789169371128082
Progress update from timestep 54600: Updated shield, loss is 0.10481506586074829
Eval num_timesteps=54600, episode_reward=12.33 +/- 12.34
Episode length: 18.60 +/- 17.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.6         |
|    mean_reward          | 12.3         |
| time/                   |              |
|    total_timesteps      | 54600        |
| train/                  |              |
|    approx_kl            | 7.857103e-05 |
|    clip_fraction        | 0.00246      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0332      |
|    explained_variance   | 0.123        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.36         |
|    n_updates            | 1360         |
|    policy_gradient_loss | -0.000754    |
|    value_loss           | 2.69         |
------------------------------------------
Progress update from timestep 54700: Updated shield, loss is 0.10860203951597214
Progress update from timestep 54800: Updated shield, loss is 0.11128385365009308
Eval num_timesteps=54800, episode_reward=23.51 +/- 12.98
Episode length: 34.80 +/- 18.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 23.5     |
| time/              |          |
|    total_timesteps | 54800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.77     |
|    ep_rew_mean     | 3.13     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 137      |
|    time_elapsed    | 8089     |
|    total_timesteps | 54800    |
---------------------------------
Progress update from timestep 54900: Updated shield, loss is 0.10884165018796921
Progress update from timestep 55000: Updated shield, loss is 0.11112085729837418
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 55000: Saved shield_loss.csv. 
Eval num_timesteps=55000, episode_reward=10.72 +/- 11.81
Episode length: 16.40 +/- 17.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | 10.7         |
| time/                   |              |
|    total_timesteps      | 55000        |
| train/                  |              |
|    approx_kl            | 2.110723e-05 |
|    clip_fraction        | 0.000223     |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00631     |
|    explained_variance   | 0.0748       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.17         |
|    n_updates            | 1370         |
|    policy_gradient_loss | -5.81e-05    |
|    value_loss           | 1.81         |
------------------------------------------
Progress update from timestep 55100: Updated shield, loss is 0.1179153248667717
Progress update from timestep 55200: Updated shield, loss is 0.0975228026509285
Eval num_timesteps=55200, episode_reward=18.45 +/- 15.07
Episode length: 26.20 +/- 19.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | 18.4     |
| time/              |          |
|    total_timesteps | 55200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.77     |
|    ep_rew_mean     | 3.15     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 138      |
|    time_elapsed    | 8182     |
|    total_timesteps | 55200    |
---------------------------------
Progress update from timestep 55300: Updated shield, loss is 0.1222849190235138
Progress update from timestep 55400: Updated shield, loss is 0.11607793718576431
Eval num_timesteps=55400, episode_reward=19.39 +/- 12.88
Episode length: 28.80 +/- 18.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 28.8         |
|    mean_reward          | 19.4         |
| time/                   |              |
|    total_timesteps      | 55400        |
| train/                  |              |
|    approx_kl            | 9.858714e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0238      |
|    explained_variance   | 0.0823       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.44         |
|    n_updates            | 1380         |
|    policy_gradient_loss | -0.000101    |
|    value_loss           | 2.14         |
------------------------------------------
Progress update from timestep 55500: Updated shield, loss is 0.10947402566671371
Progress update from timestep 55600: Updated shield, loss is 0.1137799471616745
Eval num_timesteps=55600, episode_reward=12.89 +/- 12.61
Episode length: 18.80 +/- 16.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.8     |
|    mean_reward     | 12.9     |
| time/              |          |
|    total_timesteps | 55600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.39     |
|    ep_rew_mean     | 2.77     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 139      |
|    time_elapsed    | 8278     |
|    total_timesteps | 55600    |
---------------------------------
Progress update from timestep 55700: Updated shield, loss is 0.10950449854135513
Progress update from timestep 55800: Updated shield, loss is 0.10925401002168655
Eval num_timesteps=55800, episode_reward=23.06 +/- 14.91
Episode length: 33.20 +/- 20.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.2      |
|    mean_reward          | 23.1      |
| time/                   |           |
|    total_timesteps      | 55800     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.0553    |
|    learning_rate        | 0.0001    |
|    loss                 | 0.928     |
|    n_updates            | 1390      |
|    policy_gradient_loss | -2.09e-09 |
|    value_loss           | 1.94      |
---------------------------------------
Progress update from timestep 55900: Updated shield, loss is 0.10714901983737946
Progress update from timestep 56000: Updated shield, loss is 0.11201319843530655
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 56000: Saved shield_loss.csv. 
Eval num_timesteps=56000, episode_reward=16.56 +/- 13.85
Episode length: 25.20 +/- 20.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.34     |
|    ep_rew_mean     | 2.74     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 140      |
|    time_elapsed    | 8393     |
|    total_timesteps | 56000    |
---------------------------------
Progress update from timestep 56100: Updated shield, loss is 0.12266623973846436
Progress update from timestep 56200: Updated shield, loss is 0.12101960182189941
Eval num_timesteps=56200, episode_reward=20.20 +/- 14.24
Episode length: 28.60 +/- 19.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 28.6          |
|    mean_reward          | 20.2          |
| time/                   |               |
|    total_timesteps      | 56200         |
| train/                  |               |
|    approx_kl            | 0.00022651495 |
|    clip_fraction        | 0.015         |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0359       |
|    explained_variance   | 0.101         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.735         |
|    n_updates            | 1400          |
|    policy_gradient_loss | -0.00152      |
|    value_loss           | 1.85          |
-------------------------------------------
Progress update from timestep 56300: Updated shield, loss is 0.13431094586849213
Progress update from timestep 56400: Updated shield, loss is 0.13195112347602844
Eval num_timesteps=56400, episode_reward=20.37 +/- 14.25
Episode length: 28.80 +/- 19.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.8     |
|    mean_reward     | 20.4     |
| time/              |          |
|    total_timesteps | 56400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.43     |
|    ep_rew_mean     | 2.81     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 141      |
|    time_elapsed    | 8488     |
|    total_timesteps | 56400    |
---------------------------------
Progress update from timestep 56500: Updated shield, loss is 0.10426265001296997
Progress update from timestep 56600: Updated shield, loss is 0.12339803576469421
Eval num_timesteps=56600, episode_reward=19.59 +/- 13.38
Episode length: 28.00 +/- 18.59
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 28       |
|    mean_reward          | 19.6     |
| time/                   |          |
|    total_timesteps      | 56600    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.146    |
|    learning_rate        | 0.0001   |
|    loss                 | 0.979    |
|    n_updates            | 1410     |
|    policy_gradient_loss | 3.59e-10 |
|    value_loss           | 1.92     |
--------------------------------------
Progress update from timestep 56700: Updated shield, loss is 0.12493999302387238
Progress update from timestep 56800: Updated shield, loss is 0.1070222482085228
Eval num_timesteps=56800, episode_reward=13.51 +/- 11.49
Episode length: 19.40 +/- 15.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 13.5     |
| time/              |          |
|    total_timesteps | 56800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.84     |
|    ep_rew_mean     | 3.14     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 142      |
|    time_elapsed    | 8577     |
|    total_timesteps | 56800    |
---------------------------------
Progress update from timestep 56900: Updated shield, loss is 0.11108183860778809
Progress update from timestep 57000: Updated shield, loss is 0.12743699550628662
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 57000: Saved shield_loss.csv. 
Eval num_timesteps=57000, episode_reward=11.30 +/- 12.94
Episode length: 16.60 +/- 17.32
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 16.6     |
|    mean_reward          | 11.3     |
| time/                   |          |
|    total_timesteps      | 57000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.171    |
|    learning_rate        | 0.0001   |
|    loss                 | 0.504    |
|    n_updates            | 1420     |
|    policy_gradient_loss | 1.45e-09 |
|    value_loss           | 2.22     |
--------------------------------------
Progress update from timestep 57100: Updated shield, loss is 0.10682196170091629
Progress update from timestep 57200: Updated shield, loss is 0.0986545979976654
Eval num_timesteps=57200, episode_reward=10.71 +/- 13.15
Episode length: 15.60 +/- 17.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 10.7     |
| time/              |          |
|    total_timesteps | 57200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.57     |
|    ep_rew_mean     | 3.76     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 143      |
|    time_elapsed    | 8666     |
|    total_timesteps | 57200    |
---------------------------------
Progress update from timestep 57300: Updated shield, loss is 0.11046173423528671
Progress update from timestep 57400: Updated shield, loss is 0.10964193940162659
Eval num_timesteps=57400, episode_reward=31.19 +/- 10.42
Episode length: 43.00 +/- 14.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 43           |
|    mean_reward          | 31.2         |
| time/                   |              |
|    total_timesteps      | 57400        |
| train/                  |              |
|    approx_kl            | 6.733915e-05 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00484     |
|    explained_variance   | 0.239        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.79         |
|    n_updates            | 1430         |
|    policy_gradient_loss | -0.000219    |
|    value_loss           | 2.79         |
------------------------------------------
Progress update from timestep 57500: Updated shield, loss is 0.11957193166017532
Progress update from timestep 57600: Updated shield, loss is 0.09928717464208603
Eval num_timesteps=57600, episode_reward=3.44 +/- 2.25
Episode length: 5.80 +/- 3.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5.8      |
|    mean_reward     | 3.44     |
| time/              |          |
|    total_timesteps | 57600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.28     |
|    ep_rew_mean     | 3.51     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 144      |
|    time_elapsed    | 8747     |
|    total_timesteps | 57600    |
---------------------------------
Progress update from timestep 57700: Updated shield, loss is 0.09658997505903244
Progress update from timestep 57800: Updated shield, loss is 0.10454269498586655
Eval num_timesteps=57800, episode_reward=5.67 +/- 3.58
Episode length: 9.00 +/- 4.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 9            |
|    mean_reward          | 5.67         |
| time/                   |              |
|    total_timesteps      | 57800        |
| train/                  |              |
|    approx_kl            | 9.233797e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.017       |
|    explained_variance   | 0.111        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.04         |
|    n_updates            | 1440         |
|    policy_gradient_loss | -0.00028     |
|    value_loss           | 2.04         |
------------------------------------------
Progress update from timestep 57900: Updated shield, loss is 0.11135834455490112
Progress update from timestep 58000: Updated shield, loss is 0.1100502461194992
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 58000: Saved shield_loss.csv. 
Eval num_timesteps=58000, episode_reward=17.32 +/- 15.59
Episode length: 24.60 +/- 20.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 17.3     |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.32     |
|    ep_rew_mean     | 3.56     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 145      |
|    time_elapsed    | 8822     |
|    total_timesteps | 58000    |
---------------------------------
Progress update from timestep 58100: Updated shield, loss is 0.10104448348283768
Progress update from timestep 58200: Updated shield, loss is 0.10060349106788635
Eval num_timesteps=58200, episode_reward=10.17 +/- 9.42
Episode length: 15.80 +/- 14.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.8          |
|    mean_reward          | 10.2          |
| time/                   |               |
|    total_timesteps      | 58200         |
| train/                  |               |
|    approx_kl            | 0.00012800896 |
|    clip_fraction        | 0.00112       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0441       |
|    explained_variance   | 0.0605        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.998         |
|    n_updates            | 1450          |
|    policy_gradient_loss | -0.000656     |
|    value_loss           | 3.14          |
-------------------------------------------
Progress update from timestep 58300: Updated shield, loss is 0.1031804159283638
Progress update from timestep 58400: Updated shield, loss is 0.11097943782806396
Eval num_timesteps=58400, episode_reward=19.68 +/- 14.11
Episode length: 27.80 +/- 18.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.8     |
|    mean_reward     | 19.7     |
| time/              |          |
|    total_timesteps | 58400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.13     |
|    ep_rew_mean     | 3.47     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 146      |
|    time_elapsed    | 8913     |
|    total_timesteps | 58400    |
---------------------------------
Progress update from timestep 58500: Updated shield, loss is 0.11883492022752762
Progress update from timestep 58600: Updated shield, loss is 0.1051446869969368
Eval num_timesteps=58600, episode_reward=4.76 +/- 4.36
Episode length: 7.80 +/- 6.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 7.8           |
|    mean_reward          | 4.76          |
| time/                   |               |
|    total_timesteps      | 58600         |
| train/                  |               |
|    approx_kl            | 3.2886597e-05 |
|    clip_fraction        | 0.00112       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0289       |
|    explained_variance   | 0.0609        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.841         |
|    n_updates            | 1460          |
|    policy_gradient_loss | -0.000407     |
|    value_loss           | 2.29          |
-------------------------------------------
Progress update from timestep 58700: Updated shield, loss is 0.1167709156870842
Progress update from timestep 58800: Updated shield, loss is 0.10638377815485
Eval num_timesteps=58800, episode_reward=5.81 +/- 0.94
Episode length: 9.20 +/- 1.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.2      |
|    mean_reward     | 5.81     |
| time/              |          |
|    total_timesteps | 58800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.81     |
|    ep_rew_mean     | 3.13     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 147      |
|    time_elapsed    | 8980     |
|    total_timesteps | 58800    |
---------------------------------
Progress update from timestep 58900: Updated shield, loss is 0.10011105239391327
Progress update from timestep 59000: Updated shield, loss is 0.1143079400062561
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 59000: Saved shield_loss.csv. 
Eval num_timesteps=59000, episode_reward=15.81 +/- 14.86
Episode length: 23.60 +/- 21.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.6          |
|    mean_reward          | 15.8          |
| time/                   |               |
|    total_timesteps      | 59000         |
| train/                  |               |
|    approx_kl            | 5.1015588e-05 |
|    clip_fraction        | 0.00201       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0154       |
|    explained_variance   | 0.201         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.505         |
|    n_updates            | 1470          |
|    policy_gradient_loss | -0.000686     |
|    value_loss           | 1.66          |
-------------------------------------------
Progress update from timestep 59100: Updated shield, loss is 0.11367268860340118
Progress update from timestep 59200: Updated shield, loss is 0.10646975040435791
Eval num_timesteps=59200, episode_reward=17.42 +/- 14.94
Episode length: 25.00 +/- 20.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 59200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.4      |
|    ep_rew_mean     | 2.81     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 148      |
|    time_elapsed    | 9076     |
|    total_timesteps | 59200    |
---------------------------------
Progress update from timestep 59300: Updated shield, loss is 0.11021135002374649
Progress update from timestep 59400: Updated shield, loss is 0.11158185452222824
Eval num_timesteps=59400, episode_reward=19.93 +/- 13.43
Episode length: 29.20 +/- 18.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 29.2          |
|    mean_reward          | 19.9          |
| time/                   |               |
|    total_timesteps      | 59400         |
| train/                  |               |
|    approx_kl            | 4.8854254e-05 |
|    clip_fraction        | 0.00268       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0165       |
|    explained_variance   | 0.0786        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.913         |
|    n_updates            | 1480          |
|    policy_gradient_loss | -0.000631     |
|    value_loss           | 1.99          |
-------------------------------------------
Progress update from timestep 59500: Updated shield, loss is 0.10687098652124405
Progress update from timestep 59600: Updated shield, loss is 0.1076817512512207
Eval num_timesteps=59600, episode_reward=18.47 +/- 14.05
Episode length: 26.80 +/- 19.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.8     |
|    mean_reward     | 18.5     |
| time/              |          |
|    total_timesteps | 59600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.87     |
|    ep_rew_mean     | 3.17     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 149      |
|    time_elapsed    | 9187     |
|    total_timesteps | 59600    |
---------------------------------
Progress update from timestep 59700: Updated shield, loss is 0.10757283866405487
Progress update from timestep 59800: Updated shield, loss is 0.10200714319944382
Eval num_timesteps=59800, episode_reward=16.81 +/- 15.35
Episode length: 24.40 +/- 20.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.4          |
|    mean_reward          | 16.8          |
| time/                   |               |
|    total_timesteps      | 59800         |
| train/                  |               |
|    approx_kl            | 6.0587856e-05 |
|    clip_fraction        | 0.00067       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.051        |
|    explained_variance   | 0.0928        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.643         |
|    n_updates            | 1490          |
|    policy_gradient_loss | -0.000832     |
|    value_loss           | 2.54          |
-------------------------------------------
Progress update from timestep 59900: Updated shield, loss is 0.11219990998506546
Progress update from timestep 60000: Updated shield, loss is 0.10843945294618607
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 60000: Saved shield_loss.csv. 
Eval num_timesteps=60000, episode_reward=17.48 +/- 14.56
Episode length: 26.00 +/- 20.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | 17.5     |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.74     |
|    ep_rew_mean     | 3.02     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 150      |
|    time_elapsed    | 9319     |
|    total_timesteps | 60000    |
---------------------------------
Progress update from timestep 60100: Updated shield, loss is 0.11390437930822372
Progress update from timestep 60200: Updated shield, loss is 0.11656858026981354
Eval num_timesteps=60200, episode_reward=17.94 +/- 14.85
Episode length: 25.60 +/- 20.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.6          |
|    mean_reward          | 17.9          |
| time/                   |               |
|    total_timesteps      | 60200         |
| train/                  |               |
|    approx_kl            | 0.00020624351 |
|    clip_fraction        | 0.00915       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.061        |
|    explained_variance   | 0.107         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.536         |
|    n_updates            | 1500          |
|    policy_gradient_loss | -0.00105      |
|    value_loss           | 2.03          |
-------------------------------------------
Progress update from timestep 60300: Updated shield, loss is 0.11131110042333603
Progress update from timestep 60400: Updated shield, loss is 0.1139582023024559
Eval num_timesteps=60400, episode_reward=9.54 +/- 13.66
Episode length: 14.00 +/- 18.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14       |
|    mean_reward     | 9.54     |
| time/              |          |
|    total_timesteps | 60400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.95     |
|    ep_rew_mean     | 3.26     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 151      |
|    time_elapsed    | 9413     |
|    total_timesteps | 60400    |
---------------------------------
Progress update from timestep 60500: Updated shield, loss is 0.10218735784292221
Progress update from timestep 60600: Updated shield, loss is 0.11259225010871887
Eval num_timesteps=60600, episode_reward=12.99 +/- 12.64
Episode length: 18.80 +/- 16.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.8         |
|    mean_reward          | 13           |
| time/                   |              |
|    total_timesteps      | 60600        |
| train/                  |              |
|    approx_kl            | 9.228116e-05 |
|    clip_fraction        | 0.00223      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0418      |
|    explained_variance   | 0.226        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.556        |
|    n_updates            | 1510         |
|    policy_gradient_loss | -0.000929    |
|    value_loss           | 2.03         |
------------------------------------------
Progress update from timestep 60700: Updated shield, loss is 0.11127917468547821
Progress update from timestep 60800: Updated shield, loss is 0.1158735603094101
Eval num_timesteps=60800, episode_reward=10.63 +/- 13.17
Episode length: 15.40 +/- 17.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | 10.6     |
| time/              |          |
|    total_timesteps | 60800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.64     |
|    ep_rew_mean     | 2.98     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 152      |
|    time_elapsed    | 9507     |
|    total_timesteps | 60800    |
---------------------------------
Progress update from timestep 60900: Updated shield, loss is 0.10164707899093628
Progress update from timestep 61000: Updated shield, loss is 0.11224457621574402
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 61000: Saved shield_loss.csv. 
Eval num_timesteps=61000, episode_reward=4.14 +/- 2.36
Episode length: 6.80 +/- 3.43
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 6.8           |
|    mean_reward          | 4.14          |
| time/                   |               |
|    total_timesteps      | 61000         |
| train/                  |               |
|    approx_kl            | 3.3319793e-05 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0425       |
|    explained_variance   | 0.167         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.25          |
|    n_updates            | 1520          |
|    policy_gradient_loss | -0.000687     |
|    value_loss           | 1.73          |
-------------------------------------------
Progress update from timestep 61100: Updated shield, loss is 0.1101093664765358
Progress update from timestep 61200: Updated shield, loss is 0.11333686113357544
Eval num_timesteps=61200, episode_reward=11.23 +/- 12.87
Episode length: 16.00 +/- 17.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | 11.2     |
| time/              |          |
|    total_timesteps | 61200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.07     |
|    ep_rew_mean     | 3.33     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 153      |
|    time_elapsed    | 9581     |
|    total_timesteps | 61200    |
---------------------------------
Progress update from timestep 61300: Updated shield, loss is 0.1109212189912796
Progress update from timestep 61400: Updated shield, loss is 0.11288571357727051
Eval num_timesteps=61400, episode_reward=10.58 +/- 12.62
Episode length: 15.60 +/- 17.28
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.6          |
|    mean_reward          | 10.6          |
| time/                   |               |
|    total_timesteps      | 61400         |
| train/                  |               |
|    approx_kl            | 3.2339245e-05 |
|    clip_fraction        | 0.000446      |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0329       |
|    explained_variance   | 0.101         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.918         |
|    n_updates            | 1530          |
|    policy_gradient_loss | -0.000276     |
|    value_loss           | 1.98          |
-------------------------------------------
Progress update from timestep 61500: Updated shield, loss is 0.10861779749393463
Progress update from timestep 61600: Updated shield, loss is 0.11394178122282028
Eval num_timesteps=61600, episode_reward=8.28 +/- 9.91
Episode length: 12.60 +/- 13.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 12.6     |
|    mean_reward     | 8.28     |
| time/              |          |
|    total_timesteps | 61600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.77     |
|    ep_rew_mean     | 3.03     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 154      |
|    time_elapsed    | 9672     |
|    total_timesteps | 61600    |
---------------------------------
Progress update from timestep 61700: Updated shield, loss is 0.1073000431060791
Progress update from timestep 61800: Updated shield, loss is 0.10065513104200363
Eval num_timesteps=61800, episode_reward=17.45 +/- 15.69
Episode length: 24.60 +/- 20.83
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.6          |
|    mean_reward          | 17.5          |
| time/                   |               |
|    total_timesteps      | 61800         |
| train/                  |               |
|    approx_kl            | 0.00021194307 |
|    clip_fraction        | 0.00826       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0675       |
|    explained_variance   | 0.231         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.13          |
|    n_updates            | 1540          |
|    policy_gradient_loss | -0.00123      |
|    value_loss           | 1.63          |
-------------------------------------------
Progress update from timestep 61900: Updated shield, loss is 0.10571463406085968
Progress update from timestep 62000: Updated shield, loss is 0.12217052280902863
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 62000: Saved shield_loss.csv. 
Eval num_timesteps=62000, episode_reward=10.02 +/- 13.40
Episode length: 14.60 +/- 17.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 10       |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.25     |
|    ep_rew_mean     | 3.46     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 155      |
|    time_elapsed    | 9766     |
|    total_timesteps | 62000    |
---------------------------------
Progress update from timestep 62100: Updated shield, loss is 0.10044018179178238
Progress update from timestep 62200: Updated shield, loss is 0.09581752121448517
Eval num_timesteps=62200, episode_reward=3.99 +/- 3.57
Episode length: 6.00 +/- 4.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 6           |
|    mean_reward          | 3.99        |
| time/                   |             |
|    total_timesteps      | 62200       |
| train/                  |             |
|    approx_kl            | 4.06658e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0187     |
|    explained_variance   | 0.156       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.859       |
|    n_updates            | 1550        |
|    policy_gradient_loss | -0.000198   |
|    value_loss           | 1.99        |
-----------------------------------------
Progress update from timestep 62300: Updated shield, loss is 0.11009720712900162
Progress update from timestep 62400: Updated shield, loss is 0.10488536208868027
Eval num_timesteps=62400, episode_reward=2.78 +/- 1.15
Episode length: 4.80 +/- 1.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.8      |
|    mean_reward     | 2.78     |
| time/              |          |
|    total_timesteps | 62400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.36     |
|    ep_rew_mean     | 3.55     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 156      |
|    time_elapsed    | 9822     |
|    total_timesteps | 62400    |
---------------------------------
Progress update from timestep 62500: Updated shield, loss is 0.09926694631576538
Progress update from timestep 62600: Updated shield, loss is 0.10623724013566971
Eval num_timesteps=62600, episode_reward=9.13 +/- 13.84
Episode length: 12.80 +/- 18.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 12.8          |
|    mean_reward          | 9.13          |
| time/                   |               |
|    total_timesteps      | 62600         |
| train/                  |               |
|    approx_kl            | 0.00013713365 |
|    clip_fraction        | 0.00446       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.028        |
|    explained_variance   | 0.0969        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.96          |
|    n_updates            | 1560          |
|    policy_gradient_loss | -0.00103      |
|    value_loss           | 3.22          |
-------------------------------------------
Progress update from timestep 62700: Updated shield, loss is 0.09953557699918747
Progress update from timestep 62800: Updated shield, loss is 0.10425511747598648
Eval num_timesteps=62800, episode_reward=2.74 +/- 0.96
Episode length: 4.60 +/- 1.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.6      |
|    mean_reward     | 2.74     |
| time/              |          |
|    total_timesteps | 62800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.13     |
|    ep_rew_mean     | 3.38     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 157      |
|    time_elapsed    | 9892     |
|    total_timesteps | 62800    |
---------------------------------
Progress update from timestep 62900: Updated shield, loss is 0.10907293111085892
Progress update from timestep 63000: Updated shield, loss is 0.10295945405960083
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 63000: Saved shield_loss.csv. 
Eval num_timesteps=63000, episode_reward=3.37 +/- 1.97
Episode length: 4.80 +/- 2.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 4.8           |
|    mean_reward          | 3.37          |
| time/                   |               |
|    total_timesteps      | 63000         |
| train/                  |               |
|    approx_kl            | 0.00022501366 |
|    clip_fraction        | 0.0096        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0457       |
|    explained_variance   | -0.00713      |
|    learning_rate        | 0.0001        |
|    loss                 | 0.952         |
|    n_updates            | 1570          |
|    policy_gradient_loss | -0.00119      |
|    value_loss           | 1.7           |
-------------------------------------------
Progress update from timestep 63100: Updated shield, loss is 0.0961659774184227
Progress update from timestep 63200: Updated shield, loss is 0.10592397302389145
Eval num_timesteps=63200, episode_reward=1.79 +/- 1.10
Episode length: 3.00 +/- 1.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3        |
|    mean_reward     | 1.79     |
| time/              |          |
|    total_timesteps | 63200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.55     |
|    ep_rew_mean     | 2.98     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 158      |
|    time_elapsed    | 9944     |
|    total_timesteps | 63200    |
---------------------------------
Progress update from timestep 63300: Updated shield, loss is 0.12088494002819061
Progress update from timestep 63400: Updated shield, loss is 0.0930778980255127
Eval num_timesteps=63400, episode_reward=1.81 +/- 1.43
Episode length: 3.00 +/- 1.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 3             |
|    mean_reward          | 1.81          |
| time/                   |               |
|    total_timesteps      | 63400         |
| train/                  |               |
|    approx_kl            | 2.6573161e-05 |
|    clip_fraction        | 0.000223      |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0128       |
|    explained_variance   | 0.214         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.554         |
|    n_updates            | 1580          |
|    policy_gradient_loss | -0.00034      |
|    value_loss           | 1.45          |
-------------------------------------------
Progress update from timestep 63500: Updated shield, loss is 0.10496894270181656
Progress update from timestep 63600: Updated shield, loss is 0.10816451162099838
Eval num_timesteps=63600, episode_reward=3.32 +/- 1.70
Episode length: 4.80 +/- 1.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.8      |
|    mean_reward     | 3.32     |
| time/              |          |
|    total_timesteps | 63600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.49     |
|    ep_rew_mean     | 2.89     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 159      |
|    time_elapsed    | 9995     |
|    total_timesteps | 63600    |
---------------------------------
Progress update from timestep 63700: Updated shield, loss is 0.09920050203800201
Progress update from timestep 63800: Updated shield, loss is 0.0992136299610138
Eval num_timesteps=63800, episode_reward=3.27 +/- 4.03
Episode length: 5.00 +/- 4.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 5             |
|    mean_reward          | 3.27          |
| time/                   |               |
|    total_timesteps      | 63800         |
| train/                  |               |
|    approx_kl            | 0.00022771582 |
|    clip_fraction        | 0.0107        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0453       |
|    explained_variance   | 0.111         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.02          |
|    n_updates            | 1590          |
|    policy_gradient_loss | -0.00173      |
|    value_loss           | 1.61          |
-------------------------------------------
Progress update from timestep 63900: Updated shield, loss is 0.11377816647291183
Progress update from timestep 64000: Updated shield, loss is 0.10282640904188156
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 64000: Saved shield_loss.csv. 
Eval num_timesteps=64000, episode_reward=8.38 +/- 14.17
Episode length: 12.20 +/- 18.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 12.2     |
|    mean_reward     | 8.38     |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.49     |
|    ep_rew_mean     | 2.91     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 160      |
|    time_elapsed    | 10069    |
|    total_timesteps | 64000    |
---------------------------------
Progress update from timestep 64100: Updated shield, loss is 0.10102217644453049
Progress update from timestep 64200: Updated shield, loss is 0.09543779492378235
Eval num_timesteps=64200, episode_reward=5.44 +/- 3.91
Episode length: 7.20 +/- 4.49
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 7.2           |
|    mean_reward          | 5.44          |
| time/                   |               |
|    total_timesteps      | 64200         |
| train/                  |               |
|    approx_kl            | 4.6641966e-05 |
|    clip_fraction        | 0.000223      |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0264       |
|    explained_variance   | 0.136         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.56          |
|    n_updates            | 1600          |
|    policy_gradient_loss | -0.000244     |
|    value_loss           | 1.59          |
-------------------------------------------
Progress update from timestep 64300: Updated shield, loss is 0.09768511354923248
Progress update from timestep 64400: Updated shield, loss is 0.10087410360574722
Eval num_timesteps=64400, episode_reward=1.93 +/- 0.87
Episode length: 3.20 +/- 0.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3.2      |
|    mean_reward     | 1.93     |
| time/              |          |
|    total_timesteps | 64400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.09     |
|    ep_rew_mean     | 3.46     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 161      |
|    time_elapsed    | 10124    |
|    total_timesteps | 64400    |
---------------------------------
Progress update from timestep 64500: Updated shield, loss is 0.09898783266544342
Progress update from timestep 64600: Updated shield, loss is 0.10099941492080688
Eval num_timesteps=64600, episode_reward=4.30 +/- 2.16
Episode length: 7.00 +/- 3.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 7             |
|    mean_reward          | 4.3           |
| time/                   |               |
|    total_timesteps      | 64600         |
| train/                  |               |
|    approx_kl            | 0.00029250808 |
|    clip_fraction        | 0.0109        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.066        |
|    explained_variance   | 0.0813        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.2           |
|    n_updates            | 1610          |
|    policy_gradient_loss | -0.00189      |
|    value_loss           | 2.33          |
-------------------------------------------
Progress update from timestep 64700: Updated shield, loss is 0.10306791216135025
Progress update from timestep 64800: Updated shield, loss is 0.09263143688440323
Eval num_timesteps=64800, episode_reward=23.61 +/- 16.43
Episode length: 32.20 +/- 21.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 23.6     |
| time/              |          |
|    total_timesteps | 64800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.58     |
|    ep_rew_mean     | 3.7      |
| time/              |          |
|    fps             | 6        |
|    iterations      | 162      |
|    time_elapsed    | 10218    |
|    total_timesteps | 64800    |
---------------------------------
Progress update from timestep 64900: Updated shield, loss is 0.10213413089513779
Progress update from timestep 65000: Updated shield, loss is 0.10781081765890121
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 65000: Saved shield_loss.csv. 
Eval num_timesteps=65000, episode_reward=3.29 +/- 2.73
Episode length: 5.20 +/- 3.60
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5.2      |
|    mean_reward          | 3.29     |
| time/                   |          |
|    total_timesteps      | 65000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.121    |
|    learning_rate        | 0.0001   |
|    loss                 | 0.995    |
|    n_updates            | 1620     |
|    policy_gradient_loss | 3.25e-09 |
|    value_loss           | 2.88     |
--------------------------------------
Progress update from timestep 65100: Updated shield, loss is 0.10773205012083054
Progress update from timestep 65200: Updated shield, loss is 0.1140780970454216
Eval num_timesteps=65200, episode_reward=3.96 +/- 1.65
Episode length: 6.40 +/- 2.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6.4      |
|    mean_reward     | 3.96     |
| time/              |          |
|    total_timesteps | 65200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.55     |
|    ep_rew_mean     | 2.89     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 163      |
|    time_elapsed    | 10276    |
|    total_timesteps | 65200    |
---------------------------------
Progress update from timestep 65300: Updated shield, loss is 0.09662492573261261
Progress update from timestep 65400: Updated shield, loss is 0.10468771308660507
Eval num_timesteps=65400, episode_reward=16.92 +/- 16.10
Episode length: 23.80 +/- 21.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.8          |
|    mean_reward          | 16.9          |
| time/                   |               |
|    total_timesteps      | 65400         |
| train/                  |               |
|    approx_kl            | 9.7939745e-05 |
|    clip_fraction        | 0.00737       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0242       |
|    explained_variance   | 0.164         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.343         |
|    n_updates            | 1630          |
|    policy_gradient_loss | -0.00104      |
|    value_loss           | 1.83          |
-------------------------------------------
Progress update from timestep 65500: Updated shield, loss is 0.09662551432847977
Progress update from timestep 65600: Updated shield, loss is 0.10253159701824188
Eval num_timesteps=65600, episode_reward=2.88 +/- 1.31
Episode length: 5.00 +/- 1.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 2.88     |
| time/              |          |
|    total_timesteps | 65600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.86     |
|    ep_rew_mean     | 3.17     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 164      |
|    time_elapsed    | 10353    |
|    total_timesteps | 65600    |
---------------------------------
Progress update from timestep 65700: Updated shield, loss is 0.10492664575576782
Progress update from timestep 65800: Updated shield, loss is 0.09865564852952957
Eval num_timesteps=65800, episode_reward=17.67 +/- 15.51
Episode length: 25.00 +/- 20.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25            |
|    mean_reward          | 17.7          |
| time/                   |               |
|    total_timesteps      | 65800         |
| train/                  |               |
|    approx_kl            | 1.9705987e-05 |
|    clip_fraction        | 0.00134       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0118       |
|    explained_variance   | 0.0333        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.09          |
|    n_updates            | 1640          |
|    policy_gradient_loss | -0.000509     |
|    value_loss           | 2.37          |
-------------------------------------------
Progress update from timestep 65900: Updated shield, loss is 0.10914149880409241
Progress update from timestep 66000: Updated shield, loss is 0.10183490812778473
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 66000: Saved shield_loss.csv. 
Eval num_timesteps=66000, episode_reward=16.40 +/- 16.40
Episode length: 23.20 +/- 22.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 16.4     |
| time/              |          |
|    total_timesteps | 66000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.86     |
|    ep_rew_mean     | 3.21     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 165      |
|    time_elapsed    | 10446    |
|    total_timesteps | 66000    |
---------------------------------
Progress update from timestep 66100: Updated shield, loss is 0.12126155942678452
Progress update from timestep 66200: Updated shield, loss is 0.098285011947155
Eval num_timesteps=66200, episode_reward=29.58 +/- 11.47
Episode length: 42.20 +/- 15.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.2         |
|    mean_reward          | 29.6         |
| time/                   |              |
|    total_timesteps      | 66200        |
| train/                  |              |
|    approx_kl            | 8.726892e-05 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0459      |
|    explained_variance   | 0.208        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.791        |
|    n_updates            | 1650         |
|    policy_gradient_loss | -0.000573    |
|    value_loss           | 1.37         |
------------------------------------------
Progress update from timestep 66300: Updated shield, loss is 0.11094889044761658
Progress update from timestep 66400: Updated shield, loss is 0.11014451086521149
Eval num_timesteps=66400, episode_reward=19.01 +/- 14.12
Episode length: 27.40 +/- 19.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.4     |
|    mean_reward     | 19       |
| time/              |          |
|    total_timesteps | 66400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.98     |
|    ep_rew_mean     | 3.3      |
| time/              |          |
|    fps             | 6        |
|    iterations      | 166      |
|    time_elapsed    | 10549    |
|    total_timesteps | 66400    |
---------------------------------
Progress update from timestep 66500: Updated shield, loss is 0.11159921437501907
Progress update from timestep 66600: Updated shield, loss is 0.10876484960317612
Eval num_timesteps=66600, episode_reward=15.46 +/- 14.85
Episode length: 23.60 +/- 21.90
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 23.6     |
|    mean_reward          | 15.5     |
| time/                   |          |
|    total_timesteps      | 66600    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.195    |
|    learning_rate        | 0.0001   |
|    loss                 | 0.83     |
|    n_updates            | 1660     |
|    policy_gradient_loss | -3.3e-09 |
|    value_loss           | 1.51     |
--------------------------------------
Progress update from timestep 66700: Updated shield, loss is 0.11104340106248856
Progress update from timestep 66800: Updated shield, loss is 0.10696006566286087
Eval num_timesteps=66800, episode_reward=10.46 +/- 12.08
Episode length: 15.80 +/- 17.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | 10.5     |
| time/              |          |
|    total_timesteps | 66800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.56     |
|    ep_rew_mean     | 2.99     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 167      |
|    time_elapsed    | 10645    |
|    total_timesteps | 66800    |
---------------------------------
Progress update from timestep 66900: Updated shield, loss is 0.10021752119064331
Progress update from timestep 67000: Updated shield, loss is 0.1018109992146492
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 67000: Saved shield_loss.csv. 
Eval num_timesteps=67000, episode_reward=4.85 +/- 2.59
Episode length: 8.00 +/- 3.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 8         |
|    mean_reward          | 4.85      |
| time/                   |           |
|    total_timesteps      | 67000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | -0.00595  |
|    learning_rate        | 0.0001    |
|    loss                 | 0.917     |
|    n_updates            | 1670      |
|    policy_gradient_loss | -3.73e-10 |
|    value_loss           | 2.6       |
---------------------------------------
Progress update from timestep 67100: Updated shield, loss is 0.10492895543575287
Progress update from timestep 67200: Updated shield, loss is 0.10213378816843033
Eval num_timesteps=67200, episode_reward=14.92 +/- 15.55
Episode length: 22.60 +/- 22.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 14.9     |
| time/              |          |
|    total_timesteps | 67200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.28     |
|    ep_rew_mean     | 3.54     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 168      |
|    time_elapsed    | 10720    |
|    total_timesteps | 67200    |
---------------------------------
Progress update from timestep 67300: Updated shield, loss is 0.1079091876745224
Progress update from timestep 67400: Updated shield, loss is 0.11152254045009613
Eval num_timesteps=67400, episode_reward=28.54 +/- 13.55
Episode length: 40.60 +/- 18.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 40.6         |
|    mean_reward          | 28.5         |
| time/                   |              |
|    total_timesteps      | 67400        |
| train/                  |              |
|    approx_kl            | 0.0002156701 |
|    clip_fraction        | 0.00692      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0442      |
|    explained_variance   | 0.111        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.15         |
|    n_updates            | 1680         |
|    policy_gradient_loss | -0.00171     |
|    value_loss           | 2.8          |
------------------------------------------
Progress update from timestep 67500: Updated shield, loss is 0.10264988988637924
Progress update from timestep 67600: Updated shield, loss is 0.09973748028278351
Eval num_timesteps=67600, episode_reward=22.20 +/- 13.83
Episode length: 33.60 +/- 20.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 22.2     |
| time/              |          |
|    total_timesteps | 67600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.61     |
|    ep_rew_mean     | 3.7      |
| time/              |          |
|    fps             | 6        |
|    iterations      | 169      |
|    time_elapsed    | 10852    |
|    total_timesteps | 67600    |
---------------------------------
Progress update from timestep 67700: Updated shield, loss is 0.10865677893161774
Progress update from timestep 67800: Updated shield, loss is 0.09718283265829086
Eval num_timesteps=67800, episode_reward=15.95 +/- 14.71
Episode length: 24.00 +/- 21.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24           |
|    mean_reward          | 15.9         |
| time/                   |              |
|    total_timesteps      | 67800        |
| train/                  |              |
|    approx_kl            | 5.177076e-05 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0207      |
|    explained_variance   | 0.169        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.839        |
|    n_updates            | 1690         |
|    policy_gradient_loss | -0.000343    |
|    value_loss           | 2.35         |
------------------------------------------
Progress update from timestep 67900: Updated shield, loss is 0.1081477552652359
Progress update from timestep 68000: Updated shield, loss is 0.10654191672801971
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 68000: Saved shield_loss.csv. 
Eval num_timesteps=68000, episode_reward=18.29 +/- 14.57
Episode length: 26.80 +/- 20.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.8     |
|    mean_reward     | 18.3     |
| time/              |          |
|    total_timesteps | 68000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.79     |
|    ep_rew_mean     | 3.1      |
| time/              |          |
|    fps             | 6        |
|    iterations      | 170      |
|    time_elapsed    | 10963    |
|    total_timesteps | 68000    |
---------------------------------
Progress update from timestep 68100: Updated shield, loss is 0.10185409337282181
Progress update from timestep 68200: Updated shield, loss is 0.10866577923297882
Eval num_timesteps=68200, episode_reward=18.89 +/- 14.56
Episode length: 26.60 +/- 19.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 26.6      |
|    mean_reward          | 18.9      |
| time/                   |           |
|    total_timesteps      | 68200     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.208     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.13      |
|    n_updates            | 1700      |
|    policy_gradient_loss | -2.15e-09 |
|    value_loss           | 2.23      |
---------------------------------------
Progress update from timestep 68300: Updated shield, loss is 0.1155882403254509
Progress update from timestep 68400: Updated shield, loss is 0.10203781723976135
Eval num_timesteps=68400, episode_reward=11.12 +/- 13.35
Episode length: 16.00 +/- 17.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | 11.1     |
| time/              |          |
|    total_timesteps | 68400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.09     |
|    ep_rew_mean     | 3.28     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 171      |
|    time_elapsed    | 11051    |
|    total_timesteps | 68400    |
---------------------------------
Progress update from timestep 68500: Updated shield, loss is 0.11157485097646713
Progress update from timestep 68600: Updated shield, loss is 0.10687941312789917
Eval num_timesteps=68600, episode_reward=5.64 +/- 1.94
Episode length: 9.20 +/- 2.79
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 9.2           |
|    mean_reward          | 5.64          |
| time/                   |               |
|    total_timesteps      | 68600         |
| train/                  |               |
|    approx_kl            | 0.00014494706 |
|    clip_fraction        | 0.00781       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0539       |
|    explained_variance   | 0.111         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.582         |
|    n_updates            | 1710          |
|    policy_gradient_loss | -0.00135      |
|    value_loss           | 1.82          |
-------------------------------------------
Progress update from timestep 68700: Updated shield, loss is 0.09678886085748672
Progress update from timestep 68800: Updated shield, loss is 0.09169980138540268
Eval num_timesteps=68800, episode_reward=4.48 +/- 2.93
Episode length: 7.20 +/- 3.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7.2      |
|    mean_reward     | 4.48     |
| time/              |          |
|    total_timesteps | 68800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.85     |
|    ep_rew_mean     | 3.13     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 172      |
|    time_elapsed    | 11113    |
|    total_timesteps | 68800    |
---------------------------------
Progress update from timestep 68900: Updated shield, loss is 0.09417621791362762
Progress update from timestep 69000: Updated shield, loss is 0.11123931407928467
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 69000: Saved shield_loss.csv. 
Eval num_timesteps=69000, episode_reward=23.97 +/- 14.78
Episode length: 33.80 +/- 19.96
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.8          |
|    mean_reward          | 24            |
| time/                   |               |
|    total_timesteps      | 69000         |
| train/                  |               |
|    approx_kl            | 5.8911473e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00944      |
|    explained_variance   | 0.0701        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.867         |
|    n_updates            | 1720          |
|    policy_gradient_loss | -4e-05        |
|    value_loss           | 2.16          |
-------------------------------------------
Progress update from timestep 69100: Updated shield, loss is 0.11068273335695267
Progress update from timestep 69200: Updated shield, loss is 0.10303507000207901
Eval num_timesteps=69200, episode_reward=11.22 +/- 12.11
Episode length: 16.60 +/- 17.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | 11.2     |
| time/              |          |
|    total_timesteps | 69200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.44     |
|    ep_rew_mean     | 3.6      |
| time/              |          |
|    fps             | 6        |
|    iterations      | 173      |
|    time_elapsed    | 11205    |
|    total_timesteps | 69200    |
---------------------------------
Progress update from timestep 69300: Updated shield, loss is 0.1139085665345192
Progress update from timestep 69400: Updated shield, loss is 0.11562515050172806
Eval num_timesteps=69400, episode_reward=4.33 +/- 2.08
Episode length: 7.20 +/- 2.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 7.2          |
|    mean_reward          | 4.33         |
| time/                   |              |
|    total_timesteps      | 69400        |
| train/                  |              |
|    approx_kl            | 0.0004495981 |
|    clip_fraction        | 0.00804      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0711      |
|    explained_variance   | 0.123        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.852        |
|    n_updates            | 1730         |
|    policy_gradient_loss | -0.0016      |
|    value_loss           | 2.75         |
------------------------------------------
Progress update from timestep 69500: Updated shield, loss is 0.10742302983999252
Progress update from timestep 69600: Updated shield, loss is 0.10134232044219971
Eval num_timesteps=69600, episode_reward=11.13 +/- 12.37
Episode length: 16.40 +/- 17.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | 11.1     |
| time/              |          |
|    total_timesteps | 69600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.47     |
|    ep_rew_mean     | 3.68     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 174      |
|    time_elapsed    | 11278    |
|    total_timesteps | 69600    |
---------------------------------
Progress update from timestep 69700: Updated shield, loss is 0.10708153992891312
Progress update from timestep 69800: Updated shield, loss is 0.10420671850442886
Eval num_timesteps=69800, episode_reward=16.26 +/- 15.05
Episode length: 24.00 +/- 21.47
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 24       |
|    mean_reward          | 16.3     |
| time/                   |          |
|    total_timesteps      | 69800    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.145    |
|    learning_rate        | 0.0001   |
|    loss                 | 0.819    |
|    n_updates            | 1740     |
|    policy_gradient_loss | 1.77e-09 |
|    value_loss           | 2.08     |
--------------------------------------
Progress update from timestep 69900: Updated shield, loss is 0.10700821876525879
Progress update from timestep 70000: Updated shield, loss is 0.10650504380464554
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 70000: Saved shield_loss.csv. 
Eval num_timesteps=70000, episode_reward=17.73 +/- 13.64
Episode length: 26.20 +/- 19.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | 17.7     |
| time/              |          |
|    total_timesteps | 70000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.09     |
|    ep_rew_mean     | 3.33     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 175      |
|    time_elapsed    | 11374    |
|    total_timesteps | 70000    |
---------------------------------
Progress update from timestep 70100: Updated shield, loss is 0.10228416323661804
Progress update from timestep 70200: Updated shield, loss is 0.10718297213315964
Eval num_timesteps=70200, episode_reward=19.14 +/- 13.36
Episode length: 28.00 +/- 18.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 28            |
|    mean_reward          | 19.1          |
| time/                   |               |
|    total_timesteps      | 70200         |
| train/                  |               |
|    approx_kl            | 0.00031811744 |
|    clip_fraction        | 0.0103        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0336       |
|    explained_variance   | 0.158         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.33          |
|    n_updates            | 1750          |
|    policy_gradient_loss | -0.00147      |
|    value_loss           | 2.26          |
-------------------------------------------
Progress update from timestep 70300: Updated shield, loss is 0.10688124597072601
Progress update from timestep 70400: Updated shield, loss is 0.1095147430896759
Eval num_timesteps=70400, episode_reward=15.12 +/- 15.84
Episode length: 22.40 +/- 22.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 15.1     |
| time/              |          |
|    total_timesteps | 70400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.4      |
|    ep_rew_mean     | 3.55     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 176      |
|    time_elapsed    | 11487    |
|    total_timesteps | 70400    |
---------------------------------
Progress update from timestep 70500: Updated shield, loss is 0.11234977096319199
Progress update from timestep 70600: Updated shield, loss is 0.09527195990085602
Eval num_timesteps=70600, episode_reward=3.17 +/- 1.79
Episode length: 5.40 +/- 2.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 5.4           |
|    mean_reward          | 3.17          |
| time/                   |               |
|    total_timesteps      | 70600         |
| train/                  |               |
|    approx_kl            | 3.2995296e-05 |
|    clip_fraction        | 0.000893      |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0255       |
|    explained_variance   | 0.21          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.746         |
|    n_updates            | 1760          |
|    policy_gradient_loss | -0.000316     |
|    value_loss           | 2.16          |
-------------------------------------------
Progress update from timestep 70700: Updated shield, loss is 0.101640984416008
Progress update from timestep 70800: Updated shield, loss is 0.09837140887975693
Eval num_timesteps=70800, episode_reward=18.44 +/- 14.57
Episode length: 26.20 +/- 19.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | 18.4     |
| time/              |          |
|    total_timesteps | 70800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.44     |
|    ep_rew_mean     | 3.6      |
| time/              |          |
|    fps             | 6        |
|    iterations      | 177      |
|    time_elapsed    | 11580    |
|    total_timesteps | 70800    |
---------------------------------
Progress update from timestep 70900: Updated shield, loss is 0.10109037905931473
Progress update from timestep 71000: Updated shield, loss is 0.0981443002820015
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 71000: Saved shield_loss.csv. 
Eval num_timesteps=71000, episode_reward=22.50 +/- 14.68
Episode length: 33.00 +/- 20.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33            |
|    mean_reward          | 22.5          |
| time/                   |               |
|    total_timesteps      | 71000         |
| train/                  |               |
|    approx_kl            | 3.6368943e-05 |
|    clip_fraction        | 0.00112       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0229       |
|    explained_variance   | 0.104         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.69          |
|    n_updates            | 1770          |
|    policy_gradient_loss | -0.000378     |
|    value_loss           | 3.02          |
-------------------------------------------
Progress update from timestep 71100: Updated shield, loss is 0.09936555474996567
Progress update from timestep 71200: Updated shield, loss is 0.10550647228956223
Eval num_timesteps=71200, episode_reward=3.29 +/- 0.67
Episode length: 5.60 +/- 1.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5.6      |
|    mean_reward     | 3.29     |
| time/              |          |
|    total_timesteps | 71200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.31     |
|    ep_rew_mean     | 3.54     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 178      |
|    time_elapsed    | 11656    |
|    total_timesteps | 71200    |
---------------------------------
Progress update from timestep 71300: Updated shield, loss is 0.09807445108890533
Progress update from timestep 71400: Updated shield, loss is 0.10897568613290787
Eval num_timesteps=71400, episode_reward=16.44 +/- 14.78
Episode length: 24.40 +/- 20.96
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.4          |
|    mean_reward          | 16.4          |
| time/                   |               |
|    total_timesteps      | 71400         |
| train/                  |               |
|    approx_kl            | 0.00020515213 |
|    clip_fraction        | 0.00268       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0489       |
|    explained_variance   | 0.152         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.03          |
|    n_updates            | 1780          |
|    policy_gradient_loss | -0.000613     |
|    value_loss           | 2.29          |
-------------------------------------------
Progress update from timestep 71500: Updated shield, loss is 0.10063113272190094
Progress update from timestep 71600: Updated shield, loss is 0.11280275136232376
Eval num_timesteps=71600, episode_reward=5.84 +/- 5.40
Episode length: 9.20 +/- 7.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.2      |
|    mean_reward     | 5.84     |
| time/              |          |
|    total_timesteps | 71600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.68     |
|    ep_rew_mean     | 2.95     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 179      |
|    time_elapsed    | 11734    |
|    total_timesteps | 71600    |
---------------------------------
Progress update from timestep 71700: Updated shield, loss is 0.0950767919421196
Progress update from timestep 71800: Updated shield, loss is 0.10079909861087799
Eval num_timesteps=71800, episode_reward=19.41 +/- 12.56
Episode length: 29.20 +/- 18.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 29.2          |
|    mean_reward          | 19.4          |
| time/                   |               |
|    total_timesteps      | 71800         |
| train/                  |               |
|    approx_kl            | 0.00013682226 |
|    clip_fraction        | 0.00558       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0565       |
|    explained_variance   | 0.135         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.593         |
|    n_updates            | 1790          |
|    policy_gradient_loss | -0.00111      |
|    value_loss           | 1.96          |
-------------------------------------------
Progress update from timestep 71900: Updated shield, loss is 0.12025266885757446
Progress update from timestep 72000: Updated shield, loss is 0.0947440043091774
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 72000: Saved shield_loss.csv. 
Eval num_timesteps=72000, episode_reward=7.95 +/- 3.68
Episode length: 12.40 +/- 5.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 12.4     |
|    mean_reward     | 7.95     |
| time/              |          |
|    total_timesteps | 72000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.52     |
|    ep_rew_mean     | 3.66     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 180      |
|    time_elapsed    | 11823    |
|    total_timesteps | 72000    |
---------------------------------
Progress update from timestep 72100: Updated shield, loss is 0.08563271909952164
Progress update from timestep 72200: Updated shield, loss is 0.09453003108501434
Eval num_timesteps=72200, episode_reward=23.54 +/- 14.78
Episode length: 33.40 +/- 20.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.4          |
|    mean_reward          | 23.5          |
| time/                   |               |
|    total_timesteps      | 72200         |
| train/                  |               |
|    approx_kl            | 0.00013579203 |
|    clip_fraction        | 0.0105        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0373       |
|    explained_variance   | 0.165         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.22          |
|    n_updates            | 1800          |
|    policy_gradient_loss | -0.0017       |
|    value_loss           | 2.12          |
-------------------------------------------
Progress update from timestep 72300: Updated shield, loss is 0.10701156407594681
Progress update from timestep 72400: Updated shield, loss is 0.11121267825365067
Eval num_timesteps=72400, episode_reward=16.87 +/- 14.60
Episode length: 24.80 +/- 20.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 72400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.14     |
|    ep_rew_mean     | 3.47     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 181      |
|    time_elapsed    | 11939    |
|    total_timesteps | 72400    |
---------------------------------
Progress update from timestep 72500: Updated shield, loss is 0.09729977697134018
Progress update from timestep 72600: Updated shield, loss is 0.09235754609107971
Eval num_timesteps=72600, episode_reward=15.40 +/- 14.70
Episode length: 23.60 +/- 21.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.6         |
|    mean_reward          | 15.4         |
| time/                   |              |
|    total_timesteps      | 72600        |
| train/                  |              |
|    approx_kl            | 8.632283e-05 |
|    clip_fraction        | 0.00357      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0381      |
|    explained_variance   | 0.0176       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.04         |
|    n_updates            | 1810         |
|    policy_gradient_loss | -0.000862    |
|    value_loss           | 2.66         |
------------------------------------------
Progress update from timestep 72700: Updated shield, loss is 0.10143265128135681
Progress update from timestep 72800: Updated shield, loss is 0.10324510186910629
Eval num_timesteps=72800, episode_reward=23.52 +/- 14.12
Episode length: 34.20 +/- 19.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 23.5     |
| time/              |          |
|    total_timesteps | 72800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.32     |
|    ep_rew_mean     | 2.78     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 182      |
|    time_elapsed    | 12030    |
|    total_timesteps | 72800    |
---------------------------------
Progress update from timestep 72900: Updated shield, loss is 0.10326021164655685
Progress update from timestep 73000: Updated shield, loss is 0.10267335921525955
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 73000: Saved shield_loss.csv. 
Eval num_timesteps=73000, episode_reward=5.38 +/- 3.41
Episode length: 8.60 +/- 4.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 8.6           |
|    mean_reward          | 5.38          |
| time/                   |               |
|    total_timesteps      | 73000         |
| train/                  |               |
|    approx_kl            | 0.00027524293 |
|    clip_fraction        | 0.00692       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.065        |
|    explained_variance   | 0.107         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.02          |
|    n_updates            | 1820          |
|    policy_gradient_loss | -0.00112      |
|    value_loss           | 2.11          |
-------------------------------------------
Progress update from timestep 73100: Updated shield, loss is 0.10837288945913315
Progress update from timestep 73200: Updated shield, loss is 0.10606493055820465
Eval num_timesteps=73200, episode_reward=14.19 +/- 11.75
Episode length: 20.80 +/- 16.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.8     |
|    mean_reward     | 14.2     |
| time/              |          |
|    total_timesteps | 73200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4        |
|    ep_rew_mean     | 2.51     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 183      |
|    time_elapsed    | 12108    |
|    total_timesteps | 73200    |
---------------------------------
Progress update from timestep 73300: Updated shield, loss is 0.10006310790777206
Progress update from timestep 73400: Updated shield, loss is 0.10531477630138397
Eval num_timesteps=73400, episode_reward=16.86 +/- 15.39
Episode length: 24.20 +/- 21.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.2         |
|    mean_reward          | 16.9         |
| time/                   |              |
|    total_timesteps      | 73400        |
| train/                  |              |
|    approx_kl            | 8.593686e-05 |
|    clip_fraction        | 0.00446      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0368      |
|    explained_variance   | 0.176        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.961        |
|    n_updates            | 1830         |
|    policy_gradient_loss | -0.000837    |
|    value_loss           | 1.67         |
------------------------------------------
Progress update from timestep 73500: Updated shield, loss is 0.1029895544052124
Progress update from timestep 73600: Updated shield, loss is 0.11920221149921417
Eval num_timesteps=73600, episode_reward=17.58 +/- 15.04
Episode length: 25.40 +/- 20.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 73600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.84     |
|    ep_rew_mean     | 3.13     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 184      |
|    time_elapsed    | 12202    |
|    total_timesteps | 73600    |
---------------------------------
Progress update from timestep 73700: Updated shield, loss is 0.09060956537723541
Progress update from timestep 73800: Updated shield, loss is 0.09597775340080261
Eval num_timesteps=73800, episode_reward=13.57 +/- 12.34
Episode length: 19.20 +/- 16.49
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 19.2          |
|    mean_reward          | 13.6          |
| time/                   |               |
|    total_timesteps      | 73800         |
| train/                  |               |
|    approx_kl            | 3.8945647e-05 |
|    clip_fraction        | 0.00246       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0106       |
|    explained_variance   | 0.0846        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.447         |
|    n_updates            | 1840          |
|    policy_gradient_loss | -0.000291     |
|    value_loss           | 2.13          |
-------------------------------------------
Progress update from timestep 73900: Updated shield, loss is 0.0920787900686264
Progress update from timestep 74000: Updated shield, loss is 0.09918773919343948
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 74000: Saved shield_loss.csv. 
Eval num_timesteps=74000, episode_reward=15.44 +/- 15.18
Episode length: 23.20 +/- 21.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 15.4     |
| time/              |          |
|    total_timesteps | 74000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.7      |
|    ep_rew_mean     | 3.77     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 185      |
|    time_elapsed    | 12301    |
|    total_timesteps | 74000    |
---------------------------------
Progress update from timestep 74100: Updated shield, loss is 0.08409404754638672
Progress update from timestep 74200: Updated shield, loss is 0.09032640606164932
Eval num_timesteps=74200, episode_reward=19.07 +/- 13.45
Episode length: 28.20 +/- 19.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 28.2          |
|    mean_reward          | 19.1          |
| time/                   |               |
|    total_timesteps      | 74200         |
| train/                  |               |
|    approx_kl            | 0.00013629041 |
|    clip_fraction        | 0.0103        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0222       |
|    explained_variance   | 0.0751        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.09          |
|    n_updates            | 1850          |
|    policy_gradient_loss | -0.00145      |
|    value_loss           | 2.68          |
-------------------------------------------
Progress update from timestep 74300: Updated shield, loss is 0.08948896825313568
Progress update from timestep 74400: Updated shield, loss is 0.0932667925953865
Eval num_timesteps=74400, episode_reward=17.27 +/- 14.51
Episode length: 25.20 +/- 20.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 17.3     |
| time/              |          |
|    total_timesteps | 74400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.77     |
|    ep_rew_mean     | 3.08     |
| time/              |          |
|    fps             | 6        |
|    iterations      | 186      |
|    time_elapsed    | 12392    |
|    total_timesteps | 74400    |
---------------------------------
Progress update from timestep 74500: Updated shield, loss is 0.10322778671979904
Progress update from timestep 74600: Updated shield, loss is 0.09724567830562592
Eval num_timesteps=74600, episode_reward=16.82 +/- 15.36
Episode length: 24.40 +/- 20.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.4          |
|    mean_reward          | 16.8          |
| time/                   |               |
|    total_timesteps      | 74600         |
| train/                  |               |
|    approx_kl            | 1.3609949e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00768      |
|    explained_variance   | 0.123         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.863         |
|    n_updates            | 1860          |
|    policy_gradient_loss | -5.31e-05     |
|    value_loss           | 1.98          |
-------------------------------------------
Progress update from timestep 74700: Updated shield, loss is 0.09905833005905151
Progress update from timestep 74800: Updated shield, loss is 0.09192147850990295
Eval num_timesteps=74800, episode_reward=19.13 +/- 13.05
Episode length: 27.80 +/- 18.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.8     |
|    mean_reward     | 19.1     |
| time/              |          |
|    total_timesteps | 74800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.48     |
|    ep_rew_mean     | 2.9      |
| time/              |          |
|    fps             | 5        |
|    iterations      | 187      |
|    time_elapsed    | 12487    |
|    total_timesteps | 74800    |
---------------------------------
Progress update from timestep 74900: Updated shield, loss is 0.09892494976520538
Progress update from timestep 75000: Updated shield, loss is 0.09529545903205872
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 75000: Saved shield_loss.csv. 
Eval num_timesteps=75000, episode_reward=6.28 +/- 5.03
Episode length: 9.80 +/- 7.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 9.8           |
|    mean_reward          | 6.28          |
| time/                   |               |
|    total_timesteps      | 75000         |
| train/                  |               |
|    approx_kl            | 0.00016053194 |
|    clip_fraction        | 0.00625       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0845       |
|    explained_variance   | 0.0839        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.676         |
|    n_updates            | 1870          |
|    policy_gradient_loss | -0.00097      |
|    value_loss           | 1.86          |
-------------------------------------------
Progress update from timestep 75100: Updated shield, loss is 0.09130053967237473
Progress update from timestep 75200: Updated shield, loss is 0.08706037700176239
Eval num_timesteps=75200, episode_reward=19.04 +/- 12.26
Episode length: 28.40 +/- 18.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.4     |
|    mean_reward     | 19       |
| time/              |          |
|    total_timesteps | 75200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.2      |
|    ep_rew_mean     | 3.42     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 188      |
|    time_elapsed    | 12570    |
|    total_timesteps | 75200    |
---------------------------------
Progress update from timestep 75300: Updated shield, loss is 0.10224749892950058
Progress update from timestep 75400: Updated shield, loss is 0.0953175276517868
Eval num_timesteps=75400, episode_reward=4.20 +/- 2.03
Episode length: 7.00 +/- 2.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 7             |
|    mean_reward          | 4.2           |
| time/                   |               |
|    total_timesteps      | 75400         |
| train/                  |               |
|    approx_kl            | 0.00014441514 |
|    clip_fraction        | 0.00357       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0335       |
|    explained_variance   | 0.178         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.12          |
|    n_updates            | 1880          |
|    policy_gradient_loss | -0.000882     |
|    value_loss           | 2             |
-------------------------------------------
Progress update from timestep 75500: Updated shield, loss is 0.09981486201286316
Progress update from timestep 75600: Updated shield, loss is 0.1080506220459938
Eval num_timesteps=75600, episode_reward=16.17 +/- 14.61
Episode length: 24.40 +/- 21.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 16.2     |
| time/              |          |
|    total_timesteps | 75600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.98     |
|    ep_rew_mean     | 3.2      |
| time/              |          |
|    fps             | 5        |
|    iterations      | 189      |
|    time_elapsed    | 12648    |
|    total_timesteps | 75600    |
---------------------------------
Progress update from timestep 75700: Updated shield, loss is 0.10036575794219971
Progress update from timestep 75800: Updated shield, loss is 0.09634469449520111
Eval num_timesteps=75800, episode_reward=17.85 +/- 13.88
Episode length: 26.40 +/- 19.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.4          |
|    mean_reward          | 17.8          |
| time/                   |               |
|    total_timesteps      | 75800         |
| train/                  |               |
|    approx_kl            | 0.00022960747 |
|    clip_fraction        | 0.0116        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0692       |
|    explained_variance   | 0.213         |
|    learning_rate        | 0.0001        |
|    loss                 | 1             |
|    n_updates            | 1890          |
|    policy_gradient_loss | -0.00181      |
|    value_loss           | 1.46          |
-------------------------------------------
Progress update from timestep 75900: Updated shield, loss is 0.10327266156673431
Progress update from timestep 76000: Updated shield, loss is 0.09712352603673935
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 76000: Saved shield_loss.csv. 
Eval num_timesteps=76000, episode_reward=24.28 +/- 14.46
Episode length: 34.20 +/- 19.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 24.3     |
| time/              |          |
|    total_timesteps | 76000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.17     |
|    ep_rew_mean     | 2.65     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 190      |
|    time_elapsed    | 12744    |
|    total_timesteps | 76000    |
---------------------------------
Progress update from timestep 76100: Updated shield, loss is 0.0996958464384079
Progress update from timestep 76200: Updated shield, loss is 0.1026294082403183
Eval num_timesteps=76200, episode_reward=17.77 +/- 15.03
Episode length: 25.20 +/- 20.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 25.2      |
|    mean_reward          | 17.8      |
| time/                   |           |
|    total_timesteps      | 76200     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.18      |
|    learning_rate        | 0.0001    |
|    loss                 | 0.759     |
|    n_updates            | 1900      |
|    policy_gradient_loss | -7.45e-10 |
|    value_loss           | 1.24      |
---------------------------------------
Progress update from timestep 76300: Updated shield, loss is 0.10038518905639648
Progress update from timestep 76400: Updated shield, loss is 0.09550601243972778
Eval num_timesteps=76400, episode_reward=21.37 +/- 16.52
Episode length: 31.00 +/- 23.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31       |
|    mean_reward     | 21.4     |
| time/              |          |
|    total_timesteps | 76400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.16     |
|    ep_rew_mean     | 3.46     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 191      |
|    time_elapsed    | 12855    |
|    total_timesteps | 76400    |
---------------------------------
Progress update from timestep 76500: Updated shield, loss is 0.09947487711906433
Progress update from timestep 76600: Updated shield, loss is 0.10067413747310638
Eval num_timesteps=76600, episode_reward=15.92 +/- 16.54
Episode length: 22.60 +/- 22.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.6          |
|    mean_reward          | 15.9          |
| time/                   |               |
|    total_timesteps      | 76600         |
| train/                  |               |
|    approx_kl            | 2.0416055e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.012        |
|    explained_variance   | 0.198         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.15          |
|    n_updates            | 1910          |
|    policy_gradient_loss | -0.000172     |
|    value_loss           | 1.84          |
-------------------------------------------
Progress update from timestep 76700: Updated shield, loss is 0.10339018702507019
Progress update from timestep 76800: Updated shield, loss is 0.09302398562431335
Eval num_timesteps=76800, episode_reward=17.61 +/- 14.30
Episode length: 26.00 +/- 20.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 76800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.4      |
|    ep_rew_mean     | 2.91     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 192      |
|    time_elapsed    | 12967    |
|    total_timesteps | 76800    |
---------------------------------
Progress update from timestep 76900: Updated shield, loss is 0.1038372814655304
Progress update from timestep 77000: Updated shield, loss is 0.08237138390541077
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 77000: Saved shield_loss.csv. 
Eval num_timesteps=77000, episode_reward=16.42 +/- 15.68
Episode length: 23.60 +/- 21.58
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 23.6           |
|    mean_reward          | 16.4           |
| time/                   |                |
|    total_timesteps      | 77000          |
| train/                  |                |
|    approx_kl            | 0.000120784425 |
|    clip_fraction        | 0.00357        |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.03          |
|    explained_variance   | 0.0652         |
|    learning_rate        | 0.0001         |
|    loss                 | 0.98           |
|    n_updates            | 1920           |
|    policy_gradient_loss | -0.000871      |
|    value_loss           | 1.95           |
--------------------------------------------
Progress update from timestep 77100: Updated shield, loss is 0.0969260111451149
Progress update from timestep 77200: Updated shield, loss is 0.1041487604379654
Eval num_timesteps=77200, episode_reward=10.93 +/- 13.10
Episode length: 16.00 +/- 17.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | 10.9     |
| time/              |          |
|    total_timesteps | 77200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.93     |
|    ep_rew_mean     | 3.17     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 193      |
|    time_elapsed    | 13058    |
|    total_timesteps | 77200    |
---------------------------------
Progress update from timestep 77300: Updated shield, loss is 0.09190309047698975
Progress update from timestep 77400: Updated shield, loss is 0.10603956878185272
Eval num_timesteps=77400, episode_reward=20.97 +/- 11.87
Episode length: 31.00 +/- 17.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 31            |
|    mean_reward          | 21            |
| time/                   |               |
|    total_timesteps      | 77400         |
| train/                  |               |
|    approx_kl            | 0.00012038249 |
|    clip_fraction        | 0.00469       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0167       |
|    explained_variance   | 0.14          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.558         |
|    n_updates            | 1930          |
|    policy_gradient_loss | -0.000794     |
|    value_loss           | 1.83          |
-------------------------------------------
Progress update from timestep 77500: Updated shield, loss is 0.09256093949079514
Progress update from timestep 77600: Updated shield, loss is 0.10607127100229263
Eval num_timesteps=77600, episode_reward=26.35 +/- 12.29
Episode length: 37.60 +/- 16.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.6     |
|    mean_reward     | 26.4     |
| time/              |          |
|    total_timesteps | 77600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.64     |
|    ep_rew_mean     | 2.96     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 194      |
|    time_elapsed    | 13183    |
|    total_timesteps | 77600    |
---------------------------------
Progress update from timestep 77700: Updated shield, loss is 0.09099753201007843
Progress update from timestep 77800: Updated shield, loss is 0.10471658408641815
Eval num_timesteps=77800, episode_reward=6.28 +/- 5.54
Episode length: 10.00 +/- 8.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 10           |
|    mean_reward          | 6.28         |
| time/                   |              |
|    total_timesteps      | 77800        |
| train/                  |              |
|    approx_kl            | 0.0002574069 |
|    clip_fraction        | 0.00647      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.047       |
|    explained_variance   | 0.0743       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.34         |
|    n_updates            | 1940         |
|    policy_gradient_loss | -0.000971    |
|    value_loss           | 2.1          |
------------------------------------------
Progress update from timestep 77900: Updated shield, loss is 0.11470125615596771
Progress update from timestep 78000: Updated shield, loss is 0.10020200163125992
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 78000: Saved shield_loss.csv. 
Eval num_timesteps=78000, episode_reward=4.69 +/- 2.94
Episode length: 7.40 +/- 4.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7.4      |
|    mean_reward     | 4.69     |
| time/              |          |
|    total_timesteps | 78000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.35     |
|    ep_rew_mean     | 2.72     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 195      |
|    time_elapsed    | 13246    |
|    total_timesteps | 78000    |
---------------------------------
Progress update from timestep 78100: Updated shield, loss is 0.10442081838846207
Progress update from timestep 78200: Updated shield, loss is 0.09337596595287323
Eval num_timesteps=78200, episode_reward=15.11 +/- 10.85
Episode length: 22.40 +/- 14.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.4          |
|    mean_reward          | 15.1          |
| time/                   |               |
|    total_timesteps      | 78200         |
| train/                  |               |
|    approx_kl            | 9.6264164e-05 |
|    clip_fraction        | 0.00112       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0336       |
|    explained_variance   | 0.147         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.696         |
|    n_updates            | 1950          |
|    policy_gradient_loss | -0.000655     |
|    value_loss           | 1.82          |
-------------------------------------------
Progress update from timestep 78300: Updated shield, loss is 0.10390506684780121
Progress update from timestep 78400: Updated shield, loss is 0.1108662411570549
Eval num_timesteps=78400, episode_reward=17.33 +/- 15.58
Episode length: 24.80 +/- 20.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 17.3     |
| time/              |          |
|    total_timesteps | 78400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.26     |
|    ep_rew_mean     | 2.69     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 196      |
|    time_elapsed    | 13343    |
|    total_timesteps | 78400    |
---------------------------------
Progress update from timestep 78500: Updated shield, loss is 0.09083221107721329
Progress update from timestep 78600: Updated shield, loss is 0.10866480320692062
Eval num_timesteps=78600, episode_reward=16.72 +/- 14.15
Episode length: 25.20 +/- 20.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.2          |
|    mean_reward          | 16.7          |
| time/                   |               |
|    total_timesteps      | 78600         |
| train/                  |               |
|    approx_kl            | 3.3945378e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0106       |
|    explained_variance   | 0.0223        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.14          |
|    n_updates            | 1960          |
|    policy_gradient_loss | -8.99e-05     |
|    value_loss           | 1.49          |
-------------------------------------------
Progress update from timestep 78700: Updated shield, loss is 0.1120288074016571
Progress update from timestep 78800: Updated shield, loss is 0.09747596830129623
Eval num_timesteps=78800, episode_reward=10.99 +/- 12.47
Episode length: 16.40 +/- 17.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | 11       |
| time/              |          |
|    total_timesteps | 78800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.54     |
|    ep_rew_mean     | 2.89     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 197      |
|    time_elapsed    | 13437    |
|    total_timesteps | 78800    |
---------------------------------
Progress update from timestep 78900: Updated shield, loss is 0.11106997728347778
Progress update from timestep 79000: Updated shield, loss is 0.10101687908172607
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 79000: Saved shield_loss.csv. 
Eval num_timesteps=79000, episode_reward=23.78 +/- 13.56
Episode length: 34.40 +/- 19.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.4         |
|    mean_reward          | 23.8         |
| time/                   |              |
|    total_timesteps      | 79000        |
| train/                  |              |
|    approx_kl            | 9.365952e-05 |
|    clip_fraction        | 0.00424      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.037       |
|    explained_variance   | 0.123        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.655        |
|    n_updates            | 1970         |
|    policy_gradient_loss | -0.000773    |
|    value_loss           | 1.45         |
------------------------------------------
Progress update from timestep 79100: Updated shield, loss is 0.09766941517591476
Progress update from timestep 79200: Updated shield, loss is 0.10041870176792145
Eval num_timesteps=79200, episode_reward=9.75 +/- 13.03
Episode length: 14.40 +/- 17.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.4     |
|    mean_reward     | 9.75     |
| time/              |          |
|    total_timesteps | 79200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.51     |
|    ep_rew_mean     | 2.84     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 198      |
|    time_elapsed    | 13526    |
|    total_timesteps | 79200    |
---------------------------------
Progress update from timestep 79300: Updated shield, loss is 0.10105298459529877
Progress update from timestep 79400: Updated shield, loss is 0.095566526055336
Eval num_timesteps=79400, episode_reward=6.28 +/- 3.06
Episode length: 10.00 +/- 4.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 10            |
|    mean_reward          | 6.28          |
| time/                   |               |
|    total_timesteps      | 79400         |
| train/                  |               |
|    approx_kl            | 0.00026608104 |
|    clip_fraction        | 0.00491       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0372       |
|    explained_variance   | 0.12          |
|    learning_rate        | 0.0001        |
|    loss                 | 1.35          |
|    n_updates            | 1980          |
|    policy_gradient_loss | -0.00139      |
|    value_loss           | 2.28          |
-------------------------------------------
Progress update from timestep 79500: Updated shield, loss is 0.09419885277748108
Progress update from timestep 79600: Updated shield, loss is 0.10944308340549469
Eval num_timesteps=79600, episode_reward=24.40 +/- 13.49
Episode length: 35.00 +/- 18.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 24.4     |
| time/              |          |
|    total_timesteps | 79600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.29     |
|    ep_rew_mean     | 3.56     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 199      |
|    time_elapsed    | 13625    |
|    total_timesteps | 79600    |
---------------------------------
Progress update from timestep 79700: Updated shield, loss is 0.09638331830501556
Progress update from timestep 79800: Updated shield, loss is 0.09602685272693634
Eval num_timesteps=79800, episode_reward=17.82 +/- 14.41
Episode length: 25.80 +/- 20.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.8          |
|    mean_reward          | 17.8          |
| time/                   |               |
|    total_timesteps      | 79800         |
| train/                  |               |
|    approx_kl            | 0.00020816464 |
|    clip_fraction        | 0.0029        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0497       |
|    explained_variance   | 0.145         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.19          |
|    n_updates            | 1990          |
|    policy_gradient_loss | -0.000903     |
|    value_loss           | 3.19          |
-------------------------------------------
Progress update from timestep 79900: Updated shield, loss is 0.10779794305562973
Progress update from timestep 80000: Updated shield, loss is 0.10409539192914963
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 80000: Saved shield_loss.csv. 
Eval num_timesteps=80000, episode_reward=17.53 +/- 15.13
Episode length: 25.40 +/- 21.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | 17.5     |
| time/              |          |
|    total_timesteps | 80000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.4      |
|    ep_rew_mean     | 3.57     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 200      |
|    time_elapsed    | 13721    |
|    total_timesteps | 80000    |
---------------------------------
Progress update from timestep 80100: Updated shield, loss is 0.10088074207305908
Progress update from timestep 80200: Updated shield, loss is 0.09339042007923126
Eval num_timesteps=80200, episode_reward=17.48 +/- 14.74
Episode length: 25.60 +/- 20.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.6          |
|    mean_reward          | 17.5          |
| time/                   |               |
|    total_timesteps      | 80200         |
| train/                  |               |
|    approx_kl            | 0.00016824169 |
|    clip_fraction        | 0.00625       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0258       |
|    explained_variance   | 0.154         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.999         |
|    n_updates            | 2000          |
|    policy_gradient_loss | -0.000988     |
|    value_loss           | 2.35          |
-------------------------------------------
Progress update from timestep 80300: Updated shield, loss is 0.09965304285287857
Progress update from timestep 80400: Updated shield, loss is 0.08914075046777725
Eval num_timesteps=80400, episode_reward=23.85 +/- 14.39
Episode length: 34.00 +/- 19.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 23.8     |
| time/              |          |
|    total_timesteps | 80400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.2      |
|    ep_rew_mean     | 3.51     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 201      |
|    time_elapsed    | 13840    |
|    total_timesteps | 80400    |
---------------------------------
Progress update from timestep 80500: Updated shield, loss is 0.10955413430929184
Progress update from timestep 80600: Updated shield, loss is 0.09361127763986588
Eval num_timesteps=80600, episode_reward=15.79 +/- 14.91
Episode length: 23.80 +/- 21.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.8          |
|    mean_reward          | 15.8          |
| time/                   |               |
|    total_timesteps      | 80600         |
| train/                  |               |
|    approx_kl            | 0.00014179187 |
|    clip_fraction        | 0.00714       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0324       |
|    explained_variance   | 0.121         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.761         |
|    n_updates            | 2010          |
|    policy_gradient_loss | -0.00122      |
|    value_loss           | 2.14          |
-------------------------------------------
Progress update from timestep 80700: Updated shield, loss is 0.1021260917186737
Progress update from timestep 80800: Updated shield, loss is 0.09610886126756668
Eval num_timesteps=80800, episode_reward=15.58 +/- 15.48
Episode length: 23.20 +/- 21.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 15.6     |
| time/              |          |
|    total_timesteps | 80800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.12     |
|    ep_rew_mean     | 3.4      |
| time/              |          |
|    fps             | 5        |
|    iterations      | 202      |
|    time_elapsed    | 13930    |
|    total_timesteps | 80800    |
---------------------------------
Progress update from timestep 80900: Updated shield, loss is 0.09667184948921204
Progress update from timestep 81000: Updated shield, loss is 0.09994890540838242
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 81000: Saved shield_loss.csv. 
Eval num_timesteps=81000, episode_reward=17.62 +/- 14.76
Episode length: 25.40 +/- 20.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.4          |
|    mean_reward          | 17.6          |
| time/                   |               |
|    total_timesteps      | 81000         |
| train/                  |               |
|    approx_kl            | 0.00011351212 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0197       |
|    explained_variance   | 0.0538        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.16          |
|    n_updates            | 2020          |
|    policy_gradient_loss | -0.000297     |
|    value_loss           | 2.45          |
-------------------------------------------
Progress update from timestep 81100: Updated shield, loss is 0.10811352729797363
Progress update from timestep 81200: Updated shield, loss is 0.09556330740451813
Eval num_timesteps=81200, episode_reward=15.53 +/- 9.69
Episode length: 23.40 +/- 13.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 15.5     |
| time/              |          |
|    total_timesteps | 81200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.71     |
|    ep_rew_mean     | 3.83     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 203      |
|    time_elapsed    | 14033    |
|    total_timesteps | 81200    |
---------------------------------
Progress update from timestep 81300: Updated shield, loss is 0.09719491750001907
Progress update from timestep 81400: Updated shield, loss is 0.09939540177583694
Eval num_timesteps=81400, episode_reward=18.33 +/- 14.62
Episode length: 26.60 +/- 19.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.6          |
|    mean_reward          | 18.3          |
| time/                   |               |
|    total_timesteps      | 81400         |
| train/                  |               |
|    approx_kl            | 0.00018130043 |
|    clip_fraction        | 0.00692       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0583       |
|    explained_variance   | 0.174         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.979         |
|    n_updates            | 2030          |
|    policy_gradient_loss | -0.00172      |
|    value_loss           | 2.39          |
-------------------------------------------
Progress update from timestep 81500: Updated shield, loss is 0.10538329184055328
Progress update from timestep 81600: Updated shield, loss is 0.0857216864824295
Eval num_timesteps=81600, episode_reward=28.30 +/- 12.36
Episode length: 41.20 +/- 17.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | 28.3     |
| time/              |          |
|    total_timesteps | 81600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.04     |
|    ep_rew_mean     | 3.29     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 204      |
|    time_elapsed    | 14144    |
|    total_timesteps | 81600    |
---------------------------------
Progress update from timestep 81700: Updated shield, loss is 0.10272298008203506
Progress update from timestep 81800: Updated shield, loss is 0.09875484555959702
Eval num_timesteps=81800, episode_reward=11.35 +/- 12.34
Episode length: 16.80 +/- 16.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.8          |
|    mean_reward          | 11.4          |
| time/                   |               |
|    total_timesteps      | 81800         |
| train/                  |               |
|    approx_kl            | 6.4344946e-05 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0242       |
|    explained_variance   | 0.189         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.738         |
|    n_updates            | 2040          |
|    policy_gradient_loss | -0.000383     |
|    value_loss           | 1.99          |
-------------------------------------------
Progress update from timestep 81900: Updated shield, loss is 0.10557186603546143
Progress update from timestep 82000: Updated shield, loss is 0.09108752012252808
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 82000: Saved shield_loss.csv. 
Eval num_timesteps=82000, episode_reward=13.75 +/- 5.73
Episode length: 21.00 +/- 8.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | 13.8     |
| time/              |          |
|    total_timesteps | 82000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.28     |
|    ep_rew_mean     | 3.49     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 205      |
|    time_elapsed    | 14225    |
|    total_timesteps | 82000    |
---------------------------------
Progress update from timestep 82100: Updated shield, loss is 0.1067582368850708
Progress update from timestep 82200: Updated shield, loss is 0.09445638209581375
Eval num_timesteps=82200, episode_reward=13.16 +/- 11.91
Episode length: 19.00 +/- 15.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 19           |
|    mean_reward          | 13.2         |
| time/                   |              |
|    total_timesteps      | 82200        |
| train/                  |              |
|    approx_kl            | 7.711138e-05 |
|    clip_fraction        | 0.0029       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0265      |
|    explained_variance   | 0.172        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.549        |
|    n_updates            | 2050         |
|    policy_gradient_loss | -0.000416    |
|    value_loss           | 2.71         |
------------------------------------------
Progress update from timestep 82300: Updated shield, loss is 0.09275305271148682
Progress update from timestep 82400: Updated shield, loss is 0.09821075201034546
Eval num_timesteps=82400, episode_reward=25.47 +/- 11.58
Episode length: 37.00 +/- 16.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 25.5     |
| time/              |          |
|    total_timesteps | 82400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.35     |
|    ep_rew_mean     | 2.79     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 206      |
|    time_elapsed    | 14332    |
|    total_timesteps | 82400    |
---------------------------------
Progress update from timestep 82500: Updated shield, loss is 0.08487887680530548
Progress update from timestep 82600: Updated shield, loss is 0.10076572000980377
Eval num_timesteps=82600, episode_reward=14.68 +/- 11.04
Episode length: 21.80 +/- 15.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 21.8          |
|    mean_reward          | 14.7          |
| time/                   |               |
|    total_timesteps      | 82600         |
| train/                  |               |
|    approx_kl            | 0.00028190442 |
|    clip_fraction        | 0.00558       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0677       |
|    explained_variance   | 0.151         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.647         |
|    n_updates            | 2060          |
|    policy_gradient_loss | -0.00134      |
|    value_loss           | 2.07          |
-------------------------------------------
Progress update from timestep 82700: Updated shield, loss is 0.0948752760887146
Progress update from timestep 82800: Updated shield, loss is 0.09096825867891312
Eval num_timesteps=82800, episode_reward=16.59 +/- 15.23
Episode length: 24.20 +/- 21.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 82800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.51     |
|    ep_rew_mean     | 2.94     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 207      |
|    time_elapsed    | 14424    |
|    total_timesteps | 82800    |
---------------------------------
Progress update from timestep 82900: Updated shield, loss is 0.09958851337432861
Progress update from timestep 83000: Updated shield, loss is 0.09790152311325073
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 83000: Saved shield_loss.csv. 
Eval num_timesteps=83000, episode_reward=23.66 +/- 12.50
Episode length: 35.40 +/- 18.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.4         |
|    mean_reward          | 23.7         |
| time/                   |              |
|    total_timesteps      | 83000        |
| train/                  |              |
|    approx_kl            | 9.727531e-05 |
|    clip_fraction        | 0.00558      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0337      |
|    explained_variance   | 0.0756       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.277        |
|    n_updates            | 2070         |
|    policy_gradient_loss | -0.00113     |
|    value_loss           | 1.66         |
------------------------------------------
Progress update from timestep 83100: Updated shield, loss is 0.09267118573188782
Progress update from timestep 83200: Updated shield, loss is 0.08741473406553268
Eval num_timesteps=83200, episode_reward=10.21 +/- 12.89
Episode length: 15.00 +/- 17.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | 10.2     |
| time/              |          |
|    total_timesteps | 83200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.76     |
|    ep_rew_mean     | 3.07     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 208      |
|    time_elapsed    | 14536    |
|    total_timesteps | 83200    |
---------------------------------
Progress update from timestep 83300: Updated shield, loss is 0.08621609210968018
Progress update from timestep 83400: Updated shield, loss is 0.08168217539787292
Eval num_timesteps=83400, episode_reward=22.72 +/- 14.85
Episode length: 33.00 +/- 20.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33           |
|    mean_reward          | 22.7         |
| time/                   |              |
|    total_timesteps      | 83400        |
| train/                  |              |
|    approx_kl            | 6.837571e-05 |
|    clip_fraction        | 0.00223      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0515      |
|    explained_variance   | 0.0572       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.755        |
|    n_updates            | 2080         |
|    policy_gradient_loss | -0.000912    |
|    value_loss           | 2.28         |
------------------------------------------
Progress update from timestep 83500: Updated shield, loss is 0.10224954783916473
Progress update from timestep 83600: Updated shield, loss is 0.08969192206859589
Eval num_timesteps=83600, episode_reward=4.24 +/- 2.01
Episode length: 7.00 +/- 2.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7        |
|    mean_reward     | 4.24     |
| time/              |          |
|    total_timesteps | 83600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.14     |
|    ep_rew_mean     | 2.59     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 209      |
|    time_elapsed    | 14611    |
|    total_timesteps | 83600    |
---------------------------------
Progress update from timestep 83700: Updated shield, loss is 0.09389897435903549
Progress update from timestep 83800: Updated shield, loss is 0.11668901890516281
Eval num_timesteps=83800, episode_reward=22.13 +/- 15.13
Episode length: 32.40 +/- 21.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32.4          |
|    mean_reward          | 22.1          |
| time/                   |               |
|    total_timesteps      | 83800         |
| train/                  |               |
|    approx_kl            | 0.00020394368 |
|    clip_fraction        | 0.00937       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0541       |
|    explained_variance   | 0.192         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.771         |
|    n_updates            | 2090          |
|    policy_gradient_loss | -0.00155      |
|    value_loss           | 2.11          |
-------------------------------------------
Progress update from timestep 83900: Updated shield, loss is 0.09418724477291107
Progress update from timestep 84000: Updated shield, loss is 0.09574923664331436
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 84000: Saved shield_loss.csv. 
Eval num_timesteps=84000, episode_reward=23.54 +/- 15.76
Episode length: 32.80 +/- 21.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 23.5     |
| time/              |          |
|    total_timesteps | 84000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.91     |
|    ep_rew_mean     | 3.21     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 210      |
|    time_elapsed    | 14708    |
|    total_timesteps | 84000    |
---------------------------------
Progress update from timestep 84100: Updated shield, loss is 0.08943554013967514
Progress update from timestep 84200: Updated shield, loss is 0.08869382739067078
Eval num_timesteps=84200, episode_reward=16.56 +/- 14.80
Episode length: 24.40 +/- 21.11
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 24.4           |
|    mean_reward          | 16.6           |
| time/                   |                |
|    total_timesteps      | 84200          |
| train/                  |                |
|    approx_kl            | 0.000119832875 |
|    clip_fraction        | 0.00424        |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.058         |
|    explained_variance   | 0.172          |
|    learning_rate        | 0.0001         |
|    loss                 | 0.993          |
|    n_updates            | 2100           |
|    policy_gradient_loss | -0.00115       |
|    value_loss           | 2.24           |
--------------------------------------------
Progress update from timestep 84300: Updated shield, loss is 0.09374658018350601
Progress update from timestep 84400: Updated shield, loss is 0.09504583477973938
Eval num_timesteps=84400, episode_reward=29.88 +/- 11.99
Episode length: 41.80 +/- 16.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 29.9     |
| time/              |          |
|    total_timesteps | 84400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.77     |
|    ep_rew_mean     | 3.21     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 211      |
|    time_elapsed    | 14801    |
|    total_timesteps | 84400    |
---------------------------------
Progress update from timestep 84500: Updated shield, loss is 0.08031914383172989
Progress update from timestep 84600: Updated shield, loss is 0.09263642132282257
Eval num_timesteps=84600, episode_reward=17.93 +/- 14.50
Episode length: 26.00 +/- 20.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26            |
|    mean_reward          | 17.9          |
| time/                   |               |
|    total_timesteps      | 84600         |
| train/                  |               |
|    approx_kl            | 0.00014272984 |
|    clip_fraction        | 0.00603       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0298       |
|    explained_variance   | 0.0581        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.998         |
|    n_updates            | 2110          |
|    policy_gradient_loss | -0.00113      |
|    value_loss           | 2.27          |
-------------------------------------------
Progress update from timestep 84700: Updated shield, loss is 0.09843121469020844
Progress update from timestep 84800: Updated shield, loss is 0.09037307649850845
Eval num_timesteps=84800, episode_reward=10.10 +/- 11.64
Episode length: 15.80 +/- 17.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | 10.1     |
| time/              |          |
|    total_timesteps | 84800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.78     |
|    ep_rew_mean     | 3.15     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 212      |
|    time_elapsed    | 14890    |
|    total_timesteps | 84800    |
---------------------------------
Progress update from timestep 84900: Updated shield, loss is 0.08429756760597229
Progress update from timestep 85000: Updated shield, loss is 0.0937984436750412
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 85000: Saved shield_loss.csv. 
Eval num_timesteps=85000, episode_reward=25.01 +/- 13.10
Episode length: 35.80 +/- 17.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.8          |
|    mean_reward          | 25            |
| time/                   |               |
|    total_timesteps      | 85000         |
| train/                  |               |
|    approx_kl            | 0.00010116106 |
|    clip_fraction        | 0.00379       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0334       |
|    explained_variance   | 0.154         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.941         |
|    n_updates            | 2120          |
|    policy_gradient_loss | -0.000927     |
|    value_loss           | 1.51          |
-------------------------------------------
Progress update from timestep 85100: Updated shield, loss is 0.0889240950345993
Progress update from timestep 85200: Updated shield, loss is 0.08975876122713089
Eval num_timesteps=85200, episode_reward=6.49 +/- 3.48
Episode length: 10.20 +/- 4.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10.2     |
|    mean_reward     | 6.49     |
| time/              |          |
|    total_timesteps | 85200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.4      |
|    ep_rew_mean     | 2.8      |
| time/              |          |
|    fps             | 5        |
|    iterations      | 213      |
|    time_elapsed    | 14987    |
|    total_timesteps | 85200    |
---------------------------------
Progress update from timestep 85300: Updated shield, loss is 0.10524876415729523
Progress update from timestep 85400: Updated shield, loss is 0.09750856459140778
Eval num_timesteps=85400, episode_reward=22.07 +/- 16.56
Episode length: 31.40 +/- 22.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 31.4          |
|    mean_reward          | 22.1          |
| time/                   |               |
|    total_timesteps      | 85400         |
| train/                  |               |
|    approx_kl            | 0.00044985968 |
|    clip_fraction        | 0.0194        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0849       |
|    explained_variance   | 0.0899        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.28          |
|    n_updates            | 2130          |
|    policy_gradient_loss | -0.00238      |
|    value_loss           | 1.71          |
-------------------------------------------
Progress update from timestep 85500: Updated shield, loss is 0.09941474348306656
Progress update from timestep 85600: Updated shield, loss is 0.08213010430335999
Eval num_timesteps=85600, episode_reward=12.08 +/- 12.23
Episode length: 18.00 +/- 17.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | 12.1     |
| time/              |          |
|    total_timesteps | 85600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.2      |
|    ep_rew_mean     | 2.65     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 214      |
|    time_elapsed    | 15077    |
|    total_timesteps | 85600    |
---------------------------------
Progress update from timestep 85700: Updated shield, loss is 0.09700843691825867
Progress update from timestep 85800: Updated shield, loss is 0.08446186780929565
Eval num_timesteps=85800, episode_reward=24.67 +/- 12.93
Episode length: 35.60 +/- 17.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.6          |
|    mean_reward          | 24.7          |
| time/                   |               |
|    total_timesteps      | 85800         |
| train/                  |               |
|    approx_kl            | 0.00013186796 |
|    clip_fraction        | 0.0029        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0436       |
|    explained_variance   | 0.139         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.72          |
|    n_updates            | 2140          |
|    policy_gradient_loss | -0.00102      |
|    value_loss           | 2.34          |
-------------------------------------------
Progress update from timestep 85900: Updated shield, loss is 0.09832272678613663
Progress update from timestep 86000: Updated shield, loss is 0.10636278986930847
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 86000: Saved shield_loss.csv. 
Eval num_timesteps=86000, episode_reward=16.89 +/- 15.00
Episode length: 24.80 +/- 20.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.69     |
|    ep_rew_mean     | 3.07     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 215      |
|    time_elapsed    | 15177    |
|    total_timesteps | 86000    |
---------------------------------
Progress update from timestep 86100: Updated shield, loss is 0.09921957552433014
Progress update from timestep 86200: Updated shield, loss is 0.10018769651651382
Eval num_timesteps=86200, episode_reward=18.41 +/- 14.87
Episode length: 26.40 +/- 19.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.4          |
|    mean_reward          | 18.4          |
| time/                   |               |
|    total_timesteps      | 86200         |
| train/                  |               |
|    approx_kl            | 2.7981587e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0246       |
|    explained_variance   | 0.0712        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.74          |
|    n_updates            | 2150          |
|    policy_gradient_loss | -0.000253     |
|    value_loss           | 1.99          |
-------------------------------------------
Progress update from timestep 86300: Updated shield, loss is 0.0861230194568634
Progress update from timestep 86400: Updated shield, loss is 0.08269641548395157
Eval num_timesteps=86400, episode_reward=15.75 +/- 14.92
Episode length: 23.60 +/- 21.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 15.8     |
| time/              |          |
|    total_timesteps | 86400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.17     |
|    ep_rew_mean     | 3.38     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 216      |
|    time_elapsed    | 15277    |
|    total_timesteps | 86400    |
---------------------------------
Progress update from timestep 86500: Updated shield, loss is 0.10301316529512405
Progress update from timestep 86600: Updated shield, loss is 0.09600832313299179
Eval num_timesteps=86600, episode_reward=16.52 +/- 15.67
Episode length: 23.80 +/- 21.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.8         |
|    mean_reward          | 16.5         |
| time/                   |              |
|    total_timesteps      | 86600        |
| train/                  |              |
|    approx_kl            | 0.0004854137 |
|    clip_fraction        | 0.0179       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0681      |
|    explained_variance   | 0.213        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.02         |
|    n_updates            | 2160         |
|    policy_gradient_loss | -0.00232     |
|    value_loss           | 1.89         |
------------------------------------------
Progress update from timestep 86700: Updated shield, loss is 0.09119819104671478
Progress update from timestep 86800: Updated shield, loss is 0.09524472802877426
Eval num_timesteps=86800, episode_reward=27.96 +/- 12.49
Episode length: 41.00 +/- 18.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41       |
|    mean_reward     | 28       |
| time/              |          |
|    total_timesteps | 86800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.32     |
|    ep_rew_mean     | 2.74     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 217      |
|    time_elapsed    | 15389    |
|    total_timesteps | 86800    |
---------------------------------
Progress update from timestep 86900: Updated shield, loss is 0.10299426317214966
Progress update from timestep 87000: Updated shield, loss is 0.08986405283212662
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 87000: Saved shield_loss.csv. 
Eval num_timesteps=87000, episode_reward=8.96 +/- 5.35
Episode length: 14.00 +/- 7.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14            |
|    mean_reward          | 8.96          |
| time/                   |               |
|    total_timesteps      | 87000         |
| train/                  |               |
|    approx_kl            | 0.00018459026 |
|    clip_fraction        | 0.00647       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0632       |
|    explained_variance   | 0.155         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.65          |
|    n_updates            | 2170          |
|    policy_gradient_loss | -0.00108      |
|    value_loss           | 2.04          |
-------------------------------------------
Progress update from timestep 87100: Updated shield, loss is 0.10372805595397949
Progress update from timestep 87200: Updated shield, loss is 0.10662230849266052
Eval num_timesteps=87200, episode_reward=8.96 +/- 13.33
Episode length: 13.40 +/- 18.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.4     |
|    mean_reward     | 8.96     |
| time/              |          |
|    total_timesteps | 87200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.42     |
|    ep_rew_mean     | 3.56     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 218      |
|    time_elapsed    | 15471    |
|    total_timesteps | 87200    |
---------------------------------
Progress update from timestep 87300: Updated shield, loss is 0.11032278090715408
Progress update from timestep 87400: Updated shield, loss is 0.09262241423130035
Eval num_timesteps=87400, episode_reward=17.81 +/- 13.99
Episode length: 26.20 +/- 19.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.2         |
|    mean_reward          | 17.8         |
| time/                   |              |
|    total_timesteps      | 87400        |
| train/                  |              |
|    approx_kl            | 0.0001405264 |
|    clip_fraction        | 0.00469      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0371      |
|    explained_variance   | 0.075        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.545        |
|    n_updates            | 2180         |
|    policy_gradient_loss | -0.000975    |
|    value_loss           | 1.94         |
------------------------------------------
Progress update from timestep 87500: Updated shield, loss is 0.0890757367014885
Progress update from timestep 87600: Updated shield, loss is 0.09561431407928467
Eval num_timesteps=87600, episode_reward=13.18 +/- 11.52
Episode length: 20.00 +/- 16.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20       |
|    mean_reward     | 13.2     |
| time/              |          |
|    total_timesteps | 87600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.19     |
|    ep_rew_mean     | 3.43     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 219      |
|    time_elapsed    | 15571    |
|    total_timesteps | 87600    |
---------------------------------
Progress update from timestep 87700: Updated shield, loss is 0.07903677970170975
Progress update from timestep 87800: Updated shield, loss is 0.10450508445501328
Eval num_timesteps=87800, episode_reward=17.09 +/- 14.97
Episode length: 25.00 +/- 20.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25            |
|    mean_reward          | 17.1          |
| time/                   |               |
|    total_timesteps      | 87800         |
| train/                  |               |
|    approx_kl            | 7.3553194e-05 |
|    clip_fraction        | 0.00424       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0219       |
|    explained_variance   | 0.217         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.698         |
|    n_updates            | 2190          |
|    policy_gradient_loss | -0.000799     |
|    value_loss           | 2.02          |
-------------------------------------------
Progress update from timestep 87900: Updated shield, loss is 0.09521657973527908
Progress update from timestep 88000: Updated shield, loss is 0.10659715533256531
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 88000: Saved shield_loss.csv. 
Eval num_timesteps=88000, episode_reward=28.31 +/- 13.49
Episode length: 40.60 +/- 18.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.6     |
|    mean_reward     | 28.3     |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.62     |
|    ep_rew_mean     | 2.94     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 220      |
|    time_elapsed    | 15664    |
|    total_timesteps | 88000    |
---------------------------------
Progress update from timestep 88100: Updated shield, loss is 0.09782898426055908
Progress update from timestep 88200: Updated shield, loss is 0.11229455471038818
Eval num_timesteps=88200, episode_reward=22.74 +/- 16.64
Episode length: 31.80 +/- 22.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 31.8          |
|    mean_reward          | 22.7          |
| time/                   |               |
|    total_timesteps      | 88200         |
| train/                  |               |
|    approx_kl            | 0.00017859081 |
|    clip_fraction        | 0.00871       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0567       |
|    explained_variance   | 0.108         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.06          |
|    n_updates            | 2200          |
|    policy_gradient_loss | -0.00199      |
|    value_loss           | 1.67          |
-------------------------------------------
Progress update from timestep 88300: Updated shield, loss is 0.08493919670581818
Progress update from timestep 88400: Updated shield, loss is 0.08730950951576233
Eval num_timesteps=88400, episode_reward=10.81 +/- 11.47
Episode length: 16.80 +/- 16.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | 10.8     |
| time/              |          |
|    total_timesteps | 88400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.37     |
|    ep_rew_mean     | 2.79     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 221      |
|    time_elapsed    | 15755    |
|    total_timesteps | 88400    |
---------------------------------
Progress update from timestep 88500: Updated shield, loss is 0.09052670747041702
Progress update from timestep 88600: Updated shield, loss is 0.09107116609811783
Eval num_timesteps=88600, episode_reward=11.48 +/- 12.97
Episode length: 16.60 +/- 17.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.6          |
|    mean_reward          | 11.5          |
| time/                   |               |
|    total_timesteps      | 88600         |
| train/                  |               |
|    approx_kl            | 7.0231436e-05 |
|    clip_fraction        | 0.00313       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.026        |
|    explained_variance   | 0.066         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.692         |
|    n_updates            | 2210          |
|    policy_gradient_loss | -0.000419     |
|    value_loss           | 1.72          |
-------------------------------------------
Progress update from timestep 88700: Updated shield, loss is 0.0996682271361351
Progress update from timestep 88800: Updated shield, loss is 0.08067687600851059
Eval num_timesteps=88800, episode_reward=22.47 +/- 14.26
Episode length: 33.40 +/- 20.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 22.5     |
| time/              |          |
|    total_timesteps | 88800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.97     |
|    ep_rew_mean     | 3.19     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 222      |
|    time_elapsed    | 15851    |
|    total_timesteps | 88800    |
---------------------------------
Progress update from timestep 88900: Updated shield, loss is 0.10220389068126678
Progress update from timestep 89000: Updated shield, loss is 0.09865348041057587
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 89000: Saved shield_loss.csv. 
Eval num_timesteps=89000, episode_reward=4.64 +/- 2.35
Episode length: 7.40 +/- 3.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 7.4           |
|    mean_reward          | 4.64          |
| time/                   |               |
|    total_timesteps      | 89000         |
| train/                  |               |
|    approx_kl            | 0.00017939681 |
|    clip_fraction        | 0.00781       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0301       |
|    explained_variance   | 0.123         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.466         |
|    n_updates            | 2220          |
|    policy_gradient_loss | -0.00108      |
|    value_loss           | 2.05          |
-------------------------------------------
Progress update from timestep 89100: Updated shield, loss is 0.09397434443235397
Progress update from timestep 89200: Updated shield, loss is 0.08955547213554382
Eval num_timesteps=89200, episode_reward=16.40 +/- 15.34
Episode length: 24.00 +/- 21.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16.4     |
| time/              |          |
|    total_timesteps | 89200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.63     |
|    ep_rew_mean     | 2.99     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 223      |
|    time_elapsed    | 15925    |
|    total_timesteps | 89200    |
---------------------------------
Progress update from timestep 89300: Updated shield, loss is 0.09355073422193527
Progress update from timestep 89400: Updated shield, loss is 0.09806018322706223
Eval num_timesteps=89400, episode_reward=15.75 +/- 15.37
Episode length: 23.20 +/- 21.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.2          |
|    mean_reward          | 15.8          |
| time/                   |               |
|    total_timesteps      | 89400         |
| train/                  |               |
|    approx_kl            | 0.00023825759 |
|    clip_fraction        | 0.00313       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0415       |
|    explained_variance   | 0.2           |
|    learning_rate        | 0.0001        |
|    loss                 | 1.16          |
|    n_updates            | 2230          |
|    policy_gradient_loss | -0.00103      |
|    value_loss           | 2.34          |
-------------------------------------------
Progress update from timestep 89500: Updated shield, loss is 0.07703561335802078
Progress update from timestep 89600: Updated shield, loss is 0.09933149069547653
Eval num_timesteps=89600, episode_reward=21.94 +/- 16.27
Episode length: 31.60 +/- 22.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.6     |
|    mean_reward     | 21.9     |
| time/              |          |
|    total_timesteps | 89600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.91     |
|    ep_rew_mean     | 3.2      |
| time/              |          |
|    fps             | 5        |
|    iterations      | 224      |
|    time_elapsed    | 16035    |
|    total_timesteps | 89600    |
---------------------------------
Progress update from timestep 89700: Updated shield, loss is 0.09118419140577316
Progress update from timestep 89800: Updated shield, loss is 0.09087122976779938
Eval num_timesteps=89800, episode_reward=29.72 +/- 11.21
Episode length: 42.40 +/- 15.20
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 42.4           |
|    mean_reward          | 29.7           |
| time/                   |                |
|    total_timesteps      | 89800          |
| train/                  |                |
|    approx_kl            | 0.000106672625 |
|    clip_fraction        | 0.00201        |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.0656        |
|    explained_variance   | 0.036          |
|    learning_rate        | 0.0001         |
|    loss                 | 0.508          |
|    n_updates            | 2240           |
|    policy_gradient_loss | -0.000828      |
|    value_loss           | 1.66           |
--------------------------------------------
Progress update from timestep 89900: Updated shield, loss is 0.09100959450006485
Progress update from timestep 90000: Updated shield, loss is 0.09031092375516891
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 90000: Saved shield_loss.csv. 
Eval num_timesteps=90000, episode_reward=30.43 +/- 11.99
Episode length: 42.00 +/- 16.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 30.4     |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.38     |
|    ep_rew_mean     | 2.79     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 225      |
|    time_elapsed    | 16151    |
|    total_timesteps | 90000    |
---------------------------------
Progress update from timestep 90100: Updated shield, loss is 0.08663549274206161
Progress update from timestep 90200: Updated shield, loss is 0.08506421744823456
Eval num_timesteps=90200, episode_reward=15.15 +/- 16.28
Episode length: 22.20 +/- 22.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.2          |
|    mean_reward          | 15.2          |
| time/                   |               |
|    total_timesteps      | 90200         |
| train/                  |               |
|    approx_kl            | 0.00017560752 |
|    clip_fraction        | 0.01          |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0481       |
|    explained_variance   | 0.174         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.758         |
|    n_updates            | 2250          |
|    policy_gradient_loss | -0.00117      |
|    value_loss           | 1.62          |
-------------------------------------------
Progress update from timestep 90300: Updated shield, loss is 0.0865081399679184
Progress update from timestep 90400: Updated shield, loss is 0.09077310562133789
Eval num_timesteps=90400, episode_reward=9.90 +/- 12.97
Episode length: 14.60 +/- 17.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 9.9      |
| time/              |          |
|    total_timesteps | 90400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.81     |
|    ep_rew_mean     | 3.15     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 226      |
|    time_elapsed    | 16242    |
|    total_timesteps | 90400    |
---------------------------------
Progress update from timestep 90500: Updated shield, loss is 0.09263712167739868
Progress update from timestep 90600: Updated shield, loss is 0.07943759858608246
Eval num_timesteps=90600, episode_reward=28.33 +/- 13.41
Episode length: 40.60 +/- 18.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 40.6          |
|    mean_reward          | 28.3          |
| time/                   |               |
|    total_timesteps      | 90600         |
| train/                  |               |
|    approx_kl            | 0.00029471118 |
|    clip_fraction        | 0.0067        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0648       |
|    explained_variance   | 0.155         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.615         |
|    n_updates            | 2260          |
|    policy_gradient_loss | -0.00152      |
|    value_loss           | 1.55          |
-------------------------------------------
Progress update from timestep 90700: Updated shield, loss is 0.10058752447366714
Progress update from timestep 90800: Updated shield, loss is 0.08686481416225433
Eval num_timesteps=90800, episode_reward=11.22 +/- 12.57
Episode length: 16.40 +/- 17.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | 11.2     |
| time/              |          |
|    total_timesteps | 90800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.1      |
|    ep_rew_mean     | 3.37     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 227      |
|    time_elapsed    | 16336    |
|    total_timesteps | 90800    |
---------------------------------
Progress update from timestep 90900: Updated shield, loss is 0.08474905788898468
Progress update from timestep 91000: Updated shield, loss is 0.08369389176368713
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 91000: Saved shield_loss.csv. 
Eval num_timesteps=91000, episode_reward=16.37 +/- 15.35
Episode length: 23.80 +/- 21.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.8          |
|    mean_reward          | 16.4          |
| time/                   |               |
|    total_timesteps      | 91000         |
| train/                  |               |
|    approx_kl            | 0.00093674567 |
|    clip_fraction        | 0.0196        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0882       |
|    explained_variance   | 0.112         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.68          |
|    n_updates            | 2270          |
|    policy_gradient_loss | -0.00326      |
|    value_loss           | 1.85          |
-------------------------------------------
Progress update from timestep 91100: Updated shield, loss is 0.09514523297548294
Progress update from timestep 91200: Updated shield, loss is 0.08148454129695892
Eval num_timesteps=91200, episode_reward=22.62 +/- 14.08
Episode length: 33.40 +/- 20.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 22.6     |
| time/              |          |
|    total_timesteps | 91200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.22     |
|    ep_rew_mean     | 3.42     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 228      |
|    time_elapsed    | 16429    |
|    total_timesteps | 91200    |
---------------------------------
Progress update from timestep 91300: Updated shield, loss is 0.09241054207086563
Progress update from timestep 91400: Updated shield, loss is 0.09214247018098831
Eval num_timesteps=91400, episode_reward=30.13 +/- 11.50
Episode length: 42.20 +/- 15.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.2          |
|    mean_reward          | 30.1          |
| time/                   |               |
|    total_timesteps      | 91400         |
| train/                  |               |
|    approx_kl            | 0.00023860244 |
|    clip_fraction        | 0.01          |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0467       |
|    explained_variance   | 0.114         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.665         |
|    n_updates            | 2280          |
|    policy_gradient_loss | -0.00119      |
|    value_loss           | 1.81          |
-------------------------------------------
Progress update from timestep 91500: Updated shield, loss is 0.08847768604755402
Progress update from timestep 91600: Updated shield, loss is 0.09179048985242844
Eval num_timesteps=91600, episode_reward=22.13 +/- 15.13
Episode length: 32.40 +/- 21.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.4     |
|    mean_reward     | 22.1     |
| time/              |          |
|    total_timesteps | 91600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.49     |
|    ep_rew_mean     | 2.87     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 229      |
|    time_elapsed    | 16525    |
|    total_timesteps | 91600    |
---------------------------------
Progress update from timestep 91700: Updated shield, loss is 0.09600226581096649
Progress update from timestep 91800: Updated shield, loss is 0.10072417557239532
Eval num_timesteps=91800, episode_reward=34.69 +/- 1.30
Episode length: 50.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | 34.7         |
| time/                   |              |
|    total_timesteps      | 91800        |
| train/                  |              |
|    approx_kl            | 0.0001324676 |
|    clip_fraction        | 0.00446      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0683      |
|    explained_variance   | 0.0295       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.823        |
|    n_updates            | 2290         |
|    policy_gradient_loss | -0.00106     |
|    value_loss           | 1.86         |
------------------------------------------
New best mean reward!
Progress update from timestep 91900: Updated shield, loss is 0.09217032790184021
Progress update from timestep 92000: Updated shield, loss is 0.09125597774982452
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 92000: Saved shield_loss.csv. 
Eval num_timesteps=92000, episode_reward=10.22 +/- 11.82
Episode length: 15.80 +/- 17.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | 10.2     |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.25     |
|    ep_rew_mean     | 3.42     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 230      |
|    time_elapsed    | 16636    |
|    total_timesteps | 92000    |
---------------------------------
Progress update from timestep 92100: Updated shield, loss is 0.08744660019874573
Progress update from timestep 92200: Updated shield, loss is 0.090997114777565
Eval num_timesteps=92200, episode_reward=22.29 +/- 16.77
Episode length: 31.40 +/- 22.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 31.4         |
|    mean_reward          | 22.3         |
| time/                   |              |
|    total_timesteps      | 92200        |
| train/                  |              |
|    approx_kl            | 9.393013e-05 |
|    clip_fraction        | 0.00268      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0282      |
|    explained_variance   | 0.0868       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.21         |
|    n_updates            | 2300         |
|    policy_gradient_loss | -0.00077     |
|    value_loss           | 2.3          |
------------------------------------------
Progress update from timestep 92300: Updated shield, loss is 0.0845656618475914
Progress update from timestep 92400: Updated shield, loss is 0.08080194890499115
Eval num_timesteps=92400, episode_reward=16.00 +/- 15.13
Episode length: 23.60 +/- 21.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 92400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.09     |
|    ep_rew_mean     | 2.61     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 231      |
|    time_elapsed    | 16727    |
|    total_timesteps | 92400    |
---------------------------------
Progress update from timestep 92500: Updated shield, loss is 0.09654019773006439
Progress update from timestep 92600: Updated shield, loss is 0.09100828319787979
Eval num_timesteps=92600, episode_reward=35.14 +/- 1.51
Episode length: 50.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 50            |
|    mean_reward          | 35.1          |
| time/                   |               |
|    total_timesteps      | 92600         |
| train/                  |               |
|    approx_kl            | 0.00040339053 |
|    clip_fraction        | 0.0165        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0724       |
|    explained_variance   | 0.137         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.998         |
|    n_updates            | 2310          |
|    policy_gradient_loss | -0.00257      |
|    value_loss           | 2.4           |
-------------------------------------------
New best mean reward!
Progress update from timestep 92700: Updated shield, loss is 0.09268765896558762
Progress update from timestep 92800: Updated shield, loss is 0.09947697073221207
Eval num_timesteps=92800, episode_reward=9.35 +/- 12.15
Episode length: 14.60 +/- 17.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 9.35     |
| time/              |          |
|    total_timesteps | 92800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.81     |
|    ep_rew_mean     | 3.18     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 232      |
|    time_elapsed    | 16837    |
|    total_timesteps | 92800    |
---------------------------------
Progress update from timestep 92900: Updated shield, loss is 0.09848161041736603
Progress update from timestep 93000: Updated shield, loss is 0.08188145607709885
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 93000: Saved shield_loss.csv. 
Eval num_timesteps=93000, episode_reward=16.17 +/- 14.57
Episode length: 24.20 +/- 21.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.2         |
|    mean_reward          | 16.2         |
| time/                   |              |
|    total_timesteps      | 93000        |
| train/                  |              |
|    approx_kl            | 9.550021e-05 |
|    clip_fraction        | 0.00201      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0588      |
|    explained_variance   | 0.248        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.725        |
|    n_updates            | 2320         |
|    policy_gradient_loss | -0.00104     |
|    value_loss           | 2.37         |
------------------------------------------
Progress update from timestep 93100: Updated shield, loss is 0.09150231629610062
Progress update from timestep 93200: Updated shield, loss is 0.08779461681842804
Eval num_timesteps=93200, episode_reward=11.61 +/- 12.21
Episode length: 17.00 +/- 16.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | 11.6     |
| time/              |          |
|    total_timesteps | 93200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.85     |
|    ep_rew_mean     | 3.13     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 233      |
|    time_elapsed    | 16928    |
|    total_timesteps | 93200    |
---------------------------------
Progress update from timestep 93300: Updated shield, loss is 0.0910315066576004
Progress update from timestep 93400: Updated shield, loss is 0.10868528485298157
Eval num_timesteps=93400, episode_reward=16.97 +/- 13.45
Episode length: 25.60 +/- 19.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.6          |
|    mean_reward          | 17            |
| time/                   |               |
|    total_timesteps      | 93400         |
| train/                  |               |
|    approx_kl            | 0.00031378545 |
|    clip_fraction        | 0.0123        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0753       |
|    explained_variance   | 0.16          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.312         |
|    n_updates            | 2330          |
|    policy_gradient_loss | -0.00196      |
|    value_loss           | 1.84          |
-------------------------------------------
Progress update from timestep 93500: Updated shield, loss is 0.09657654166221619
Progress update from timestep 93600: Updated shield, loss is 0.08830691128969193
Eval num_timesteps=93600, episode_reward=17.50 +/- 14.46
Episode length: 25.60 +/- 20.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 17.5     |
| time/              |          |
|    total_timesteps | 93600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.55     |
|    ep_rew_mean     | 2.92     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 234      |
|    time_elapsed    | 17023    |
|    total_timesteps | 93600    |
---------------------------------
Progress update from timestep 93700: Updated shield, loss is 0.08692487329244614
Progress update from timestep 93800: Updated shield, loss is 0.0871707871556282
Eval num_timesteps=93800, episode_reward=23.45 +/- 13.96
Episode length: 34.00 +/- 19.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34          |
|    mean_reward          | 23.4        |
| time/                   |             |
|    total_timesteps      | 93800       |
| train/                  |             |
|    approx_kl            | 0.000272168 |
|    clip_fraction        | 0.00871     |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0588     |
|    explained_variance   | 0.106       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.23        |
|    n_updates            | 2340        |
|    policy_gradient_loss | -0.00167    |
|    value_loss           | 2.56        |
-----------------------------------------
Progress update from timestep 93900: Updated shield, loss is 0.0872364267706871
Progress update from timestep 94000: Updated shield, loss is 0.0952162817120552
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 94000: Saved shield_loss.csv. 
Eval num_timesteps=94000, episode_reward=10.08 +/- 13.31
Episode length: 14.60 +/- 17.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 10.1     |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.13     |
|    ep_rew_mean     | 3.39     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 235      |
|    time_elapsed    | 17114    |
|    total_timesteps | 94000    |
---------------------------------
Progress update from timestep 94100: Updated shield, loss is 0.08121588826179504
Progress update from timestep 94200: Updated shield, loss is 0.09415902197360992
Eval num_timesteps=94200, episode_reward=11.93 +/- 12.67
Episode length: 17.60 +/- 16.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17.6          |
|    mean_reward          | 11.9          |
| time/                   |               |
|    total_timesteps      | 94200         |
| train/                  |               |
|    approx_kl            | 4.6451776e-05 |
|    clip_fraction        | 0.00067       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0283       |
|    explained_variance   | 0.0485        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.807         |
|    n_updates            | 2350          |
|    policy_gradient_loss | -0.000381     |
|    value_loss           | 1.95          |
-------------------------------------------
Progress update from timestep 94300: Updated shield, loss is 0.08060823380947113
Progress update from timestep 94400: Updated shield, loss is 0.0809749886393547
Eval num_timesteps=94400, episode_reward=17.73 +/- 15.38
Episode length: 25.40 +/- 21.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | 17.7     |
| time/              |          |
|    total_timesteps | 94400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.2      |
|    ep_rew_mean     | 3.42     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 236      |
|    time_elapsed    | 17211    |
|    total_timesteps | 94400    |
---------------------------------
Progress update from timestep 94500: Updated shield, loss is 0.0831221342086792
Progress update from timestep 94600: Updated shield, loss is 0.08871230483055115
Eval num_timesteps=94600, episode_reward=22.99 +/- 15.45
Episode length: 32.80 +/- 21.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32.8          |
|    mean_reward          | 23            |
| time/                   |               |
|    total_timesteps      | 94600         |
| train/                  |               |
|    approx_kl            | 0.00015932733 |
|    clip_fraction        | 0.00647       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0696       |
|    explained_variance   | 0.265         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.7           |
|    n_updates            | 2360          |
|    policy_gradient_loss | -0.00157      |
|    value_loss           | 2.03          |
-------------------------------------------
Progress update from timestep 94700: Updated shield, loss is 0.08655324578285217
Progress update from timestep 94800: Updated shield, loss is 0.08061777055263519
Eval num_timesteps=94800, episode_reward=25.01 +/- 12.60
Episode length: 36.00 +/- 17.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 25       |
| time/              |          |
|    total_timesteps | 94800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.62     |
|    ep_rew_mean     | 2.96     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 237      |
|    time_elapsed    | 17330    |
|    total_timesteps | 94800    |
---------------------------------
Progress update from timestep 94900: Updated shield, loss is 0.08138570189476013
Progress update from timestep 95000: Updated shield, loss is 0.09647838026285172
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 95000: Saved shield_loss.csv. 
Eval num_timesteps=95000, episode_reward=13.17 +/- 13.64
Episode length: 19.40 +/- 19.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 19.4          |
|    mean_reward          | 13.2          |
| time/                   |               |
|    total_timesteps      | 95000         |
| train/                  |               |
|    approx_kl            | 0.00036329677 |
|    clip_fraction        | 0.0134        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0404       |
|    explained_variance   | 0.137         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.628         |
|    n_updates            | 2370          |
|    policy_gradient_loss | -0.00222      |
|    value_loss           | 1.69          |
-------------------------------------------
Progress update from timestep 95100: Updated shield, loss is 0.08221731334924698
Progress update from timestep 95200: Updated shield, loss is 0.09503557533025742
Eval num_timesteps=95200, episode_reward=10.74 +/- 5.66
Episode length: 16.40 +/- 8.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | 10.7     |
| time/              |          |
|    total_timesteps | 95200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.91     |
|    ep_rew_mean     | 3.21     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 238      |
|    time_elapsed    | 17411    |
|    total_timesteps | 95200    |
---------------------------------
Progress update from timestep 95300: Updated shield, loss is 0.0917205959558487
Progress update from timestep 95400: Updated shield, loss is 0.08186548203229904
Eval num_timesteps=95400, episode_reward=15.04 +/- 16.39
Episode length: 22.00 +/- 22.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22           |
|    mean_reward          | 15           |
| time/                   |              |
|    total_timesteps      | 95400        |
| train/                  |              |
|    approx_kl            | 0.0003179736 |
|    clip_fraction        | 0.01         |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.078       |
|    explained_variance   | 0.0584       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.12         |
|    n_updates            | 2380         |
|    policy_gradient_loss | -0.00178     |
|    value_loss           | 1.94         |
------------------------------------------
Progress update from timestep 95500: Updated shield, loss is 0.08569955080747604
Progress update from timestep 95600: Updated shield, loss is 0.09499090164899826
Eval num_timesteps=95600, episode_reward=11.80 +/- 11.93
Episode length: 17.40 +/- 16.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 11.8     |
| time/              |          |
|    total_timesteps | 95600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.8      |
|    ep_rew_mean     | 3.2      |
| time/              |          |
|    fps             | 5        |
|    iterations      | 239      |
|    time_elapsed    | 17500    |
|    total_timesteps | 95600    |
---------------------------------
Progress update from timestep 95700: Updated shield, loss is 0.09401269257068634
Progress update from timestep 95800: Updated shield, loss is 0.09657685458660126
Eval num_timesteps=95800, episode_reward=18.48 +/- 14.30
Episode length: 26.60 +/- 19.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.6         |
|    mean_reward          | 18.5         |
| time/                   |              |
|    total_timesteps      | 95800        |
| train/                  |              |
|    approx_kl            | 0.0002747907 |
|    clip_fraction        | 0.0096       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0314      |
|    explained_variance   | 0.246        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.41         |
|    n_updates            | 2390         |
|    policy_gradient_loss | -0.0015      |
|    value_loss           | 2.11         |
------------------------------------------
Progress update from timestep 95900: Updated shield, loss is 0.086356982588768
Progress update from timestep 96000: Updated shield, loss is 0.08613929897546768
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 96000: Saved shield_loss.csv. 
Eval num_timesteps=96000, episode_reward=18.66 +/- 14.35
Episode length: 27.20 +/- 20.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.2     |
|    mean_reward     | 18.7     |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.74     |
|    ep_rew_mean     | 3.85     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 240      |
|    time_elapsed    | 17590    |
|    total_timesteps | 96000    |
---------------------------------
Progress update from timestep 96100: Updated shield, loss is 0.09682118147611618
Progress update from timestep 96200: Updated shield, loss is 0.09714025259017944
Eval num_timesteps=96200, episode_reward=12.67 +/- 11.76
Episode length: 19.00 +/- 16.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 19           |
|    mean_reward          | 12.7         |
| time/                   |              |
|    total_timesteps      | 96200        |
| train/                  |              |
|    approx_kl            | 5.963977e-05 |
|    clip_fraction        | 0.00067      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0253      |
|    explained_variance   | 0.16         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.59         |
|    n_updates            | 2400         |
|    policy_gradient_loss | -0.000408    |
|    value_loss           | 2.58         |
------------------------------------------
Progress update from timestep 96300: Updated shield, loss is 0.08025924861431122
Progress update from timestep 96400: Updated shield, loss is 0.08112131059169769
Eval num_timesteps=96400, episode_reward=17.14 +/- 15.11
Episode length: 24.60 +/- 20.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 17.1     |
| time/              |          |
|    total_timesteps | 96400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.52     |
|    ep_rew_mean     | 3.66     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 241      |
|    time_elapsed    | 17682    |
|    total_timesteps | 96400    |
---------------------------------
Progress update from timestep 96500: Updated shield, loss is 0.08326131850481033
Progress update from timestep 96600: Updated shield, loss is 0.08539271354675293
Eval num_timesteps=96600, episode_reward=18.12 +/- 14.67
Episode length: 26.40 +/- 19.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 26.4        |
|    mean_reward          | 18.1        |
| time/                   |             |
|    total_timesteps      | 96600       |
| train/                  |             |
|    approx_kl            | 3.95854e-05 |
|    clip_fraction        | 0.00112     |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0427     |
|    explained_variance   | 0.198       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.562       |
|    n_updates            | 2410        |
|    policy_gradient_loss | -0.0006     |
|    value_loss           | 1.67        |
-----------------------------------------
Progress update from timestep 96700: Updated shield, loss is 0.08392838388681412
Progress update from timestep 96800: Updated shield, loss is 0.08197910338640213
Eval num_timesteps=96800, episode_reward=14.64 +/- 15.30
Episode length: 22.40 +/- 22.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 14.6     |
| time/              |          |
|    total_timesteps | 96800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.17     |
|    ep_rew_mean     | 3.34     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 242      |
|    time_elapsed    | 17794    |
|    total_timesteps | 96800    |
---------------------------------
Progress update from timestep 96900: Updated shield, loss is 0.08433195948600769
Progress update from timestep 97000: Updated shield, loss is 0.07959865033626556
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 97000: Saved shield_loss.csv. 
Eval num_timesteps=97000, episode_reward=16.83 +/- 14.88
Episode length: 24.40 +/- 20.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.4          |
|    mean_reward          | 16.8          |
| time/                   |               |
|    total_timesteps      | 97000         |
| train/                  |               |
|    approx_kl            | 3.9382972e-05 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00629      |
|    explained_variance   | 0.184         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.747         |
|    n_updates            | 2420          |
|    policy_gradient_loss | -0.000214     |
|    value_loss           | 1.77          |
-------------------------------------------
Progress update from timestep 97100: Updated shield, loss is 0.0839889869093895
Progress update from timestep 97200: Updated shield, loss is 0.09094610065221786
Eval num_timesteps=97200, episode_reward=21.49 +/- 14.66
Episode length: 32.60 +/- 21.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.6     |
|    mean_reward     | 21.5     |
| time/              |          |
|    total_timesteps | 97200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.33     |
|    ep_rew_mean     | 3.49     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 243      |
|    time_elapsed    | 17887    |
|    total_timesteps | 97200    |
---------------------------------
Progress update from timestep 97300: Updated shield, loss is 0.08275892585515976
Progress update from timestep 97400: Updated shield, loss is 0.09636444598436356
Eval num_timesteps=97400, episode_reward=20.00 +/- 11.39
Episode length: 29.80 +/- 16.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 29.8      |
|    mean_reward          | 20        |
| time/                   |           |
|    total_timesteps      | 97400     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.0838    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.37      |
|    n_updates            | 2430      |
|    policy_gradient_loss | -3.37e-09 |
|    value_loss           | 2.49      |
---------------------------------------
Progress update from timestep 97500: Updated shield, loss is 0.093185655772686
Progress update from timestep 97600: Updated shield, loss is 0.0870094820857048
Eval num_timesteps=97600, episode_reward=16.98 +/- 16.11
Episode length: 23.80 +/- 21.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 97600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.36     |
|    ep_rew_mean     | 2.83     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 244      |
|    time_elapsed    | 17985    |
|    total_timesteps | 97600    |
---------------------------------
Progress update from timestep 97700: Updated shield, loss is 0.09029886871576309
Progress update from timestep 97800: Updated shield, loss is 0.10130896419286728
Eval num_timesteps=97800, episode_reward=4.01 +/- 4.25
Episode length: 6.40 +/- 5.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 6.4           |
|    mean_reward          | 4.01          |
| time/                   |               |
|    total_timesteps      | 97800         |
| train/                  |               |
|    approx_kl            | 0.00018862695 |
|    clip_fraction        | 0.00558       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0493       |
|    explained_variance   | 0.0991        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.03          |
|    n_updates            | 2440          |
|    policy_gradient_loss | -0.000913     |
|    value_loss           | 1.74          |
-------------------------------------------
Progress update from timestep 97900: Updated shield, loss is 0.08444060385227203
Progress update from timestep 98000: Updated shield, loss is 0.0820751041173935
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 98000: Saved shield_loss.csv. 
Eval num_timesteps=98000, episode_reward=9.93 +/- 12.46
Episode length: 15.00 +/- 17.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | 9.93     |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.26     |
|    ep_rew_mean     | 2.71     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 245      |
|    time_elapsed    | 18067    |
|    total_timesteps | 98000    |
---------------------------------
Progress update from timestep 98100: Updated shield, loss is 0.08165319263935089
Progress update from timestep 98200: Updated shield, loss is 0.08689098060131073
Eval num_timesteps=98200, episode_reward=18.70 +/- 13.27
Episode length: 27.60 +/- 18.79
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 27.6          |
|    mean_reward          | 18.7          |
| time/                   |               |
|    total_timesteps      | 98200         |
| train/                  |               |
|    approx_kl            | 0.00022276292 |
|    clip_fraction        | 0.00379       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0602       |
|    explained_variance   | 0.162         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.783         |
|    n_updates            | 2450          |
|    policy_gradient_loss | -0.000867     |
|    value_loss           | 2.11          |
-------------------------------------------
Progress update from timestep 98300: Updated shield, loss is 0.09090512245893478
Progress update from timestep 98400: Updated shield, loss is 0.10032210499048233
Eval num_timesteps=98400, episode_reward=9.20 +/- 12.24
Episode length: 14.40 +/- 18.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.4     |
|    mean_reward     | 9.2      |
| time/              |          |
|    total_timesteps | 98400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.9      |
|    ep_rew_mean     | 3.18     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 246      |
|    time_elapsed    | 18184    |
|    total_timesteps | 98400    |
---------------------------------
Progress update from timestep 98500: Updated shield, loss is 0.09375040233135223
Progress update from timestep 98600: Updated shield, loss is 0.09124849736690521
Eval num_timesteps=98600, episode_reward=6.69 +/- 3.96
Episode length: 10.20 +/- 5.38
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 10.2          |
|    mean_reward          | 6.69          |
| time/                   |               |
|    total_timesteps      | 98600         |
| train/                  |               |
|    approx_kl            | 0.00012656806 |
|    clip_fraction        | 0.00558       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0648       |
|    explained_variance   | 0.0561        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.853         |
|    n_updates            | 2460          |
|    policy_gradient_loss | -0.00119      |
|    value_loss           | 2.08          |
-------------------------------------------
Progress update from timestep 98700: Updated shield, loss is 0.08604630082845688
Progress update from timestep 98800: Updated shield, loss is 0.08724226802587509
Eval num_timesteps=98800, episode_reward=15.45 +/- 15.55
Episode length: 23.00 +/- 22.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 15.4     |
| time/              |          |
|    total_timesteps | 98800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.23     |
|    ep_rew_mean     | 3.37     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 247      |
|    time_elapsed    | 18261    |
|    total_timesteps | 98800    |
---------------------------------
Progress update from timestep 98900: Updated shield, loss is 0.08353544771671295
Progress update from timestep 99000: Updated shield, loss is 0.08353722840547562
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 99000: Saved shield_loss.csv. 
Eval num_timesteps=99000, episode_reward=9.40 +/- 12.62
Episode length: 14.40 +/- 17.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 14.4       |
|    mean_reward          | 9.4        |
| time/                   |            |
|    total_timesteps      | 99000      |
| train/                  |            |
|    approx_kl            | 0.00038903 |
|    clip_fraction        | 0.0156     |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.0926    |
|    explained_variance   | 0.212      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.977      |
|    n_updates            | 2470       |
|    policy_gradient_loss | -0.0023    |
|    value_loss           | 1.86       |
----------------------------------------
Progress update from timestep 99100: Updated shield, loss is 0.09390845149755478
Progress update from timestep 99200: Updated shield, loss is 0.09397493302822113
Eval num_timesteps=99200, episode_reward=18.13 +/- 14.34
Episode length: 26.20 +/- 19.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | 18.1     |
| time/              |          |
|    total_timesteps | 99200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.36     |
|    ep_rew_mean     | 3.52     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 248      |
|    time_elapsed    | 18351    |
|    total_timesteps | 99200    |
---------------------------------
Progress update from timestep 99300: Updated shield, loss is 0.08901772648096085
Progress update from timestep 99400: Updated shield, loss is 0.09422469884157181
Eval num_timesteps=99400, episode_reward=16.80 +/- 15.90
Episode length: 23.80 +/- 21.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.8          |
|    mean_reward          | 16.8          |
| time/                   |               |
|    total_timesteps      | 99400         |
| train/                  |               |
|    approx_kl            | 0.00011904604 |
|    clip_fraction        | 0.00313       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0592       |
|    explained_variance   | 0.0611        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.19          |
|    n_updates            | 2480          |
|    policy_gradient_loss | -0.000902     |
|    value_loss           | 2.51          |
-------------------------------------------
Progress update from timestep 99500: Updated shield, loss is 0.09123507887125015
Progress update from timestep 99600: Updated shield, loss is 0.08686332404613495
Eval num_timesteps=99600, episode_reward=23.43 +/- 11.33
Episode length: 34.60 +/- 16.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 23.4     |
| time/              |          |
|    total_timesteps | 99600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.28     |
|    ep_rew_mean     | 2.73     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 249      |
|    time_elapsed    | 18450    |
|    total_timesteps | 99600    |
---------------------------------
Progress update from timestep 99700: Updated shield, loss is 0.07694195210933685
Progress update from timestep 99800: Updated shield, loss is 0.07691875845193863
Eval num_timesteps=99800, episode_reward=29.51 +/- 11.05
Episode length: 42.20 +/- 15.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.2          |
|    mean_reward          | 29.5          |
| time/                   |               |
|    total_timesteps      | 99800         |
| train/                  |               |
|    approx_kl            | 0.00016123123 |
|    clip_fraction        | 0.00603       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0462       |
|    explained_variance   | 0.104         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.621         |
|    n_updates            | 2490          |
|    policy_gradient_loss | -0.00117      |
|    value_loss           | 1.43          |
-------------------------------------------
Progress update from timestep 99900: Updated shield, loss is 0.08188603073358536
Progress update from timestep 100000: Updated shield, loss is 0.08470993489027023
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 100000: Saved shield_loss.csv. 
Eval num_timesteps=100000, episode_reward=15.51 +/- 15.99
Episode length: 22.60 +/- 22.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 15.5     |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.48     |
|    ep_rew_mean     | 2.89     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 250      |
|    time_elapsed    | 18561    |
|    total_timesteps | 100000   |
---------------------------------
Progress update from timestep 100100: Updated shield, loss is 0.09123438596725464
Progress update from timestep 100200: Updated shield, loss is 0.08344860374927521
Eval num_timesteps=100200, episode_reward=7.52 +/- 4.86
Episode length: 11.80 +/- 7.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 11.8         |
|    mean_reward          | 7.52         |
| time/                   |              |
|    total_timesteps      | 100200       |
| train/                  |              |
|    approx_kl            | 0.0003071643 |
|    clip_fraction        | 0.019        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0679      |
|    explained_variance   | 0.13         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.852        |
|    n_updates            | 2500         |
|    policy_gradient_loss | -0.00228     |
|    value_loss           | 1.78         |
------------------------------------------
Progress update from timestep 100300: Updated shield, loss is 0.08472836017608643
Progress update from timestep 100400: Updated shield, loss is 0.07857543230056763
Eval num_timesteps=100400, episode_reward=10.11 +/- 12.85
Episode length: 15.00 +/- 17.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | 10.1     |
| time/              |          |
|    total_timesteps | 100400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.47     |
|    ep_rew_mean     | 2.85     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 251      |
|    time_elapsed    | 18639    |
|    total_timesteps | 100400   |
---------------------------------
Progress update from timestep 100500: Updated shield, loss is 0.1004643589258194
Progress update from timestep 100600: Updated shield, loss is 0.0899733453989029
Eval num_timesteps=100600, episode_reward=15.87 +/- 16.56
Episode length: 22.60 +/- 22.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22.6         |
|    mean_reward          | 15.9         |
| time/                   |              |
|    total_timesteps      | 100600       |
| train/                  |              |
|    approx_kl            | 0.0001525678 |
|    clip_fraction        | 0.00558      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0497      |
|    explained_variance   | 0.0778       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.55         |
|    n_updates            | 2510         |
|    policy_gradient_loss | -0.00134     |
|    value_loss           | 2.21         |
------------------------------------------
Progress update from timestep 100700: Updated shield, loss is 0.08755055069923401
Progress update from timestep 100800: Updated shield, loss is 0.08328813314437866
Eval num_timesteps=100800, episode_reward=4.22 +/- 3.24
Episode length: 7.00 +/- 4.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7        |
|    mean_reward     | 4.22     |
| time/              |          |
|    total_timesteps | 100800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.45     |
|    ep_rew_mean     | 2.83     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 252      |
|    time_elapsed    | 18713    |
|    total_timesteps | 100800   |
---------------------------------
Progress update from timestep 100900: Updated shield, loss is 0.08335725218057632
Progress update from timestep 101000: Updated shield, loss is 0.08355224132537842
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 101000: Saved shield_loss.csv. 
Eval num_timesteps=101000, episode_reward=31.89 +/- 6.30
Episode length: 45.80 +/- 8.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 45.8          |
|    mean_reward          | 31.9          |
| time/                   |               |
|    total_timesteps      | 101000        |
| train/                  |               |
|    approx_kl            | 5.2910033e-05 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0217       |
|    explained_variance   | 0.125         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.969         |
|    n_updates            | 2520          |
|    policy_gradient_loss | -0.000422     |
|    value_loss           | 1.77          |
-------------------------------------------
Progress update from timestep 101100: Updated shield, loss is 0.07766784727573395
Progress update from timestep 101200: Updated shield, loss is 0.08069972693920135
Eval num_timesteps=101200, episode_reward=13.28 +/- 11.51
Episode length: 20.00 +/- 16.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20       |
|    mean_reward     | 13.3     |
| time/              |          |
|    total_timesteps | 101200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.65     |
|    ep_rew_mean     | 2.96     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 253      |
|    time_elapsed    | 18816    |
|    total_timesteps | 101200   |
---------------------------------
Progress update from timestep 101300: Updated shield, loss is 0.090507373213768
Progress update from timestep 101400: Updated shield, loss is 0.07809755951166153
Eval num_timesteps=101400, episode_reward=11.38 +/- 12.20
Episode length: 16.80 +/- 16.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.8          |
|    mean_reward          | 11.4          |
| time/                   |               |
|    total_timesteps      | 101400        |
| train/                  |               |
|    approx_kl            | 0.00010580396 |
|    clip_fraction        | 0.00268       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0511       |
|    explained_variance   | 0.148         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.833         |
|    n_updates            | 2530          |
|    policy_gradient_loss | -0.000997     |
|    value_loss           | 1.72          |
-------------------------------------------
Progress update from timestep 101500: Updated shield, loss is 0.08327849954366684
Progress update from timestep 101600: Updated shield, loss is 0.09394124150276184
Eval num_timesteps=101600, episode_reward=21.89 +/- 12.25
Episode length: 31.60 +/- 16.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.6     |
|    mean_reward     | 21.9     |
| time/              |          |
|    total_timesteps | 101600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.75     |
|    ep_rew_mean     | 3.01     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 254      |
|    time_elapsed    | 18915    |
|    total_timesteps | 101600   |
---------------------------------
Progress update from timestep 101700: Updated shield, loss is 0.08690280467271805
Progress update from timestep 101800: Updated shield, loss is 0.08638142049312592
Eval num_timesteps=101800, episode_reward=12.37 +/- 12.34
Episode length: 18.00 +/- 16.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 18            |
|    mean_reward          | 12.4          |
| time/                   |               |
|    total_timesteps      | 101800        |
| train/                  |               |
|    approx_kl            | 0.00022564841 |
|    clip_fraction        | 0.0103        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0481       |
|    explained_variance   | 0.124         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.992         |
|    n_updates            | 2540          |
|    policy_gradient_loss | -0.00143      |
|    value_loss           | 1.75          |
-------------------------------------------
Progress update from timestep 101900: Updated shield, loss is 0.08439478278160095
Progress update from timestep 102000: Updated shield, loss is 0.08231249451637268
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 102000: Saved shield_loss.csv. 
Eval num_timesteps=102000, episode_reward=16.01 +/- 11.89
Episode length: 23.80 +/- 16.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.42     |
|    ep_rew_mean     | 2.85     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 255      |
|    time_elapsed    | 19012    |
|    total_timesteps | 102000   |
---------------------------------
Progress update from timestep 102100: Updated shield, loss is 0.08393914997577667
Progress update from timestep 102200: Updated shield, loss is 0.09171848744153976
Eval num_timesteps=102200, episode_reward=15.40 +/- 16.50
Episode length: 22.20 +/- 22.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.2          |
|    mean_reward          | 15.4          |
| time/                   |               |
|    total_timesteps      | 102200        |
| train/                  |               |
|    approx_kl            | 0.00042418778 |
|    clip_fraction        | 0.0176        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0797       |
|    explained_variance   | 0.0498        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.964         |
|    n_updates            | 2550          |
|    policy_gradient_loss | -0.00224      |
|    value_loss           | 1.77          |
-------------------------------------------
Progress update from timestep 102300: Updated shield, loss is 0.07990221679210663
Progress update from timestep 102400: Updated shield, loss is 0.0807940885424614
Eval num_timesteps=102400, episode_reward=11.13 +/- 13.76
Episode length: 16.20 +/- 18.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | 11.1     |
| time/              |          |
|    total_timesteps | 102400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.64     |
|    ep_rew_mean     | 3.04     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 256      |
|    time_elapsed    | 19104    |
|    total_timesteps | 102400   |
---------------------------------
Progress update from timestep 102500: Updated shield, loss is 0.09977074712514877
Progress update from timestep 102600: Updated shield, loss is 0.09600852429866791
Eval num_timesteps=102600, episode_reward=16.01 +/- 16.47
Episode length: 22.80 +/- 22.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.8          |
|    mean_reward          | 16            |
| time/                   |               |
|    total_timesteps      | 102600        |
| train/                  |               |
|    approx_kl            | 0.00035228694 |
|    clip_fraction        | 0.01          |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0686       |
|    explained_variance   | 0.113         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.897         |
|    n_updates            | 2560          |
|    policy_gradient_loss | -0.00167      |
|    value_loss           | 1.73          |
-------------------------------------------
Progress update from timestep 102700: Updated shield, loss is 0.08520065993070602
Progress update from timestep 102800: Updated shield, loss is 0.07923965901136398
Eval num_timesteps=102800, episode_reward=12.50 +/- 10.51
Episode length: 19.20 +/- 15.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.2     |
|    mean_reward     | 12.5     |
| time/              |          |
|    total_timesteps | 102800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.47     |
|    ep_rew_mean     | 2.85     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 257      |
|    time_elapsed    | 19215    |
|    total_timesteps | 102800   |
---------------------------------
Progress update from timestep 102900: Updated shield, loss is 0.11265494674444199
Progress update from timestep 103000: Updated shield, loss is 0.08539354056119919
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 103000: Saved shield_loss.csv. 
Eval num_timesteps=103000, episode_reward=22.67 +/- 15.37
Episode length: 32.60 +/- 21.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32.6          |
|    mean_reward          | 22.7          |
| time/                   |               |
|    total_timesteps      | 103000        |
| train/                  |               |
|    approx_kl            | 0.00034601786 |
|    clip_fraction        | 0.0114        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0901       |
|    explained_variance   | 0.0963        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.13          |
|    n_updates            | 2570          |
|    policy_gradient_loss | -0.00187      |
|    value_loss           | 2.52          |
-------------------------------------------
Progress update from timestep 103100: Updated shield, loss is 0.1004597470164299
Progress update from timestep 103200: Updated shield, loss is 0.09269174188375473
Eval num_timesteps=103200, episode_reward=21.27 +/- 16.19
Episode length: 31.20 +/- 23.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.2     |
|    mean_reward     | 21.3     |
| time/              |          |
|    total_timesteps | 103200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.83     |
|    ep_rew_mean     | 3.12     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 258      |
|    time_elapsed    | 19308    |
|    total_timesteps | 103200   |
---------------------------------
Progress update from timestep 103300: Updated shield, loss is 0.09885858744382858
Progress update from timestep 103400: Updated shield, loss is 0.1010589599609375
Eval num_timesteps=103400, episode_reward=2.06 +/- 1.19
Episode length: 3.80 +/- 1.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 3.8          |
|    mean_reward          | 2.06         |
| time/                   |              |
|    total_timesteps      | 103400       |
| train/                  |              |
|    approx_kl            | 0.0001375638 |
|    clip_fraction        | 0.00469      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0351      |
|    explained_variance   | 0.201        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.254        |
|    n_updates            | 2580         |
|    policy_gradient_loss | -0.000912    |
|    value_loss           | 1.74         |
------------------------------------------
Progress update from timestep 103500: Updated shield, loss is 0.07461307942867279
Progress update from timestep 103600: Updated shield, loss is 0.08181114494800568
Eval num_timesteps=103600, episode_reward=11.01 +/- 13.17
Episode length: 16.00 +/- 17.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | 11       |
| time/              |          |
|    total_timesteps | 103600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.5      |
|    ep_rew_mean     | 2.86     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 259      |
|    time_elapsed    | 19386    |
|    total_timesteps | 103600   |
---------------------------------
Progress update from timestep 103700: Updated shield, loss is 0.09003717452287674
Progress update from timestep 103800: Updated shield, loss is 0.08512591570615768
Eval num_timesteps=103800, episode_reward=16.83 +/- 13.80
Episode length: 25.40 +/- 20.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 25.4        |
|    mean_reward          | 16.8        |
| time/                   |             |
|    total_timesteps      | 103800      |
| train/                  |             |
|    approx_kl            | 0.000230047 |
|    clip_fraction        | 0.00201     |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0615     |
|    explained_variance   | 0.107       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.21        |
|    n_updates            | 2590        |
|    policy_gradient_loss | -0.000729   |
|    value_loss           | 2.22        |
-----------------------------------------
Progress update from timestep 103900: Updated shield, loss is 0.08256639540195465
Progress update from timestep 104000: Updated shield, loss is 0.09223761409521103
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 104000: Saved shield_loss.csv. 
Eval num_timesteps=104000, episode_reward=16.57 +/- 15.19
Episode length: 24.40 +/- 21.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.7      |
|    ep_rew_mean     | 3.07     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 260      |
|    time_elapsed    | 19481    |
|    total_timesteps | 104000   |
---------------------------------
Progress update from timestep 104100: Updated shield, loss is 0.09029694646596909
Progress update from timestep 104200: Updated shield, loss is 0.07510436326265335
Eval num_timesteps=104200, episode_reward=17.92 +/- 14.16
Episode length: 26.20 +/- 19.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.2          |
|    mean_reward          | 17.9          |
| time/                   |               |
|    total_timesteps      | 104200        |
| train/                  |               |
|    approx_kl            | 0.00046703804 |
|    clip_fraction        | 0.0103        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0558       |
|    explained_variance   | -0.0357       |
|    learning_rate        | 0.0001        |
|    loss                 | 1.23          |
|    n_updates            | 2600          |
|    policy_gradient_loss | -0.00223      |
|    value_loss           | 2.44          |
-------------------------------------------
Progress update from timestep 104300: Updated shield, loss is 0.09384461492300034
Progress update from timestep 104400: Updated shield, loss is 0.09183623641729355
Eval num_timesteps=104400, episode_reward=2.78 +/- 1.09
Episode length: 5.00 +/- 1.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 2.78     |
| time/              |          |
|    total_timesteps | 104400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.26     |
|    ep_rew_mean     | 3.51     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 261      |
|    time_elapsed    | 19553    |
|    total_timesteps | 104400   |
---------------------------------
Progress update from timestep 104500: Updated shield, loss is 0.0812603160738945
Progress update from timestep 104600: Updated shield, loss is 0.09614045172929764
Eval num_timesteps=104600, episode_reward=22.76 +/- 16.25
Episode length: 32.00 +/- 22.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32            |
|    mean_reward          | 22.8          |
| time/                   |               |
|    total_timesteps      | 104600        |
| train/                  |               |
|    approx_kl            | 0.00039509978 |
|    clip_fraction        | 0.0112        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.068        |
|    explained_variance   | 0.142         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.07          |
|    n_updates            | 2610          |
|    policy_gradient_loss | -0.00213      |
|    value_loss           | 1.96          |
-------------------------------------------
Progress update from timestep 104700: Updated shield, loss is 0.08923652023077011
Progress update from timestep 104800: Updated shield, loss is 0.08728761225938797
Eval num_timesteps=104800, episode_reward=15.16 +/- 14.87
Episode length: 23.20 +/- 21.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 15.2     |
| time/              |          |
|    total_timesteps | 104800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.6      |
|    ep_rew_mean     | 2.92     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 262      |
|    time_elapsed    | 19645    |
|    total_timesteps | 104800   |
---------------------------------
Progress update from timestep 104900: Updated shield, loss is 0.08676162362098694
Progress update from timestep 105000: Updated shield, loss is 0.09913437068462372
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 105000: Saved shield_loss.csv. 
Eval num_timesteps=105000, episode_reward=11.56 +/- 12.38
Episode length: 17.40 +/- 17.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17.4          |
|    mean_reward          | 11.6          |
| time/                   |               |
|    total_timesteps      | 105000        |
| train/                  |               |
|    approx_kl            | 0.00020833153 |
|    clip_fraction        | 0.00804       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0453       |
|    explained_variance   | 0.148         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.937         |
|    n_updates            | 2620          |
|    policy_gradient_loss | -0.00125      |
|    value_loss           | 1.46          |
-------------------------------------------
Progress update from timestep 105100: Updated shield, loss is 0.09528892487287521
Progress update from timestep 105200: Updated shield, loss is 0.09931954741477966
Eval num_timesteps=105200, episode_reward=16.79 +/- 15.89
Episode length: 24.00 +/- 21.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 105200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.81     |
|    ep_rew_mean     | 3.11     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 263      |
|    time_elapsed    | 19736    |
|    total_timesteps | 105200   |
---------------------------------
Progress update from timestep 105300: Updated shield, loss is 0.08739565312862396
Progress update from timestep 105400: Updated shield, loss is 0.10517869889736176
Eval num_timesteps=105400, episode_reward=23.61 +/- 16.03
Episode length: 32.60 +/- 21.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32.6          |
|    mean_reward          | 23.6          |
| time/                   |               |
|    total_timesteps      | 105400        |
| train/                  |               |
|    approx_kl            | 0.00017791161 |
|    clip_fraction        | 0.00625       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0424       |
|    explained_variance   | 0.137         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.13          |
|    n_updates            | 2630          |
|    policy_gradient_loss | -0.00172      |
|    value_loss           | 1.73          |
-------------------------------------------
Progress update from timestep 105500: Updated shield, loss is 0.08710388839244843
Progress update from timestep 105600: Updated shield, loss is 0.08661827445030212
Eval num_timesteps=105600, episode_reward=16.97 +/- 15.57
Episode length: 24.60 +/- 21.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 105600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.66     |
|    ep_rew_mean     | 3.04     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 264      |
|    time_elapsed    | 19850    |
|    total_timesteps | 105600   |
---------------------------------
Progress update from timestep 105700: Updated shield, loss is 0.09143471717834473
Progress update from timestep 105800: Updated shield, loss is 0.08961246907711029
Eval num_timesteps=105800, episode_reward=23.36 +/- 15.49
Episode length: 32.80 +/- 21.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.8         |
|    mean_reward          | 23.4         |
| time/                   |              |
|    total_timesteps      | 105800       |
| train/                  |              |
|    approx_kl            | 5.878242e-05 |
|    clip_fraction        | 0.000223     |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.04        |
|    explained_variance   | 0.088        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.804        |
|    n_updates            | 2640         |
|    policy_gradient_loss | -0.000453    |
|    value_loss           | 2.26         |
------------------------------------------
Progress update from timestep 105900: Updated shield, loss is 0.08937167376279831
Progress update from timestep 106000: Updated shield, loss is 0.08928993344306946
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 106000: Saved shield_loss.csv. 
Eval num_timesteps=106000, episode_reward=13.44 +/- 11.85
Episode length: 19.20 +/- 15.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.2     |
|    mean_reward     | 13.4     |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.4      |
|    ep_rew_mean     | 2.8      |
| time/              |          |
|    fps             | 5        |
|    iterations      | 265      |
|    time_elapsed    | 19946    |
|    total_timesteps | 106000   |
---------------------------------
Progress update from timestep 106100: Updated shield, loss is 0.07761644572019577
Progress update from timestep 106200: Updated shield, loss is 0.0935695692896843
Eval num_timesteps=106200, episode_reward=16.77 +/- 14.98
Episode length: 24.60 +/- 21.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.6          |
|    mean_reward          | 16.8          |
| time/                   |               |
|    total_timesteps      | 106200        |
| train/                  |               |
|    approx_kl            | 0.00037849508 |
|    clip_fraction        | 0.0196        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0678       |
|    explained_variance   | 0.118         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.714         |
|    n_updates            | 2650          |
|    policy_gradient_loss | -0.00263      |
|    value_loss           | 1.73          |
-------------------------------------------
Progress update from timestep 106300: Updated shield, loss is 0.07556790113449097
Progress update from timestep 106400: Updated shield, loss is 0.08697109669446945
Eval num_timesteps=106400, episode_reward=10.85 +/- 12.76
Episode length: 15.60 +/- 17.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 10.9     |
| time/              |          |
|    total_timesteps | 106400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.53     |
|    ep_rew_mean     | 2.96     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 266      |
|    time_elapsed    | 20037    |
|    total_timesteps | 106400   |
---------------------------------
Progress update from timestep 106500: Updated shield, loss is 0.08828067034482956
Progress update from timestep 106600: Updated shield, loss is 0.0846211165189743
Eval num_timesteps=106600, episode_reward=16.86 +/- 15.91
Episode length: 23.80 +/- 21.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.8         |
|    mean_reward          | 16.9         |
| time/                   |              |
|    total_timesteps      | 106600       |
| train/                  |              |
|    approx_kl            | 0.0004495703 |
|    clip_fraction        | 0.015        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.127       |
|    explained_variance   | 0.0404       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.38         |
|    n_updates            | 2660         |
|    policy_gradient_loss | -0.00218     |
|    value_loss           | 1.58         |
------------------------------------------
Progress update from timestep 106700: Updated shield, loss is 0.07934176921844482
Progress update from timestep 106800: Updated shield, loss is 0.08656993508338928
Eval num_timesteps=106800, episode_reward=16.64 +/- 14.83
Episode length: 24.40 +/- 21.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 106800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.66     |
|    ep_rew_mean     | 3.03     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 267      |
|    time_elapsed    | 20129    |
|    total_timesteps | 106800   |
---------------------------------
Progress update from timestep 106900: Updated shield, loss is 0.08089573681354523
Progress update from timestep 107000: Updated shield, loss is 0.08208520710468292
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 107000: Saved shield_loss.csv. 
Eval num_timesteps=107000, episode_reward=12.22 +/- 12.77
Episode length: 17.60 +/- 16.85
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17.6          |
|    mean_reward          | 12.2          |
| time/                   |               |
|    total_timesteps      | 107000        |
| train/                  |               |
|    approx_kl            | 0.00014329182 |
|    clip_fraction        | 0.0058        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0422       |
|    explained_variance   | 0.187         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.361         |
|    n_updates            | 2670          |
|    policy_gradient_loss | -0.00108      |
|    value_loss           | 1.54          |
-------------------------------------------
Progress update from timestep 107100: Updated shield, loss is 0.08061034977436066
Progress update from timestep 107200: Updated shield, loss is 0.0812065601348877
Eval num_timesteps=107200, episode_reward=22.34 +/- 14.45
Episode length: 33.00 +/- 20.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 22.3     |
| time/              |          |
|    total_timesteps | 107200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.38     |
|    ep_rew_mean     | 3.54     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 268      |
|    time_elapsed    | 20221    |
|    total_timesteps | 107200   |
---------------------------------
Progress update from timestep 107300: Updated shield, loss is 0.07892030477523804
Progress update from timestep 107400: Updated shield, loss is 0.0886392816901207
Eval num_timesteps=107400, episode_reward=9.66 +/- 13.13
Episode length: 13.80 +/- 18.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 13.8          |
|    mean_reward          | 9.66          |
| time/                   |               |
|    total_timesteps      | 107400        |
| train/                  |               |
|    approx_kl            | 0.00011279873 |
|    clip_fraction        | 0.00513       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0371       |
|    explained_variance   | 0.108         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.55          |
|    n_updates            | 2680          |
|    policy_gradient_loss | -0.0011       |
|    value_loss           | 2.77          |
-------------------------------------------
Progress update from timestep 107500: Updated shield, loss is 0.08234800398349762
Progress update from timestep 107600: Updated shield, loss is 0.08036268502473831
Eval num_timesteps=107600, episode_reward=10.53 +/- 11.99
Episode length: 16.00 +/- 17.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | 10.5     |
| time/              |          |
|    total_timesteps | 107600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.49     |
|    ep_rew_mean     | 2.88     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 269      |
|    time_elapsed    | 20313    |
|    total_timesteps | 107600   |
---------------------------------
Progress update from timestep 107700: Updated shield, loss is 0.08172959089279175
Progress update from timestep 107800: Updated shield, loss is 0.0816856101155281
Eval num_timesteps=107800, episode_reward=23.09 +/- 14.58
Episode length: 33.40 +/- 20.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.4          |
|    mean_reward          | 23.1          |
| time/                   |               |
|    total_timesteps      | 107800        |
| train/                  |               |
|    approx_kl            | 0.00053031667 |
|    clip_fraction        | 0.0127        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.108        |
|    explained_variance   | 0.188         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.71          |
|    n_updates            | 2690          |
|    policy_gradient_loss | -0.0024       |
|    value_loss           | 1.68          |
-------------------------------------------
Progress update from timestep 107900: Updated shield, loss is 0.07891722023487091
Progress update from timestep 108000: Updated shield, loss is 0.0895007848739624
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 108000: Saved shield_loss.csv. 
Eval num_timesteps=108000, episode_reward=15.39 +/- 15.83
Episode length: 22.60 +/- 22.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 15.4     |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.7      |
|    ep_rew_mean     | 3.07     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 270      |
|    time_elapsed    | 20425    |
|    total_timesteps | 108000   |
---------------------------------
Progress update from timestep 108100: Updated shield, loss is 0.08193149417638779
Progress update from timestep 108200: Updated shield, loss is 0.07402466237545013
Eval num_timesteps=108200, episode_reward=14.96 +/- 15.97
Episode length: 22.20 +/- 22.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.2          |
|    mean_reward          | 15            |
| time/                   |               |
|    total_timesteps      | 108200        |
| train/                  |               |
|    approx_kl            | 0.00041090552 |
|    clip_fraction        | 0.0205        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.116        |
|    explained_variance   | 0.182         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.783         |
|    n_updates            | 2700          |
|    policy_gradient_loss | -0.00296      |
|    value_loss           | 1.81          |
-------------------------------------------
Progress update from timestep 108300: Updated shield, loss is 0.0815194845199585
Progress update from timestep 108400: Updated shield, loss is 0.07634219527244568
Eval num_timesteps=108400, episode_reward=19.33 +/- 14.46
Episode length: 27.80 +/- 19.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.8     |
|    mean_reward     | 19.3     |
| time/              |          |
|    total_timesteps | 108400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.77     |
|    ep_rew_mean     | 3.11     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 271      |
|    time_elapsed    | 20519    |
|    total_timesteps | 108400   |
---------------------------------
Progress update from timestep 108500: Updated shield, loss is 0.08026506006717682
Progress update from timestep 108600: Updated shield, loss is 0.0702710971236229
Eval num_timesteps=108600, episode_reward=15.47 +/- 16.66
Episode length: 22.20 +/- 22.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22.2         |
|    mean_reward          | 15.5         |
| time/                   |              |
|    total_timesteps      | 108600       |
| train/                  |              |
|    approx_kl            | 0.0007200607 |
|    clip_fraction        | 0.0366       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.12        |
|    explained_variance   | 0.194        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.53         |
|    n_updates            | 2710         |
|    policy_gradient_loss | -0.0029      |
|    value_loss           | 1.53         |
------------------------------------------
Progress update from timestep 108700: Updated shield, loss is 0.08588819950819016
Progress update from timestep 108800: Updated shield, loss is 0.07442363351583481
Eval num_timesteps=108800, episode_reward=14.19 +/- 10.48
Episode length: 21.40 +/- 15.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 14.2     |
| time/              |          |
|    total_timesteps | 108800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.26     |
|    ep_rew_mean     | 3.43     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 272      |
|    time_elapsed    | 20630    |
|    total_timesteps | 108800   |
---------------------------------
Progress update from timestep 108900: Updated shield, loss is 0.06926342099905014
Progress update from timestep 109000: Updated shield, loss is 0.08046531677246094
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 109000: Saved shield_loss.csv. 
Eval num_timesteps=109000, episode_reward=11.43 +/- 12.76
Episode length: 16.60 +/- 16.93
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.6          |
|    mean_reward          | 11.4          |
| time/                   |               |
|    total_timesteps      | 109000        |
| train/                  |               |
|    approx_kl            | 0.00026252586 |
|    clip_fraction        | 0.0127        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0736       |
|    explained_variance   | -0.0152       |
|    learning_rate        | 0.0001        |
|    loss                 | 1.41          |
|    n_updates            | 2720          |
|    policy_gradient_loss | -0.00172      |
|    value_loss           | 3.13          |
-------------------------------------------
Progress update from timestep 109100: Updated shield, loss is 0.0752297043800354
Progress update from timestep 109200: Updated shield, loss is 0.08230685442686081
Eval num_timesteps=109200, episode_reward=16.37 +/- 14.89
Episode length: 24.00 +/- 21.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16.4     |
| time/              |          |
|    total_timesteps | 109200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.95     |
|    ep_rew_mean     | 3.22     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 273      |
|    time_elapsed    | 20723    |
|    total_timesteps | 109200   |
---------------------------------
Progress update from timestep 109300: Updated shield, loss is 0.08410131186246872
Progress update from timestep 109400: Updated shield, loss is 0.0740971490740776
Eval num_timesteps=109400, episode_reward=24.92 +/- 14.30
Episode length: 34.80 +/- 19.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.8         |
|    mean_reward          | 24.9         |
| time/                   |              |
|    total_timesteps      | 109400       |
| train/                  |              |
|    approx_kl            | 0.0003284293 |
|    clip_fraction        | 0.0067       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0887      |
|    explained_variance   | 0.248        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.71         |
|    n_updates            | 2730         |
|    policy_gradient_loss | -0.0014      |
|    value_loss           | 1.62         |
------------------------------------------
Progress update from timestep 109500: Updated shield, loss is 0.07256169617176056
Progress update from timestep 109600: Updated shield, loss is 0.07344365119934082
Eval num_timesteps=109600, episode_reward=9.84 +/- 13.49
Episode length: 14.40 +/- 17.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.4     |
|    mean_reward     | 9.84     |
| time/              |          |
|    total_timesteps | 109600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.55     |
|    ep_rew_mean     | 2.9      |
| time/              |          |
|    fps             | 5        |
|    iterations      | 274      |
|    time_elapsed    | 20817    |
|    total_timesteps | 109600   |
---------------------------------
Progress update from timestep 109700: Updated shield, loss is 0.07554449141025543
Progress update from timestep 109800: Updated shield, loss is 0.0762590691447258
Eval num_timesteps=109800, episode_reward=15.58 +/- 16.46
Episode length: 22.40 +/- 22.57
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.4          |
|    mean_reward          | 15.6          |
| time/                   |               |
|    total_timesteps      | 109800        |
| train/                  |               |
|    approx_kl            | 0.00013732843 |
|    clip_fraction        | 0.0058        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0472       |
|    explained_variance   | 0.116         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.626         |
|    n_updates            | 2740          |
|    policy_gradient_loss | -0.000937     |
|    value_loss           | 1.77          |
-------------------------------------------
Progress update from timestep 109900: Updated shield, loss is 0.07792361825704575
Progress update from timestep 110000: Updated shield, loss is 0.0897296741604805
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 110000: Saved shield_loss.csv. 
Eval num_timesteps=110000, episode_reward=24.37 +/- 13.57
Episode length: 35.00 +/- 18.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 24.4     |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.81     |
|    ep_rew_mean     | 3.14     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 275      |
|    time_elapsed    | 20910    |
|    total_timesteps | 110000   |
---------------------------------
Progress update from timestep 110100: Updated shield, loss is 0.07047661393880844
Progress update from timestep 110200: Updated shield, loss is 0.08048343658447266
Eval num_timesteps=110200, episode_reward=22.03 +/- 15.34
Episode length: 32.20 +/- 21.91
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32.2          |
|    mean_reward          | 22            |
| time/                   |               |
|    total_timesteps      | 110200        |
| train/                  |               |
|    approx_kl            | 4.1483636e-05 |
|    clip_fraction        | 0.00201       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0153       |
|    explained_variance   | 0.149         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.768         |
|    n_updates            | 2750          |
|    policy_gradient_loss | -0.000756     |
|    value_loss           | 2.48          |
-------------------------------------------
Progress update from timestep 110300: Updated shield, loss is 0.07087229937314987
Progress update from timestep 110400: Updated shield, loss is 0.07046359032392502
Eval num_timesteps=110400, episode_reward=16.01 +/- 15.71
Episode length: 23.40 +/- 22.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 110400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5        |
|    ep_rew_mean     | 3.3      |
| time/              |          |
|    fps             | 5        |
|    iterations      | 276      |
|    time_elapsed    | 21004    |
|    total_timesteps | 110400   |
---------------------------------
Progress update from timestep 110500: Updated shield, loss is 0.08148544281721115
Progress update from timestep 110600: Updated shield, loss is 0.07855678349733353
Eval num_timesteps=110600, episode_reward=30.47 +/- 10.27
Episode length: 43.00 +/- 14.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 43            |
|    mean_reward          | 30.5          |
| time/                   |               |
|    total_timesteps      | 110600        |
| train/                  |               |
|    approx_kl            | 0.00010609148 |
|    clip_fraction        | 0.00536       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0411       |
|    explained_variance   | 0.225         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.844         |
|    n_updates            | 2760          |
|    policy_gradient_loss | -0.000766     |
|    value_loss           | 1.86          |
-------------------------------------------
Progress update from timestep 110700: Updated shield, loss is 0.08181530237197876
Progress update from timestep 110800: Updated shield, loss is 0.0801285058259964
Eval num_timesteps=110800, episode_reward=13.00 +/- 12.52
Episode length: 18.60 +/- 16.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 13       |
| time/              |          |
|    total_timesteps | 110800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.24     |
|    ep_rew_mean     | 2.7      |
| time/              |          |
|    fps             | 5        |
|    iterations      | 277      |
|    time_elapsed    | 21121    |
|    total_timesteps | 110800   |
---------------------------------
Progress update from timestep 110900: Updated shield, loss is 0.07353174686431885
Progress update from timestep 111000: Updated shield, loss is 0.07462099194526672
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 111000: Saved shield_loss.csv. 
Eval num_timesteps=111000, episode_reward=15.80 +/- 15.75
Episode length: 23.00 +/- 22.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23            |
|    mean_reward          | 15.8          |
| time/                   |               |
|    total_timesteps      | 111000        |
| train/                  |               |
|    approx_kl            | 0.00027383838 |
|    clip_fraction        | 0.00536       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0874       |
|    explained_variance   | 0.145         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.856         |
|    n_updates            | 2770          |
|    policy_gradient_loss | -0.00172      |
|    value_loss           | 1.74          |
-------------------------------------------
Progress update from timestep 111100: Updated shield, loss is 0.0855640321969986
Progress update from timestep 111200: Updated shield, loss is 0.08633984625339508
Eval num_timesteps=111200, episode_reward=8.91 +/- 12.84
Episode length: 13.60 +/- 18.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.6     |
|    mean_reward     | 8.91     |
| time/              |          |
|    total_timesteps | 111200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.67     |
|    ep_rew_mean     | 3.01     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 278      |
|    time_elapsed    | 21213    |
|    total_timesteps | 111200   |
---------------------------------
Progress update from timestep 111300: Updated shield, loss is 0.08472298085689545
Progress update from timestep 111400: Updated shield, loss is 0.08125685900449753
Eval num_timesteps=111400, episode_reward=12.89 +/- 11.17
Episode length: 19.40 +/- 15.86
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 19.4          |
|    mean_reward          | 12.9          |
| time/                   |               |
|    total_timesteps      | 111400        |
| train/                  |               |
|    approx_kl            | 0.00019218812 |
|    clip_fraction        | 0.0058        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0564       |
|    explained_variance   | 0.236         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.48          |
|    n_updates            | 2780          |
|    policy_gradient_loss | -0.00174      |
|    value_loss           | 1.37          |
-------------------------------------------
Progress update from timestep 111500: Updated shield, loss is 0.07150626182556152
Progress update from timestep 111600: Updated shield, loss is 0.08699974417686462
Eval num_timesteps=111600, episode_reward=22.52 +/- 14.67
Episode length: 33.00 +/- 20.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 22.5     |
| time/              |          |
|    total_timesteps | 111600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.88     |
|    ep_rew_mean     | 3.1      |
| time/              |          |
|    fps             | 5        |
|    iterations      | 279      |
|    time_elapsed    | 21306    |
|    total_timesteps | 111600   |
---------------------------------
Progress update from timestep 111700: Updated shield, loss is 0.07895346730947495
Progress update from timestep 111800: Updated shield, loss is 0.08299483358860016
Eval num_timesteps=111800, episode_reward=17.92 +/- 14.60
Episode length: 26.00 +/- 20.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26            |
|    mean_reward          | 17.9          |
| time/                   |               |
|    total_timesteps      | 111800        |
| train/                  |               |
|    approx_kl            | 0.00032349263 |
|    clip_fraction        | 0.0203        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0734       |
|    explained_variance   | 0.15          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.45          |
|    n_updates            | 2790          |
|    policy_gradient_loss | -0.00147      |
|    value_loss           | 1.98          |
-------------------------------------------
Progress update from timestep 111900: Updated shield, loss is 0.08832728117704391
Progress update from timestep 112000: Updated shield, loss is 0.07523129880428314
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 112000: Saved shield_loss.csv. 
Eval num_timesteps=112000, episode_reward=15.75 +/- 16.67
Episode length: 22.40 +/- 22.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 15.8     |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.82     |
|    ep_rew_mean     | 3.21     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 280      |
|    time_elapsed    | 21404    |
|    total_timesteps | 112000   |
---------------------------------
Progress update from timestep 112100: Updated shield, loss is 0.08510135859251022
Progress update from timestep 112200: Updated shield, loss is 0.07349969446659088
Eval num_timesteps=112200, episode_reward=12.46 +/- 12.61
Episode length: 18.00 +/- 16.71
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 18            |
|    mean_reward          | 12.5          |
| time/                   |               |
|    total_timesteps      | 112200        |
| train/                  |               |
|    approx_kl            | 0.00016829518 |
|    clip_fraction        | 0.00915       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0599       |
|    explained_variance   | 0.0684        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.4           |
|    n_updates            | 2800          |
|    policy_gradient_loss | -0.00179      |
|    value_loss           | 2.2           |
-------------------------------------------
Progress update from timestep 112300: Updated shield, loss is 0.08011055737733841
Progress update from timestep 112400: Updated shield, loss is 0.08502336591482162
Eval num_timesteps=112400, episode_reward=11.05 +/- 12.84
Episode length: 16.00 +/- 17.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | 11       |
| time/              |          |
|    total_timesteps | 112400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.88     |
|    ep_rew_mean     | 3.24     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 281      |
|    time_elapsed    | 21504    |
|    total_timesteps | 112400   |
---------------------------------
Progress update from timestep 112500: Updated shield, loss is 0.0733899176120758
Progress update from timestep 112600: Updated shield, loss is 0.07069256901741028
Eval num_timesteps=112600, episode_reward=9.60 +/- 3.35
Episode length: 14.60 +/- 4.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.6          |
|    mean_reward          | 9.6           |
| time/                   |               |
|    total_timesteps      | 112600        |
| train/                  |               |
|    approx_kl            | 0.00012610354 |
|    clip_fraction        | 0.00179       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0516       |
|    explained_variance   | 0.231         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.914         |
|    n_updates            | 2810          |
|    policy_gradient_loss | -0.000538     |
|    value_loss           | 2.96          |
-------------------------------------------
Progress update from timestep 112700: Updated shield, loss is 0.07238972187042236
Progress update from timestep 112800: Updated shield, loss is 0.07291541993618011
Eval num_timesteps=112800, episode_reward=11.59 +/- 12.80
Episode length: 17.20 +/- 17.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | 11.6     |
| time/              |          |
|    total_timesteps | 112800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.23     |
|    ep_rew_mean     | 2.63     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 282      |
|    time_elapsed    | 21580    |
|    total_timesteps | 112800   |
---------------------------------
Progress update from timestep 112900: Updated shield, loss is 0.07730068266391754
Progress update from timestep 113000: Updated shield, loss is 0.07321978360414505
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 113000: Saved shield_loss.csv. 
Eval num_timesteps=113000, episode_reward=27.94 +/- 13.65
Episode length: 40.40 +/- 19.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 40.4         |
|    mean_reward          | 27.9         |
| time/                   |              |
|    total_timesteps      | 113000       |
| train/                  |              |
|    approx_kl            | 7.742457e-05 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0565      |
|    explained_variance   | 0.0929       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.16         |
|    n_updates            | 2820         |
|    policy_gradient_loss | -0.000451    |
|    value_loss           | 2            |
------------------------------------------
Progress update from timestep 113100: Updated shield, loss is 0.07920380681753159
Progress update from timestep 113200: Updated shield, loss is 0.06464607268571854
Eval num_timesteps=113200, episode_reward=25.48 +/- 12.01
Episode length: 36.60 +/- 16.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 25.5     |
| time/              |          |
|    total_timesteps | 113200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.88     |
|    ep_rew_mean     | 3.14     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 283      |
|    time_elapsed    | 21700    |
|    total_timesteps | 113200   |
---------------------------------
Progress update from timestep 113300: Updated shield, loss is 0.08355769515037537
Progress update from timestep 113400: Updated shield, loss is 0.07983631640672684
Eval num_timesteps=113400, episode_reward=10.43 +/- 12.66
Episode length: 15.40 +/- 17.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.4          |
|    mean_reward          | 10.4          |
| time/                   |               |
|    total_timesteps      | 113400        |
| train/                  |               |
|    approx_kl            | 0.00020424483 |
|    clip_fraction        | 0.00625       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0549       |
|    explained_variance   | 0.0709        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.47          |
|    n_updates            | 2830          |
|    policy_gradient_loss | -0.00126      |
|    value_loss           | 1.77          |
-------------------------------------------
Progress update from timestep 113500: Updated shield, loss is 0.07807496935129166
Progress update from timestep 113600: Updated shield, loss is 0.07264739274978638
Eval num_timesteps=113600, episode_reward=16.60 +/- 15.22
Episode length: 24.60 +/- 21.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 113600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.26     |
|    ep_rew_mean     | 2.66     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 284      |
|    time_elapsed    | 21791    |
|    total_timesteps | 113600   |
---------------------------------
Progress update from timestep 113700: Updated shield, loss is 0.07623276114463806
Progress update from timestep 113800: Updated shield, loss is 0.07260049879550934
Eval num_timesteps=113800, episode_reward=24.15 +/- 14.25
Episode length: 34.20 +/- 19.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.2         |
|    mean_reward          | 24.2         |
| time/                   |              |
|    total_timesteps      | 113800       |
| train/                  |              |
|    approx_kl            | 0.0014856484 |
|    clip_fraction        | 0.0187       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0972      |
|    explained_variance   | 0.244        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.623        |
|    n_updates            | 2840         |
|    policy_gradient_loss | -0.00293     |
|    value_loss           | 1.38         |
------------------------------------------
Progress update from timestep 113900: Updated shield, loss is 0.0729672759771347
Progress update from timestep 114000: Updated shield, loss is 0.0732782781124115
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 114000: Saved shield_loss.csv. 
Eval num_timesteps=114000, episode_reward=23.09 +/- 14.00
Episode length: 34.20 +/- 20.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 23.1     |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.49     |
|    ep_rew_mean     | 2.88     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 285      |
|    time_elapsed    | 21884    |
|    total_timesteps | 114000   |
---------------------------------
Progress update from timestep 114100: Updated shield, loss is 0.08299010246992111
Progress update from timestep 114200: Updated shield, loss is 0.07182173430919647
Eval num_timesteps=114200, episode_reward=9.45 +/- 11.98
Episode length: 14.80 +/- 17.63
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.8          |
|    mean_reward          | 9.45          |
| time/                   |               |
|    total_timesteps      | 114200        |
| train/                  |               |
|    approx_kl            | 0.00047601806 |
|    clip_fraction        | 0.0143        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0381       |
|    explained_variance   | 0.0649        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.481         |
|    n_updates            | 2850          |
|    policy_gradient_loss | -0.00142      |
|    value_loss           | 1.81          |
-------------------------------------------
Progress update from timestep 114300: Updated shield, loss is 0.07831349968910217
Progress update from timestep 114400: Updated shield, loss is 0.08359163254499435
Eval num_timesteps=114400, episode_reward=10.35 +/- 12.65
Episode length: 15.40 +/- 17.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | 10.4     |
| time/              |          |
|    total_timesteps | 114400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.98     |
|    ep_rew_mean     | 2.48     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 286      |
|    time_elapsed    | 21975    |
|    total_timesteps | 114400   |
---------------------------------
Progress update from timestep 114500: Updated shield, loss is 0.0702228769659996
Progress update from timestep 114600: Updated shield, loss is 0.0695004090666771
Eval num_timesteps=114600, episode_reward=16.31 +/- 15.85
Episode length: 23.60 +/- 21.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 23.6        |
|    mean_reward          | 16.3        |
| time/                   |             |
|    total_timesteps      | 114600      |
| train/                  |             |
|    approx_kl            | 6.39514e-05 |
|    clip_fraction        | 0.00112     |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.027      |
|    explained_variance   | 0.178       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.608       |
|    n_updates            | 2860        |
|    policy_gradient_loss | -0.000554   |
|    value_loss           | 1.56        |
-----------------------------------------
Progress update from timestep 114700: Updated shield, loss is 0.07711876928806305
Progress update from timestep 114800: Updated shield, loss is 0.07781888544559479
Eval num_timesteps=114800, episode_reward=17.08 +/- 15.24
Episode length: 24.40 +/- 21.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 17.1     |
| time/              |          |
|    total_timesteps | 114800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.58     |
|    ep_rew_mean     | 2.94     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 287      |
|    time_elapsed    | 22068    |
|    total_timesteps | 114800   |
---------------------------------
Progress update from timestep 114900: Updated shield, loss is 0.07937662303447723
Progress update from timestep 115000: Updated shield, loss is 0.08487063646316528
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 115000: Saved shield_loss.csv. 
Eval num_timesteps=115000, episode_reward=17.94 +/- 14.90
Episode length: 25.40 +/- 20.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.4          |
|    mean_reward          | 17.9          |
| time/                   |               |
|    total_timesteps      | 115000        |
| train/                  |               |
|    approx_kl            | 0.00014968708 |
|    clip_fraction        | 0.00446       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0586       |
|    explained_variance   | 0.251         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.04          |
|    n_updates            | 2870          |
|    policy_gradient_loss | -0.000989     |
|    value_loss           | 1.63          |
-------------------------------------------
Progress update from timestep 115100: Updated shield, loss is 0.07264315336942673
Progress update from timestep 115200: Updated shield, loss is 0.0864124670624733
Eval num_timesteps=115200, episode_reward=15.66 +/- 14.58
Episode length: 23.80 +/- 21.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | 15.7     |
| time/              |          |
|    total_timesteps | 115200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.79     |
|    ep_rew_mean     | 3.15     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 288      |
|    time_elapsed    | 22167    |
|    total_timesteps | 115200   |
---------------------------------
Progress update from timestep 115300: Updated shield, loss is 0.07949406653642654
Progress update from timestep 115400: Updated shield, loss is 0.093408964574337
Eval num_timesteps=115400, episode_reward=17.73 +/- 13.86
Episode length: 26.60 +/- 20.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.6          |
|    mean_reward          | 17.7          |
| time/                   |               |
|    total_timesteps      | 115400        |
| train/                  |               |
|    approx_kl            | 0.00018269742 |
|    clip_fraction        | 0.00424       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0585       |
|    explained_variance   | 0.116         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.38          |
|    n_updates            | 2880          |
|    policy_gradient_loss | -0.00101      |
|    value_loss           | 2.69          |
-------------------------------------------
Progress update from timestep 115500: Updated shield, loss is 0.06772684305906296
Progress update from timestep 115600: Updated shield, loss is 0.07597547769546509
Eval num_timesteps=115600, episode_reward=22.07 +/- 15.26
Episode length: 32.40 +/- 21.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.4     |
|    mean_reward     | 22.1     |
| time/              |          |
|    total_timesteps | 115600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.58     |
|    ep_rew_mean     | 3        |
| time/              |          |
|    fps             | 5        |
|    iterations      | 289      |
|    time_elapsed    | 22281    |
|    total_timesteps | 115600   |
---------------------------------
Progress update from timestep 115700: Updated shield, loss is 0.09252608567476273
Progress update from timestep 115800: Updated shield, loss is 0.07724742591381073
Eval num_timesteps=115800, episode_reward=23.47 +/- 14.85
Episode length: 33.40 +/- 20.33
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 33.4           |
|    mean_reward          | 23.5           |
| time/                   |                |
|    total_timesteps      | 115800         |
| train/                  |                |
|    approx_kl            | 0.000107453605 |
|    clip_fraction        | 0.00223        |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.0448        |
|    explained_variance   | 0.174          |
|    learning_rate        | 0.0001         |
|    loss                 | 0.774          |
|    n_updates            | 2890           |
|    policy_gradient_loss | -0.0007        |
|    value_loss           | 2.6            |
--------------------------------------------
Progress update from timestep 115900: Updated shield, loss is 0.08031179755926132
Progress update from timestep 116000: Updated shield, loss is 0.07995861023664474
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 116000: Saved shield_loss.csv. 
Eval num_timesteps=116000, episode_reward=23.09 +/- 11.69
Episode length: 34.60 +/- 17.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 23.1     |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.85     |
|    ep_rew_mean     | 3.19     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 290      |
|    time_elapsed    | 22395    |
|    total_timesteps | 116000   |
---------------------------------
Progress update from timestep 116100: Updated shield, loss is 0.07631618529558182
Progress update from timestep 116200: Updated shield, loss is 0.06949890404939651
Eval num_timesteps=116200, episode_reward=22.34 +/- 15.81
Episode length: 32.20 +/- 21.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32.2          |
|    mean_reward          | 22.3          |
| time/                   |               |
|    total_timesteps      | 116200        |
| train/                  |               |
|    approx_kl            | 0.00023810858 |
|    clip_fraction        | 0.00424       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.051        |
|    explained_variance   | 0.208         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.737         |
|    n_updates            | 2900          |
|    policy_gradient_loss | -0.000664     |
|    value_loss           | 2.36          |
-------------------------------------------
Progress update from timestep 116300: Updated shield, loss is 0.0772850513458252
Progress update from timestep 116400: Updated shield, loss is 0.08002544939517975
Eval num_timesteps=116400, episode_reward=12.24 +/- 11.98
Episode length: 17.80 +/- 16.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | 12.2     |
| time/              |          |
|    total_timesteps | 116400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.51     |
|    ep_rew_mean     | 2.92     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 291      |
|    time_elapsed    | 22486    |
|    total_timesteps | 116400   |
---------------------------------
Progress update from timestep 116500: Updated shield, loss is 0.07374465465545654
Progress update from timestep 116600: Updated shield, loss is 0.07958386093378067
Eval num_timesteps=116600, episode_reward=16.83 +/- 14.65
Episode length: 25.00 +/- 20.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25            |
|    mean_reward          | 16.8          |
| time/                   |               |
|    total_timesteps      | 116600        |
| train/                  |               |
|    approx_kl            | 0.00011970409 |
|    clip_fraction        | 0.00491       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0605       |
|    explained_variance   | 0.13          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.886         |
|    n_updates            | 2910          |
|    policy_gradient_loss | -0.00109      |
|    value_loss           | 2.5           |
-------------------------------------------
Progress update from timestep 116700: Updated shield, loss is 0.08620838075876236
Progress update from timestep 116800: Updated shield, loss is 0.07982578128576279
Eval num_timesteps=116800, episode_reward=15.71 +/- 15.82
Episode length: 23.00 +/- 22.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 15.7     |
| time/              |          |
|    total_timesteps | 116800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.23     |
|    ep_rew_mean     | 2.68     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 292      |
|    time_elapsed    | 22579    |
|    total_timesteps | 116800   |
---------------------------------
Progress update from timestep 116900: Updated shield, loss is 0.07838939130306244
Progress update from timestep 117000: Updated shield, loss is 0.08005127310752869
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 117000: Saved shield_loss.csv. 
Eval num_timesteps=117000, episode_reward=9.41 +/- 7.15
Episode length: 14.40 +/- 10.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.4          |
|    mean_reward          | 9.41          |
| time/                   |               |
|    total_timesteps      | 117000        |
| train/                  |               |
|    approx_kl            | 0.00037795425 |
|    clip_fraction        | 0.00469       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0309       |
|    explained_variance   | 0.166         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.985         |
|    n_updates            | 2920          |
|    policy_gradient_loss | -0.001        |
|    value_loss           | 1.69          |
-------------------------------------------
Progress update from timestep 117100: Updated shield, loss is 0.090252585709095
Progress update from timestep 117200: Updated shield, loss is 0.08703266084194183
Eval num_timesteps=117200, episode_reward=30.09 +/- 9.93
Episode length: 43.00 +/- 14.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43       |
|    mean_reward     | 30.1     |
| time/              |          |
|    total_timesteps | 117200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.34     |
|    ep_rew_mean     | 2.79     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 293      |
|    time_elapsed    | 22669    |
|    total_timesteps | 117200   |
---------------------------------
Progress update from timestep 117300: Updated shield, loss is 0.08008208125829697
Progress update from timestep 117400: Updated shield, loss is 0.08812543004751205
Eval num_timesteps=117400, episode_reward=11.59 +/- 12.33
Episode length: 17.20 +/- 16.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.2        |
|    mean_reward          | 11.6        |
| time/                   |             |
|    total_timesteps      | 117400      |
| train/                  |             |
|    approx_kl            | 0.000559545 |
|    clip_fraction        | 0.0172      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0397     |
|    explained_variance   | 0.115       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.1         |
|    n_updates            | 2930        |
|    policy_gradient_loss | -0.00223    |
|    value_loss           | 1.75        |
-----------------------------------------
Progress update from timestep 117500: Updated shield, loss is 0.08142269402742386
Progress update from timestep 117600: Updated shield, loss is 0.07877542078495026
Eval num_timesteps=117600, episode_reward=11.53 +/- 11.72
Episode length: 17.20 +/- 16.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | 11.5     |
| time/              |          |
|    total_timesteps | 117600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.53     |
|    ep_rew_mean     | 2.94     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 294      |
|    time_elapsed    | 22769    |
|    total_timesteps | 117600   |
---------------------------------
Progress update from timestep 117700: Updated shield, loss is 0.0826597809791565
Progress update from timestep 117800: Updated shield, loss is 0.081041619181633
Eval num_timesteps=117800, episode_reward=20.03 +/- 12.90
Episode length: 29.00 +/- 17.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29           |
|    mean_reward          | 20           |
| time/                   |              |
|    total_timesteps      | 117800       |
| train/                  |              |
|    approx_kl            | 8.697888e-05 |
|    clip_fraction        | 0.00692      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0615      |
|    explained_variance   | 0.192        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.767        |
|    n_updates            | 2940         |
|    policy_gradient_loss | -0.00121     |
|    value_loss           | 1.71         |
------------------------------------------
Progress update from timestep 117900: Updated shield, loss is 0.07343553751707077
Progress update from timestep 118000: Updated shield, loss is 0.08083102852106094
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 118000: Saved shield_loss.csv. 
Eval num_timesteps=118000, episode_reward=31.41 +/- 5.10
Episode length: 46.60 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.6     |
|    mean_reward     | 31.4     |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.73     |
|    ep_rew_mean     | 3.1      |
| time/              |          |
|    fps             | 5        |
|    iterations      | 295      |
|    time_elapsed    | 22895    |
|    total_timesteps | 118000   |
---------------------------------
Progress update from timestep 118100: Updated shield, loss is 0.0794561579823494
Progress update from timestep 118200: Updated shield, loss is 0.07010547816753387
Eval num_timesteps=118200, episode_reward=4.07 +/- 2.88
Episode length: 6.80 +/- 4.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 6.8           |
|    mean_reward          | 4.07          |
| time/                   |               |
|    total_timesteps      | 118200        |
| train/                  |               |
|    approx_kl            | 0.00015054157 |
|    clip_fraction        | 0.00201       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0542       |
|    explained_variance   | 0.0568        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.964         |
|    n_updates            | 2950          |
|    policy_gradient_loss | -0.00103      |
|    value_loss           | 2.57          |
-------------------------------------------
Progress update from timestep 118300: Updated shield, loss is 0.0849887803196907
Progress update from timestep 118400: Updated shield, loss is 0.07151433825492859
Eval num_timesteps=118400, episode_reward=10.25 +/- 12.82
Episode length: 15.40 +/- 17.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | 10.2     |
| time/              |          |
|    total_timesteps | 118400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.1      |
|    ep_rew_mean     | 3.33     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 296      |
|    time_elapsed    | 22967    |
|    total_timesteps | 118400   |
---------------------------------
Progress update from timestep 118500: Updated shield, loss is 0.07571172714233398
Progress update from timestep 118600: Updated shield, loss is 0.09193920344114304
Eval num_timesteps=118600, episode_reward=11.19 +/- 11.90
Episode length: 16.80 +/- 16.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.8          |
|    mean_reward          | 11.2          |
| time/                   |               |
|    total_timesteps      | 118600        |
| train/                  |               |
|    approx_kl            | 1.3249394e-05 |
|    clip_fraction        | 0.000893      |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0122       |
|    explained_variance   | 0.173         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.871         |
|    n_updates            | 2960          |
|    policy_gradient_loss | -0.000224     |
|    value_loss           | 2.35          |
-------------------------------------------
Progress update from timestep 118700: Updated shield, loss is 0.07869134843349457
Progress update from timestep 118800: Updated shield, loss is 0.07604961842298508
Eval num_timesteps=118800, episode_reward=2.76 +/- 1.36
Episode length: 4.80 +/- 1.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.8      |
|    mean_reward     | 2.76     |
| time/              |          |
|    total_timesteps | 118800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.81     |
|    ep_rew_mean     | 3.14     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 297      |
|    time_elapsed    | 23044    |
|    total_timesteps | 118800   |
---------------------------------
Progress update from timestep 118900: Updated shield, loss is 0.07693543285131454
Progress update from timestep 119000: Updated shield, loss is 0.08421863615512848
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 119000: Saved shield_loss.csv. 
Eval num_timesteps=119000, episode_reward=17.39 +/- 13.61
Episode length: 26.00 +/- 19.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26           |
|    mean_reward          | 17.4         |
| time/                   |              |
|    total_timesteps      | 119000       |
| train/                  |              |
|    approx_kl            | 5.470469e-05 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0587      |
|    explained_variance   | 0.129        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.21         |
|    n_updates            | 2970         |
|    policy_gradient_loss | -0.000532    |
|    value_loss           | 2.14         |
------------------------------------------
Progress update from timestep 119100: Updated shield, loss is 0.07517873495817184
Progress update from timestep 119200: Updated shield, loss is 0.07832177728414536
Eval num_timesteps=119200, episode_reward=9.45 +/- 12.55
Episode length: 14.40 +/- 17.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.4     |
|    mean_reward     | 9.45     |
| time/              |          |
|    total_timesteps | 119200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.19     |
|    ep_rew_mean     | 2.68     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 298      |
|    time_elapsed    | 23133    |
|    total_timesteps | 119200   |
---------------------------------
Progress update from timestep 119300: Updated shield, loss is 0.07298094779253006
Progress update from timestep 119400: Updated shield, loss is 0.0806211307644844
Eval num_timesteps=119400, episode_reward=11.88 +/- 12.39
Episode length: 17.40 +/- 17.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.4         |
|    mean_reward          | 11.9         |
| time/                   |              |
|    total_timesteps      | 119400       |
| train/                  |              |
|    approx_kl            | 5.727022e-05 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.035       |
|    explained_variance   | 0.109        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.608        |
|    n_updates            | 2980         |
|    policy_gradient_loss | -0.000563    |
|    value_loss           | 2.01         |
------------------------------------------
Progress update from timestep 119500: Updated shield, loss is 0.07389072328805923
Progress update from timestep 119600: Updated shield, loss is 0.07440750300884247
Eval num_timesteps=119600, episode_reward=15.60 +/- 15.90
Episode length: 22.80 +/- 22.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 15.6     |
| time/              |          |
|    total_timesteps | 119600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.26     |
|    ep_rew_mean     | 3.42     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 299      |
|    time_elapsed    | 23224    |
|    total_timesteps | 119600   |
---------------------------------
Progress update from timestep 119700: Updated shield, loss is 0.08668714761734009
Progress update from timestep 119800: Updated shield, loss is 0.07309501618146896
Eval num_timesteps=119800, episode_reward=24.15 +/- 13.46
Episode length: 35.00 +/- 18.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35           |
|    mean_reward          | 24.2         |
| time/                   |              |
|    total_timesteps      | 119800       |
| train/                  |              |
|    approx_kl            | 6.282143e-05 |
|    clip_fraction        | 0.0029       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0126      |
|    explained_variance   | 0.074        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.27         |
|    n_updates            | 2990         |
|    policy_gradient_loss | -0.000327    |
|    value_loss           | 2.22         |
------------------------------------------
Progress update from timestep 119900: Updated shield, loss is 0.07188873738050461
Progress update from timestep 120000: Updated shield, loss is 0.07862512767314911
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 120000: Saved shield_loss.csv. 
Eval num_timesteps=120000, episode_reward=8.74 +/- 13.47
Episode length: 13.00 +/- 18.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13       |
|    mean_reward     | 8.74     |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.29     |
|    ep_rew_mean     | 3.48     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 300      |
|    time_elapsed    | 23322    |
|    total_timesteps | 120000   |
---------------------------------
Progress update from timestep 120100: Updated shield, loss is 0.07421979308128357
Progress update from timestep 120200: Updated shield, loss is 0.07672140002250671
Eval num_timesteps=120200, episode_reward=24.83 +/- 11.83
Episode length: 36.60 +/- 16.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.6         |
|    mean_reward          | 24.8         |
| time/                   |              |
|    total_timesteps      | 120200       |
| train/                  |              |
|    approx_kl            | 7.162995e-05 |
|    clip_fraction        | 0.00402      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0242      |
|    explained_variance   | 0.244        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.698        |
|    n_updates            | 3000         |
|    policy_gradient_loss | -0.000485    |
|    value_loss           | 2.47         |
------------------------------------------
Progress update from timestep 120300: Updated shield, loss is 0.07278206199407578
Progress update from timestep 120400: Updated shield, loss is 0.07062333077192307
Eval num_timesteps=120400, episode_reward=23.18 +/- 15.73
Episode length: 32.60 +/- 21.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.6     |
|    mean_reward     | 23.2     |
| time/              |          |
|    total_timesteps | 120400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.22     |
|    ep_rew_mean     | 2.64     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 301      |
|    time_elapsed    | 23440    |
|    total_timesteps | 120400   |
---------------------------------
Progress update from timestep 120500: Updated shield, loss is 0.08009084314107895
Progress update from timestep 120600: Updated shield, loss is 0.07277444005012512
Eval num_timesteps=120600, episode_reward=16.37 +/- 15.71
Episode length: 23.60 +/- 21.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.6          |
|    mean_reward          | 16.4          |
| time/                   |               |
|    total_timesteps      | 120600        |
| train/                  |               |
|    approx_kl            | 0.00026227135 |
|    clip_fraction        | 0.0114        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0401       |
|    explained_variance   | 0.203         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.456         |
|    n_updates            | 3010          |
|    policy_gradient_loss | -0.0014       |
|    value_loss           | 1.41          |
-------------------------------------------
Progress update from timestep 120700: Updated shield, loss is 0.08684851229190826
Progress update from timestep 120800: Updated shield, loss is 0.06532977521419525
Eval num_timesteps=120800, episode_reward=17.99 +/- 14.74
Episode length: 26.00 +/- 20.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | 18       |
| time/              |          |
|    total_timesteps | 120800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.49     |
|    ep_rew_mean     | 2.95     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 302      |
|    time_elapsed    | 23538    |
|    total_timesteps | 120800   |
---------------------------------
Progress update from timestep 120900: Updated shield, loss is 0.07071059197187424
Progress update from timestep 121000: Updated shield, loss is 0.07473491132259369
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 121000: Saved shield_loss.csv. 
Eval num_timesteps=121000, episode_reward=15.36 +/- 16.06
Episode length: 22.60 +/- 22.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.6          |
|    mean_reward          | 15.4          |
| time/                   |               |
|    total_timesteps      | 121000        |
| train/                  |               |
|    approx_kl            | 0.00012400227 |
|    clip_fraction        | 0.00201       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0314       |
|    explained_variance   | 0.131         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.71          |
|    n_updates            | 3020          |
|    policy_gradient_loss | -0.000518     |
|    value_loss           | 3.29          |
-------------------------------------------
Progress update from timestep 121100: Updated shield, loss is 0.06558119505643845
Progress update from timestep 121200: Updated shield, loss is 0.08372955769300461
Eval num_timesteps=121200, episode_reward=13.81 +/- 11.51
Episode length: 20.60 +/- 15.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 13.8     |
| time/              |          |
|    total_timesteps | 121200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.81     |
|    ep_rew_mean     | 3.07     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 303      |
|    time_elapsed    | 23630    |
|    total_timesteps | 121200   |
---------------------------------
Progress update from timestep 121300: Updated shield, loss is 0.07262016087770462
Progress update from timestep 121400: Updated shield, loss is 0.07537757605314255
Eval num_timesteps=121400, episode_reward=9.94 +/- 12.47
Episode length: 15.20 +/- 17.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.2         |
|    mean_reward          | 9.94         |
| time/                   |              |
|    total_timesteps      | 121400       |
| train/                  |              |
|    approx_kl            | 0.0004099851 |
|    clip_fraction        | 0.0114       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0556      |
|    explained_variance   | 0.162        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.866        |
|    n_updates            | 3030         |
|    policy_gradient_loss | -0.00179     |
|    value_loss           | 2.31         |
------------------------------------------
Progress update from timestep 121500: Updated shield, loss is 0.07001300156116486
Progress update from timestep 121600: Updated shield, loss is 0.07266277819871902
Eval num_timesteps=121600, episode_reward=24.07 +/- 13.79
Episode length: 34.80 +/- 18.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 24.1     |
| time/              |          |
|    total_timesteps | 121600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.37     |
|    ep_rew_mean     | 3.55     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 304      |
|    time_elapsed    | 23725    |
|    total_timesteps | 121600   |
---------------------------------
Progress update from timestep 121700: Updated shield, loss is 0.07579844444990158
Progress update from timestep 121800: Updated shield, loss is 0.07154039293527603
Eval num_timesteps=121800, episode_reward=20.49 +/- 13.23
Episode length: 29.40 +/- 18.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 29.4          |
|    mean_reward          | 20.5          |
| time/                   |               |
|    total_timesteps      | 121800        |
| train/                  |               |
|    approx_kl            | 2.4841765e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0391       |
|    explained_variance   | 0.23          |
|    learning_rate        | 0.0001        |
|    loss                 | 1.31          |
|    n_updates            | 3040          |
|    policy_gradient_loss | -0.000328     |
|    value_loss           | 2.36          |
-------------------------------------------
Progress update from timestep 121900: Updated shield, loss is 0.06813802570104599
Progress update from timestep 122000: Updated shield, loss is 0.0692143589258194
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 122000: Saved shield_loss.csv. 
Eval num_timesteps=122000, episode_reward=12.25 +/- 10.79
Episode length: 18.40 +/- 16.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.4     |
|    mean_reward     | 12.2     |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.76     |
|    ep_rew_mean     | 3.1      |
| time/              |          |
|    fps             | 5        |
|    iterations      | 305      |
|    time_elapsed    | 23814    |
|    total_timesteps | 122000   |
---------------------------------
Progress update from timestep 122100: Updated shield, loss is 0.08009015768766403
Progress update from timestep 122200: Updated shield, loss is 0.07325854897499084
Eval num_timesteps=122200, episode_reward=9.58 +/- 12.62
Episode length: 14.60 +/- 17.95
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 14.6           |
|    mean_reward          | 9.58           |
| time/                   |                |
|    total_timesteps      | 122200         |
| train/                  |                |
|    approx_kl            | 0.000108989356 |
|    clip_fraction        | 0.00357        |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.0455        |
|    explained_variance   | 0.112          |
|    learning_rate        | 0.0001         |
|    loss                 | 1.02           |
|    n_updates            | 3050           |
|    policy_gradient_loss | -0.000792      |
|    value_loss           | 1.76           |
--------------------------------------------
Progress update from timestep 122300: Updated shield, loss is 0.07181328535079956
Progress update from timestep 122400: Updated shield, loss is 0.07371002435684204
Eval num_timesteps=122400, episode_reward=17.86 +/- 13.67
Episode length: 26.20 +/- 19.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 122400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.26     |
|    ep_rew_mean     | 2.67     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 306      |
|    time_elapsed    | 23907    |
|    total_timesteps | 122400   |
---------------------------------
Progress update from timestep 122500: Updated shield, loss is 0.07424432784318924
Progress update from timestep 122600: Updated shield, loss is 0.07559656351804733
Eval num_timesteps=122600, episode_reward=17.85 +/- 14.71
Episode length: 25.80 +/- 20.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.8         |
|    mean_reward          | 17.8         |
| time/                   |              |
|    total_timesteps      | 122600       |
| train/                  |              |
|    approx_kl            | 9.912412e-05 |
|    clip_fraction        | 0.00469      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0832      |
|    explained_variance   | 0.155        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.723        |
|    n_updates            | 3060         |
|    policy_gradient_loss | -0.00111     |
|    value_loss           | 1.54         |
------------------------------------------
Progress update from timestep 122700: Updated shield, loss is 0.0660635232925415
Progress update from timestep 122800: Updated shield, loss is 0.07861191034317017
Eval num_timesteps=122800, episode_reward=30.35 +/- 11.61
Episode length: 42.20 +/- 15.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 30.3     |
| time/              |          |
|    total_timesteps | 122800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.51     |
|    ep_rew_mean     | 2.89     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 307      |
|    time_elapsed    | 24026    |
|    total_timesteps | 122800   |
---------------------------------
Progress update from timestep 122900: Updated shield, loss is 0.0758061632514
Progress update from timestep 123000: Updated shield, loss is 0.08341057598590851
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 123000: Saved shield_loss.csv. 
Eval num_timesteps=123000, episode_reward=13.23 +/- 12.02
Episode length: 19.80 +/- 17.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 19.8          |
|    mean_reward          | 13.2          |
| time/                   |               |
|    total_timesteps      | 123000        |
| train/                  |               |
|    approx_kl            | 0.00018725076 |
|    clip_fraction        | 0.00826       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.103        |
|    explained_variance   | 0.215         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.41          |
|    n_updates            | 3070          |
|    policy_gradient_loss | -0.000968     |
|    value_loss           | 2.52          |
-------------------------------------------
Progress update from timestep 123100: Updated shield, loss is 0.08817378431558609
Progress update from timestep 123200: Updated shield, loss is 0.08792351931333542
Eval num_timesteps=123200, episode_reward=24.66 +/- 14.30
Episode length: 34.60 +/- 18.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 24.7     |
| time/              |          |
|    total_timesteps | 123200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.97     |
|    ep_rew_mean     | 3.25     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 308      |
|    time_elapsed    | 24121    |
|    total_timesteps | 123200   |
---------------------------------
Progress update from timestep 123300: Updated shield, loss is 0.10139285027980804
Progress update from timestep 123400: Updated shield, loss is 0.09744756668806076
Eval num_timesteps=123400, episode_reward=10.44 +/- 13.33
Episode length: 15.20 +/- 17.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.2        |
|    mean_reward          | 10.4        |
| time/                   |             |
|    total_timesteps      | 123400      |
| train/                  |             |
|    approx_kl            | 0.000225624 |
|    clip_fraction        | 0.00759     |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0635     |
|    explained_variance   | 0.335       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.686       |
|    n_updates            | 3080        |
|    policy_gradient_loss | -0.00179    |
|    value_loss           | 2.05        |
-----------------------------------------
Progress update from timestep 123500: Updated shield, loss is 0.08279465138912201
Progress update from timestep 123600: Updated shield, loss is 0.09584508091211319
Eval num_timesteps=123600, episode_reward=29.43 +/- 10.71
Episode length: 42.40 +/- 15.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 29.4     |
| time/              |          |
|    total_timesteps | 123600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.81     |
|    ep_rew_mean     | 3.11     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 309      |
|    time_elapsed    | 24216    |
|    total_timesteps | 123600   |
---------------------------------
Progress update from timestep 123700: Updated shield, loss is 0.09193148463964462
Progress update from timestep 123800: Updated shield, loss is 0.07514026761054993
Eval num_timesteps=123800, episode_reward=9.24 +/- 13.78
Episode length: 13.40 +/- 18.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 13.4         |
|    mean_reward          | 9.24         |
| time/                   |              |
|    total_timesteps      | 123800       |
| train/                  |              |
|    approx_kl            | 8.251704e-05 |
|    clip_fraction        | 0.00402      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0482      |
|    explained_variance   | 0.213        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.23         |
|    n_updates            | 3090         |
|    policy_gradient_loss | -0.00085     |
|    value_loss           | 2.06         |
------------------------------------------
Progress update from timestep 123900: Updated shield, loss is 0.06978724151849747
Progress update from timestep 124000: Updated shield, loss is 0.08619629591703415
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 124000: Saved shield_loss.csv. 
Eval num_timesteps=124000, episode_reward=22.41 +/- 15.68
Episode length: 32.20 +/- 21.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 22.4     |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.96     |
|    ep_rew_mean     | 3.14     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 310      |
|    time_elapsed    | 24308    |
|    total_timesteps | 124000   |
---------------------------------
Progress update from timestep 124100: Updated shield, loss is 0.08233829587697983
Progress update from timestep 124200: Updated shield, loss is 0.09558810293674469
Eval num_timesteps=124200, episode_reward=9.91 +/- 11.78
Episode length: 15.40 +/- 17.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.4          |
|    mean_reward          | 9.91          |
| time/                   |               |
|    total_timesteps      | 124200        |
| train/                  |               |
|    approx_kl            | 0.00033947013 |
|    clip_fraction        | 0.0127        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0818       |
|    explained_variance   | 0.146         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.972         |
|    n_updates            | 3100          |
|    policy_gradient_loss | -0.00206      |
|    value_loss           | 2.24          |
-------------------------------------------
Progress update from timestep 124300: Updated shield, loss is 0.1004284992814064
Progress update from timestep 124400: Updated shield, loss is 0.07714056223630905
Eval num_timesteps=124400, episode_reward=29.88 +/- 11.99
Episode length: 41.80 +/- 16.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 29.9     |
| time/              |          |
|    total_timesteps | 124400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.07     |
|    ep_rew_mean     | 3.26     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 311      |
|    time_elapsed    | 24420    |
|    total_timesteps | 124400   |
---------------------------------
Progress update from timestep 124500: Updated shield, loss is 0.07553324848413467
Progress update from timestep 124600: Updated shield, loss is 0.10106926411390305
Eval num_timesteps=124600, episode_reward=21.69 +/- 12.60
Episode length: 32.00 +/- 17.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32           |
|    mean_reward          | 21.7         |
| time/                   |              |
|    total_timesteps      | 124600       |
| train/                  |              |
|    approx_kl            | 3.746937e-05 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0412      |
|    explained_variance   | 0.0792       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.939        |
|    n_updates            | 3110         |
|    policy_gradient_loss | -0.000363    |
|    value_loss           | 2.24         |
------------------------------------------
Progress update from timestep 124700: Updated shield, loss is 0.1036398708820343
Progress update from timestep 124800: Updated shield, loss is 0.0867578536272049
Eval num_timesteps=124800, episode_reward=10.60 +/- 12.66
Episode length: 15.60 +/- 17.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 10.6     |
| time/              |          |
|    total_timesteps | 124800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.33     |
|    ep_rew_mean     | 3.45     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 312      |
|    time_elapsed    | 24534    |
|    total_timesteps | 124800   |
---------------------------------
Progress update from timestep 124900: Updated shield, loss is 0.10261750966310501
Progress update from timestep 125000: Updated shield, loss is 0.07603231817483902
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 125000: Saved shield_loss.csv. 
Eval num_timesteps=125000, episode_reward=6.53 +/- 1.49
Episode length: 10.00 +/- 2.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 10            |
|    mean_reward          | 6.53          |
| time/                   |               |
|    total_timesteps      | 125000        |
| train/                  |               |
|    approx_kl            | 0.00020300553 |
|    clip_fraction        | 0.00446       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.079        |
|    explained_variance   | 0.0958        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.879         |
|    n_updates            | 3120          |
|    policy_gradient_loss | -0.00179      |
|    value_loss           | 2.4           |
-------------------------------------------
Progress update from timestep 125100: Updated shield, loss is 0.09680112451314926
Progress update from timestep 125200: Updated shield, loss is 0.10355451703071594
Eval num_timesteps=125200, episode_reward=15.00 +/- 16.37
Episode length: 22.00 +/- 22.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22       |
|    mean_reward     | 15       |
| time/              |          |
|    total_timesteps | 125200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.38     |
|    ep_rew_mean     | 2.8      |
| time/              |          |
|    fps             | 5        |
|    iterations      | 313      |
|    time_elapsed    | 24610    |
|    total_timesteps | 125200   |
---------------------------------
Progress update from timestep 125300: Updated shield, loss is 0.09501543641090393
Progress update from timestep 125400: Updated shield, loss is 0.07845649123191833
Eval num_timesteps=125400, episode_reward=16.24 +/- 14.24
Episode length: 24.60 +/- 21.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.6          |
|    mean_reward          | 16.2          |
| time/                   |               |
|    total_timesteps      | 125400        |
| train/                  |               |
|    approx_kl            | 0.00016729979 |
|    clip_fraction        | 0.00915       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0729       |
|    explained_variance   | 0.0936        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.05          |
|    n_updates            | 3130          |
|    policy_gradient_loss | -0.00128      |
|    value_loss           | 2.73          |
-------------------------------------------
Progress update from timestep 125500: Updated shield, loss is 0.08116243779659271
Progress update from timestep 125600: Updated shield, loss is 0.09849193692207336
Eval num_timesteps=125600, episode_reward=17.28 +/- 15.91
Episode length: 24.20 +/- 21.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 17.3     |
| time/              |          |
|    total_timesteps | 125600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.53     |
|    ep_rew_mean     | 2.87     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 314      |
|    time_elapsed    | 24728    |
|    total_timesteps | 125600   |
---------------------------------
Progress update from timestep 125700: Updated shield, loss is 0.09034545719623566
Progress update from timestep 125800: Updated shield, loss is 0.09716929495334625
Eval num_timesteps=125800, episode_reward=18.44 +/- 13.66
Episode length: 27.20 +/- 19.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 27.2          |
|    mean_reward          | 18.4          |
| time/                   |               |
|    total_timesteps      | 125800        |
| train/                  |               |
|    approx_kl            | 3.7237936e-05 |
|    clip_fraction        | 0.00179       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0172       |
|    explained_variance   | 0.118         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.33          |
|    n_updates            | 3140          |
|    policy_gradient_loss | -0.000439     |
|    value_loss           | 1.63          |
-------------------------------------------
Progress update from timestep 125900: Updated shield, loss is 0.08662369847297668
Progress update from timestep 126000: Updated shield, loss is 0.0877242386341095
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 126000: Saved shield_loss.csv. 
Eval num_timesteps=126000, episode_reward=16.61 +/- 15.05
Episode length: 24.40 +/- 20.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.99     |
|    ep_rew_mean     | 3.35     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 315      |
|    time_elapsed    | 24821    |
|    total_timesteps | 126000   |
---------------------------------
Progress update from timestep 126100: Updated shield, loss is 0.09343785792589188
Progress update from timestep 126200: Updated shield, loss is 0.08276264369487762
Eval num_timesteps=126200, episode_reward=16.30 +/- 14.55
Episode length: 24.40 +/- 21.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.4          |
|    mean_reward          | 16.3          |
| time/                   |               |
|    total_timesteps      | 126200        |
| train/                  |               |
|    approx_kl            | 2.8913175e-05 |
|    clip_fraction        | 0.00067       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0165       |
|    explained_variance   | 0.28          |
|    learning_rate        | 0.0001        |
|    loss                 | 1.98          |
|    n_updates            | 3150          |
|    policy_gradient_loss | -0.000172     |
|    value_loss           | 3.07          |
-------------------------------------------
Progress update from timestep 126300: Updated shield, loss is 0.07738469541072845
Progress update from timestep 126400: Updated shield, loss is 0.09165950119495392
Eval num_timesteps=126400, episode_reward=23.57 +/- 12.93
Episode length: 34.80 +/- 18.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 23.6     |
| time/              |          |
|    total_timesteps | 126400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.77     |
|    ep_rew_mean     | 3.13     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 316      |
|    time_elapsed    | 24916    |
|    total_timesteps | 126400   |
---------------------------------
Progress update from timestep 126500: Updated shield, loss is 0.09192526340484619
Progress update from timestep 126600: Updated shield, loss is 0.0827372595667839
Eval num_timesteps=126600, episode_reward=23.77 +/- 12.88
Episode length: 35.00 +/- 18.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35           |
|    mean_reward          | 23.8         |
| time/                   |              |
|    total_timesteps      | 126600       |
| train/                  |              |
|    approx_kl            | 7.173093e-05 |
|    clip_fraction        | 0.00313      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.031       |
|    explained_variance   | 0.208        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.637        |
|    n_updates            | 3160         |
|    policy_gradient_loss | -0.000748    |
|    value_loss           | 2.41         |
------------------------------------------
Progress update from timestep 126700: Updated shield, loss is 0.07790498435497284
Progress update from timestep 126800: Updated shield, loss is 0.07213588058948517
Eval num_timesteps=126800, episode_reward=16.82 +/- 15.36
Episode length: 24.20 +/- 21.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 126800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.68     |
|    ep_rew_mean     | 3.8      |
| time/              |          |
|    fps             | 5        |
|    iterations      | 317      |
|    time_elapsed    | 25006    |
|    total_timesteps | 126800   |
---------------------------------
Progress update from timestep 126900: Updated shield, loss is 0.0847981795668602
Progress update from timestep 127000: Updated shield, loss is 0.08101506531238556
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 127000: Saved shield_loss.csv. 
Eval num_timesteps=127000, episode_reward=16.83 +/- 14.45
Episode length: 25.00 +/- 20.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25           |
|    mean_reward          | 16.8         |
| time/                   |              |
|    total_timesteps      | 127000       |
| train/                  |              |
|    approx_kl            | 0.0002911174 |
|    clip_fraction        | 0.0136       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0567      |
|    explained_variance   | 0.312        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.741        |
|    n_updates            | 3170         |
|    policy_gradient_loss | -0.00201     |
|    value_loss           | 1.91         |
------------------------------------------
Progress update from timestep 127100: Updated shield, loss is 0.07296448945999146
Progress update from timestep 127200: Updated shield, loss is 0.091422900557518
Eval num_timesteps=127200, episode_reward=16.32 +/- 15.41
Episode length: 24.00 +/- 21.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16.3     |
| time/              |          |
|    total_timesteps | 127200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.49     |
|    ep_rew_mean     | 2.89     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 318      |
|    time_elapsed    | 25099    |
|    total_timesteps | 127200   |
---------------------------------
Progress update from timestep 127300: Updated shield, loss is 0.0755961611866951
Progress update from timestep 127400: Updated shield, loss is 0.07872535288333893
Eval num_timesteps=127400, episode_reward=16.81 +/- 15.88
Episode length: 24.00 +/- 21.38
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24            |
|    mean_reward          | 16.8          |
| time/                   |               |
|    total_timesteps      | 127400        |
| train/                  |               |
|    approx_kl            | 0.00020212714 |
|    clip_fraction        | 0.00357       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0624       |
|    explained_variance   | 0.168         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.25          |
|    n_updates            | 3180          |
|    policy_gradient_loss | -0.000769     |
|    value_loss           | 1.87          |
-------------------------------------------
Progress update from timestep 127500: Updated shield, loss is 0.07364507019519806
Progress update from timestep 127600: Updated shield, loss is 0.08442514389753342
Eval num_timesteps=127600, episode_reward=16.53 +/- 10.63
Episode length: 24.20 +/- 14.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 127600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.29     |
|    ep_rew_mean     | 3.45     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 319      |
|    time_elapsed    | 25188    |
|    total_timesteps | 127600   |
---------------------------------
Progress update from timestep 127700: Updated shield, loss is 0.0630960464477539
Progress update from timestep 127800: Updated shield, loss is 0.08233910799026489
Eval num_timesteps=127800, episode_reward=17.47 +/- 13.30
Episode length: 26.60 +/- 19.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.6          |
|    mean_reward          | 17.5          |
| time/                   |               |
|    total_timesteps      | 127800        |
| train/                  |               |
|    approx_kl            | 0.00010694254 |
|    clip_fraction        | 0.0029        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0227       |
|    explained_variance   | 0.0581        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.681         |
|    n_updates            | 3190          |
|    policy_gradient_loss | -0.000634     |
|    value_loss           | 2.43          |
-------------------------------------------
Progress update from timestep 127900: Updated shield, loss is 0.07484446465969086
Progress update from timestep 128000: Updated shield, loss is 0.07553908228874207
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 128000: Saved shield_loss.csv. 
Eval num_timesteps=128000, episode_reward=29.30 +/- 14.24
Episode length: 40.40 +/- 19.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.4     |
|    mean_reward     | 29.3     |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.81     |
|    ep_rew_mean     | 3.14     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 320      |
|    time_elapsed    | 25278    |
|    total_timesteps | 128000   |
---------------------------------
Progress update from timestep 128100: Updated shield, loss is 0.06767220050096512
Progress update from timestep 128200: Updated shield, loss is 0.08234170824289322
Eval num_timesteps=128200, episode_reward=29.11 +/- 12.41
Episode length: 41.40 +/- 17.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 41.4          |
|    mean_reward          | 29.1          |
| time/                   |               |
|    total_timesteps      | 128200        |
| train/                  |               |
|    approx_kl            | 0.00042521805 |
|    clip_fraction        | 0.0109        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0633       |
|    explained_variance   | 0.109         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.746         |
|    n_updates            | 3200          |
|    policy_gradient_loss | -0.00166      |
|    value_loss           | 1.72          |
-------------------------------------------
Progress update from timestep 128300: Updated shield, loss is 0.0741528645157814
Progress update from timestep 128400: Updated shield, loss is 0.06590160727500916
Eval num_timesteps=128400, episode_reward=22.14 +/- 14.21
Episode length: 33.00 +/- 20.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 22.1     |
| time/              |          |
|    total_timesteps | 128400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.89     |
|    ep_rew_mean     | 3.14     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 321      |
|    time_elapsed    | 25393    |
|    total_timesteps | 128400   |
---------------------------------
Progress update from timestep 128500: Updated shield, loss is 0.07243995368480682
Progress update from timestep 128600: Updated shield, loss is 0.07326959073543549
Eval num_timesteps=128600, episode_reward=12.20 +/- 11.19
Episode length: 18.20 +/- 15.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.2         |
|    mean_reward          | 12.2         |
| time/                   |              |
|    total_timesteps      | 128600       |
| train/                  |              |
|    approx_kl            | 6.080206e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00306     |
|    explained_variance   | 0.148        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.01         |
|    n_updates            | 3210         |
|    policy_gradient_loss | -5.47e-06    |
|    value_loss           | 1.98         |
------------------------------------------
Progress update from timestep 128700: Updated shield, loss is 0.06266848742961884
Progress update from timestep 128800: Updated shield, loss is 0.07226478308439255
Eval num_timesteps=128800, episode_reward=10.10 +/- 11.69
Episode length: 15.60 +/- 17.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 10.1     |
| time/              |          |
|    total_timesteps | 128800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.34     |
|    ep_rew_mean     | 2.74     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 322      |
|    time_elapsed    | 25484    |
|    total_timesteps | 128800   |
---------------------------------
Progress update from timestep 128900: Updated shield, loss is 0.07650581747293472
Progress update from timestep 129000: Updated shield, loss is 0.08484058082103729
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 129000: Saved shield_loss.csv. 
Eval num_timesteps=129000, episode_reward=9.84 +/- 13.49
Episode length: 14.40 +/- 17.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.4          |
|    mean_reward          | 9.84          |
| time/                   |               |
|    total_timesteps      | 129000        |
| train/                  |               |
|    approx_kl            | 0.00042032156 |
|    clip_fraction        | 0.0154        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0627       |
|    explained_variance   | 0.176         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.321         |
|    n_updates            | 3220          |
|    policy_gradient_loss | -0.00173      |
|    value_loss           | 1.45          |
-------------------------------------------
Progress update from timestep 129100: Updated shield, loss is 0.06791819632053375
Progress update from timestep 129200: Updated shield, loss is 0.06752345710992813
Eval num_timesteps=129200, episode_reward=6.52 +/- 4.33
Episode length: 10.20 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10.2     |
|    mean_reward     | 6.52     |
| time/              |          |
|    total_timesteps | 129200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.09     |
|    ep_rew_mean     | 3.32     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 323      |
|    time_elapsed    | 25561    |
|    total_timesteps | 129200   |
---------------------------------
Progress update from timestep 129300: Updated shield, loss is 0.06529750674962997
Progress update from timestep 129400: Updated shield, loss is 0.07957851141691208
Eval num_timesteps=129400, episode_reward=22.83 +/- 14.81
Episode length: 33.00 +/- 20.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33            |
|    mean_reward          | 22.8          |
| time/                   |               |
|    total_timesteps      | 129400        |
| train/                  |               |
|    approx_kl            | 4.3217227e-05 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.021        |
|    explained_variance   | 0.119         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.48          |
|    n_updates            | 3230          |
|    policy_gradient_loss | -0.000275     |
|    value_loss           | 2.25          |
-------------------------------------------
Progress update from timestep 129500: Updated shield, loss is 0.07262980937957764
Progress update from timestep 129600: Updated shield, loss is 0.08311609923839569
Eval num_timesteps=129600, episode_reward=19.53 +/- 13.95
Episode length: 28.20 +/- 19.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.2     |
|    mean_reward     | 19.5     |
| time/              |          |
|    total_timesteps | 129600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.74     |
|    ep_rew_mean     | 3.07     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 324      |
|    time_elapsed    | 25659    |
|    total_timesteps | 129600   |
---------------------------------
Progress update from timestep 129700: Updated shield, loss is 0.07014975696802139
Progress update from timestep 129800: Updated shield, loss is 0.07502150535583496
Eval num_timesteps=129800, episode_reward=22.57 +/- 14.71
Episode length: 33.00 +/- 21.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33            |
|    mean_reward          | 22.6          |
| time/                   |               |
|    total_timesteps      | 129800        |
| train/                  |               |
|    approx_kl            | 0.00033856655 |
|    clip_fraction        | 0.0125        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0662       |
|    explained_variance   | 0.183         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.977         |
|    n_updates            | 3240          |
|    policy_gradient_loss | -0.00181      |
|    value_loss           | 1.9           |
-------------------------------------------
Progress update from timestep 129900: Updated shield, loss is 0.070822574198246
Progress update from timestep 130000: Updated shield, loss is 0.07184452563524246
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 130000: Saved shield_loss.csv. 
Eval num_timesteps=130000, episode_reward=11.57 +/- 12.17
Episode length: 17.00 +/- 16.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | 11.6     |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.18     |
|    ep_rew_mean     | 3.38     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 325      |
|    time_elapsed    | 25750    |
|    total_timesteps | 130000   |
---------------------------------
Progress update from timestep 130100: Updated shield, loss is 0.06358710676431656
Progress update from timestep 130200: Updated shield, loss is 0.06871647387742996
Eval num_timesteps=130200, episode_reward=4.43 +/- 2.54
Episode length: 7.00 +/- 3.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 7             |
|    mean_reward          | 4.43          |
| time/                   |               |
|    total_timesteps      | 130200        |
| train/                  |               |
|    approx_kl            | 0.00016133195 |
|    clip_fraction        | 0.00759       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0459       |
|    explained_variance   | 0.136         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.18          |
|    n_updates            | 3250          |
|    policy_gradient_loss | -0.00116      |
|    value_loss           | 2.7           |
-------------------------------------------
Progress update from timestep 130300: Updated shield, loss is 0.06968772411346436
Progress update from timestep 130400: Updated shield, loss is 0.07562973350286484
Eval num_timesteps=130400, episode_reward=16.29 +/- 13.97
Episode length: 24.60 +/- 20.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 16.3     |
| time/              |          |
|    total_timesteps | 130400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.29     |
|    ep_rew_mean     | 3.58     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 326      |
|    time_elapsed    | 25823    |
|    total_timesteps | 130400   |
---------------------------------
Progress update from timestep 130500: Updated shield, loss is 0.06651998311281204
Progress update from timestep 130600: Updated shield, loss is 0.07171257585287094
Eval num_timesteps=130600, episode_reward=16.18 +/- 14.58
Episode length: 24.40 +/- 21.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.4         |
|    mean_reward          | 16.2         |
| time/                   |              |
|    total_timesteps      | 130600       |
| train/                  |              |
|    approx_kl            | 7.888575e-05 |
|    clip_fraction        | 0.00335      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0289      |
|    explained_variance   | 0.116        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.27         |
|    n_updates            | 3260         |
|    policy_gradient_loss | -0.000248    |
|    value_loss           | 2.13         |
------------------------------------------
Progress update from timestep 130700: Updated shield, loss is 0.07589266449213028
Progress update from timestep 130800: Updated shield, loss is 0.06592250615358353
Eval num_timesteps=130800, episode_reward=24.99 +/- 14.65
Episode length: 34.60 +/- 19.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 25       |
| time/              |          |
|    total_timesteps | 130800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.13     |
|    ep_rew_mean     | 3.44     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 327      |
|    time_elapsed    | 25917    |
|    total_timesteps | 130800   |
---------------------------------
Progress update from timestep 130900: Updated shield, loss is 0.07868457585573196
Progress update from timestep 131000: Updated shield, loss is 0.06158018112182617
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 131000: Saved shield_loss.csv. 
Eval num_timesteps=131000, episode_reward=10.80 +/- 12.37
Episode length: 16.20 +/- 17.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.2          |
|    mean_reward          | 10.8          |
| time/                   |               |
|    total_timesteps      | 131000        |
| train/                  |               |
|    approx_kl            | 9.2433336e-05 |
|    clip_fraction        | 0.00357       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0348       |
|    explained_variance   | 0.228         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.36          |
|    n_updates            | 3270          |
|    policy_gradient_loss | -0.000608     |
|    value_loss           | 2.11          |
-------------------------------------------
Progress update from timestep 131100: Updated shield, loss is 0.06694115698337555
Progress update from timestep 131200: Updated shield, loss is 0.07060593366622925
Eval num_timesteps=131200, episode_reward=21.36 +/- 11.02
Episode length: 31.00 +/- 15.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31       |
|    mean_reward     | 21.4     |
| time/              |          |
|    total_timesteps | 131200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.88     |
|    ep_rew_mean     | 3.16     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 328      |
|    time_elapsed    | 26020    |
|    total_timesteps | 131200   |
---------------------------------
Progress update from timestep 131300: Updated shield, loss is 0.07210534811019897
Progress update from timestep 131400: Updated shield, loss is 0.08491910248994827
Eval num_timesteps=131400, episode_reward=5.40 +/- 3.08
Episode length: 8.80 +/- 4.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 8.8         |
|    mean_reward          | 5.4         |
| time/                   |             |
|    total_timesteps      | 131400      |
| train/                  |             |
|    approx_kl            | 7.81353e-05 |
|    clip_fraction        | 0.00692     |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0364     |
|    explained_variance   | 0.215       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.23        |
|    n_updates            | 3280        |
|    policy_gradient_loss | -0.00074    |
|    value_loss           | 1.99        |
-----------------------------------------
Progress update from timestep 131500: Updated shield, loss is 0.08445006608963013
Progress update from timestep 131600: Updated shield, loss is 0.06899898499250412
Eval num_timesteps=131600, episode_reward=12.65 +/- 13.57
Episode length: 19.00 +/- 19.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 12.7     |
| time/              |          |
|    total_timesteps | 131600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.67     |
|    ep_rew_mean     | 2.99     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 329      |
|    time_elapsed    | 26099    |
|    total_timesteps | 131600   |
---------------------------------
Progress update from timestep 131700: Updated shield, loss is 0.07532758265733719
Progress update from timestep 131800: Updated shield, loss is 0.08540373295545578
Eval num_timesteps=131800, episode_reward=15.38 +/- 15.29
Episode length: 23.20 +/- 22.10
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.2          |
|    mean_reward          | 15.4          |
| time/                   |               |
|    total_timesteps      | 131800        |
| train/                  |               |
|    approx_kl            | 9.0473666e-05 |
|    clip_fraction        | 0.00223       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0442       |
|    explained_variance   | 0.241         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.765         |
|    n_updates            | 3290          |
|    policy_gradient_loss | -0.000668     |
|    value_loss           | 1.65          |
-------------------------------------------
Progress update from timestep 131900: Updated shield, loss is 0.08490799367427826
Progress update from timestep 132000: Updated shield, loss is 0.07956095784902573
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 132000: Saved shield_loss.csv. 
Eval num_timesteps=132000, episode_reward=13.86 +/- 11.05
Episode length: 20.40 +/- 15.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 13.9     |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.02     |
|    ep_rew_mean     | 3.29     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 330      |
|    time_elapsed    | 26193    |
|    total_timesteps | 132000   |
---------------------------------
Progress update from timestep 132100: Updated shield, loss is 0.08117464184761047
Progress update from timestep 132200: Updated shield, loss is 0.0729975551366806
Eval num_timesteps=132200, episode_reward=18.16 +/- 13.46
Episode length: 26.80 +/- 19.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.8          |
|    mean_reward          | 18.2          |
| time/                   |               |
|    total_timesteps      | 132200        |
| train/                  |               |
|    approx_kl            | 3.7342244e-05 |
|    clip_fraction        | 0.00112       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0271       |
|    explained_variance   | 0.353         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.902         |
|    n_updates            | 3300          |
|    policy_gradient_loss | -0.000366     |
|    value_loss           | 2.57          |
-------------------------------------------
Progress update from timestep 132300: Updated shield, loss is 0.07565012574195862
Progress update from timestep 132400: Updated shield, loss is 0.07850712537765503
Eval num_timesteps=132400, episode_reward=17.29 +/- 15.35
Episode length: 24.80 +/- 21.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 17.3     |
| time/              |          |
|    total_timesteps | 132400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.23     |
|    ep_rew_mean     | 2.65     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 331      |
|    time_elapsed    | 26313    |
|    total_timesteps | 132400   |
---------------------------------
Progress update from timestep 132500: Updated shield, loss is 0.07542561739683151
Progress update from timestep 132600: Updated shield, loss is 0.07606686651706696
Eval num_timesteps=132600, episode_reward=10.98 +/- 11.92
Episode length: 16.40 +/- 17.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | 11           |
| time/                   |              |
|    total_timesteps      | 132600       |
| train/                  |              |
|    approx_kl            | 7.409366e-05 |
|    clip_fraction        | 0.00223      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0457      |
|    explained_variance   | 0.165        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.779        |
|    n_updates            | 3310         |
|    policy_gradient_loss | -0.000623    |
|    value_loss           | 2.03         |
------------------------------------------
Progress update from timestep 132700: Updated shield, loss is 0.06506027281284332
Progress update from timestep 132800: Updated shield, loss is 0.08052833378314972
Eval num_timesteps=132800, episode_reward=16.65 +/- 13.80
Episode length: 25.20 +/- 20.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 132800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.82     |
|    ep_rew_mean     | 3.11     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 332      |
|    time_elapsed    | 26402    |
|    total_timesteps | 132800   |
---------------------------------
Progress update from timestep 132900: Updated shield, loss is 0.06980174034833908
Progress update from timestep 133000: Updated shield, loss is 0.07031295448541641
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 133000: Saved shield_loss.csv. 
Eval num_timesteps=133000, episode_reward=12.39 +/- 12.45
Episode length: 18.20 +/- 16.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 18.2          |
|    mean_reward          | 12.4          |
| time/                   |               |
|    total_timesteps      | 133000        |
| train/                  |               |
|    approx_kl            | 0.00020383009 |
|    clip_fraction        | 0.00402       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0214       |
|    explained_variance   | 0.0128        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.918         |
|    n_updates            | 3320          |
|    policy_gradient_loss | -0.000814     |
|    value_loss           | 2.47          |
-------------------------------------------
Progress update from timestep 133100: Updated shield, loss is 0.07430964708328247
Progress update from timestep 133200: Updated shield, loss is 0.06407558172941208
Eval num_timesteps=133200, episode_reward=12.28 +/- 12.02
Episode length: 18.60 +/- 17.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 12.3     |
| time/              |          |
|    total_timesteps | 133200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.44     |
|    ep_rew_mean     | 2.83     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 333      |
|    time_elapsed    | 26497    |
|    total_timesteps | 133200   |
---------------------------------
Progress update from timestep 133300: Updated shield, loss is 0.07116729766130447
Progress update from timestep 133400: Updated shield, loss is 0.07127246260643005
Eval num_timesteps=133400, episode_reward=11.18 +/- 12.37
Episode length: 16.80 +/- 17.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.8         |
|    mean_reward          | 11.2         |
| time/                   |              |
|    total_timesteps      | 133400       |
| train/                  |              |
|    approx_kl            | 0.0001586651 |
|    clip_fraction        | 0.00313      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0262      |
|    explained_variance   | 0.181        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.783        |
|    n_updates            | 3330         |
|    policy_gradient_loss | -0.000627    |
|    value_loss           | 1.59         |
------------------------------------------
Progress update from timestep 133500: Updated shield, loss is 0.0696604996919632
Progress update from timestep 133600: Updated shield, loss is 0.07252059131860733
Eval num_timesteps=133600, episode_reward=23.02 +/- 15.03
Episode length: 33.20 +/- 20.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | 23       |
| time/              |          |
|    total_timesteps | 133600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.3      |
|    ep_rew_mean     | 2.69     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 334      |
|    time_elapsed    | 26587    |
|    total_timesteps | 133600   |
---------------------------------
Progress update from timestep 133700: Updated shield, loss is 0.06950213015079498
Progress update from timestep 133800: Updated shield, loss is 0.06343206018209457
Eval num_timesteps=133800, episode_reward=28.41 +/- 12.14
Episode length: 41.40 +/- 17.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 41.4          |
|    mean_reward          | 28.4          |
| time/                   |               |
|    total_timesteps      | 133800        |
| train/                  |               |
|    approx_kl            | 0.00015302867 |
|    clip_fraction        | 0.00379       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0474       |
|    explained_variance   | 0.0914        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.216         |
|    n_updates            | 3340          |
|    policy_gradient_loss | -0.000706     |
|    value_loss           | 1.5           |
-------------------------------------------
Progress update from timestep 133900: Updated shield, loss is 0.07161208987236023
Progress update from timestep 134000: Updated shield, loss is 0.07262250781059265
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 134000: Saved shield_loss.csv. 
Eval num_timesteps=134000, episode_reward=15.74 +/- 17.12
Episode length: 22.00 +/- 22.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22       |
|    mean_reward     | 15.7     |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.85     |
|    ep_rew_mean     | 3.18     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 335      |
|    time_elapsed    | 26698    |
|    total_timesteps | 134000   |
---------------------------------
Progress update from timestep 134100: Updated shield, loss is 0.06818005442619324
Progress update from timestep 134200: Updated shield, loss is 0.06465079635381699
Eval num_timesteps=134200, episode_reward=16.12 +/- 15.52
Episode length: 23.60 +/- 21.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.6          |
|    mean_reward          | 16.1          |
| time/                   |               |
|    total_timesteps      | 134200        |
| train/                  |               |
|    approx_kl            | 0.00024567032 |
|    clip_fraction        | 0.0152        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0492       |
|    explained_variance   | 0.182         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.45          |
|    n_updates            | 3350          |
|    policy_gradient_loss | -0.00136      |
|    value_loss           | 1.79          |
-------------------------------------------
Progress update from timestep 134300: Updated shield, loss is 0.07060963660478592
Progress update from timestep 134400: Updated shield, loss is 0.06723111122846603
Eval num_timesteps=134400, episode_reward=20.45 +/- 13.01
Episode length: 29.80 +/- 18.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.8     |
|    mean_reward     | 20.5     |
| time/              |          |
|    total_timesteps | 134400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.38     |
|    ep_rew_mean     | 2.75     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 336      |
|    time_elapsed    | 26795    |
|    total_timesteps | 134400   |
---------------------------------
Progress update from timestep 134500: Updated shield, loss is 0.06785521656274796
Progress update from timestep 134600: Updated shield, loss is 0.06274805217981339
Eval num_timesteps=134600, episode_reward=17.46 +/- 15.45
Episode length: 24.80 +/- 20.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.8          |
|    mean_reward          | 17.5          |
| time/                   |               |
|    total_timesteps      | 134600        |
| train/                  |               |
|    approx_kl            | 0.00010402388 |
|    clip_fraction        | 0.00357       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0242       |
|    explained_variance   | 0.0569        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.975         |
|    n_updates            | 3360          |
|    policy_gradient_loss | -0.000742     |
|    value_loss           | 2.08          |
-------------------------------------------
Progress update from timestep 134700: Updated shield, loss is 0.0672154650092125
Progress update from timestep 134800: Updated shield, loss is 0.06406432390213013
Eval num_timesteps=134800, episode_reward=15.91 +/- 15.73
Episode length: 23.20 +/- 21.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 15.9     |
| time/              |          |
|    total_timesteps | 134800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.8      |
|    ep_rew_mean     | 3.12     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 337      |
|    time_elapsed    | 26890    |
|    total_timesteps | 134800   |
---------------------------------
Progress update from timestep 134900: Updated shield, loss is 0.0746193528175354
Progress update from timestep 135000: Updated shield, loss is 0.06190633773803711
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 135000: Saved shield_loss.csv. 
Eval num_timesteps=135000, episode_reward=17.57 +/- 13.47
Episode length: 26.20 +/- 19.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.2          |
|    mean_reward          | 17.6          |
| time/                   |               |
|    total_timesteps      | 135000        |
| train/                  |               |
|    approx_kl            | 2.5348604e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00692      |
|    explained_variance   | 0.182         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.587         |
|    n_updates            | 3370          |
|    policy_gradient_loss | -2.3e-05      |
|    value_loss           | 2.29          |
-------------------------------------------
Progress update from timestep 135100: Updated shield, loss is 0.07107805460691452
Progress update from timestep 135200: Updated shield, loss is 0.06294802576303482
Eval num_timesteps=135200, episode_reward=19.42 +/- 12.79
Episode length: 28.40 +/- 18.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.4     |
|    mean_reward     | 19.4     |
| time/              |          |
|    total_timesteps | 135200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.91     |
|    ep_rew_mean     | 2.42     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 338      |
|    time_elapsed    | 26980    |
|    total_timesteps | 135200   |
---------------------------------
Progress update from timestep 135300: Updated shield, loss is 0.060798440128564835
Progress update from timestep 135400: Updated shield, loss is 0.07394734025001526
Eval num_timesteps=135400, episode_reward=10.63 +/- 13.22
Episode length: 15.20 +/- 17.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.2          |
|    mean_reward          | 10.6          |
| time/                   |               |
|    total_timesteps      | 135400        |
| train/                  |               |
|    approx_kl            | 0.00022082652 |
|    clip_fraction        | 0.0127        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0691       |
|    explained_variance   | 0.144         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.98          |
|    n_updates            | 3380          |
|    policy_gradient_loss | -0.00207      |
|    value_loss           | 1.8           |
-------------------------------------------
Progress update from timestep 135500: Updated shield, loss is 0.07513753324747086
Progress update from timestep 135600: Updated shield, loss is 0.06172148510813713
Eval num_timesteps=135600, episode_reward=24.05 +/- 14.72
Episode length: 34.00 +/- 19.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 24       |
| time/              |          |
|    total_timesteps | 135600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.57     |
|    ep_rew_mean     | 2.86     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 339      |
|    time_elapsed    | 27075    |
|    total_timesteps | 135600   |
---------------------------------
Progress update from timestep 135700: Updated shield, loss is 0.06125771626830101
Progress update from timestep 135800: Updated shield, loss is 0.06410949677228928
Eval num_timesteps=135800, episode_reward=8.75 +/- 4.57
Episode length: 13.40 +/- 6.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.4        |
|    mean_reward          | 8.75        |
| time/                   |             |
|    total_timesteps      | 135800      |
| train/                  |             |
|    approx_kl            | 0.000186151 |
|    clip_fraction        | 0.0058      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0667     |
|    explained_variance   | 0.151       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.942       |
|    n_updates            | 3390        |
|    policy_gradient_loss | -0.00121    |
|    value_loss           | 2.17        |
-----------------------------------------
Progress update from timestep 135900: Updated shield, loss is 0.059927165508270264
Progress update from timestep 136000: Updated shield, loss is 0.06618445366621017
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 136000: Saved shield_loss.csv. 
Eval num_timesteps=136000, episode_reward=15.31 +/- 15.68
Episode length: 22.80 +/- 22.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 15.3     |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.68     |
|    ep_rew_mean     | 3.09     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 340      |
|    time_elapsed    | 27160    |
|    total_timesteps | 136000   |
---------------------------------
Progress update from timestep 136100: Updated shield, loss is 0.0771750882267952
Progress update from timestep 136200: Updated shield, loss is 0.06395655125379562
Eval num_timesteps=136200, episode_reward=17.10 +/- 14.40
Episode length: 25.20 +/- 20.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.2         |
|    mean_reward          | 17.1         |
| time/                   |              |
|    total_timesteps      | 136200       |
| train/                  |              |
|    approx_kl            | 1.622058e-05 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0221      |
|    explained_variance   | 0.129        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.778        |
|    n_updates            | 3400         |
|    policy_gradient_loss | -0.00032     |
|    value_loss           | 2.36         |
------------------------------------------
Progress update from timestep 136300: Updated shield, loss is 0.07422763854265213
Progress update from timestep 136400: Updated shield, loss is 0.06817840784788132
Eval num_timesteps=136400, episode_reward=11.40 +/- 13.05
Episode length: 16.40 +/- 17.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | 11.4     |
| time/              |          |
|    total_timesteps | 136400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.28     |
|    ep_rew_mean     | 2.72     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 341      |
|    time_elapsed    | 27256    |
|    total_timesteps | 136400   |
---------------------------------
Progress update from timestep 136500: Updated shield, loss is 0.06629317253828049
Progress update from timestep 136600: Updated shield, loss is 0.07358389347791672
Eval num_timesteps=136600, episode_reward=23.69 +/- 13.70
Episode length: 34.40 +/- 19.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.4         |
|    mean_reward          | 23.7         |
| time/                   |              |
|    total_timesteps      | 136600       |
| train/                  |              |
|    approx_kl            | 8.234468e-05 |
|    clip_fraction        | 0.0029       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0222      |
|    explained_variance   | 0.118        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.04         |
|    n_updates            | 3410         |
|    policy_gradient_loss | -0.000395    |
|    value_loss           | 1.89         |
------------------------------------------
Progress update from timestep 136700: Updated shield, loss is 0.061474498361349106
Progress update from timestep 136800: Updated shield, loss is 0.06263964623212814
Eval num_timesteps=136800, episode_reward=17.73 +/- 15.05
Episode length: 25.20 +/- 20.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 17.7     |
| time/              |          |
|    total_timesteps | 136800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.8      |
|    ep_rew_mean     | 2.33     |
| time/              |          |
|    fps             | 5        |
|    iterations      | 342      |
|    time_elapsed    | 27355    |
|    total_timesteps | 136800   |
---------------------------------
Progress update from timestep 136900: Updated shield, loss is 0.06401943415403366
Progress update from timestep 137000: Updated shield, loss is 0.0753893181681633
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 137000: Saved shield_loss.csv. 
Eval num_timesteps=137000, episode_reward=17.58 +/- 13.92
Episode length: 26.00 +/- 20.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26           |
|    mean_reward          | 17.6         |
| time/                   |              |
|    total_timesteps      | 137000       |
| train/                  |              |
|    approx_kl            | 0.0006554191 |
|    clip_fraction        | 0.0279       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0853      |
|    explained_variance   | 0.169        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.419        |
|    n_updates            | 3420         |
|    policy_gradient_loss | -0.00338     |
|    value_loss           | 1.14         |
------------------------------------------
Progress update from timestep 137100: Updated shield, loss is 0.06156862527132034
Progress update from timestep 137200: Updated shield, loss is 0.0667017474770546
Eval num_timesteps=137200, episode_reward=10.16 +/- 13.29
Episode length: 14.80 +/- 17.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 10.2     |
| time/              |          |
|    total_timesteps | 137200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.9      |
|    ep_rew_mean     | 2.46     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 343      |
|    time_elapsed    | 27449    |
|    total_timesteps | 137200   |
---------------------------------
Progress update from timestep 137300: Updated shield, loss is 0.06806630641222
Progress update from timestep 137400: Updated shield, loss is 0.0679839700460434
Eval num_timesteps=137400, episode_reward=22.80 +/- 13.88
Episode length: 33.60 +/- 20.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.6         |
|    mean_reward          | 22.8         |
| time/                   |              |
|    total_timesteps      | 137400       |
| train/                  |              |
|    approx_kl            | 0.0004223647 |
|    clip_fraction        | 0.00826      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0371      |
|    explained_variance   | 0.2          |
|    learning_rate        | 0.0001       |
|    loss                 | 0.94         |
|    n_updates            | 3430         |
|    policy_gradient_loss | -0.00141     |
|    value_loss           | 1.4          |
------------------------------------------
Progress update from timestep 137500: Updated shield, loss is 0.07570710778236389
Progress update from timestep 137600: Updated shield, loss is 0.05206279456615448
Eval num_timesteps=137600, episode_reward=11.13 +/- 12.10
Episode length: 16.80 +/- 17.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | 11.1     |
| time/              |          |
|    total_timesteps | 137600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.19     |
|    ep_rew_mean     | 2.65     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 344      |
|    time_elapsed    | 27542    |
|    total_timesteps | 137600   |
---------------------------------
Progress update from timestep 137700: Updated shield, loss is 0.061344072222709656
Progress update from timestep 137800: Updated shield, loss is 0.05764580890536308
Eval num_timesteps=137800, episode_reward=22.66 +/- 14.54
Episode length: 33.20 +/- 20.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.2         |
|    mean_reward          | 22.7         |
| time/                   |              |
|    total_timesteps      | 137800       |
| train/                  |              |
|    approx_kl            | 0.0001392895 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0355      |
|    explained_variance   | 0.172        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.729        |
|    n_updates            | 3440         |
|    policy_gradient_loss | -0.000523    |
|    value_loss           | 1.7          |
------------------------------------------
Progress update from timestep 137900: Updated shield, loss is 0.05911993235349655
Progress update from timestep 138000: Updated shield, loss is 0.06475367397069931
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 138000: Saved shield_loss.csv. 
Eval num_timesteps=138000, episode_reward=12.24 +/- 11.71
Episode length: 18.00 +/- 16.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | 12.2     |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.65     |
|    ep_rew_mean     | 2.99     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 345      |
|    time_elapsed    | 27631    |
|    total_timesteps | 138000   |
---------------------------------
Progress update from timestep 138100: Updated shield, loss is 0.06007443740963936
Progress update from timestep 138200: Updated shield, loss is 0.0686594620347023
Eval num_timesteps=138200, episode_reward=11.28 +/- 11.32
Episode length: 17.40 +/- 16.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.4         |
|    mean_reward          | 11.3         |
| time/                   |              |
|    total_timesteps      | 138200       |
| train/                  |              |
|    approx_kl            | 5.979761e-05 |
|    clip_fraction        | 0.00335      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0239      |
|    explained_variance   | 0.163        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.793        |
|    n_updates            | 3450         |
|    policy_gradient_loss | -0.000553    |
|    value_loss           | 1.87         |
------------------------------------------
Progress update from timestep 138300: Updated shield, loss is 0.06700805574655533
Progress update from timestep 138400: Updated shield, loss is 0.06829608231782913
Eval num_timesteps=138400, episode_reward=16.05 +/- 14.66
Episode length: 24.00 +/- 21.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 138400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.75     |
|    ep_rew_mean     | 3.09     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 346      |
|    time_elapsed    | 27723    |
|    total_timesteps | 138400   |
---------------------------------
Progress update from timestep 138500: Updated shield, loss is 0.07594022154808044
Progress update from timestep 138600: Updated shield, loss is 0.07267001271247864
Eval num_timesteps=138600, episode_reward=6.15 +/- 3.40
Episode length: 9.60 +/- 4.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 9.6           |
|    mean_reward          | 6.15          |
| time/                   |               |
|    total_timesteps      | 138600        |
| train/                  |               |
|    approx_kl            | 4.3451037e-05 |
|    clip_fraction        | 0.00357       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0284       |
|    explained_variance   | 0.162         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.783         |
|    n_updates            | 3460          |
|    policy_gradient_loss | -0.000524     |
|    value_loss           | 2.71          |
-------------------------------------------
Progress update from timestep 138700: Updated shield, loss is 0.07793222367763519
Progress update from timestep 138800: Updated shield, loss is 0.07402141392230988
Eval num_timesteps=138800, episode_reward=10.28 +/- 11.90
Episode length: 16.00 +/- 17.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | 10.3     |
| time/              |          |
|    total_timesteps | 138800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.97     |
|    ep_rew_mean     | 2.44     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 347      |
|    time_elapsed    | 27799    |
|    total_timesteps | 138800   |
---------------------------------
Progress update from timestep 138900: Updated shield, loss is 0.06998244673013687
Progress update from timestep 139000: Updated shield, loss is 0.07439407706260681
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 139000: Saved shield_loss.csv. 
Eval num_timesteps=139000, episode_reward=10.29 +/- 11.57
Episode length: 16.00 +/- 17.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | 10.3         |
| time/                   |              |
|    total_timesteps      | 139000       |
| train/                  |              |
|    approx_kl            | 9.453475e-05 |
|    clip_fraction        | 0.00246      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0192      |
|    explained_variance   | 0.145        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.48         |
|    n_updates            | 3470         |
|    policy_gradient_loss | -0.00028     |
|    value_loss           | 2.2          |
------------------------------------------
Progress update from timestep 139100: Updated shield, loss is 0.07790698856115341
Progress update from timestep 139200: Updated shield, loss is 0.06762056052684784
Eval num_timesteps=139200, episode_reward=10.47 +/- 12.76
Episode length: 15.60 +/- 17.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 10.5     |
| time/              |          |
|    total_timesteps | 139200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.61     |
|    ep_rew_mean     | 2.9      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 348      |
|    time_elapsed    | 27888    |
|    total_timesteps | 139200   |
---------------------------------
Progress update from timestep 139300: Updated shield, loss is 0.06411760300397873
Progress update from timestep 139400: Updated shield, loss is 0.0652097836136818
Eval num_timesteps=139400, episode_reward=27.88 +/- 13.25
Episode length: 40.60 +/- 18.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 40.6          |
|    mean_reward          | 27.9          |
| time/                   |               |
|    total_timesteps      | 139400        |
| train/                  |               |
|    approx_kl            | 2.2148048e-05 |
|    clip_fraction        | 0.000446      |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0242       |
|    explained_variance   | 0.104         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.877         |
|    n_updates            | 3480          |
|    policy_gradient_loss | -0.000162     |
|    value_loss           | 2.39          |
-------------------------------------------
Progress update from timestep 139500: Updated shield, loss is 0.05928756296634674
Progress update from timestep 139600: Updated shield, loss is 0.05790913850069046
Eval num_timesteps=139600, episode_reward=8.02 +/- 6.56
Episode length: 12.00 +/- 8.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 12       |
|    mean_reward     | 8.02     |
| time/              |          |
|    total_timesteps | 139600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.78     |
|    ep_rew_mean     | 3.06     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 349      |
|    time_elapsed    | 27969    |
|    total_timesteps | 139600   |
---------------------------------
Progress update from timestep 139700: Updated shield, loss is 0.06803534179925919
Progress update from timestep 139800: Updated shield, loss is 0.05281495302915573
Eval num_timesteps=139800, episode_reward=17.75 +/- 14.46
Episode length: 25.80 +/- 20.25
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.8          |
|    mean_reward          | 17.8          |
| time/                   |               |
|    total_timesteps      | 139800        |
| train/                  |               |
|    approx_kl            | 0.00032276797 |
|    clip_fraction        | 0.0103        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0402       |
|    explained_variance   | 0.104         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.733         |
|    n_updates            | 3490          |
|    policy_gradient_loss | -0.00135      |
|    value_loss           | 1.95          |
-------------------------------------------
Progress update from timestep 139900: Updated shield, loss is 0.05750705301761627
Progress update from timestep 140000: Updated shield, loss is 0.05519897863268852
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 140000: Saved shield_loss.csv. 
Eval num_timesteps=140000, episode_reward=15.78 +/- 15.44
Episode length: 23.40 +/- 22.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 15.8     |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.3      |
|    ep_rew_mean     | 2.78     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 350      |
|    time_elapsed    | 28071    |
|    total_timesteps | 140000   |
---------------------------------
Progress update from timestep 140100: Updated shield, loss is 0.07591662555932999
Progress update from timestep 140200: Updated shield, loss is 0.06181276962161064
Eval num_timesteps=140200, episode_reward=28.16 +/- 12.07
Episode length: 41.40 +/- 17.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.4         |
|    mean_reward          | 28.2         |
| time/                   |              |
|    total_timesteps      | 140200       |
| train/                  |              |
|    approx_kl            | 7.182599e-05 |
|    clip_fraction        | 0.00201      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0264      |
|    explained_variance   | 0.21         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.62         |
|    n_updates            | 3500         |
|    policy_gradient_loss | -0.000441    |
|    value_loss           | 1.57         |
------------------------------------------
Progress update from timestep 140300: Updated shield, loss is 0.0710354745388031
Progress update from timestep 140400: Updated shield, loss is 0.06571167707443237
Eval num_timesteps=140400, episode_reward=9.44 +/- 12.03
Episode length: 14.80 +/- 17.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 9.44     |
| time/              |          |
|    total_timesteps | 140400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.42     |
|    ep_rew_mean     | 2.82     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 351      |
|    time_elapsed    | 28164    |
|    total_timesteps | 140400   |
---------------------------------
Progress update from timestep 140500: Updated shield, loss is 0.07044468075037003
Progress update from timestep 140600: Updated shield, loss is 0.058314524590969086
Eval num_timesteps=140600, episode_reward=14.76 +/- 16.14
Episode length: 22.00 +/- 22.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22           |
|    mean_reward          | 14.8         |
| time/                   |              |
|    total_timesteps      | 140600       |
| train/                  |              |
|    approx_kl            | 0.0001541584 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.013       |
|    explained_variance   | 0.268        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.856        |
|    n_updates            | 3510         |
|    policy_gradient_loss | -0.000167    |
|    value_loss           | 1.64         |
------------------------------------------
Progress update from timestep 140700: Updated shield, loss is 0.05989983305335045
Progress update from timestep 140800: Updated shield, loss is 0.06541592627763748
Eval num_timesteps=140800, episode_reward=27.72 +/- 13.53
Episode length: 40.40 +/- 19.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.4     |
|    mean_reward     | 27.7     |
| time/              |          |
|    total_timesteps | 140800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.45     |
|    ep_rew_mean     | 2.79     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 352      |
|    time_elapsed    | 28254    |
|    total_timesteps | 140800   |
---------------------------------
Progress update from timestep 140900: Updated shield, loss is 0.06307907402515411
Progress update from timestep 141000: Updated shield, loss is 0.06646528095006943
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 141000: Saved shield_loss.csv. 
Eval num_timesteps=141000, episode_reward=19.09 +/- 14.41
Episode length: 27.80 +/- 19.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 27.8         |
|    mean_reward          | 19.1         |
| time/                   |              |
|    total_timesteps      | 141000       |
| train/                  |              |
|    approx_kl            | 0.0002631445 |
|    clip_fraction        | 0.0058       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0347      |
|    explained_variance   | 0.00727      |
|    learning_rate        | 0.0001       |
|    loss                 | 0.509        |
|    n_updates            | 3520         |
|    policy_gradient_loss | -0.0013      |
|    value_loss           | 1.88         |
------------------------------------------
Progress update from timestep 141100: Updated shield, loss is 0.062472153455019
Progress update from timestep 141200: Updated shield, loss is 0.06994057446718216
Eval num_timesteps=141200, episode_reward=18.81 +/- 12.61
Episode length: 27.80 +/- 18.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.8     |
|    mean_reward     | 18.8     |
| time/              |          |
|    total_timesteps | 141200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.93     |
|    ep_rew_mean     | 3.25     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 353      |
|    time_elapsed    | 28352    |
|    total_timesteps | 141200   |
---------------------------------
Progress update from timestep 141300: Updated shield, loss is 0.05707649886608124
Progress update from timestep 141400: Updated shield, loss is 0.07682449370622635
Eval num_timesteps=141400, episode_reward=22.58 +/- 15.05
Episode length: 32.80 +/- 21.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.8         |
|    mean_reward          | 22.6         |
| time/                   |              |
|    total_timesteps      | 141400       |
| train/                  |              |
|    approx_kl            | 6.143381e-05 |
|    clip_fraction        | 0.00357      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0169      |
|    explained_variance   | 0.0559       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.891        |
|    n_updates            | 3530         |
|    policy_gradient_loss | -0.000281    |
|    value_loss           | 2.31         |
------------------------------------------
Progress update from timestep 141500: Updated shield, loss is 0.07406012713909149
Progress update from timestep 141600: Updated shield, loss is 0.06214737519621849
Eval num_timesteps=141600, episode_reward=18.66 +/- 14.36
Episode length: 26.80 +/- 19.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.8     |
|    mean_reward     | 18.7     |
| time/              |          |
|    total_timesteps | 141600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.29     |
|    ep_rew_mean     | 2.65     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 354      |
|    time_elapsed    | 28464    |
|    total_timesteps | 141600   |
---------------------------------
Progress update from timestep 141700: Updated shield, loss is 0.056084759533405304
Progress update from timestep 141800: Updated shield, loss is 0.066865473985672
Eval num_timesteps=141800, episode_reward=28.73 +/- 11.51
Episode length: 41.80 +/- 16.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 41.8          |
|    mean_reward          | 28.7          |
| time/                   |               |
|    total_timesteps      | 141800        |
| train/                  |               |
|    approx_kl            | 0.00020007751 |
|    clip_fraction        | 0.00714       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0468       |
|    explained_variance   | 0.102         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.589         |
|    n_updates            | 3540          |
|    policy_gradient_loss | -0.00118      |
|    value_loss           | 1.97          |
-------------------------------------------
Progress update from timestep 141900: Updated shield, loss is 0.06942961364984512
Progress update from timestep 142000: Updated shield, loss is 0.06964625418186188
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 142000: Saved shield_loss.csv. 
Eval num_timesteps=142000, episode_reward=12.10 +/- 13.22
Episode length: 17.40 +/- 17.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 12.1     |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.91     |
|    ep_rew_mean     | 2.41     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 355      |
|    time_elapsed    | 28576    |
|    total_timesteps | 142000   |
---------------------------------
Progress update from timestep 142100: Updated shield, loss is 0.08210720121860504
Progress update from timestep 142200: Updated shield, loss is 0.07583627849817276
Eval num_timesteps=142200, episode_reward=9.21 +/- 12.83
Episode length: 14.00 +/- 18.28
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14            |
|    mean_reward          | 9.21          |
| time/                   |               |
|    total_timesteps      | 142200        |
| train/                  |               |
|    approx_kl            | 0.00042273398 |
|    clip_fraction        | 0.00848       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0516       |
|    explained_variance   | 0.157         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.22          |
|    n_updates            | 3550          |
|    policy_gradient_loss | -0.00241      |
|    value_loss           | 1.99          |
-------------------------------------------
Progress update from timestep 142300: Updated shield, loss is 0.06175504997372627
Progress update from timestep 142400: Updated shield, loss is 0.08072876930236816
Eval num_timesteps=142400, episode_reward=12.52 +/- 11.12
Episode length: 19.00 +/- 16.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 12.5     |
| time/              |          |
|    total_timesteps | 142400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.51     |
|    ep_rew_mean     | 2.93     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 356      |
|    time_elapsed    | 28674    |
|    total_timesteps | 142400   |
---------------------------------
Progress update from timestep 142500: Updated shield, loss is 0.06806515157222748
Progress update from timestep 142600: Updated shield, loss is 0.07394517213106155
Eval num_timesteps=142600, episode_reward=3.36 +/- 0.93
Episode length: 5.80 +/- 1.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 5.8           |
|    mean_reward          | 3.36          |
| time/                   |               |
|    total_timesteps      | 142600        |
| train/                  |               |
|    approx_kl            | 3.9448696e-05 |
|    clip_fraction        | 0.00268       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0271       |
|    explained_variance   | 0.145         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.604         |
|    n_updates            | 3560          |
|    policy_gradient_loss | -0.000576     |
|    value_loss           | 2.21          |
-------------------------------------------
Progress update from timestep 142700: Updated shield, loss is 0.06355519592761993
Progress update from timestep 142800: Updated shield, loss is 0.07466824352741241
Eval num_timesteps=142800, episode_reward=24.15 +/- 13.12
Episode length: 35.00 +/- 18.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 24.2     |
| time/              |          |
|    total_timesteps | 142800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.77     |
|    ep_rew_mean     | 3.14     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 357      |
|    time_elapsed    | 28752    |
|    total_timesteps | 142800   |
---------------------------------
Progress update from timestep 142900: Updated shield, loss is 0.05872016400098801
Progress update from timestep 143000: Updated shield, loss is 0.06558843702077866
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 143000: Saved shield_loss.csv. 
Eval num_timesteps=143000, episode_reward=13.69 +/- 12.83
Episode length: 20.20 +/- 18.30
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 20.2          |
|    mean_reward          | 13.7          |
| time/                   |               |
|    total_timesteps      | 143000        |
| train/                  |               |
|    approx_kl            | 0.00023037438 |
|    clip_fraction        | 0.00692       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0387       |
|    explained_variance   | 0.127         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.22          |
|    n_updates            | 3570          |
|    policy_gradient_loss | -0.00117      |
|    value_loss           | 2.43          |
-------------------------------------------
Progress update from timestep 143100: Updated shield, loss is 0.07116880267858505
Progress update from timestep 143200: Updated shield, loss is 0.05475655198097229
Eval num_timesteps=143200, episode_reward=22.55 +/- 15.08
Episode length: 32.80 +/- 21.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 22.6     |
| time/              |          |
|    total_timesteps | 143200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.58     |
|    ep_rew_mean     | 2.88     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 358      |
|    time_elapsed    | 28859    |
|    total_timesteps | 143200   |
---------------------------------
Progress update from timestep 143300: Updated shield, loss is 0.06981215626001358
Progress update from timestep 143400: Updated shield, loss is 0.06378636509180069
Eval num_timesteps=143400, episode_reward=23.68 +/- 12.90
Episode length: 35.00 +/- 18.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35            |
|    mean_reward          | 23.7          |
| time/                   |               |
|    total_timesteps      | 143400        |
| train/                  |               |
|    approx_kl            | 3.0289604e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00119      |
|    explained_variance   | 0.131         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.663         |
|    n_updates            | 3580          |
|    policy_gradient_loss | -1.64e-05     |
|    value_loss           | 1.97          |
-------------------------------------------
Progress update from timestep 143500: Updated shield, loss is 0.06998641043901443
Progress update from timestep 143600: Updated shield, loss is 0.06473766267299652
Eval num_timesteps=143600, episode_reward=8.98 +/- 4.66
Episode length: 13.80 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.8     |
|    mean_reward     | 8.98     |
| time/              |          |
|    total_timesteps | 143600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.39     |
|    ep_rew_mean     | 2.77     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 359      |
|    time_elapsed    | 28938    |
|    total_timesteps | 143600   |
---------------------------------
Progress update from timestep 143700: Updated shield, loss is 0.05842800810933113
Progress update from timestep 143800: Updated shield, loss is 0.07336457073688507
Eval num_timesteps=143800, episode_reward=9.56 +/- 12.62
Episode length: 14.60 +/- 17.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.6         |
|    mean_reward          | 9.56         |
| time/                   |              |
|    total_timesteps      | 143800       |
| train/                  |              |
|    approx_kl            | 7.037547e-05 |
|    clip_fraction        | 0.00134      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.027       |
|    explained_variance   | 0.206        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.722        |
|    n_updates            | 3590         |
|    policy_gradient_loss | -0.00034     |
|    value_loss           | 2.4          |
------------------------------------------
Progress update from timestep 143900: Updated shield, loss is 0.06495092809200287
Progress update from timestep 144000: Updated shield, loss is 0.0615648590028286
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 144000: Saved shield_loss.csv. 
Eval num_timesteps=144000, episode_reward=12.35 +/- 11.15
Episode length: 18.80 +/- 16.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.8     |
|    mean_reward     | 12.4     |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.44     |
|    ep_rew_mean     | 2.78     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 360      |
|    time_elapsed    | 29033    |
|    total_timesteps | 144000   |
---------------------------------
Progress update from timestep 144100: Updated shield, loss is 0.0675082728266716
Progress update from timestep 144200: Updated shield, loss is 0.07113376259803772
Eval num_timesteps=144200, episode_reward=10.56 +/- 13.17
Episode length: 15.20 +/- 17.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.2         |
|    mean_reward          | 10.6         |
| time/                   |              |
|    total_timesteps      | 144200       |
| train/                  |              |
|    approx_kl            | 0.0003384675 |
|    clip_fraction        | 0.00826      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0345      |
|    explained_variance   | 0.00779      |
|    learning_rate        | 0.0001       |
|    loss                 | 0.263        |
|    n_updates            | 3600         |
|    policy_gradient_loss | -0.00153     |
|    value_loss           | 1.63         |
------------------------------------------
Progress update from timestep 144300: Updated shield, loss is 0.05916452035307884
Progress update from timestep 144400: Updated shield, loss is 0.061025332659482956
Eval num_timesteps=144400, episode_reward=22.48 +/- 16.12
Episode length: 32.00 +/- 22.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32       |
|    mean_reward     | 22.5     |
| time/              |          |
|    total_timesteps | 144400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5        |
|    ep_rew_mean     | 3.18     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 361      |
|    time_elapsed    | 29128    |
|    total_timesteps | 144400   |
---------------------------------
Progress update from timestep 144500: Updated shield, loss is 0.05734843760728836
Progress update from timestep 144600: Updated shield, loss is 0.06176954135298729
Eval num_timesteps=144600, episode_reward=17.06 +/- 15.67
Episode length: 24.40 +/- 21.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.4          |
|    mean_reward          | 17.1          |
| time/                   |               |
|    total_timesteps      | 144600        |
| train/                  |               |
|    approx_kl            | 2.7442034e-05 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0204       |
|    explained_variance   | 0.278         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.74          |
|    n_updates            | 3610          |
|    policy_gradient_loss | -0.000287     |
|    value_loss           | 2.66          |
-------------------------------------------
Progress update from timestep 144700: Updated shield, loss is 0.06262272596359253
Progress update from timestep 144800: Updated shield, loss is 0.06286783516407013
Eval num_timesteps=144800, episode_reward=23.60 +/- 14.31
Episode length: 34.00 +/- 19.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 23.6     |
| time/              |          |
|    total_timesteps | 144800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.66     |
|    ep_rew_mean     | 2.25     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 362      |
|    time_elapsed    | 29227    |
|    total_timesteps | 144800   |
---------------------------------
Progress update from timestep 144900: Updated shield, loss is 0.061687763780355453
Progress update from timestep 145000: Updated shield, loss is 0.06908401101827621
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 145000: Saved shield_loss.csv. 
Eval num_timesteps=145000, episode_reward=15.75 +/- 15.29
Episode length: 23.20 +/- 21.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.2         |
|    mean_reward          | 15.8         |
| time/                   |              |
|    total_timesteps      | 145000       |
| train/                  |              |
|    approx_kl            | 0.0002602095 |
|    clip_fraction        | 0.00893      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0546      |
|    explained_variance   | 0.121        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.17         |
|    n_updates            | 3620         |
|    policy_gradient_loss | -0.00185     |
|    value_loss           | 1.81         |
------------------------------------------
Progress update from timestep 145100: Updated shield, loss is 0.06012923642992973
Progress update from timestep 145200: Updated shield, loss is 0.06599145382642746
Eval num_timesteps=145200, episode_reward=23.73 +/- 14.53
Episode length: 33.60 +/- 20.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 23.7     |
| time/              |          |
|    total_timesteps | 145200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.01     |
|    ep_rew_mean     | 2.5      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 363      |
|    time_elapsed    | 29341    |
|    total_timesteps | 145200   |
---------------------------------
Progress update from timestep 145300: Updated shield, loss is 0.061357319355010986
Progress update from timestep 145400: Updated shield, loss is 0.05588291957974434
Eval num_timesteps=145400, episode_reward=10.11 +/- 12.29
Episode length: 15.40 +/- 17.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.4         |
|    mean_reward          | 10.1         |
| time/                   |              |
|    total_timesteps      | 145400       |
| train/                  |              |
|    approx_kl            | 0.0002721616 |
|    clip_fraction        | 0.00357      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.026       |
|    explained_variance   | 0.161        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.348        |
|    n_updates            | 3630         |
|    policy_gradient_loss | -0.000411    |
|    value_loss           | 1.2          |
------------------------------------------
Progress update from timestep 145500: Updated shield, loss is 0.0652942880988121
Progress update from timestep 145600: Updated shield, loss is 0.06592828035354614
Eval num_timesteps=145600, episode_reward=18.58 +/- 14.53
Episode length: 26.40 +/- 19.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.4     |
|    mean_reward     | 18.6     |
| time/              |          |
|    total_timesteps | 145600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.55     |
|    ep_rew_mean     | 2.89     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 364      |
|    time_elapsed    | 29434    |
|    total_timesteps | 145600   |
---------------------------------
Progress update from timestep 145700: Updated shield, loss is 0.061741288751363754
Progress update from timestep 145800: Updated shield, loss is 0.055215105414390564
Eval num_timesteps=145800, episode_reward=18.32 +/- 14.27
Episode length: 26.40 +/- 19.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.4          |
|    mean_reward          | 18.3          |
| time/                   |               |
|    total_timesteps      | 145800        |
| train/                  |               |
|    approx_kl            | 0.00016861952 |
|    clip_fraction        | 0.00402       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0335       |
|    explained_variance   | 0.139         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.17          |
|    n_updates            | 3640          |
|    policy_gradient_loss | -0.000604     |
|    value_loss           | 1.75          |
-------------------------------------------
Progress update from timestep 145900: Updated shield, loss is 0.06154205650091171
Progress update from timestep 146000: Updated shield, loss is 0.05658251419663429
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 146000: Saved shield_loss.csv. 
Eval num_timesteps=146000, episode_reward=21.98 +/- 14.43
Episode length: 33.00 +/- 20.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 22       |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.73     |
|    ep_rew_mean     | 2.97     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 365      |
|    time_elapsed    | 29553    |
|    total_timesteps | 146000   |
---------------------------------
Progress update from timestep 146100: Updated shield, loss is 0.05725933238863945
Progress update from timestep 146200: Updated shield, loss is 0.06520968675613403
Eval num_timesteps=146200, episode_reward=17.72 +/- 13.63
Episode length: 26.40 +/- 19.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.4          |
|    mean_reward          | 17.7          |
| time/                   |               |
|    total_timesteps      | 146200        |
| train/                  |               |
|    approx_kl            | 0.00022312252 |
|    clip_fraction        | 0.0103        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0552       |
|    explained_variance   | 0.346         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.3           |
|    n_updates            | 3650          |
|    policy_gradient_loss | -0.0016       |
|    value_loss           | 2.19          |
-------------------------------------------
Progress update from timestep 146300: Updated shield, loss is 0.05774041637778282
Progress update from timestep 146400: Updated shield, loss is 0.059317439794540405
Eval num_timesteps=146400, episode_reward=16.49 +/- 14.83
Episode length: 24.40 +/- 21.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 146400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.16     |
|    ep_rew_mean     | 2.63     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 366      |
|    time_elapsed    | 29648    |
|    total_timesteps | 146400   |
---------------------------------
Progress update from timestep 146500: Updated shield, loss is 0.06468752771615982
Progress update from timestep 146600: Updated shield, loss is 0.06602446734905243
Eval num_timesteps=146600, episode_reward=18.19 +/- 13.81
Episode length: 26.60 +/- 19.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.6         |
|    mean_reward          | 18.2         |
| time/                   |              |
|    total_timesteps      | 146600       |
| train/                  |              |
|    approx_kl            | 6.455707e-05 |
|    clip_fraction        | 0.00469      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0343      |
|    explained_variance   | -0.0575      |
|    learning_rate        | 0.0001       |
|    loss                 | 0.974        |
|    n_updates            | 3660         |
|    policy_gradient_loss | -0.000462    |
|    value_loss           | 2.07         |
------------------------------------------
Progress update from timestep 146700: Updated shield, loss is 0.06780422478914261
Progress update from timestep 146800: Updated shield, loss is 0.06134645640850067
Eval num_timesteps=146800, episode_reward=21.65 +/- 15.76
Episode length: 31.80 +/- 22.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.8     |
|    mean_reward     | 21.7     |
| time/              |          |
|    total_timesteps | 146800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.1      |
|    ep_rew_mean     | 2.54     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 367      |
|    time_elapsed    | 29760    |
|    total_timesteps | 146800   |
---------------------------------
Progress update from timestep 146900: Updated shield, loss is 0.06505508720874786
Progress update from timestep 147000: Updated shield, loss is 0.07302980124950409
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 147000: Saved shield_loss.csv. 
Eval num_timesteps=147000, episode_reward=8.55 +/- 13.02
Episode length: 13.20 +/- 18.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 13.2          |
|    mean_reward          | 8.55          |
| time/                   |               |
|    total_timesteps      | 147000        |
| train/                  |               |
|    approx_kl            | 8.4345746e-05 |
|    clip_fraction        | 0.0029        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0143       |
|    explained_variance   | 0.0683        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.598         |
|    n_updates            | 3670          |
|    policy_gradient_loss | -0.000343     |
|    value_loss           | 1.69          |
-------------------------------------------
Progress update from timestep 147100: Updated shield, loss is 0.06510934233665466
Progress update from timestep 147200: Updated shield, loss is 0.061821602284908295
Eval num_timesteps=147200, episode_reward=10.56 +/- 12.57
Episode length: 15.60 +/- 17.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 10.6     |
| time/              |          |
|    total_timesteps | 147200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.48     |
|    ep_rew_mean     | 2.84     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 368      |
|    time_elapsed    | 29853    |
|    total_timesteps | 147200   |
---------------------------------
Progress update from timestep 147300: Updated shield, loss is 0.06721200048923492
Progress update from timestep 147400: Updated shield, loss is 0.06659044325351715
Eval num_timesteps=147400, episode_reward=10.97 +/- 12.53
Episode length: 16.40 +/- 17.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | 11           |
| time/                   |              |
|    total_timesteps      | 147400       |
| train/                  |              |
|    approx_kl            | 4.448329e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0103      |
|    explained_variance   | 0.106        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.583        |
|    n_updates            | 3680         |
|    policy_gradient_loss | -8.99e-05    |
|    value_loss           | 2.29         |
------------------------------------------
Progress update from timestep 147500: Updated shield, loss is 0.07637254148721695
Progress update from timestep 147600: Updated shield, loss is 0.05849495530128479
Eval num_timesteps=147600, episode_reward=22.80 +/- 15.21
Episode length: 32.80 +/- 21.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 22.8     |
| time/              |          |
|    total_timesteps | 147600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.65     |
|    ep_rew_mean     | 2.99     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 369      |
|    time_elapsed    | 29946    |
|    total_timesteps | 147600   |
---------------------------------
Progress update from timestep 147700: Updated shield, loss is 0.061319638043642044
Progress update from timestep 147800: Updated shield, loss is 0.06942048668861389
Eval num_timesteps=147800, episode_reward=6.11 +/- 3.55
Episode length: 9.40 +/- 4.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 9.4          |
|    mean_reward          | 6.11         |
| time/                   |              |
|    total_timesteps      | 147800       |
| train/                  |              |
|    approx_kl            | 8.881552e-05 |
|    clip_fraction        | 0.00491      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0366      |
|    explained_variance   | 0.239        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.964        |
|    n_updates            | 3690         |
|    policy_gradient_loss | -0.00132     |
|    value_loss           | 2.12         |
------------------------------------------
Progress update from timestep 147900: Updated shield, loss is 0.06130437180399895
Progress update from timestep 148000: Updated shield, loss is 0.06396479904651642
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 148000: Saved shield_loss.csv. 
Eval num_timesteps=148000, episode_reward=4.97 +/- 2.70
Episode length: 8.00 +/- 3.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8        |
|    mean_reward     | 4.97     |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.19     |
|    ep_rew_mean     | 3.39     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 370      |
|    time_elapsed    | 30005    |
|    total_timesteps | 148000   |
---------------------------------
Progress update from timestep 148100: Updated shield, loss is 0.07077091187238693
Progress update from timestep 148200: Updated shield, loss is 0.06443984061479568
Eval num_timesteps=148200, episode_reward=5.92 +/- 4.16
Episode length: 9.40 +/- 6.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 9.4           |
|    mean_reward          | 5.92          |
| time/                   |               |
|    total_timesteps      | 148200        |
| train/                  |               |
|    approx_kl            | 1.1255774e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0122       |
|    explained_variance   | 0.206         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.39          |
|    n_updates            | 3700          |
|    policy_gradient_loss | -3.67e-05     |
|    value_loss           | 2.07          |
-------------------------------------------
Progress update from timestep 148300: Updated shield, loss is 0.0585324764251709
Progress update from timestep 148400: Updated shield, loss is 0.06522823125123978
Eval num_timesteps=148400, episode_reward=6.69 +/- 4.28
Episode length: 10.80 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10.8     |
|    mean_reward     | 6.69     |
| time/              |          |
|    total_timesteps | 148400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.38     |
|    ep_rew_mean     | 2.75     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 371      |
|    time_elapsed    | 30069    |
|    total_timesteps | 148400   |
---------------------------------
Progress update from timestep 148500: Updated shield, loss is 0.06567700207233429
Progress update from timestep 148600: Updated shield, loss is 0.060640208423137665
Eval num_timesteps=148600, episode_reward=13.27 +/- 11.20
Episode length: 19.80 +/- 15.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19.8        |
|    mean_reward          | 13.3        |
| time/                   |             |
|    total_timesteps      | 148600      |
| train/                  |             |
|    approx_kl            | 2.29615e-05 |
|    clip_fraction        | 0.00134     |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.00862    |
|    explained_variance   | 0.218       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.768       |
|    n_updates            | 3710        |
|    policy_gradient_loss | -0.000143   |
|    value_loss           | 1.8         |
-----------------------------------------
Progress update from timestep 148700: Updated shield, loss is 0.06515180319547653
Progress update from timestep 148800: Updated shield, loss is 0.06287877261638641
Eval num_timesteps=148800, episode_reward=18.20 +/- 14.69
Episode length: 26.40 +/- 20.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.4     |
|    mean_reward     | 18.2     |
| time/              |          |
|    total_timesteps | 148800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.75     |
|    ep_rew_mean     | 3.04     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 372      |
|    time_elapsed    | 30158    |
|    total_timesteps | 148800   |
---------------------------------
Progress update from timestep 148900: Updated shield, loss is 0.06821029633283615
Progress update from timestep 149000: Updated shield, loss is 0.060014307498931885
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 149000: Saved shield_loss.csv. 
Eval num_timesteps=149000, episode_reward=15.32 +/- 15.23
Episode length: 23.20 +/- 21.96
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.2          |
|    mean_reward          | 15.3          |
| time/                   |               |
|    total_timesteps      | 149000        |
| train/                  |               |
|    approx_kl            | 4.7760765e-05 |
|    clip_fraction        | 0.00112       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0185       |
|    explained_variance   | 0.142         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.54          |
|    n_updates            | 3720          |
|    policy_gradient_loss | -0.000182     |
|    value_loss           | 2.47          |
-------------------------------------------
Progress update from timestep 149100: Updated shield, loss is 0.06482651829719543
Progress update from timestep 149200: Updated shield, loss is 0.06517530232667923
Eval num_timesteps=149200, episode_reward=28.83 +/- 14.13
Episode length: 40.40 +/- 19.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.4     |
|    mean_reward     | 28.8     |
| time/              |          |
|    total_timesteps | 149200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.88     |
|    ep_rew_mean     | 3.2      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 373      |
|    time_elapsed    | 30270    |
|    total_timesteps | 149200   |
---------------------------------
Progress update from timestep 149300: Updated shield, loss is 0.07656022906303406
Progress update from timestep 149400: Updated shield, loss is 0.055602192878723145
Eval num_timesteps=149400, episode_reward=24.39 +/- 14.03
Episode length: 34.60 +/- 19.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.6         |
|    mean_reward          | 24.4         |
| time/                   |              |
|    total_timesteps      | 149400       |
| train/                  |              |
|    approx_kl            | 8.895128e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0122      |
|    explained_variance   | 0.371        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.403        |
|    n_updates            | 3730         |
|    policy_gradient_loss | -5.83e-05    |
|    value_loss           | 1.29         |
------------------------------------------
Progress update from timestep 149500: Updated shield, loss is 0.06689640879631042
Progress update from timestep 149600: Updated shield, loss is 0.05998196452856064
Eval num_timesteps=149600, episode_reward=18.20 +/- 14.22
Episode length: 26.20 +/- 19.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | 18.2     |
| time/              |          |
|    total_timesteps | 149600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.92     |
|    ep_rew_mean     | 2.37     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 374      |
|    time_elapsed    | 30371    |
|    total_timesteps | 149600   |
---------------------------------
Progress update from timestep 149700: Updated shield, loss is 0.05656615272164345
Progress update from timestep 149800: Updated shield, loss is 0.06520475447177887
Eval num_timesteps=149800, episode_reward=18.19 +/- 14.28
Episode length: 26.20 +/- 19.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.2         |
|    mean_reward          | 18.2         |
| time/                   |              |
|    total_timesteps      | 149800       |
| train/                  |              |
|    approx_kl            | 0.0004808271 |
|    clip_fraction        | 0.0067       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0218      |
|    explained_variance   | 0.0783       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.409        |
|    n_updates            | 3740         |
|    policy_gradient_loss | -0.000656    |
|    value_loss           | 1.39         |
------------------------------------------
Progress update from timestep 149900: Updated shield, loss is 0.06273607909679413
Progress update from timestep 150000: Updated shield, loss is 0.06560367345809937
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 150000: Saved shield_loss.csv. 
Eval num_timesteps=150000, episode_reward=11.64 +/- 11.70
Episode length: 17.60 +/- 16.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | 11.6     |
| time/              |          |
|    total_timesteps | 150000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.79     |
|    ep_rew_mean     | 3.13     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 375      |
|    time_elapsed    | 30466    |
|    total_timesteps | 150000   |
---------------------------------
Progress update from timestep 150100: Updated shield, loss is 0.08085834234952927
Progress update from timestep 150200: Updated shield, loss is 0.0631890743970871
Eval num_timesteps=150200, episode_reward=15.56 +/- 15.01
Episode length: 23.40 +/- 21.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.4          |
|    mean_reward          | 15.6          |
| time/                   |               |
|    total_timesteps      | 150200        |
| train/                  |               |
|    approx_kl            | 2.3495331e-05 |
|    clip_fraction        | 0.00246       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0108       |
|    explained_variance   | 0.0974        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.805         |
|    n_updates            | 3750          |
|    policy_gradient_loss | -0.000233     |
|    value_loss           | 2.01          |
-------------------------------------------
Progress update from timestep 150300: Updated shield, loss is 0.07328386604785919
Progress update from timestep 150400: Updated shield, loss is 0.07072500139474869
Eval num_timesteps=150400, episode_reward=17.67 +/- 13.82
Episode length: 26.20 +/- 19.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | 17.7     |
| time/              |          |
|    total_timesteps | 150400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.35     |
|    ep_rew_mean     | 2.75     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 376      |
|    time_elapsed    | 30556    |
|    total_timesteps | 150400   |
---------------------------------
Progress update from timestep 150500: Updated shield, loss is 0.06416992843151093
Progress update from timestep 150600: Updated shield, loss is 0.06618878990411758
Eval num_timesteps=150600, episode_reward=10.36 +/- 12.23
Episode length: 15.60 +/- 17.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.6        |
|    mean_reward          | 10.4        |
| time/                   |             |
|    total_timesteps      | 150600      |
| train/                  |             |
|    approx_kl            | 0.000324252 |
|    clip_fraction        | 0.00446     |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0328     |
|    explained_variance   | 0.155       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.02        |
|    n_updates            | 3760        |
|    policy_gradient_loss | -0.000588   |
|    value_loss           | 1.43        |
-----------------------------------------
Progress update from timestep 150700: Updated shield, loss is 0.0561307854950428
Progress update from timestep 150800: Updated shield, loss is 0.06122104823589325
Eval num_timesteps=150800, episode_reward=31.61 +/- 6.92
Episode length: 45.40 +/- 9.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.4     |
|    mean_reward     | 31.6     |
| time/              |          |
|    total_timesteps | 150800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.52     |
|    ep_rew_mean     | 2.85     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 377      |
|    time_elapsed    | 30670    |
|    total_timesteps | 150800   |
---------------------------------
Progress update from timestep 150900: Updated shield, loss is 0.07250766456127167
Progress update from timestep 151000: Updated shield, loss is 0.05552026256918907
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 151000: Saved shield_loss.csv. 
Eval num_timesteps=151000, episode_reward=17.89 +/- 14.12
Episode length: 26.20 +/- 19.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.2          |
|    mean_reward          | 17.9          |
| time/                   |               |
|    total_timesteps      | 151000        |
| train/                  |               |
|    approx_kl            | 7.7791213e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0187       |
|    explained_variance   | 0.206         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.836         |
|    n_updates            | 3770          |
|    policy_gradient_loss | -4.85e-05     |
|    value_loss           | 1.81          |
-------------------------------------------
Progress update from timestep 151100: Updated shield, loss is 0.06798058748245239
Progress update from timestep 151200: Updated shield, loss is 0.07041080296039581
Eval num_timesteps=151200, episode_reward=21.82 +/- 12.16
Episode length: 31.40 +/- 15.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.4     |
|    mean_reward     | 21.8     |
| time/              |          |
|    total_timesteps | 151200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.72     |
|    ep_rew_mean     | 3.03     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 378      |
|    time_elapsed    | 30773    |
|    total_timesteps | 151200   |
---------------------------------
Progress update from timestep 151300: Updated shield, loss is 0.06854704767465591
Progress update from timestep 151400: Updated shield, loss is 0.06054135784506798
Eval num_timesteps=151400, episode_reward=12.74 +/- 11.22
Episode length: 19.00 +/- 16.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 19            |
|    mean_reward          | 12.7          |
| time/                   |               |
|    total_timesteps      | 151400        |
| train/                  |               |
|    approx_kl            | 0.00014231917 |
|    clip_fraction        | 0.00692       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.022        |
|    explained_variance   | 0.165         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.595         |
|    n_updates            | 3780          |
|    policy_gradient_loss | -0.00101      |
|    value_loss           | 1.73          |
-------------------------------------------
Progress update from timestep 151500: Updated shield, loss is 0.06077433377504349
Progress update from timestep 151600: Updated shield, loss is 0.061327073723077774
Eval num_timesteps=151600, episode_reward=5.24 +/- 2.70
Episode length: 8.60 +/- 3.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8.6      |
|    mean_reward     | 5.24     |
| time/              |          |
|    total_timesteps | 151600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.55     |
|    ep_rew_mean     | 3        |
| time/              |          |
|    fps             | 4        |
|    iterations      | 379      |
|    time_elapsed    | 30847    |
|    total_timesteps | 151600   |
---------------------------------
Progress update from timestep 151700: Updated shield, loss is 0.06257961690425873
Progress update from timestep 151800: Updated shield, loss is 0.06257721781730652
Eval num_timesteps=151800, episode_reward=22.92 +/- 13.72
Episode length: 34.00 +/- 19.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34           |
|    mean_reward          | 22.9         |
| time/                   |              |
|    total_timesteps      | 151800       |
| train/                  |              |
|    approx_kl            | 0.0002777391 |
|    clip_fraction        | 0.00893      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0273      |
|    explained_variance   | -0.0138      |
|    learning_rate        | 0.0001       |
|    loss                 | 0.552        |
|    n_updates            | 3790         |
|    policy_gradient_loss | -0.00134     |
|    value_loss           | 2.46         |
------------------------------------------
Progress update from timestep 151900: Updated shield, loss is 0.06577124446630478
Progress update from timestep 152000: Updated shield, loss is 0.05683279410004616
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 152000: Saved shield_loss.csv. 
Eval num_timesteps=152000, episode_reward=15.15 +/- 15.37
Episode length: 22.80 +/- 22.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 15.1     |
| time/              |          |
|    total_timesteps | 152000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.12     |
|    ep_rew_mean     | 2.63     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 380      |
|    time_elapsed    | 30959    |
|    total_timesteps | 152000   |
---------------------------------
Progress update from timestep 152100: Updated shield, loss is 0.05780833959579468
Progress update from timestep 152200: Updated shield, loss is 0.05585508793592453
Eval num_timesteps=152200, episode_reward=17.25 +/- 15.91
Episode length: 24.20 +/- 21.10
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.2          |
|    mean_reward          | 17.2          |
| time/                   |               |
|    total_timesteps      | 152200        |
| train/                  |               |
|    approx_kl            | 0.00013308704 |
|    clip_fraction        | 0.00558       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.028        |
|    explained_variance   | 0.134         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.56          |
|    n_updates            | 3800          |
|    policy_gradient_loss | -0.000797     |
|    value_loss           | 1.81          |
-------------------------------------------
Progress update from timestep 152300: Updated shield, loss is 0.059531014412641525
Progress update from timestep 152400: Updated shield, loss is 0.05558269843459129
Eval num_timesteps=152400, episode_reward=21.38 +/- 15.19
Episode length: 32.00 +/- 22.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32       |
|    mean_reward     | 21.4     |
| time/              |          |
|    total_timesteps | 152400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.76     |
|    ep_rew_mean     | 3.05     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 381      |
|    time_elapsed    | 31055    |
|    total_timesteps | 152400   |
---------------------------------
Progress update from timestep 152500: Updated shield, loss is 0.05641276761889458
Progress update from timestep 152600: Updated shield, loss is 0.05141986533999443
Eval num_timesteps=152600, episode_reward=17.57 +/- 13.12
Episode length: 26.80 +/- 19.30
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.8          |
|    mean_reward          | 17.6          |
| time/                   |               |
|    total_timesteps      | 152600        |
| train/                  |               |
|    approx_kl            | 1.4826127e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0235       |
|    explained_variance   | 0.0576        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.882         |
|    n_updates            | 3810          |
|    policy_gradient_loss | -0.000132     |
|    value_loss           | 2.16          |
-------------------------------------------
Progress update from timestep 152700: Updated shield, loss is 0.0697806105017662
Progress update from timestep 152800: Updated shield, loss is 0.05480451509356499
Eval num_timesteps=152800, episode_reward=14.18 +/- 13.59
Episode length: 21.00 +/- 19.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | 14.2     |
| time/              |          |
|    total_timesteps | 152800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.24     |
|    ep_rew_mean     | 2.67     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 382      |
|    time_elapsed    | 31161    |
|    total_timesteps | 152800   |
---------------------------------
Progress update from timestep 152900: Updated shield, loss is 0.07975578308105469
Progress update from timestep 153000: Updated shield, loss is 0.06585510075092316
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 153000: Saved shield_loss.csv. 
Eval num_timesteps=153000, episode_reward=2.92 +/- 2.45
Episode length: 5.20 +/- 3.71
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 5.2           |
|    mean_reward          | 2.92          |
| time/                   |               |
|    total_timesteps      | 153000        |
| train/                  |               |
|    approx_kl            | 2.4217235e-05 |
|    clip_fraction        | 0.00134       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00925      |
|    explained_variance   | 0.155         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.806         |
|    n_updates            | 3820          |
|    policy_gradient_loss | -0.000177     |
|    value_loss           | 1.51          |
-------------------------------------------
Progress update from timestep 153100: Updated shield, loss is 0.09038697183132172
Progress update from timestep 153200: Updated shield, loss is 0.06463713943958282
Eval num_timesteps=153200, episode_reward=9.78 +/- 12.41
Episode length: 14.80 +/- 17.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 9.78     |
| time/              |          |
|    total_timesteps | 153200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.28     |
|    ep_rew_mean     | 3.41     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 383      |
|    time_elapsed    | 31235    |
|    total_timesteps | 153200   |
---------------------------------
Progress update from timestep 153300: Updated shield, loss is 0.10091391205787659
Progress update from timestep 153400: Updated shield, loss is 0.060027629137039185
Eval num_timesteps=153400, episode_reward=10.89 +/- 12.39
Episode length: 16.20 +/- 16.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.2          |
|    mean_reward          | 10.9          |
| time/                   |               |
|    total_timesteps      | 153400        |
| train/                  |               |
|    approx_kl            | 0.00022208724 |
|    clip_fraction        | 0.00603       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0255       |
|    explained_variance   | 0.136         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.785         |
|    n_updates            | 3830          |
|    policy_gradient_loss | -0.000476     |
|    value_loss           | 2.29          |
-------------------------------------------
Progress update from timestep 153500: Updated shield, loss is 0.07324529439210892
Progress update from timestep 153600: Updated shield, loss is 0.06588464230298996
Eval num_timesteps=153600, episode_reward=5.42 +/- 2.59
Episode length: 8.80 +/- 3.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8.8      |
|    mean_reward     | 5.42     |
| time/              |          |
|    total_timesteps | 153600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.16     |
|    ep_rew_mean     | 3.36     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 384      |
|    time_elapsed    | 31309    |
|    total_timesteps | 153600   |
---------------------------------
Progress update from timestep 153700: Updated shield, loss is 0.06807315349578857
Progress update from timestep 153800: Updated shield, loss is 0.08867962658405304
Eval num_timesteps=153800, episode_reward=4.93 +/- 2.40
Episode length: 8.00 +/- 3.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 8             |
|    mean_reward          | 4.93          |
| time/                   |               |
|    total_timesteps      | 153800        |
| train/                  |               |
|    approx_kl            | 2.2276904e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00753      |
|    explained_variance   | 0.212         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.999         |
|    n_updates            | 3840          |
|    policy_gradient_loss | -5.75e-06     |
|    value_loss           | 2.08          |
-------------------------------------------
Progress update from timestep 153900: Updated shield, loss is 0.05682266131043434
Progress update from timestep 154000: Updated shield, loss is 0.07498402893543243
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 154000: Saved shield_loss.csv. 
Eval num_timesteps=154000, episode_reward=13.83 +/- 10.78
Episode length: 21.40 +/- 15.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 13.8     |
| time/              |          |
|    total_timesteps | 154000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.61     |
|    ep_rew_mean     | 3.7      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 385      |
|    time_elapsed    | 31391    |
|    total_timesteps | 154000   |
---------------------------------
Progress update from timestep 154100: Updated shield, loss is 0.07388012111186981
Progress update from timestep 154200: Updated shield, loss is 0.058380961418151855
Eval num_timesteps=154200, episode_reward=9.99 +/- 13.58
Episode length: 14.60 +/- 18.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.6          |
|    mean_reward          | 9.99          |
| time/                   |               |
|    total_timesteps      | 154200        |
| train/                  |               |
|    approx_kl            | 0.00019945033 |
|    clip_fraction        | 0.00469       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0241       |
|    explained_variance   | 0.227         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.594         |
|    n_updates            | 3850          |
|    policy_gradient_loss | -0.000587     |
|    value_loss           | 2.12          |
-------------------------------------------
Progress update from timestep 154300: Updated shield, loss is 0.06772147119045258
Progress update from timestep 154400: Updated shield, loss is 0.05399169400334358
Eval num_timesteps=154400, episode_reward=28.92 +/- 11.16
Episode length: 42.20 +/- 15.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 28.9     |
| time/              |          |
|    total_timesteps | 154400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.05     |
|    ep_rew_mean     | 2.5      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 386      |
|    time_elapsed    | 31487    |
|    total_timesteps | 154400   |
---------------------------------
Progress update from timestep 154500: Updated shield, loss is 0.07570531964302063
Progress update from timestep 154600: Updated shield, loss is 0.06429120898246765
Eval num_timesteps=154600, episode_reward=10.43 +/- 13.47
Episode length: 15.20 +/- 17.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.2          |
|    mean_reward          | 10.4          |
| time/                   |               |
|    total_timesteps      | 154600        |
| train/                  |               |
|    approx_kl            | 0.00013524476 |
|    clip_fraction        | 0.00268       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0245       |
|    explained_variance   | 0.203         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.913         |
|    n_updates            | 3860          |
|    policy_gradient_loss | -0.000386     |
|    value_loss           | 1.9           |
-------------------------------------------
Progress update from timestep 154700: Updated shield, loss is 0.0715419352054596
Progress update from timestep 154800: Updated shield, loss is 0.06268814951181412
Eval num_timesteps=154800, episode_reward=9.74 +/- 12.47
Episode length: 15.00 +/- 17.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | 9.74     |
| time/              |          |
|    total_timesteps | 154800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.74     |
|    ep_rew_mean     | 2.99     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 387      |
|    time_elapsed    | 31577    |
|    total_timesteps | 154800   |
---------------------------------
Progress update from timestep 154900: Updated shield, loss is 0.06524921953678131
Progress update from timestep 155000: Updated shield, loss is 0.07049652189016342
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 155000: Saved shield_loss.csv. 
Eval num_timesteps=155000, episode_reward=28.77 +/- 12.54
Episode length: 41.20 +/- 17.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.2        |
|    mean_reward          | 28.8        |
| time/                   |             |
|    total_timesteps      | 155000      |
| train/                  |             |
|    approx_kl            | 2.05173e-05 |
|    clip_fraction        | 0.00134     |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0118     |
|    explained_variance   | 0.128       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.06        |
|    n_updates            | 3870        |
|    policy_gradient_loss | -0.000226   |
|    value_loss           | 2.41        |
-----------------------------------------
Progress update from timestep 155100: Updated shield, loss is 0.050113871693611145
Progress update from timestep 155200: Updated shield, loss is 0.06866791099309921
Eval num_timesteps=155200, episode_reward=16.92 +/- 15.80
Episode length: 24.00 +/- 21.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 155200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.43     |
|    ep_rew_mean     | 2.82     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 388      |
|    time_elapsed    | 31670    |
|    total_timesteps | 155200   |
---------------------------------
Progress update from timestep 155300: Updated shield, loss is 0.06156593933701515
Progress update from timestep 155400: Updated shield, loss is 0.05225665122270584
Eval num_timesteps=155400, episode_reward=5.00 +/- 3.00
Episode length: 8.20 +/- 4.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 8.2           |
|    mean_reward          | 5             |
| time/                   |               |
|    total_timesteps      | 155400        |
| train/                  |               |
|    approx_kl            | 1.1956402e-05 |
|    clip_fraction        | 0.00134       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0124       |
|    explained_variance   | 0.238         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.65          |
|    n_updates            | 3880          |
|    policy_gradient_loss | -0.000303     |
|    value_loss           | 1.54          |
-------------------------------------------
Progress update from timestep 155500: Updated shield, loss is 0.047805480659008026
Progress update from timestep 155600: Updated shield, loss is 0.059830937534570694
Eval num_timesteps=155600, episode_reward=22.05 +/- 15.24
Episode length: 32.40 +/- 21.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.4     |
|    mean_reward     | 22       |
| time/              |          |
|    total_timesteps | 155600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.81     |
|    ep_rew_mean     | 2.28     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 389      |
|    time_elapsed    | 31750    |
|    total_timesteps | 155600   |
---------------------------------
Progress update from timestep 155700: Updated shield, loss is 0.05553711950778961
Progress update from timestep 155800: Updated shield, loss is 0.04792376980185509
Eval num_timesteps=155800, episode_reward=17.16 +/- 15.54
Episode length: 24.40 +/- 20.96
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.4          |
|    mean_reward          | 17.2          |
| time/                   |               |
|    total_timesteps      | 155800        |
| train/                  |               |
|    approx_kl            | 3.9910952e-05 |
|    clip_fraction        | 0.00357       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00815      |
|    explained_variance   | -0.0228       |
|    learning_rate        | 0.0001        |
|    loss                 | 0.521         |
|    n_updates            | 3890          |
|    policy_gradient_loss | -0.000358     |
|    value_loss           | 1.71          |
-------------------------------------------
Progress update from timestep 155900: Updated shield, loss is 0.060621071606874466
Progress update from timestep 156000: Updated shield, loss is 0.06128625571727753
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 156000: Saved shield_loss.csv. 
Eval num_timesteps=156000, episode_reward=21.97 +/- 14.87
Episode length: 32.40 +/- 21.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.4     |
|    mean_reward     | 22       |
| time/              |          |
|    total_timesteps | 156000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.34     |
|    ep_rew_mean     | 2.71     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 390      |
|    time_elapsed    | 31839    |
|    total_timesteps | 156000   |
---------------------------------
Progress update from timestep 156100: Updated shield, loss is 0.056099556386470795
Progress update from timestep 156200: Updated shield, loss is 0.05936162546277046
Eval num_timesteps=156200, episode_reward=17.48 +/- 16.00
Episode length: 24.40 +/- 21.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.4         |
|    mean_reward          | 17.5         |
| time/                   |              |
|    total_timesteps      | 156200       |
| train/                  |              |
|    approx_kl            | 0.0003832758 |
|    clip_fraction        | 0.00469      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0101      |
|    explained_variance   | 0.125        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.724        |
|    n_updates            | 3900         |
|    policy_gradient_loss | -0.000773    |
|    value_loss           | 1.33         |
------------------------------------------
Progress update from timestep 156300: Updated shield, loss is 0.05695095658302307
Progress update from timestep 156400: Updated shield, loss is 0.06686967611312866
Eval num_timesteps=156400, episode_reward=19.20 +/- 12.20
Episode length: 28.20 +/- 18.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.2     |
|    mean_reward     | 19.2     |
| time/              |          |
|    total_timesteps | 156400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.88     |
|    ep_rew_mean     | 3.15     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 391      |
|    time_elapsed    | 31929    |
|    total_timesteps | 156400   |
---------------------------------
Progress update from timestep 156500: Updated shield, loss is 0.06820788979530334
Progress update from timestep 156600: Updated shield, loss is 0.058177102357149124
Eval num_timesteps=156600, episode_reward=28.93 +/- 10.53
Episode length: 42.40 +/- 15.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.4          |
|    mean_reward          | 28.9          |
| time/                   |               |
|    total_timesteps      | 156600        |
| train/                  |               |
|    approx_kl            | 2.9595685e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0187       |
|    explained_variance   | 0.238         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.51          |
|    n_updates            | 3910          |
|    policy_gradient_loss | -3.51e-05     |
|    value_loss           | 2.65          |
-------------------------------------------
Progress update from timestep 156700: Updated shield, loss is 0.05556469038128853
Progress update from timestep 156800: Updated shield, loss is 0.0537731759250164
Eval num_timesteps=156800, episode_reward=17.38 +/- 13.67
Episode length: 26.00 +/- 19.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 156800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.27     |
|    ep_rew_mean     | 3.44     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 392      |
|    time_elapsed    | 32044    |
|    total_timesteps | 156800   |
---------------------------------
Progress update from timestep 156900: Updated shield, loss is 0.05786973983049393
Progress update from timestep 157000: Updated shield, loss is 0.06806378066539764
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 157000: Saved shield_loss.csv. 
Eval num_timesteps=157000, episode_reward=6.13 +/- 4.03
Episode length: 9.40 +/- 5.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 9.4          |
|    mean_reward          | 6.13         |
| time/                   |              |
|    total_timesteps      | 157000       |
| train/                  |              |
|    approx_kl            | 3.392698e-05 |
|    clip_fraction        | 0.000446     |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0246      |
|    explained_variance   | 0.11         |
|    learning_rate        | 0.0001       |
|    loss                 | 1.46         |
|    n_updates            | 3920         |
|    policy_gradient_loss | -0.000415    |
|    value_loss           | 2.58         |
------------------------------------------
Progress update from timestep 157100: Updated shield, loss is 0.05968411639332771
Progress update from timestep 157200: Updated shield, loss is 0.05796007439494133
Eval num_timesteps=157200, episode_reward=8.83 +/- 6.39
Episode length: 13.80 +/- 9.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.8     |
|    mean_reward     | 8.83     |
| time/              |          |
|    total_timesteps | 157200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.67     |
|    ep_rew_mean     | 2.95     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 393      |
|    time_elapsed    | 32114    |
|    total_timesteps | 157200   |
---------------------------------
Progress update from timestep 157300: Updated shield, loss is 0.07190041244029999
Progress update from timestep 157400: Updated shield, loss is 0.060511939227581024
Eval num_timesteps=157400, episode_reward=9.78 +/- 13.68
Episode length: 14.20 +/- 18.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.2          |
|    mean_reward          | 9.78          |
| time/                   |               |
|    total_timesteps      | 157400        |
| train/                  |               |
|    approx_kl            | 8.4884516e-05 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0141       |
|    explained_variance   | 0.0945        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.507         |
|    n_updates            | 3930          |
|    policy_gradient_loss | -0.000446     |
|    value_loss           | 1.53          |
-------------------------------------------
Progress update from timestep 157500: Updated shield, loss is 0.062043339014053345
Progress update from timestep 157600: Updated shield, loss is 0.06456422060728073
Eval num_timesteps=157600, episode_reward=11.01 +/- 12.44
Episode length: 16.60 +/- 17.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | 11       |
| time/              |          |
|    total_timesteps | 157600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.89     |
|    ep_rew_mean     | 3.16     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 394      |
|    time_elapsed    | 32205    |
|    total_timesteps | 157600   |
---------------------------------
Progress update from timestep 157700: Updated shield, loss is 0.06368492543697357
Progress update from timestep 157800: Updated shield, loss is 0.06151632219552994
Eval num_timesteps=157800, episode_reward=23.73 +/- 14.09
Episode length: 34.00 +/- 19.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34            |
|    mean_reward          | 23.7          |
| time/                   |               |
|    total_timesteps      | 157800        |
| train/                  |               |
|    approx_kl            | 2.6688629e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.014        |
|    explained_variance   | 0.109         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.996         |
|    n_updates            | 3940          |
|    policy_gradient_loss | -8.7e-05      |
|    value_loss           | 2.12          |
-------------------------------------------
Progress update from timestep 157900: Updated shield, loss is 0.06218991428613663
Progress update from timestep 158000: Updated shield, loss is 0.058478739112615585
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 158000: Saved shield_loss.csv. 
Eval num_timesteps=158000, episode_reward=18.05 +/- 13.37
Episode length: 27.00 +/- 19.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27       |
|    mean_reward     | 18       |
| time/              |          |
|    total_timesteps | 158000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.56     |
|    ep_rew_mean     | 2.9      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 395      |
|    time_elapsed    | 32300    |
|    total_timesteps | 158000   |
---------------------------------
Progress update from timestep 158100: Updated shield, loss is 0.0761057436466217
Progress update from timestep 158200: Updated shield, loss is 0.06387725472450256
Eval num_timesteps=158200, episode_reward=14.05 +/- 10.89
Episode length: 21.40 +/- 16.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 21.4          |
|    mean_reward          | 14            |
| time/                   |               |
|    total_timesteps      | 158200        |
| train/                  |               |
|    approx_kl            | 8.7961605e-05 |
|    clip_fraction        | 0.00335       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0105       |
|    explained_variance   | 0.166         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.516         |
|    n_updates            | 3950          |
|    policy_gradient_loss | -0.000321     |
|    value_loss           | 1.28          |
-------------------------------------------
Progress update from timestep 158300: Updated shield, loss is 0.06264008581638336
Progress update from timestep 158400: Updated shield, loss is 0.0601203478872776
Eval num_timesteps=158400, episode_reward=9.77 +/- 12.45
Episode length: 14.80 +/- 17.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 9.77     |
| time/              |          |
|    total_timesteps | 158400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.85     |
|    ep_rew_mean     | 3.15     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 396      |
|    time_elapsed    | 32390    |
|    total_timesteps | 158400   |
---------------------------------
Progress update from timestep 158500: Updated shield, loss is 0.07460090517997742
Progress update from timestep 158600: Updated shield, loss is 0.057308465242385864
Eval num_timesteps=158600, episode_reward=26.31 +/- 12.93
Episode length: 37.40 +/- 17.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 37.4         |
|    mean_reward          | 26.3         |
| time/                   |              |
|    total_timesteps      | 158600       |
| train/                  |              |
|    approx_kl            | 0.0003344671 |
|    clip_fraction        | 0.0118       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.015       |
|    explained_variance   | 0.179        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.6          |
|    n_updates            | 3960         |
|    policy_gradient_loss | -0.00134     |
|    value_loss           | 1.83         |
------------------------------------------
Progress update from timestep 158700: Updated shield, loss is 0.05925801396369934
Progress update from timestep 158800: Updated shield, loss is 0.05948346108198166
Eval num_timesteps=158800, episode_reward=22.46 +/- 15.66
Episode length: 32.20 +/- 21.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 22.5     |
| time/              |          |
|    total_timesteps | 158800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.31     |
|    ep_rew_mean     | 2.67     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 397      |
|    time_elapsed    | 32516    |
|    total_timesteps | 158800   |
---------------------------------
Progress update from timestep 158900: Updated shield, loss is 0.06704609096050262
Progress update from timestep 159000: Updated shield, loss is 0.06183110922574997
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 159000: Saved shield_loss.csv. 
Eval num_timesteps=159000, episode_reward=10.88 +/- 13.14
Episode length: 15.60 +/- 17.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.6          |
|    mean_reward          | 10.9          |
| time/                   |               |
|    total_timesteps      | 159000        |
| train/                  |               |
|    approx_kl            | 0.00010208909 |
|    clip_fraction        | 0.00513       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0166       |
|    explained_variance   | 0.101         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.64          |
|    n_updates            | 3970          |
|    policy_gradient_loss | -0.00069      |
|    value_loss           | 2.02          |
-------------------------------------------
Progress update from timestep 159100: Updated shield, loss is 0.07006117701530457
Progress update from timestep 159200: Updated shield, loss is 0.07033450901508331
Eval num_timesteps=159200, episode_reward=23.48 +/- 14.89
Episode length: 33.40 +/- 20.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 23.5     |
| time/              |          |
|    total_timesteps | 159200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.79     |
|    ep_rew_mean     | 3.1      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 398      |
|    time_elapsed    | 32608    |
|    total_timesteps | 159200   |
---------------------------------
Progress update from timestep 159300: Updated shield, loss is 0.06488283723592758
Progress update from timestep 159400: Updated shield, loss is 0.05682734400033951
Eval num_timesteps=159400, episode_reward=22.91 +/- 15.10
Episode length: 33.00 +/- 20.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33            |
|    mean_reward          | 22.9          |
| time/                   |               |
|    total_timesteps      | 159400        |
| train/                  |               |
|    approx_kl            | 6.2624225e-05 |
|    clip_fraction        | 0.0058        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0301       |
|    explained_variance   | 0.143         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.844         |
|    n_updates            | 3980          |
|    policy_gradient_loss | -0.000706     |
|    value_loss           | 1.4           |
-------------------------------------------
Progress update from timestep 159500: Updated shield, loss is 0.06728532910346985
Progress update from timestep 159600: Updated shield, loss is 0.07174110412597656
Eval num_timesteps=159600, episode_reward=14.73 +/- 12.31
Episode length: 21.40 +/- 16.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 14.7     |
| time/              |          |
|    total_timesteps | 159600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.02     |
|    ep_rew_mean     | 2.55     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 399      |
|    time_elapsed    | 32702    |
|    total_timesteps | 159600   |
---------------------------------
Progress update from timestep 159700: Updated shield, loss is 0.07836177200078964
Progress update from timestep 159800: Updated shield, loss is 0.06878796219825745
Eval num_timesteps=159800, episode_reward=29.53 +/- 12.70
Episode length: 41.40 +/- 17.20
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 41.4           |
|    mean_reward          | 29.5           |
| time/                   |                |
|    total_timesteps      | 159800         |
| train/                  |                |
|    approx_kl            | 1.36304425e-05 |
|    clip_fraction        | 0.00156        |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.00909       |
|    explained_variance   | 0.142          |
|    learning_rate        | 0.0001         |
|    loss                 | 0.472          |
|    n_updates            | 3990           |
|    policy_gradient_loss | -5.28e-05      |
|    value_loss           | 1.41           |
--------------------------------------------
Progress update from timestep 159900: Updated shield, loss is 0.06645029038190842
Progress update from timestep 160000: Updated shield, loss is 0.06790447235107422
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 160000: Saved shield_loss.csv. 
Eval num_timesteps=160000, episode_reward=10.04 +/- 11.71
Episode length: 15.60 +/- 17.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 10       |
| time/              |          |
|    total_timesteps | 160000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.48     |
|    ep_rew_mean     | 2.89     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 400      |
|    time_elapsed    | 32794    |
|    total_timesteps | 160000   |
---------------------------------
Progress update from timestep 160100: Updated shield, loss is 0.07686010748147964
Progress update from timestep 160200: Updated shield, loss is 0.05867750197649002
Eval num_timesteps=160200, episode_reward=4.68 +/- 1.91
Episode length: 7.60 +/- 2.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 7.6         |
|    mean_reward          | 4.68        |
| time/                   |             |
|    total_timesteps      | 160200      |
| train/                  |             |
|    approx_kl            | 3.18111e-05 |
|    clip_fraction        | 0.00067     |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0213     |
|    explained_variance   | 0.183       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.02        |
|    n_updates            | 4000        |
|    policy_gradient_loss | -0.000367   |
|    value_loss           | 1.71        |
-----------------------------------------
Progress update from timestep 160300: Updated shield, loss is 0.06930328160524368
Progress update from timestep 160400: Updated shield, loss is 0.06039658561348915
Eval num_timesteps=160400, episode_reward=7.05 +/- 4.87
Episode length: 11.20 +/- 7.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 11.2     |
|    mean_reward     | 7.05     |
| time/              |          |
|    total_timesteps | 160400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.93     |
|    ep_rew_mean     | 3.19     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 401      |
|    time_elapsed    | 32857    |
|    total_timesteps | 160400   |
---------------------------------
Progress update from timestep 160500: Updated shield, loss is 0.057003453373909
Progress update from timestep 160600: Updated shield, loss is 0.05950203910470009
Eval num_timesteps=160600, episode_reward=27.81 +/- 12.25
Episode length: 41.20 +/- 17.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 41.2          |
|    mean_reward          | 27.8          |
| time/                   |               |
|    total_timesteps      | 160600        |
| train/                  |               |
|    approx_kl            | 7.9324476e-05 |
|    clip_fraction        | 0.00469       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.015        |
|    explained_variance   | 0.176         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.358         |
|    n_updates            | 4010          |
|    policy_gradient_loss | -0.000177     |
|    value_loss           | 1.85          |
-------------------------------------------
Progress update from timestep 160700: Updated shield, loss is 0.052346933633089066
Progress update from timestep 160800: Updated shield, loss is 0.06408850848674774
Eval num_timesteps=160800, episode_reward=13.03 +/- 11.88
Episode length: 19.40 +/- 16.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 13       |
| time/              |          |
|    total_timesteps | 160800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.05     |
|    ep_rew_mean     | 3.29     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 402      |
|    time_elapsed    | 32949    |
|    total_timesteps | 160800   |
---------------------------------
Progress update from timestep 160900: Updated shield, loss is 0.05065260827541351
Progress update from timestep 161000: Updated shield, loss is 0.060439370572566986
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 161000: Saved shield_loss.csv. 
Eval num_timesteps=161000, episode_reward=16.37 +/- 14.40
Episode length: 24.60 +/- 20.83
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.6          |
|    mean_reward          | 16.4          |
| time/                   |               |
|    total_timesteps      | 161000        |
| train/                  |               |
|    approx_kl            | 0.00010816879 |
|    clip_fraction        | 0.00446       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0095       |
|    explained_variance   | 0.0483        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.773         |
|    n_updates            | 4020          |
|    policy_gradient_loss | -0.000483     |
|    value_loss           | 2.42          |
-------------------------------------------
Progress update from timestep 161100: Updated shield, loss is 0.056778669357299805
Progress update from timestep 161200: Updated shield, loss is 0.06545094400644302
Eval num_timesteps=161200, episode_reward=29.55 +/- 11.02
Episode length: 42.40 +/- 15.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 29.5     |
| time/              |          |
|    total_timesteps | 161200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.01     |
|    ep_rew_mean     | 3.21     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 403      |
|    time_elapsed    | 33061    |
|    total_timesteps | 161200   |
---------------------------------
Progress update from timestep 161300: Updated shield, loss is 0.06131136789917946
Progress update from timestep 161400: Updated shield, loss is 0.061169810593128204
Eval num_timesteps=161400, episode_reward=16.33 +/- 14.54
Episode length: 24.60 +/- 21.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.6          |
|    mean_reward          | 16.3          |
| time/                   |               |
|    total_timesteps      | 161400        |
| train/                  |               |
|    approx_kl            | 0.00013750361 |
|    clip_fraction        | 0.00268       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0134       |
|    explained_variance   | 0.205         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.38          |
|    n_updates            | 4030          |
|    policy_gradient_loss | -0.000414     |
|    value_loss           | 2.2           |
-------------------------------------------
Progress update from timestep 161500: Updated shield, loss is 0.06602820008993149
Progress update from timestep 161600: Updated shield, loss is 0.06057361140847206
Eval num_timesteps=161600, episode_reward=22.41 +/- 16.63
Episode length: 31.60 +/- 22.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.6     |
|    mean_reward     | 22.4     |
| time/              |          |
|    total_timesteps | 161600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.02     |
|    ep_rew_mean     | 3.25     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 404      |
|    time_elapsed    | 33172    |
|    total_timesteps | 161600   |
---------------------------------
Progress update from timestep 161700: Updated shield, loss is 0.05536743998527527
Progress update from timestep 161800: Updated shield, loss is 0.05476817116141319
Eval num_timesteps=161800, episode_reward=20.15 +/- 13.65
Episode length: 28.20 +/- 17.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 28.2          |
|    mean_reward          | 20.2          |
| time/                   |               |
|    total_timesteps      | 161800        |
| train/                  |               |
|    approx_kl            | 1.3263863e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0111       |
|    explained_variance   | 0.217         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.567         |
|    n_updates            | 4040          |
|    policy_gradient_loss | -1.93e-05     |
|    value_loss           | 1.67          |
-------------------------------------------
Progress update from timestep 161900: Updated shield, loss is 0.06490032374858856
Progress update from timestep 162000: Updated shield, loss is 0.06311770528554916
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 162000: Saved shield_loss.csv. 
Eval num_timesteps=162000, episode_reward=21.51 +/- 14.09
Episode length: 31.40 +/- 20.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.4     |
|    mean_reward     | 21.5     |
| time/              |          |
|    total_timesteps | 162000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.67     |
|    ep_rew_mean     | 2.98     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 405      |
|    time_elapsed    | 33267    |
|    total_timesteps | 162000   |
---------------------------------
Progress update from timestep 162100: Updated shield, loss is 0.0639241635799408
Progress update from timestep 162200: Updated shield, loss is 0.06399788707494736
Eval num_timesteps=162200, episode_reward=17.86 +/- 14.99
Episode length: 25.40 +/- 20.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.4         |
|    mean_reward          | 17.9         |
| time/                   |              |
|    total_timesteps      | 162200       |
| train/                  |              |
|    approx_kl            | 6.841347e-05 |
|    clip_fraction        | 0.00313      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0219      |
|    explained_variance   | 0.275        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.532        |
|    n_updates            | 4050         |
|    policy_gradient_loss | -0.000404    |
|    value_loss           | 1.9          |
------------------------------------------
Progress update from timestep 162300: Updated shield, loss is 0.05209718644618988
Progress update from timestep 162400: Updated shield, loss is 0.06016870215535164
Eval num_timesteps=162400, episode_reward=14.51 +/- 15.41
Episode length: 22.20 +/- 22.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 14.5     |
| time/              |          |
|    total_timesteps | 162400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.86     |
|    ep_rew_mean     | 2.34     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 406      |
|    time_elapsed    | 33357    |
|    total_timesteps | 162400   |
---------------------------------
Progress update from timestep 162500: Updated shield, loss is 0.05650332570075989
Progress update from timestep 162600: Updated shield, loss is 0.06132826954126358
Eval num_timesteps=162600, episode_reward=23.54 +/- 14.77
Episode length: 33.60 +/- 20.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.6         |
|    mean_reward          | 23.5         |
| time/                   |              |
|    total_timesteps      | 162600       |
| train/                  |              |
|    approx_kl            | 0.0001262403 |
|    clip_fraction        | 0.00558      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0102      |
|    explained_variance   | 0.134        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.17         |
|    n_updates            | 4060         |
|    policy_gradient_loss | -0.000406    |
|    value_loss           | 1.86         |
------------------------------------------
Progress update from timestep 162700: Updated shield, loss is 0.05388461425900459
Progress update from timestep 162800: Updated shield, loss is 0.05240580812096596
Eval num_timesteps=162800, episode_reward=9.67 +/- 13.05
Episode length: 14.40 +/- 17.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.4     |
|    mean_reward     | 9.67     |
| time/              |          |
|    total_timesteps | 162800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.72     |
|    ep_rew_mean     | 3.01     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 407      |
|    time_elapsed    | 33469    |
|    total_timesteps | 162800   |
---------------------------------
Progress update from timestep 162900: Updated shield, loss is 0.05720934271812439
Progress update from timestep 163000: Updated shield, loss is 0.054716240614652634
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 163000: Saved shield_loss.csv. 
Eval num_timesteps=163000, episode_reward=6.49 +/- 6.43
Episode length: 10.00 +/- 9.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 10            |
|    mean_reward          | 6.49          |
| time/                   |               |
|    total_timesteps      | 163000        |
| train/                  |               |
|    approx_kl            | 3.5226927e-05 |
|    clip_fraction        | 0.00134       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0124       |
|    explained_variance   | 0.161         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.1           |
|    n_updates            | 4070          |
|    policy_gradient_loss | -0.000217     |
|    value_loss           | 2.04          |
-------------------------------------------
Progress update from timestep 163100: Updated shield, loss is 0.05229678377509117
Progress update from timestep 163200: Updated shield, loss is 0.04687676206231117
Eval num_timesteps=163200, episode_reward=13.29 +/- 12.39
Episode length: 19.40 +/- 16.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 13.3     |
| time/              |          |
|    total_timesteps | 163200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.73     |
|    ep_rew_mean     | 3.05     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 408      |
|    time_elapsed    | 33554    |
|    total_timesteps | 163200   |
---------------------------------
Progress update from timestep 163300: Updated shield, loss is 0.06036338582634926
Progress update from timestep 163400: Updated shield, loss is 0.05879300832748413
Eval num_timesteps=163400, episode_reward=21.67 +/- 15.70
Episode length: 31.80 +/- 22.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 31.8         |
|    mean_reward          | 21.7         |
| time/                   |              |
|    total_timesteps      | 163400       |
| train/                  |              |
|    approx_kl            | 7.207622e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00876     |
|    explained_variance   | 0.215        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.715        |
|    n_updates            | 4080         |
|    policy_gradient_loss | -0.000134    |
|    value_loss           | 1.86         |
------------------------------------------
Progress update from timestep 163500: Updated shield, loss is 0.061180777847766876
Progress update from timestep 163600: Updated shield, loss is 0.062291357666254044
Eval num_timesteps=163600, episode_reward=12.93 +/- 12.71
Episode length: 19.00 +/- 17.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 12.9     |
| time/              |          |
|    total_timesteps | 163600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.58     |
|    ep_rew_mean     | 2.95     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 409      |
|    time_elapsed    | 33657    |
|    total_timesteps | 163600   |
---------------------------------
Progress update from timestep 163700: Updated shield, loss is 0.05276747792959213
Progress update from timestep 163800: Updated shield, loss is 0.05065497010946274
Eval num_timesteps=163800, episode_reward=22.41 +/- 14.33
Episode length: 33.20 +/- 20.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.2          |
|    mean_reward          | 22.4          |
| time/                   |               |
|    total_timesteps      | 163800        |
| train/                  |               |
|    approx_kl            | 3.7132548e-05 |
|    clip_fraction        | 0.000446      |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0176       |
|    explained_variance   | 0.238         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.983         |
|    n_updates            | 4090          |
|    policy_gradient_loss | -0.000266     |
|    value_loss           | 1.83          |
-------------------------------------------
Progress update from timestep 163900: Updated shield, loss is 0.06179387867450714
Progress update from timestep 164000: Updated shield, loss is 0.059708837419748306
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 164000: Saved shield_loss.csv. 
Eval num_timesteps=164000, episode_reward=23.49 +/- 14.42
Episode length: 33.80 +/- 19.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 23.5     |
| time/              |          |
|    total_timesteps | 164000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.82     |
|    ep_rew_mean     | 2.33     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 410      |
|    time_elapsed    | 33753    |
|    total_timesteps | 164000   |
---------------------------------
Progress update from timestep 164100: Updated shield, loss is 0.0550365149974823
Progress update from timestep 164200: Updated shield, loss is 0.050740327686071396
Eval num_timesteps=164200, episode_reward=21.41 +/- 16.92
Episode length: 30.80 +/- 23.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 30.8          |
|    mean_reward          | 21.4          |
| time/                   |               |
|    total_timesteps      | 164200        |
| train/                  |               |
|    approx_kl            | 0.00010599173 |
|    clip_fraction        | 0.00179       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0152       |
|    explained_variance   | 0.291         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.358         |
|    n_updates            | 4100          |
|    policy_gradient_loss | -0.00038      |
|    value_loss           | 1.71          |
-------------------------------------------
Progress update from timestep 164300: Updated shield, loss is 0.056349050253629684
Progress update from timestep 164400: Updated shield, loss is 0.05288074165582657
Eval num_timesteps=164400, episode_reward=12.48 +/- 11.96
Episode length: 18.60 +/- 16.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 12.5     |
| time/              |          |
|    total_timesteps | 164400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.85     |
|    ep_rew_mean     | 3.09     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 411      |
|    time_elapsed    | 33844    |
|    total_timesteps | 164400   |
---------------------------------
Progress update from timestep 164500: Updated shield, loss is 0.05978984013199806
Progress update from timestep 164600: Updated shield, loss is 0.05959640070796013
Eval num_timesteps=164600, episode_reward=16.18 +/- 14.49
Episode length: 24.40 +/- 20.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.4         |
|    mean_reward          | 16.2         |
| time/                   |              |
|    total_timesteps      | 164600       |
| train/                  |              |
|    approx_kl            | 0.0007027655 |
|    clip_fraction        | 0.00826      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0465      |
|    explained_variance   | 0.286        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.865        |
|    n_updates            | 4110         |
|    policy_gradient_loss | -0.00108     |
|    value_loss           | 1.53         |
------------------------------------------
Progress update from timestep 164700: Updated shield, loss is 0.061513502150774
Progress update from timestep 164800: Updated shield, loss is 0.04557621479034424
Eval num_timesteps=164800, episode_reward=23.38 +/- 14.57
Episode length: 33.60 +/- 20.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 23.4     |
| time/              |          |
|    total_timesteps | 164800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.57     |
|    ep_rew_mean     | 2.2      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 412      |
|    time_elapsed    | 33943    |
|    total_timesteps | 164800   |
---------------------------------
Progress update from timestep 164900: Updated shield, loss is 0.05809598043560982
Progress update from timestep 165000: Updated shield, loss is 0.054600998759269714
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 165000: Saved shield_loss.csv. 
Eval num_timesteps=165000, episode_reward=11.03 +/- 7.93
Episode length: 17.00 +/- 11.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17            |
|    mean_reward          | 11            |
| time/                   |               |
|    total_timesteps      | 165000        |
| train/                  |               |
|    approx_kl            | 2.1466153e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00256      |
|    explained_variance   | 0.257         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.757         |
|    n_updates            | 4120          |
|    policy_gradient_loss | -6.28e-06     |
|    value_loss           | 1.15          |
-------------------------------------------
Progress update from timestep 165100: Updated shield, loss is 0.06088445708155632
Progress update from timestep 165200: Updated shield, loss is 0.06053610518574715
Eval num_timesteps=165200, episode_reward=15.92 +/- 15.24
Episode length: 23.60 +/- 21.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 15.9     |
| time/              |          |
|    total_timesteps | 165200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.91     |
|    ep_rew_mean     | 2.46     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 413      |
|    time_elapsed    | 34030    |
|    total_timesteps | 165200   |
---------------------------------
Progress update from timestep 165300: Updated shield, loss is 0.053114697337150574
Progress update from timestep 165400: Updated shield, loss is 0.05494961515069008
Eval num_timesteps=165400, episode_reward=22.52 +/- 14.78
Episode length: 33.00 +/- 21.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33            |
|    mean_reward          | 22.5          |
| time/                   |               |
|    total_timesteps      | 165400        |
| train/                  |               |
|    approx_kl            | 2.4565403e-05 |
|    clip_fraction        | 0.00134       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0123       |
|    explained_variance   | 0.0718        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.333         |
|    n_updates            | 4130          |
|    policy_gradient_loss | -0.000121     |
|    value_loss           | 1.35          |
-------------------------------------------
Progress update from timestep 165500: Updated shield, loss is 0.05362016335129738
Progress update from timestep 165600: Updated shield, loss is 0.051933787763118744
Eval num_timesteps=165600, episode_reward=27.89 +/- 13.16
Episode length: 40.60 +/- 18.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.6     |
|    mean_reward     | 27.9     |
| time/              |          |
|    total_timesteps | 165600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.05     |
|    ep_rew_mean     | 2.56     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 414      |
|    time_elapsed    | 34143    |
|    total_timesteps | 165600   |
---------------------------------
Progress update from timestep 165700: Updated shield, loss is 0.0592268630862236
Progress update from timestep 165800: Updated shield, loss is 0.0593457855284214
Eval num_timesteps=165800, episode_reward=29.53 +/- 12.70
Episode length: 41.40 +/- 17.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 41.4          |
|    mean_reward          | 29.5          |
| time/                   |               |
|    total_timesteps      | 165800        |
| train/                  |               |
|    approx_kl            | 0.00013261315 |
|    clip_fraction        | 0.0067        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0157       |
|    explained_variance   | 0.168         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.648         |
|    n_updates            | 4140          |
|    policy_gradient_loss | -0.000404     |
|    value_loss           | 1.8           |
-------------------------------------------
Progress update from timestep 165900: Updated shield, loss is 0.0580781027674675
Progress update from timestep 166000: Updated shield, loss is 0.06220182776451111
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 166000: Saved shield_loss.csv. 
Eval num_timesteps=166000, episode_reward=18.26 +/- 13.39
Episode length: 27.00 +/- 18.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27       |
|    mean_reward     | 18.3     |
| time/              |          |
|    total_timesteps | 166000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.6      |
|    ep_rew_mean     | 2.97     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 415      |
|    time_elapsed    | 34276    |
|    total_timesteps | 166000   |
---------------------------------
Progress update from timestep 166100: Updated shield, loss is 0.0659886971116066
Progress update from timestep 166200: Updated shield, loss is 0.05684471130371094
Eval num_timesteps=166200, episode_reward=16.24 +/- 15.87
Episode length: 23.40 +/- 21.83
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.4          |
|    mean_reward          | 16.2          |
| time/                   |               |
|    total_timesteps      | 166200        |
| train/                  |               |
|    approx_kl            | 3.8978345e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00535      |
|    explained_variance   | 0.148         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.32          |
|    n_updates            | 4150          |
|    policy_gradient_loss | -1.39e-06     |
|    value_loss           | 2.66          |
-------------------------------------------
Progress update from timestep 166300: Updated shield, loss is 0.05269908159971237
Progress update from timestep 166400: Updated shield, loss is 0.06399074196815491
Eval num_timesteps=166400, episode_reward=27.86 +/- 13.22
Episode length: 40.60 +/- 18.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.6     |
|    mean_reward     | 27.9     |
| time/              |          |
|    total_timesteps | 166400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.44     |
|    ep_rew_mean     | 3.56     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 416      |
|    time_elapsed    | 34370    |
|    total_timesteps | 166400   |
---------------------------------
Progress update from timestep 166500: Updated shield, loss is 0.05985218659043312
Progress update from timestep 166600: Updated shield, loss is 0.06949472427368164
Eval num_timesteps=166600, episode_reward=4.97 +/- 2.42
Episode length: 8.20 +/- 3.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 8.2           |
|    mean_reward          | 4.97          |
| time/                   |               |
|    total_timesteps      | 166600        |
| train/                  |               |
|    approx_kl            | 0.00027016908 |
|    clip_fraction        | 0.00313       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00742      |
|    explained_variance   | 0.125         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.813         |
|    n_updates            | 4160          |
|    policy_gradient_loss | -0.000356     |
|    value_loss           | 2.16          |
-------------------------------------------
Progress update from timestep 166700: Updated shield, loss is 0.05415089428424835
Progress update from timestep 166800: Updated shield, loss is 0.0835794061422348
Eval num_timesteps=166800, episode_reward=16.74 +/- 14.95
Episode length: 24.40 +/- 20.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 166800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.71     |
|    ep_rew_mean     | 3.06     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 417      |
|    time_elapsed    | 34448    |
|    total_timesteps | 166800   |
---------------------------------
Progress update from timestep 166900: Updated shield, loss is 0.05169151350855827
Progress update from timestep 167000: Updated shield, loss is 0.06976189464330673
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 167000: Saved shield_loss.csv. 
Eval num_timesteps=167000, episode_reward=4.10 +/- 1.98
Episode length: 6.80 +/- 2.79
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 6.8           |
|    mean_reward          | 4.1           |
| time/                   |               |
|    total_timesteps      | 167000        |
| train/                  |               |
|    approx_kl            | 9.3379225e-05 |
|    clip_fraction        | 0.0058        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0218       |
|    explained_variance   | 0.235         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.718         |
|    n_updates            | 4170          |
|    policy_gradient_loss | -0.00121      |
|    value_loss           | 1.43          |
-------------------------------------------
Progress update from timestep 167100: Updated shield, loss is 0.06319153308868408
Progress update from timestep 167200: Updated shield, loss is 0.06745313853025436
Eval num_timesteps=167200, episode_reward=13.82 +/- 12.28
Episode length: 20.40 +/- 17.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 13.8     |
| time/              |          |
|    total_timesteps | 167200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.56     |
|    ep_rew_mean     | 2.93     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 418      |
|    time_elapsed    | 34527    |
|    total_timesteps | 167200   |
---------------------------------
Progress update from timestep 167300: Updated shield, loss is 0.06677589565515518
Progress update from timestep 167400: Updated shield, loss is 0.05229176580905914
Eval num_timesteps=167400, episode_reward=15.17 +/- 16.26
Episode length: 22.20 +/- 22.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22.2         |
|    mean_reward          | 15.2         |
| time/                   |              |
|    total_timesteps      | 167400       |
| train/                  |              |
|    approx_kl            | 9.879635e-05 |
|    clip_fraction        | 0.00201      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0164      |
|    explained_variance   | 0.0757       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.989        |
|    n_updates            | 4180         |
|    policy_gradient_loss | -0.000724    |
|    value_loss           | 1.9          |
------------------------------------------
Progress update from timestep 167500: Updated shield, loss is 0.08779174834489822
Progress update from timestep 167600: Updated shield, loss is 0.054658468812704086
Eval num_timesteps=167600, episode_reward=6.63 +/- 4.12
Episode length: 10.40 +/- 5.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10.4     |
|    mean_reward     | 6.63     |
| time/              |          |
|    total_timesteps | 167600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.43     |
|    ep_rew_mean     | 2.81     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 419      |
|    time_elapsed    | 34611    |
|    total_timesteps | 167600   |
---------------------------------
Progress update from timestep 167700: Updated shield, loss is 0.05637764185667038
Progress update from timestep 167800: Updated shield, loss is 0.06896217167377472
Eval num_timesteps=167800, episode_reward=12.64 +/- 12.26
Episode length: 18.20 +/- 16.25
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 18.2          |
|    mean_reward          | 12.6          |
| time/                   |               |
|    total_timesteps      | 167800        |
| train/                  |               |
|    approx_kl            | 0.00015266449 |
|    clip_fraction        | 0.00536       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0155       |
|    explained_variance   | 0.275         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.797         |
|    n_updates            | 4190          |
|    policy_gradient_loss | -0.00112      |
|    value_loss           | 1.57          |
-------------------------------------------
Progress update from timestep 167900: Updated shield, loss is 0.054048459976911545
Progress update from timestep 168000: Updated shield, loss is 0.06599686294794083
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 168000: Saved shield_loss.csv. 
Eval num_timesteps=168000, episode_reward=13.76 +/- 11.11
Episode length: 20.20 +/- 15.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.2     |
|    mean_reward     | 13.8     |
| time/              |          |
|    total_timesteps | 168000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.59     |
|    ep_rew_mean     | 2.94     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 420      |
|    time_elapsed    | 34707    |
|    total_timesteps | 168000   |
---------------------------------
Progress update from timestep 168100: Updated shield, loss is 0.07675785571336746
Progress update from timestep 168200: Updated shield, loss is 0.07035838067531586
Eval num_timesteps=168200, episode_reward=13.64 +/- 12.03
Episode length: 19.80 +/- 16.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 19.8          |
|    mean_reward          | 13.6          |
| time/                   |               |
|    total_timesteps      | 168200        |
| train/                  |               |
|    approx_kl            | 1.9328061e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00345      |
|    explained_variance   | 0.147         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.65          |
|    n_updates            | 4200          |
|    policy_gradient_loss | -1.98e-05     |
|    value_loss           | 1.59          |
-------------------------------------------
Progress update from timestep 168300: Updated shield, loss is 0.0816730409860611
Progress update from timestep 168400: Updated shield, loss is 0.05960050970315933
Eval num_timesteps=168400, episode_reward=23.71 +/- 14.57
Episode length: 33.80 +/- 19.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 23.7     |
| time/              |          |
|    total_timesteps | 168400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.29     |
|    ep_rew_mean     | 2.68     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 421      |
|    time_elapsed    | 34800    |
|    total_timesteps | 168400   |
---------------------------------
Progress update from timestep 168500: Updated shield, loss is 0.06957869231700897
Progress update from timestep 168600: Updated shield, loss is 0.050844330340623856
Eval num_timesteps=168600, episode_reward=12.37 +/- 10.63
Episode length: 19.00 +/- 15.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 19            |
|    mean_reward          | 12.4          |
| time/                   |               |
|    total_timesteps      | 168600        |
| train/                  |               |
|    approx_kl            | 0.00013279979 |
|    clip_fraction        | 0.00335       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0268       |
|    explained_variance   | 0.11          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.771         |
|    n_updates            | 4210          |
|    policy_gradient_loss | -0.000654     |
|    value_loss           | 2.43          |
-------------------------------------------
Progress update from timestep 168700: Updated shield, loss is 0.05904075503349304
Progress update from timestep 168800: Updated shield, loss is 0.06577916443347931
Eval num_timesteps=168800, episode_reward=5.01 +/- 2.17
Episode length: 8.00 +/- 3.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8        |
|    mean_reward     | 5.01     |
| time/              |          |
|    total_timesteps | 168800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.31     |
|    ep_rew_mean     | 2.75     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 422      |
|    time_elapsed    | 34878    |
|    total_timesteps | 168800   |
---------------------------------
Progress update from timestep 168900: Updated shield, loss is 0.06362144649028778
Progress update from timestep 169000: Updated shield, loss is 0.07724632322788239
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 169000: Saved shield_loss.csv. 
Eval num_timesteps=169000, episode_reward=9.49 +/- 12.53
Episode length: 14.40 +/- 17.85
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.4          |
|    mean_reward          | 9.49          |
| time/                   |               |
|    total_timesteps      | 169000        |
| train/                  |               |
|    approx_kl            | 0.00016242622 |
|    clip_fraction        | 0.00201       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00459      |
|    explained_variance   | 0.274         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.01          |
|    n_updates            | 4220          |
|    policy_gradient_loss | -0.000175     |
|    value_loss           | 1.94          |
-------------------------------------------
Progress update from timestep 169100: Updated shield, loss is 0.07353652268648148
Progress update from timestep 169200: Updated shield, loss is 0.05473257601261139
Eval num_timesteps=169200, episode_reward=22.77 +/- 14.37
Episode length: 33.20 +/- 20.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | 22.8     |
| time/              |          |
|    total_timesteps | 169200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.96     |
|    ep_rew_mean     | 3.3      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 423      |
|    time_elapsed    | 34970    |
|    total_timesteps | 169200   |
---------------------------------
Progress update from timestep 169300: Updated shield, loss is 0.06060578301548958
Progress update from timestep 169400: Updated shield, loss is 0.07447237521409988
Eval num_timesteps=169400, episode_reward=16.64 +/- 14.13
Episode length: 25.00 +/- 20.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25           |
|    mean_reward          | 16.6         |
| time/                   |              |
|    total_timesteps      | 169400       |
| train/                  |              |
|    approx_kl            | 0.0001229193 |
|    clip_fraction        | 0.00268      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00723     |
|    explained_variance   | 0.205        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.09         |
|    n_updates            | 4230         |
|    policy_gradient_loss | -0.000817    |
|    value_loss           | 2.12         |
------------------------------------------
Progress update from timestep 169500: Updated shield, loss is 0.049544841051101685
Progress update from timestep 169600: Updated shield, loss is 0.05968025326728821
Eval num_timesteps=169600, episode_reward=17.65 +/- 13.42
Episode length: 26.20 +/- 19.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | 17.7     |
| time/              |          |
|    total_timesteps | 169600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.4      |
|    ep_rew_mean     | 2.79     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 424      |
|    time_elapsed    | 35066    |
|    total_timesteps | 169600   |
---------------------------------
Progress update from timestep 169700: Updated shield, loss is 0.07166868448257446
Progress update from timestep 169800: Updated shield, loss is 0.05322502180933952
Eval num_timesteps=169800, episode_reward=22.53 +/- 16.00
Episode length: 32.00 +/- 22.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32           |
|    mean_reward          | 22.5         |
| time/                   |              |
|    total_timesteps      | 169800       |
| train/                  |              |
|    approx_kl            | 7.430033e-05 |
|    clip_fraction        | 0.00446      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0197      |
|    explained_variance   | 0.193        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.24         |
|    n_updates            | 4240         |
|    policy_gradient_loss | -0.000523    |
|    value_loss           | 1.97         |
------------------------------------------
Progress update from timestep 169900: Updated shield, loss is 0.06366080790758133
Progress update from timestep 170000: Updated shield, loss is 0.07236668467521667
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 170000: Saved shield_loss.csv. 
Eval num_timesteps=170000, episode_reward=12.35 +/- 11.25
Episode length: 18.60 +/- 16.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 12.4     |
| time/              |          |
|    total_timesteps | 170000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.84     |
|    ep_rew_mean     | 3.17     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 425      |
|    time_elapsed    | 35158    |
|    total_timesteps | 170000   |
---------------------------------
Progress update from timestep 170100: Updated shield, loss is 0.06692002713680267
Progress update from timestep 170200: Updated shield, loss is 0.0790553018450737
Eval num_timesteps=170200, episode_reward=22.34 +/- 12.15
Episode length: 32.00 +/- 16.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32           |
|    mean_reward          | 22.3         |
| time/                   |              |
|    total_timesteps      | 170200       |
| train/                  |              |
|    approx_kl            | 8.887686e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00504     |
|    explained_variance   | 0.102        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.17         |
|    n_updates            | 4250         |
|    policy_gradient_loss | -1.26e-05    |
|    value_loss           | 1.68         |
------------------------------------------
Progress update from timestep 170300: Updated shield, loss is 0.04936487227678299
Progress update from timestep 170400: Updated shield, loss is 0.060304198414087296
Eval num_timesteps=170400, episode_reward=30.32 +/- 10.00
Episode length: 43.20 +/- 13.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.2     |
|    mean_reward     | 30.3     |
| time/              |          |
|    total_timesteps | 170400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4        |
|    ep_rew_mean     | 2.48     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 426      |
|    time_elapsed    | 35284    |
|    total_timesteps | 170400   |
---------------------------------
Progress update from timestep 170500: Updated shield, loss is 0.06548330187797546
Progress update from timestep 170600: Updated shield, loss is 0.06505941599607468
Eval num_timesteps=170600, episode_reward=22.91 +/- 14.26
Episode length: 33.60 +/- 20.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.6          |
|    mean_reward          | 22.9          |
| time/                   |               |
|    total_timesteps      | 170600        |
| train/                  |               |
|    approx_kl            | 1.5494069e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0164       |
|    explained_variance   | 0.11          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.807         |
|    n_updates            | 4260          |
|    policy_gradient_loss | -6.69e-05     |
|    value_loss           | 1.57          |
-------------------------------------------
Progress update from timestep 170700: Updated shield, loss is 0.05903627723455429
Progress update from timestep 170800: Updated shield, loss is 0.05691991373896599
Eval num_timesteps=170800, episode_reward=3.85 +/- 0.85
Episode length: 6.40 +/- 1.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6.4      |
|    mean_reward     | 3.85     |
| time/              |          |
|    total_timesteps | 170800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.84     |
|    ep_rew_mean     | 3.03     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 427      |
|    time_elapsed    | 35380    |
|    total_timesteps | 170800   |
---------------------------------
Progress update from timestep 170900: Updated shield, loss is 0.06701410561800003
Progress update from timestep 171000: Updated shield, loss is 0.05638709291815758
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 171000: Saved shield_loss.csv. 
Eval num_timesteps=171000, episode_reward=24.70 +/- 12.07
Episode length: 36.20 +/- 17.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.2          |
|    mean_reward          | 24.7          |
| time/                   |               |
|    total_timesteps      | 171000        |
| train/                  |               |
|    approx_kl            | 1.8518143e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00973      |
|    explained_variance   | 0.354         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.12          |
|    n_updates            | 4270          |
|    policy_gradient_loss | -3.36e-05     |
|    value_loss           | 2.48          |
-------------------------------------------
Progress update from timestep 171100: Updated shield, loss is 0.05890840291976929
Progress update from timestep 171200: Updated shield, loss is 0.0588793121278286
Eval num_timesteps=171200, episode_reward=10.62 +/- 12.02
Episode length: 16.20 +/- 17.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | 10.6     |
| time/              |          |
|    total_timesteps | 171200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.8      |
|    ep_rew_mean     | 3.01     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 428      |
|    time_elapsed    | 35493    |
|    total_timesteps | 171200   |
---------------------------------
Progress update from timestep 171300: Updated shield, loss is 0.06713605672121048
Progress update from timestep 171400: Updated shield, loss is 0.06899533420801163
Eval num_timesteps=171400, episode_reward=17.07 +/- 15.61
Episode length: 24.20 +/- 21.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.2          |
|    mean_reward          | 17.1          |
| time/                   |               |
|    total_timesteps      | 171400        |
| train/                  |               |
|    approx_kl            | 9.0335896e-05 |
|    clip_fraction        | 0.00201       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0173       |
|    explained_variance   | 0.334         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.06          |
|    n_updates            | 4280          |
|    policy_gradient_loss | -0.000405     |
|    value_loss           | 1.87          |
-------------------------------------------
Progress update from timestep 171500: Updated shield, loss is 0.052030112594366074
Progress update from timestep 171600: Updated shield, loss is 0.07180583477020264
Eval num_timesteps=171600, episode_reward=32.44 +/- 5.38
Episode length: 46.60 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.6     |
|    mean_reward     | 32.4     |
| time/              |          |
|    total_timesteps | 171600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.21     |
|    ep_rew_mean     | 2.6      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 429      |
|    time_elapsed    | 35608    |
|    total_timesteps | 171600   |
---------------------------------
Progress update from timestep 171700: Updated shield, loss is 0.05379117280244827
Progress update from timestep 171800: Updated shield, loss is 0.053963061422109604
Eval num_timesteps=171800, episode_reward=13.70 +/- 12.07
Episode length: 19.80 +/- 15.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 19.8         |
|    mean_reward          | 13.7         |
| time/                   |              |
|    total_timesteps      | 171800       |
| train/                  |              |
|    approx_kl            | 0.0004609345 |
|    clip_fraction        | 0.00536      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0148      |
|    explained_variance   | 0.167        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.639        |
|    n_updates            | 4290         |
|    policy_gradient_loss | -0.00118     |
|    value_loss           | 1.27         |
------------------------------------------
Progress update from timestep 171900: Updated shield, loss is 0.061118192970752716
Progress update from timestep 172000: Updated shield, loss is 0.06525938212871552
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 172000: Saved shield_loss.csv. 
Eval num_timesteps=172000, episode_reward=16.77 +/- 15.09
Episode length: 24.60 +/- 20.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.96     |
|    ep_rew_mean     | 2.46     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 430      |
|    time_elapsed    | 35698    |
|    total_timesteps | 172000   |
---------------------------------
Progress update from timestep 172100: Updated shield, loss is 0.05321819707751274
Progress update from timestep 172200: Updated shield, loss is 0.06488098204135895
Eval num_timesteps=172200, episode_reward=16.08 +/- 14.39
Episode length: 24.40 +/- 21.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.4          |
|    mean_reward          | 16.1          |
| time/                   |               |
|    total_timesteps      | 172200        |
| train/                  |               |
|    approx_kl            | 0.00017888982 |
|    clip_fraction        | 0.00647       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0186       |
|    explained_variance   | 0.203         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.807         |
|    n_updates            | 4300          |
|    policy_gradient_loss | -0.000423     |
|    value_loss           | 1.72          |
-------------------------------------------
Progress update from timestep 172300: Updated shield, loss is 0.06203049421310425
Progress update from timestep 172400: Updated shield, loss is 0.05739099904894829
Eval num_timesteps=172400, episode_reward=22.78 +/- 15.02
Episode length: 33.00 +/- 21.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 22.8     |
| time/              |          |
|    total_timesteps | 172400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.45     |
|    ep_rew_mean     | 2.84     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 431      |
|    time_elapsed    | 35795    |
|    total_timesteps | 172400   |
---------------------------------
Progress update from timestep 172500: Updated shield, loss is 0.05969932675361633
Progress update from timestep 172600: Updated shield, loss is 0.04573829472064972
Eval num_timesteps=172600, episode_reward=5.36 +/- 3.07
Episode length: 8.40 +/- 4.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 8.4           |
|    mean_reward          | 5.36          |
| time/                   |               |
|    total_timesteps      | 172600        |
| train/                  |               |
|    approx_kl            | 4.3106866e-05 |
|    clip_fraction        | 0.00313       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0101       |
|    explained_variance   | 0.201         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.03          |
|    n_updates            | 4310          |
|    policy_gradient_loss | -0.000204     |
|    value_loss           | 1.45          |
-------------------------------------------
Progress update from timestep 172700: Updated shield, loss is 0.05806393176317215
Progress update from timestep 172800: Updated shield, loss is 0.06076890975236893
Eval num_timesteps=172800, episode_reward=10.11 +/- 12.33
Episode length: 15.20 +/- 17.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | 10.1     |
| time/              |          |
|    total_timesteps | 172800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.73     |
|    ep_rew_mean     | 3.08     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 432      |
|    time_elapsed    | 35870    |
|    total_timesteps | 172800   |
---------------------------------
Progress update from timestep 172900: Updated shield, loss is 0.05966087803244591
Progress update from timestep 173000: Updated shield, loss is 0.05542212724685669
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 173000: Saved shield_loss.csv. 
Eval num_timesteps=173000, episode_reward=10.97 +/- 12.34
Episode length: 16.00 +/- 17.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16            |
|    mean_reward          | 11            |
| time/                   |               |
|    total_timesteps      | 173000        |
| train/                  |               |
|    approx_kl            | 0.00013642335 |
|    clip_fraction        | 0.00513       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0166       |
|    explained_variance   | -0.0353       |
|    learning_rate        | 0.0001        |
|    loss                 | 1.14          |
|    n_updates            | 4320          |
|    policy_gradient_loss | -0.000595     |
|    value_loss           | 3.36          |
-------------------------------------------
Progress update from timestep 173100: Updated shield, loss is 0.051239896565675735
Progress update from timestep 173200: Updated shield, loss is 0.05791006609797478
Eval num_timesteps=173200, episode_reward=21.66 +/- 12.85
Episode length: 30.60 +/- 16.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.6     |
|    mean_reward     | 21.7     |
| time/              |          |
|    total_timesteps | 173200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.83     |
|    ep_rew_mean     | 3.08     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 433      |
|    time_elapsed    | 35972    |
|    total_timesteps | 173200   |
---------------------------------
Progress update from timestep 173300: Updated shield, loss is 0.06149141862988472
Progress update from timestep 173400: Updated shield, loss is 0.0647188350558281
Eval num_timesteps=173400, episode_reward=19.92 +/- 12.99
Episode length: 29.20 +/- 17.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29.2         |
|    mean_reward          | 19.9         |
| time/                   |              |
|    total_timesteps      | 173400       |
| train/                  |              |
|    approx_kl            | 9.686123e-05 |
|    clip_fraction        | 0.00179      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00758     |
|    explained_variance   | 0.228        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.922        |
|    n_updates            | 4330         |
|    policy_gradient_loss | -0.000122    |
|    value_loss           | 2.61         |
------------------------------------------
Progress update from timestep 173500: Updated shield, loss is 0.05512012541294098
Progress update from timestep 173600: Updated shield, loss is 0.04939746484160423
Eval num_timesteps=173600, episode_reward=28.69 +/- 13.30
Episode length: 40.80 +/- 18.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.8     |
|    mean_reward     | 28.7     |
| time/              |          |
|    total_timesteps | 173600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.5      |
|    ep_rew_mean     | 2.79     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 434      |
|    time_elapsed    | 36067    |
|    total_timesteps | 173600   |
---------------------------------
Progress update from timestep 173700: Updated shield, loss is 0.06885792315006256
Progress update from timestep 173800: Updated shield, loss is 0.060808856040239334
Eval num_timesteps=173800, episode_reward=9.59 +/- 12.49
Episode length: 14.40 +/- 17.86
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.4          |
|    mean_reward          | 9.59          |
| time/                   |               |
|    total_timesteps      | 173800        |
| train/                  |               |
|    approx_kl            | 0.00027388017 |
|    clip_fraction        | 0.0058        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0245       |
|    explained_variance   | 0.115         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.89          |
|    n_updates            | 4340          |
|    policy_gradient_loss | -0.000957     |
|    value_loss           | 1.8           |
-------------------------------------------
Progress update from timestep 173900: Updated shield, loss is 0.059647075831890106
Progress update from timestep 174000: Updated shield, loss is 0.06648048013448715
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 174000: Saved shield_loss.csv. 
Eval num_timesteps=174000, episode_reward=3.92 +/- 3.79
Episode length: 6.60 +/- 5.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6.6      |
|    mean_reward     | 3.92     |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.16     |
|    ep_rew_mean     | 2.56     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 435      |
|    time_elapsed    | 36142    |
|    total_timesteps | 174000   |
---------------------------------
Progress update from timestep 174100: Updated shield, loss is 0.052099861204624176
Progress update from timestep 174200: Updated shield, loss is 0.060331910848617554
Eval num_timesteps=174200, episode_reward=22.38 +/- 15.38
Episode length: 32.40 +/- 21.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32.4          |
|    mean_reward          | 22.4          |
| time/                   |               |
|    total_timesteps      | 174200        |
| train/                  |               |
|    approx_kl            | 0.00038056524 |
|    clip_fraction        | 0.0114        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0352       |
|    explained_variance   | 0.12          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.885         |
|    n_updates            | 4350          |
|    policy_gradient_loss | -0.00121      |
|    value_loss           | 1.93          |
-------------------------------------------
Progress update from timestep 174300: Updated shield, loss is 0.04974253475666046
Progress update from timestep 174400: Updated shield, loss is 0.051183853298425674
Eval num_timesteps=174400, episode_reward=35.14 +/- 1.51
Episode length: 50.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 35.1     |
| time/              |          |
|    total_timesteps | 174400   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.46     |
|    ep_rew_mean     | 3.5      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 436      |
|    time_elapsed    | 36257    |
|    total_timesteps | 174400   |
---------------------------------
Progress update from timestep 174500: Updated shield, loss is 0.06453615427017212
Progress update from timestep 174600: Updated shield, loss is 0.06359405070543289
Eval num_timesteps=174600, episode_reward=23.83 +/- 14.48
Episode length: 34.40 +/- 19.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.4        |
|    mean_reward          | 23.8        |
| time/                   |             |
|    total_timesteps      | 174600      |
| train/                  |             |
|    approx_kl            | 9.47059e-05 |
|    clip_fraction        | 0.00558     |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0136     |
|    explained_variance   | 0.239       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.78        |
|    n_updates            | 4360        |
|    policy_gradient_loss | -0.000917   |
|    value_loss           | 2.81        |
-----------------------------------------
Progress update from timestep 174700: Updated shield, loss is 0.05003312975168228
Progress update from timestep 174800: Updated shield, loss is 0.06680980324745178
Eval num_timesteps=174800, episode_reward=15.61 +/- 15.89
Episode length: 22.80 +/- 22.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 15.6     |
| time/              |          |
|    total_timesteps | 174800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.47     |
|    ep_rew_mean     | 2.8      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 437      |
|    time_elapsed    | 36347    |
|    total_timesteps | 174800   |
---------------------------------
Progress update from timestep 174900: Updated shield, loss is 0.053934067487716675
Progress update from timestep 175000: Updated shield, loss is 0.06015738844871521
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 175000: Saved shield_loss.csv. 
Eval num_timesteps=175000, episode_reward=23.01 +/- 14.96
Episode length: 33.20 +/- 20.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.2          |
|    mean_reward          | 23            |
| time/                   |               |
|    total_timesteps      | 175000        |
| train/                  |               |
|    approx_kl            | 8.8425455e-05 |
|    clip_fraction        | 0.00201       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0051       |
|    explained_variance   | 0.0359        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.604         |
|    n_updates            | 4370          |
|    policy_gradient_loss | -0.000328     |
|    value_loss           | 2.02          |
-------------------------------------------
Progress update from timestep 175100: Updated shield, loss is 0.06832332909107208
Progress update from timestep 175200: Updated shield, loss is 0.06324632465839386
Eval num_timesteps=175200, episode_reward=6.16 +/- 3.43
Episode length: 9.80 +/- 5.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.8      |
|    mean_reward     | 6.16     |
| time/              |          |
|    total_timesteps | 175200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.84     |
|    ep_rew_mean     | 3.09     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 438      |
|    time_elapsed    | 36426    |
|    total_timesteps | 175200   |
---------------------------------
Progress update from timestep 175300: Updated shield, loss is 0.06737922877073288
Progress update from timestep 175400: Updated shield, loss is 0.061413127928972244
Eval num_timesteps=175400, episode_reward=8.61 +/- 12.56
Episode length: 13.40 +/- 18.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 13.4         |
|    mean_reward          | 8.61         |
| time/                   |              |
|    total_timesteps      | 175400       |
| train/                  |              |
|    approx_kl            | 9.437956e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00325     |
|    explained_variance   | 0.147        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.59         |
|    n_updates            | 4380         |
|    policy_gradient_loss | -1.59e-06    |
|    value_loss           | 2.61         |
------------------------------------------
Progress update from timestep 175500: Updated shield, loss is 0.061945028603076935
Progress update from timestep 175600: Updated shield, loss is 0.05904403328895569
Eval num_timesteps=175600, episode_reward=5.88 +/- 2.11
Episode length: 9.20 +/- 2.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.2      |
|    mean_reward     | 5.88     |
| time/              |          |
|    total_timesteps | 175600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.33     |
|    ep_rew_mean     | 2.7      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 439      |
|    time_elapsed    | 36504    |
|    total_timesteps | 175600   |
---------------------------------
Progress update from timestep 175700: Updated shield, loss is 0.057584479451179504
Progress update from timestep 175800: Updated shield, loss is 0.04647199809551239
Eval num_timesteps=175800, episode_reward=3.33 +/- 2.61
Episode length: 5.60 +/- 3.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 5.6           |
|    mean_reward          | 3.33          |
| time/                   |               |
|    total_timesteps      | 175800        |
| train/                  |               |
|    approx_kl            | 1.3313755e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00665      |
|    explained_variance   | -0.0137       |
|    learning_rate        | 0.0001        |
|    loss                 | 1.34          |
|    n_updates            | 4390          |
|    policy_gradient_loss | -1.16e-05     |
|    value_loss           | 2.25          |
-------------------------------------------
Progress update from timestep 175900: Updated shield, loss is 0.06886954605579376
Progress update from timestep 176000: Updated shield, loss is 0.04872078821063042
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 176000: Saved shield_loss.csv. 
Eval num_timesteps=176000, episode_reward=12.86 +/- 11.69
Episode length: 19.20 +/- 16.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.2     |
|    mean_reward     | 12.9     |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.75     |
|    ep_rew_mean     | 3        |
| time/              |          |
|    fps             | 4        |
|    iterations      | 440      |
|    time_elapsed    | 36576    |
|    total_timesteps | 176000   |
---------------------------------
Progress update from timestep 176100: Updated shield, loss is 0.06027897074818611
Progress update from timestep 176200: Updated shield, loss is 0.06268227845430374
Eval num_timesteps=176200, episode_reward=22.79 +/- 14.37
Episode length: 33.40 +/- 20.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.4         |
|    mean_reward          | 22.8         |
| time/                   |              |
|    total_timesteps      | 176200       |
| train/                  |              |
|    approx_kl            | 1.247307e-11 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00197     |
|    explained_variance   | 0.188        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.714        |
|    n_updates            | 4400         |
|    policy_gradient_loss | -6.59e-07    |
|    value_loss           | 2.03         |
------------------------------------------
Progress update from timestep 176300: Updated shield, loss is 0.057991333305835724
Progress update from timestep 176400: Updated shield, loss is 0.059523437172174454
Eval num_timesteps=176400, episode_reward=29.33 +/- 12.52
Episode length: 41.40 +/- 17.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | 29.3     |
| time/              |          |
|    total_timesteps | 176400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.19     |
|    ep_rew_mean     | 3.26     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 441      |
|    time_elapsed    | 36709    |
|    total_timesteps | 176400   |
---------------------------------
Progress update from timestep 176500: Updated shield, loss is 0.05767880752682686
Progress update from timestep 176600: Updated shield, loss is 0.05854562297463417
Eval num_timesteps=176600, episode_reward=10.91 +/- 12.00
Episode length: 16.40 +/- 17.10
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.4          |
|    mean_reward          | 10.9          |
| time/                   |               |
|    total_timesteps      | 176600        |
| train/                  |               |
|    approx_kl            | -1.247307e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.000411     |
|    explained_variance   | 0.331         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.892         |
|    n_updates            | 4410          |
|    policy_gradient_loss | -1.23e-07     |
|    value_loss           | 2.02          |
-------------------------------------------
Progress update from timestep 176700: Updated shield, loss is 0.06782995909452438
Progress update from timestep 176800: Updated shield, loss is 0.0526890866458416
Eval num_timesteps=176800, episode_reward=21.67 +/- 12.72
Episode length: 31.40 +/- 16.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.4     |
|    mean_reward     | 21.7     |
| time/              |          |
|    total_timesteps | 176800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.04     |
|    ep_rew_mean     | 2.46     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 442      |
|    time_elapsed    | 36797    |
|    total_timesteps | 176800   |
---------------------------------
Progress update from timestep 176900: Updated shield, loss is 0.04525027796626091
Progress update from timestep 177000: Updated shield, loss is 0.05330606549978256
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 177000: Saved shield_loss.csv. 
Eval num_timesteps=177000, episode_reward=17.82 +/- 15.07
Episode length: 25.20 +/- 20.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.2          |
|    mean_reward          | 17.8          |
| time/                   |               |
|    total_timesteps      | 177000        |
| train/                  |               |
|    approx_kl            | 3.0565894e-05 |
|    clip_fraction        | 0.0029        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.016        |
|    explained_variance   | 0.182         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.583         |
|    n_updates            | 4420          |
|    policy_gradient_loss | -0.000353     |
|    value_loss           | 1.43          |
-------------------------------------------
Progress update from timestep 177100: Updated shield, loss is 0.06299003958702087
Progress update from timestep 177200: Updated shield, loss is 0.0638182982802391
Eval num_timesteps=177200, episode_reward=22.44 +/- 15.68
Episode length: 32.20 +/- 21.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 22.4     |
| time/              |          |
|    total_timesteps | 177200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.53     |
|    ep_rew_mean     | 2.97     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 443      |
|    time_elapsed    | 36893    |
|    total_timesteps | 177200   |
---------------------------------
Progress update from timestep 177300: Updated shield, loss is 0.04928392544388771
Progress update from timestep 177400: Updated shield, loss is 0.05049177631735802
Eval num_timesteps=177400, episode_reward=17.85 +/- 13.79
Episode length: 26.40 +/- 19.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.4         |
|    mean_reward          | 17.9         |
| time/                   |              |
|    total_timesteps      | 177400       |
| train/                  |              |
|    approx_kl            | 9.876855e-05 |
|    clip_fraction        | 0.00179      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00638     |
|    explained_variance   | -0.0139      |
|    learning_rate        | 0.0001       |
|    loss                 | 1.46         |
|    n_updates            | 4430         |
|    policy_gradient_loss | -0.000361    |
|    value_loss           | 2.35         |
------------------------------------------
Progress update from timestep 177500: Updated shield, loss is 0.054757729172706604
Progress update from timestep 177600: Updated shield, loss is 0.058011628687381744
Eval num_timesteps=177600, episode_reward=16.53 +/- 14.25
Episode length: 24.80 +/- 20.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 177600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.32     |
|    ep_rew_mean     | 2.76     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 444      |
|    time_elapsed    | 36990    |
|    total_timesteps | 177600   |
---------------------------------
Progress update from timestep 177700: Updated shield, loss is 0.05420363321900368
Progress update from timestep 177800: Updated shield, loss is 0.055279117077589035
Eval num_timesteps=177800, episode_reward=16.06 +/- 16.09
Episode length: 23.20 +/- 22.10
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.2          |
|    mean_reward          | 16.1          |
| time/                   |               |
|    total_timesteps      | 177800        |
| train/                  |               |
|    approx_kl            | 0.00020050278 |
|    clip_fraction        | 0.00603       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0123       |
|    explained_variance   | 0.292         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.439         |
|    n_updates            | 4440          |
|    policy_gradient_loss | -0.00068      |
|    value_loss           | 1.37          |
-------------------------------------------
Progress update from timestep 177900: Updated shield, loss is 0.05267597362399101
Progress update from timestep 178000: Updated shield, loss is 0.05651799589395523
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 178000: Saved shield_loss.csv. 
Eval num_timesteps=178000, episode_reward=6.38 +/- 2.56
Episode length: 9.80 +/- 3.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.8      |
|    mean_reward     | 6.38     |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.2      |
|    ep_rew_mean     | 2.62     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 445      |
|    time_elapsed    | 37070    |
|    total_timesteps | 178000   |
---------------------------------
Progress update from timestep 178100: Updated shield, loss is 0.05247167870402336
Progress update from timestep 178200: Updated shield, loss is 0.0554872564971447
Eval num_timesteps=178200, episode_reward=11.61 +/- 11.44
Episode length: 17.60 +/- 16.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.6         |
|    mean_reward          | 11.6         |
| time/                   |              |
|    total_timesteps      | 178200       |
| train/                  |              |
|    approx_kl            | 9.384946e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00283     |
|    explained_variance   | 0.257        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.783        |
|    n_updates            | 4450         |
|    policy_gradient_loss | -4.49e-06    |
|    value_loss           | 1.32         |
------------------------------------------
Progress update from timestep 178300: Updated shield, loss is 0.048163775354623795
Progress update from timestep 178400: Updated shield, loss is 0.057852234691381454
Eval num_timesteps=178400, episode_reward=11.28 +/- 11.51
Episode length: 17.40 +/- 16.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 11.3     |
| time/              |          |
|    total_timesteps | 178400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.68     |
|    ep_rew_mean     | 2.22     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 446      |
|    time_elapsed    | 37165    |
|    total_timesteps | 178400   |
---------------------------------
Progress update from timestep 178500: Updated shield, loss is 0.060401514172554016
Progress update from timestep 178600: Updated shield, loss is 0.0570799894630909
Eval num_timesteps=178600, episode_reward=16.39 +/- 14.32
Episode length: 24.60 +/- 20.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.6         |
|    mean_reward          | 16.4         |
| time/                   |              |
|    total_timesteps      | 178600       |
| train/                  |              |
|    approx_kl            | 7.767604e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00247     |
|    explained_variance   | 0.0566       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.66         |
|    n_updates            | 4460         |
|    policy_gradient_loss | -8.89e-06    |
|    value_loss           | 1.58         |
------------------------------------------
Progress update from timestep 178700: Updated shield, loss is 0.06970079988241196
Progress update from timestep 178800: Updated shield, loss is 0.06146598979830742
Eval num_timesteps=178800, episode_reward=12.93 +/- 10.91
Episode length: 19.40 +/- 15.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 12.9     |
| time/              |          |
|    total_timesteps | 178800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.19     |
|    ep_rew_mean     | 2.69     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 447      |
|    time_elapsed    | 37257    |
|    total_timesteps | 178800   |
---------------------------------
Progress update from timestep 178900: Updated shield, loss is 0.057158272713422775
Progress update from timestep 179000: Updated shield, loss is 0.061977386474609375
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 179000: Saved shield_loss.csv. 
Eval num_timesteps=179000, episode_reward=10.80 +/- 11.58
Episode length: 16.60 +/- 17.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.6          |
|    mean_reward          | 10.8          |
| time/                   |               |
|    total_timesteps      | 179000        |
| train/                  |               |
|    approx_kl            | 0.00094109547 |
|    clip_fraction        | 0.00982       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0369       |
|    explained_variance   | 0.217         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.03          |
|    n_updates            | 4470          |
|    policy_gradient_loss | -0.0015       |
|    value_loss           | 1.61          |
-------------------------------------------
Progress update from timestep 179100: Updated shield, loss is 0.059907179325819016
Progress update from timestep 179200: Updated shield, loss is 0.05994314327836037
Eval num_timesteps=179200, episode_reward=9.65 +/- 12.07
Episode length: 14.80 +/- 17.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 9.65     |
| time/              |          |
|    total_timesteps | 179200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.95     |
|    ep_rew_mean     | 2.45     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 448      |
|    time_elapsed    | 37347    |
|    total_timesteps | 179200   |
---------------------------------
Progress update from timestep 179300: Updated shield, loss is 0.056631527841091156
Progress update from timestep 179400: Updated shield, loss is 0.05478863790631294
Eval num_timesteps=179400, episode_reward=21.60 +/- 16.26
Episode length: 31.40 +/- 22.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 31.4          |
|    mean_reward          | 21.6          |
| time/                   |               |
|    total_timesteps      | 179400        |
| train/                  |               |
|    approx_kl            | 3.2222097e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00141      |
|    explained_variance   | 0.126         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.03          |
|    n_updates            | 4480          |
|    policy_gradient_loss | -1.11e-06     |
|    value_loss           | 1.65          |
-------------------------------------------
Progress update from timestep 179500: Updated shield, loss is 0.07686338573694229
Progress update from timestep 179600: Updated shield, loss is 0.0569731742143631
Eval num_timesteps=179600, episode_reward=9.87 +/- 11.77
Episode length: 15.20 +/- 17.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | 9.87     |
| time/              |          |
|    total_timesteps | 179600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.47     |
|    ep_rew_mean     | 2.83     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 449      |
|    time_elapsed    | 37440    |
|    total_timesteps | 179600   |
---------------------------------
Progress update from timestep 179700: Updated shield, loss is 0.05775827914476395
Progress update from timestep 179800: Updated shield, loss is 0.05142660439014435
Eval num_timesteps=179800, episode_reward=17.41 +/- 14.89
Episode length: 25.20 +/- 20.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.2          |
|    mean_reward          | 17.4          |
| time/                   |               |
|    total_timesteps      | 179800        |
| train/                  |               |
|    approx_kl            | 0.00024550123 |
|    clip_fraction        | 0.00513       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0115       |
|    explained_variance   | 0.273         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.563         |
|    n_updates            | 4490          |
|    policy_gradient_loss | -0.000923     |
|    value_loss           | 1.66          |
-------------------------------------------
Progress update from timestep 179900: Updated shield, loss is 0.05276588723063469
Progress update from timestep 180000: Updated shield, loss is 0.05683088302612305
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 180000: Saved shield_loss.csv. 
Eval num_timesteps=180000, episode_reward=19.25 +/- 13.67
Episode length: 27.80 +/- 18.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.8     |
|    mean_reward     | 19.2     |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.55     |
|    ep_rew_mean     | 2.91     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 450      |
|    time_elapsed    | 37551    |
|    total_timesteps | 180000   |
---------------------------------
Progress update from timestep 180100: Updated shield, loss is 0.05803566426038742
Progress update from timestep 180200: Updated shield, loss is 0.057814378291368484
Eval num_timesteps=180200, episode_reward=10.76 +/- 12.54
Episode length: 16.40 +/- 18.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.4          |
|    mean_reward          | 10.8          |
| time/                   |               |
|    total_timesteps      | 180200        |
| train/                  |               |
|    approx_kl            | 0.00030114476 |
|    clip_fraction        | 0.00915       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0277       |
|    explained_variance   | 0.153         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.16          |
|    n_updates            | 4500          |
|    policy_gradient_loss | -0.00106      |
|    value_loss           | 2.16          |
-------------------------------------------
Progress update from timestep 180300: Updated shield, loss is 0.056804854422807693
Progress update from timestep 180400: Updated shield, loss is 0.05792633816599846
Eval num_timesteps=180400, episode_reward=11.57 +/- 12.04
Episode length: 17.20 +/- 16.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | 11.6     |
| time/              |          |
|    total_timesteps | 180400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.92     |
|    ep_rew_mean     | 3.29     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 451      |
|    time_elapsed    | 37641    |
|    total_timesteps | 180400   |
---------------------------------
Progress update from timestep 180500: Updated shield, loss is 0.04982028529047966
Progress update from timestep 180600: Updated shield, loss is 0.04902678728103638
Eval num_timesteps=180600, episode_reward=29.41 +/- 11.29
Episode length: 42.20 +/- 15.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.2         |
|    mean_reward          | 29.4         |
| time/                   |              |
|    total_timesteps      | 180600       |
| train/                  |              |
|    approx_kl            | 8.087695e-05 |
|    clip_fraction        | 0.00313      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0105      |
|    explained_variance   | 0.0696       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.603        |
|    n_updates            | 4510         |
|    policy_gradient_loss | -0.000429    |
|    value_loss           | 2.37         |
------------------------------------------
Progress update from timestep 180700: Updated shield, loss is 0.060174696147441864
Progress update from timestep 180800: Updated shield, loss is 0.057977672666311264
Eval num_timesteps=180800, episode_reward=13.05 +/- 13.42
Episode length: 18.60 +/- 17.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 13.1     |
| time/              |          |
|    total_timesteps | 180800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.54     |
|    ep_rew_mean     | 2.9      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 452      |
|    time_elapsed    | 37752    |
|    total_timesteps | 180800   |
---------------------------------
Progress update from timestep 180900: Updated shield, loss is 0.041870035231113434
Progress update from timestep 181000: Updated shield, loss is 0.04337093234062195
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 181000: Saved shield_loss.csv. 
Eval num_timesteps=181000, episode_reward=24.17 +/- 13.15
Episode length: 35.00 +/- 18.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35            |
|    mean_reward          | 24.2          |
| time/                   |               |
|    total_timesteps      | 181000        |
| train/                  |               |
|    approx_kl            | 2.4703954e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00316      |
|    explained_variance   | 0.147         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.31          |
|    n_updates            | 4520          |
|    policy_gradient_loss | -1.03e-05     |
|    value_loss           | 1.87          |
-------------------------------------------
Progress update from timestep 181100: Updated shield, loss is 0.059559814631938934
Progress update from timestep 181200: Updated shield, loss is 0.056093353778123856
Eval num_timesteps=181200, episode_reward=9.90 +/- 11.79
Episode length: 15.40 +/- 17.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | 9.9      |
| time/              |          |
|    total_timesteps | 181200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.77     |
|    ep_rew_mean     | 3.01     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 453      |
|    time_elapsed    | 37862    |
|    total_timesteps | 181200   |
---------------------------------
Progress update from timestep 181300: Updated shield, loss is 0.06436266005039215
Progress update from timestep 181400: Updated shield, loss is 0.06272020936012268
Eval num_timesteps=181400, episode_reward=5.14 +/- 5.19
Episode length: 8.20 +/- 7.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 8.2           |
|    mean_reward          | 5.14          |
| time/                   |               |
|    total_timesteps      | 181400        |
| train/                  |               |
|    approx_kl            | 6.0182564e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00238      |
|    explained_variance   | 0.187         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.55          |
|    n_updates            | 4530          |
|    policy_gradient_loss | -1.02e-06     |
|    value_loss           | 2.63          |
-------------------------------------------
Progress update from timestep 181500: Updated shield, loss is 0.059553805738687515
Progress update from timestep 181600: Updated shield, loss is 0.06110403686761856
Eval num_timesteps=181600, episode_reward=12.62 +/- 11.99
Episode length: 18.80 +/- 17.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.8     |
|    mean_reward     | 12.6     |
| time/              |          |
|    total_timesteps | 181600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.29     |
|    ep_rew_mean     | 2.72     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 454      |
|    time_elapsed    | 37953    |
|    total_timesteps | 181600   |
---------------------------------
Progress update from timestep 181700: Updated shield, loss is 0.05129576474428177
Progress update from timestep 181800: Updated shield, loss is 0.06144228205084801
Eval num_timesteps=181800, episode_reward=11.98 +/- 13.11
Episode length: 18.20 +/- 18.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 18.2          |
|    mean_reward          | 12            |
| time/                   |               |
|    total_timesteps      | 181800        |
| train/                  |               |
|    approx_kl            | 3.1182676e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00107      |
|    explained_variance   | 0.209         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.812         |
|    n_updates            | 4540          |
|    policy_gradient_loss | -2.2e-07      |
|    value_loss           | 1.41          |
-------------------------------------------
Progress update from timestep 181900: Updated shield, loss is 0.05160612612962723
Progress update from timestep 182000: Updated shield, loss is 0.05342516303062439
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 182000: Saved shield_loss.csv. 
Eval num_timesteps=182000, episode_reward=28.16 +/- 12.68
Episode length: 41.00 +/- 18.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41       |
|    mean_reward     | 28.2     |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.06     |
|    ep_rew_mean     | 2.54     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 455      |
|    time_elapsed    | 38064    |
|    total_timesteps | 182000   |
---------------------------------
Progress update from timestep 182100: Updated shield, loss is 0.05950527638196945
Progress update from timestep 182200: Updated shield, loss is 0.04873703047633171
Eval num_timesteps=182200, episode_reward=23.80 +/- 13.58
Episode length: 34.60 +/- 18.93
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.6          |
|    mean_reward          | 23.8          |
| time/                   |               |
|    total_timesteps      | 182200        |
| train/                  |               |
|    approx_kl            | 0.00025201432 |
|    clip_fraction        | 0.0058        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.011        |
|    explained_variance   | 0.29          |
|    learning_rate        | 0.0001        |
|    loss                 | 1.03          |
|    n_updates            | 4550          |
|    policy_gradient_loss | -0.000582     |
|    value_loss           | 1.83          |
-------------------------------------------
Progress update from timestep 182300: Updated shield, loss is 0.04798552393913269
Progress update from timestep 182400: Updated shield, loss is 0.054509274661540985
Eval num_timesteps=182400, episode_reward=21.75 +/- 15.13
Episode length: 32.20 +/- 21.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 21.8     |
| time/              |          |
|    total_timesteps | 182400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.11     |
|    ep_rew_mean     | 2.56     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 456      |
|    time_elapsed    | 38178    |
|    total_timesteps | 182400   |
---------------------------------
Progress update from timestep 182500: Updated shield, loss is 0.057751141488552094
Progress update from timestep 182600: Updated shield, loss is 0.05730133876204491
Eval num_timesteps=182600, episode_reward=22.99 +/- 14.15
Episode length: 33.60 +/- 20.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.6          |
|    mean_reward          | 23            |
| time/                   |               |
|    total_timesteps      | 182600        |
| train/                  |               |
|    approx_kl            | 1.2591695e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00958      |
|    explained_variance   | 0.242         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.351         |
|    n_updates            | 4560          |
|    policy_gradient_loss | -8.82e-05     |
|    value_loss           | 1.31          |
-------------------------------------------
Progress update from timestep 182700: Updated shield, loss is 0.0595855638384819
Progress update from timestep 182800: Updated shield, loss is 0.06277298182249069
Eval num_timesteps=182800, episode_reward=18.35 +/- 14.77
Episode length: 26.00 +/- 19.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | 18.3     |
| time/              |          |
|    total_timesteps | 182800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.07     |
|    ep_rew_mean     | 2.53     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 457      |
|    time_elapsed    | 38295    |
|    total_timesteps | 182800   |
---------------------------------
Progress update from timestep 182900: Updated shield, loss is 0.05052466690540314
Progress update from timestep 183000: Updated shield, loss is 0.05592486634850502
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 183000: Saved shield_loss.csv. 
Eval num_timesteps=183000, episode_reward=28.21 +/- 11.45
Episode length: 41.80 +/- 16.40
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 41.8           |
|    mean_reward          | 28.2           |
| time/                   |                |
|    total_timesteps      | 183000         |
| train/                  |                |
|    approx_kl            | 1.48258705e-05 |
|    clip_fraction        | 0.00156        |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.00788       |
|    explained_variance   | 0.217          |
|    learning_rate        | 0.0001         |
|    loss                 | 0.763          |
|    n_updates            | 4570           |
|    policy_gradient_loss | -0.000246      |
|    value_loss           | 1.29           |
--------------------------------------------
Progress update from timestep 183100: Updated shield, loss is 0.0616292767226696
Progress update from timestep 183200: Updated shield, loss is 0.06462982296943665
Eval num_timesteps=183200, episode_reward=12.32 +/- 12.74
Episode length: 18.20 +/- 17.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | 12.3     |
| time/              |          |
|    total_timesteps | 183200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.74     |
|    ep_rew_mean     | 3.08     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 458      |
|    time_elapsed    | 38407    |
|    total_timesteps | 183200   |
---------------------------------
Progress update from timestep 183300: Updated shield, loss is 0.05617399886250496
Progress update from timestep 183400: Updated shield, loss is 0.05364232510328293
Eval num_timesteps=183400, episode_reward=18.89 +/- 13.89
Episode length: 28.00 +/- 19.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 28           |
|    mean_reward          | 18.9         |
| time/                   |              |
|    total_timesteps      | 183400       |
| train/                  |              |
|    approx_kl            | 8.954875e-05 |
|    clip_fraction        | 0.00268      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0105      |
|    explained_variance   | 0.174        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.15         |
|    n_updates            | 4580         |
|    policy_gradient_loss | -0.00048     |
|    value_loss           | 2.16         |
------------------------------------------
Progress update from timestep 183500: Updated shield, loss is 0.05233796313405037
Progress update from timestep 183600: Updated shield, loss is 0.05729614943265915
Eval num_timesteps=183600, episode_reward=16.81 +/- 14.05
Episode length: 25.20 +/- 20.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 183600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.14     |
|    ep_rew_mean     | 2.59     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 459      |
|    time_elapsed    | 38501    |
|    total_timesteps | 183600   |
---------------------------------
Progress update from timestep 183700: Updated shield, loss is 0.0583464577794075
Progress update from timestep 183800: Updated shield, loss is 0.05409200116991997
Eval num_timesteps=183800, episode_reward=16.22 +/- 14.08
Episode length: 24.80 +/- 20.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.8         |
|    mean_reward          | 16.2         |
| time/                   |              |
|    total_timesteps      | 183800       |
| train/                  |              |
|    approx_kl            | 6.632384e-05 |
|    clip_fraction        | 0.00179      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00697     |
|    explained_variance   | 0.0879       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.897        |
|    n_updates            | 4590         |
|    policy_gradient_loss | -0.000168    |
|    value_loss           | 1.54         |
------------------------------------------
Progress update from timestep 183900: Updated shield, loss is 0.056082386523485184
Progress update from timestep 184000: Updated shield, loss is 0.04803228750824928
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 184000: Saved shield_loss.csv. 
Eval num_timesteps=184000, episode_reward=14.64 +/- 16.21
Episode length: 21.80 +/- 23.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | 14.6     |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.27     |
|    ep_rew_mean     | 2.67     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 460      |
|    time_elapsed    | 38593    |
|    total_timesteps | 184000   |
---------------------------------
Progress update from timestep 184100: Updated shield, loss is 0.047557532787323
Progress update from timestep 184200: Updated shield, loss is 0.04964979365468025
Eval num_timesteps=184200, episode_reward=22.36 +/- 13.93
Episode length: 33.40 +/- 20.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.4          |
|    mean_reward          | 22.4          |
| time/                   |               |
|    total_timesteps      | 184200        |
| train/                  |               |
|    approx_kl            | 0.00011187765 |
|    clip_fraction        | 0.00759       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00823      |
|    explained_variance   | 0.304         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.245         |
|    n_updates            | 4600          |
|    policy_gradient_loss | -0.000462     |
|    value_loss           | 1.26          |
-------------------------------------------
Progress update from timestep 184300: Updated shield, loss is 0.0503767728805542
Progress update from timestep 184400: Updated shield, loss is 0.05059899762272835
Eval num_timesteps=184400, episode_reward=17.35 +/- 14.78
Episode length: 25.20 +/- 20.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 17.3     |
| time/              |          |
|    total_timesteps | 184400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.34     |
|    ep_rew_mean     | 2.72     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 461      |
|    time_elapsed    | 38708    |
|    total_timesteps | 184400   |
---------------------------------
Progress update from timestep 184500: Updated shield, loss is 0.05149107426404953
Progress update from timestep 184600: Updated shield, loss is 0.058283619582653046
Eval num_timesteps=184600, episode_reward=15.39 +/- 12.40
Episode length: 22.80 +/- 18.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22.8         |
|    mean_reward          | 15.4         |
| time/                   |              |
|    total_timesteps      | 184600       |
| train/                  |              |
|    approx_kl            | 3.468617e-05 |
|    clip_fraction        | 0.00134      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0092      |
|    explained_variance   | 0.108        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.611        |
|    n_updates            | 4610         |
|    policy_gradient_loss | -0.000211    |
|    value_loss           | 1.29         |
------------------------------------------
Progress update from timestep 184700: Updated shield, loss is 0.06093011423945427
Progress update from timestep 184800: Updated shield, loss is 0.05854345113039017
Eval num_timesteps=184800, episode_reward=28.19 +/- 13.73
Episode length: 40.40 +/- 19.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.4     |
|    mean_reward     | 28.2     |
| time/              |          |
|    total_timesteps | 184800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.45     |
|    ep_rew_mean     | 2.77     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 462      |
|    time_elapsed    | 38819    |
|    total_timesteps | 184800   |
---------------------------------
Progress update from timestep 184900: Updated shield, loss is 0.05100259929895401
Progress update from timestep 185000: Updated shield, loss is 0.056081946939229965
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 185000: Saved shield_loss.csv. 
Eval num_timesteps=185000, episode_reward=29.54 +/- 12.11
Episode length: 41.80 +/- 16.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 41.8          |
|    mean_reward          | 29.5          |
| time/                   |               |
|    total_timesteps      | 185000        |
| train/                  |               |
|    approx_kl            | 0.00011878344 |
|    clip_fraction        | 0.00402       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0152       |
|    explained_variance   | 0.171         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.718         |
|    n_updates            | 4620          |
|    policy_gradient_loss | -0.000595     |
|    value_loss           | 1.84          |
-------------------------------------------
Progress update from timestep 185100: Updated shield, loss is 0.04610501974821091
Progress update from timestep 185200: Updated shield, loss is 0.0485403835773468
Eval num_timesteps=185200, episode_reward=9.51 +/- 12.07
Episode length: 14.80 +/- 17.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 9.51     |
| time/              |          |
|    total_timesteps | 185200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.78     |
|    ep_rew_mean     | 3.09     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 463      |
|    time_elapsed    | 38929    |
|    total_timesteps | 185200   |
---------------------------------
Progress update from timestep 185300: Updated shield, loss is 0.046806950122117996
Progress update from timestep 185400: Updated shield, loss is 0.057890526950359344
Eval num_timesteps=185400, episode_reward=16.70 +/- 15.61
Episode length: 24.00 +/- 21.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24            |
|    mean_reward          | 16.7          |
| time/                   |               |
|    total_timesteps      | 185400        |
| train/                  |               |
|    approx_kl            | 0.00018183279 |
|    clip_fraction        | 0.00335       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00527      |
|    explained_variance   | 0.116         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.996         |
|    n_updates            | 4630          |
|    policy_gradient_loss | -0.000222     |
|    value_loss           | 1.98          |
-------------------------------------------
Progress update from timestep 185500: Updated shield, loss is 0.05904112383723259
Progress update from timestep 185600: Updated shield, loss is 0.051026925444602966
Eval num_timesteps=185600, episode_reward=28.62 +/- 11.74
Episode length: 41.60 +/- 16.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 28.6     |
| time/              |          |
|    total_timesteps | 185600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.1      |
|    ep_rew_mean     | 2.6      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 464      |
|    time_elapsed    | 39044    |
|    total_timesteps | 185600   |
---------------------------------
Progress update from timestep 185700: Updated shield, loss is 0.056938286870718
Progress update from timestep 185800: Updated shield, loss is 0.0631752610206604
Eval num_timesteps=185800, episode_reward=34.92 +/- 1.13
Episode length: 50.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 50            |
|    mean_reward          | 34.9          |
| time/                   |               |
|    total_timesteps      | 185800        |
| train/                  |               |
|    approx_kl            | 0.00028033383 |
|    clip_fraction        | 0.00469       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0173       |
|    explained_variance   | 0.179         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.01          |
|    n_updates            | 4640          |
|    policy_gradient_loss | -0.000783     |
|    value_loss           | 1.57          |
-------------------------------------------
Progress update from timestep 185900: Updated shield, loss is 0.04939623177051544
Progress update from timestep 186000: Updated shield, loss is 0.05626784637570381
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 186000: Saved shield_loss.csv. 
Eval num_timesteps=186000, episode_reward=4.33 +/- 1.39
Episode length: 7.20 +/- 2.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7.2      |
|    mean_reward     | 4.33     |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.63     |
|    ep_rew_mean     | 2.96     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 465      |
|    time_elapsed    | 39140    |
|    total_timesteps | 186000   |
---------------------------------
Progress update from timestep 186100: Updated shield, loss is 0.05878268554806709
Progress update from timestep 186200: Updated shield, loss is 0.05381530895829201
Eval num_timesteps=186200, episode_reward=25.63 +/- 12.45
Episode length: 36.80 +/- 16.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.8          |
|    mean_reward          | 25.6          |
| time/                   |               |
|    total_timesteps      | 186200        |
| train/                  |               |
|    approx_kl            | 0.00018102395 |
|    clip_fraction        | 0.00603       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0188       |
|    explained_variance   | 0.2           |
|    learning_rate        | 0.0001        |
|    loss                 | 1.36          |
|    n_updates            | 4650          |
|    policy_gradient_loss | -0.000386     |
|    value_loss           | 2.01          |
-------------------------------------------
Progress update from timestep 186300: Updated shield, loss is 0.05666905269026756
Progress update from timestep 186400: Updated shield, loss is 0.05577579885721207
Eval num_timesteps=186400, episode_reward=24.72 +/- 12.46
Episode length: 35.80 +/- 17.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 24.7     |
| time/              |          |
|    total_timesteps | 186400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.52     |
|    ep_rew_mean     | 2.86     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 466      |
|    time_elapsed    | 39246    |
|    total_timesteps | 186400   |
---------------------------------
Progress update from timestep 186500: Updated shield, loss is 0.05543401464819908
Progress update from timestep 186600: Updated shield, loss is 0.059281278401613235
Eval num_timesteps=186600, episode_reward=24.29 +/- 13.34
Episode length: 35.20 +/- 18.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.2          |
|    mean_reward          | 24.3          |
| time/                   |               |
|    total_timesteps      | 186600        |
| train/                  |               |
|    approx_kl            | 4.2438343e-05 |
|    clip_fraction        | 0.00179       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0207       |
|    explained_variance   | 0.0871        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.746         |
|    n_updates            | 4660          |
|    policy_gradient_loss | -0.000372     |
|    value_loss           | 1.79          |
-------------------------------------------
Progress update from timestep 186700: Updated shield, loss is 0.04997284337878227
Progress update from timestep 186800: Updated shield, loss is 0.05891282111406326
Eval num_timesteps=186800, episode_reward=21.43 +/- 15.55
Episode length: 31.80 +/- 22.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.8     |
|    mean_reward     | 21.4     |
| time/              |          |
|    total_timesteps | 186800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.21     |
|    ep_rew_mean     | 2.65     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 467      |
|    time_elapsed    | 39366    |
|    total_timesteps | 186800   |
---------------------------------
Progress update from timestep 186900: Updated shield, loss is 0.05149015411734581
Progress update from timestep 187000: Updated shield, loss is 0.04999165236949921
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 187000: Saved shield_loss.csv. 
Eval num_timesteps=187000, episode_reward=16.30 +/- 14.11
Episode length: 24.60 +/- 20.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.6          |
|    mean_reward          | 16.3          |
| time/                   |               |
|    total_timesteps      | 187000        |
| train/                  |               |
|    approx_kl            | 0.00018528184 |
|    clip_fraction        | 0.00982       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0197       |
|    explained_variance   | 0.174         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.603         |
|    n_updates            | 4670          |
|    policy_gradient_loss | -0.000845     |
|    value_loss           | 1.5           |
-------------------------------------------
Progress update from timestep 187100: Updated shield, loss is 0.04791349172592163
Progress update from timestep 187200: Updated shield, loss is 0.05444997176527977
Eval num_timesteps=187200, episode_reward=19.38 +/- 13.24
Episode length: 28.20 +/- 18.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.2     |
|    mean_reward     | 19.4     |
| time/              |          |
|    total_timesteps | 187200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.45     |
|    ep_rew_mean     | 2.76     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 468      |
|    time_elapsed    | 39464    |
|    total_timesteps | 187200   |
---------------------------------
Progress update from timestep 187300: Updated shield, loss is 0.0625789612531662
Progress update from timestep 187400: Updated shield, loss is 0.059140272438526154
Eval num_timesteps=187400, episode_reward=14.69 +/- 15.72
Episode length: 22.20 +/- 22.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.2          |
|    mean_reward          | 14.7          |
| time/                   |               |
|    total_timesteps      | 187400        |
| train/                  |               |
|    approx_kl            | 0.00013470344 |
|    clip_fraction        | 0.0058        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00617      |
|    explained_variance   | 0.239         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.41          |
|    n_updates            | 4680          |
|    policy_gradient_loss | -0.000451     |
|    value_loss           | 1.76          |
-------------------------------------------
Progress update from timestep 187500: Updated shield, loss is 0.06452780961990356
Progress update from timestep 187600: Updated shield, loss is 0.05926716700196266
Eval num_timesteps=187600, episode_reward=19.07 +/- 14.56
Episode length: 27.20 +/- 19.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.2     |
|    mean_reward     | 19.1     |
| time/              |          |
|    total_timesteps | 187600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.18     |
|    ep_rew_mean     | 2.63     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 469      |
|    time_elapsed    | 39555    |
|    total_timesteps | 187600   |
---------------------------------
Progress update from timestep 187700: Updated shield, loss is 0.07299686968326569
Progress update from timestep 187800: Updated shield, loss is 0.05721411481499672
Eval num_timesteps=187800, episode_reward=15.05 +/- 15.43
Episode length: 22.60 +/- 22.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.6          |
|    mean_reward          | 15            |
| time/                   |               |
|    total_timesteps      | 187800        |
| train/                  |               |
|    approx_kl            | 0.00015282138 |
|    clip_fraction        | 0.00223       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0131       |
|    explained_variance   | 0.162         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.2           |
|    n_updates            | 4690          |
|    policy_gradient_loss | -0.000355     |
|    value_loss           | 1.79          |
-------------------------------------------
Progress update from timestep 187900: Updated shield, loss is 0.05375881865620613
Progress update from timestep 188000: Updated shield, loss is 0.05726175010204315
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 188000: Saved shield_loss.csv. 
Eval num_timesteps=188000, episode_reward=17.13 +/- 15.08
Episode length: 24.80 +/- 20.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 17.1     |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.12     |
|    ep_rew_mean     | 2.6      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 470      |
|    time_elapsed    | 39647    |
|    total_timesteps | 188000   |
---------------------------------
Progress update from timestep 188100: Updated shield, loss is 0.05527573451399803
Progress update from timestep 188200: Updated shield, loss is 0.05730653554201126
Eval num_timesteps=188200, episode_reward=16.20 +/- 14.48
Episode length: 24.40 +/- 20.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.4          |
|    mean_reward          | 16.2          |
| time/                   |               |
|    total_timesteps      | 188200        |
| train/                  |               |
|    approx_kl            | 4.5636066e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00684      |
|    explained_variance   | 0.161         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.722         |
|    n_updates            | 4700          |
|    policy_gradient_loss | -0.000101     |
|    value_loss           | 1.67          |
-------------------------------------------
Progress update from timestep 188300: Updated shield, loss is 0.046512868255376816
Progress update from timestep 188400: Updated shield, loss is 0.046141255646944046
Eval num_timesteps=188400, episode_reward=16.76 +/- 14.06
Episode length: 25.00 +/- 20.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 188400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.37     |
|    ep_rew_mean     | 2.76     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 471      |
|    time_elapsed    | 39742    |
|    total_timesteps | 188400   |
---------------------------------
Progress update from timestep 188500: Updated shield, loss is 0.05782359838485718
Progress update from timestep 188600: Updated shield, loss is 0.05408367142081261
Eval num_timesteps=188600, episode_reward=22.51 +/- 14.66
Episode length: 33.00 +/- 20.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33            |
|    mean_reward          | 22.5          |
| time/                   |               |
|    total_timesteps      | 188600        |
| train/                  |               |
|    approx_kl            | 1.0753865e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00479      |
|    explained_variance   | 0.174         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.698         |
|    n_updates            | 4710          |
|    policy_gradient_loss | -2.65e-05     |
|    value_loss           | 1.35          |
-------------------------------------------
Progress update from timestep 188700: Updated shield, loss is 0.0538778193295002
Progress update from timestep 188800: Updated shield, loss is 0.060007985681295395
Eval num_timesteps=188800, episode_reward=5.28 +/- 2.75
Episode length: 8.40 +/- 3.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8.4      |
|    mean_reward     | 5.28     |
| time/              |          |
|    total_timesteps | 188800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.99     |
|    ep_rew_mean     | 2.43     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 472      |
|    time_elapsed    | 39822    |
|    total_timesteps | 188800   |
---------------------------------
Progress update from timestep 188900: Updated shield, loss is 0.05876921862363815
Progress update from timestep 189000: Updated shield, loss is 0.06324450671672821
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 189000: Saved shield_loss.csv. 
Eval num_timesteps=189000, episode_reward=3.20 +/- 1.29
Episode length: 5.60 +/- 1.85
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 5.6           |
|    mean_reward          | 3.2           |
| time/                   |               |
|    total_timesteps      | 189000        |
| train/                  |               |
|    approx_kl            | 0.00018293396 |
|    clip_fraction        | 0.00469       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0188       |
|    explained_variance   | 0.0515        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.338         |
|    n_updates            | 4720          |
|    policy_gradient_loss | -0.000367     |
|    value_loss           | 1.6           |
-------------------------------------------
Progress update from timestep 189100: Updated shield, loss is 0.05214519426226616
Progress update from timestep 189200: Updated shield, loss is 0.05924725905060768
Eval num_timesteps=189200, episode_reward=21.91 +/- 16.75
Episode length: 31.20 +/- 23.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.2     |
|    mean_reward     | 21.9     |
| time/              |          |
|    total_timesteps | 189200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.22     |
|    ep_rew_mean     | 2.7      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 473      |
|    time_elapsed    | 39897    |
|    total_timesteps | 189200   |
---------------------------------
Progress update from timestep 189300: Updated shield, loss is 0.05305560678243637
Progress update from timestep 189400: Updated shield, loss is 0.05464037135243416
Eval num_timesteps=189400, episode_reward=10.62 +/- 12.63
Episode length: 15.80 +/- 17.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.8          |
|    mean_reward          | 10.6          |
| time/                   |               |
|    total_timesteps      | 189400        |
| train/                  |               |
|    approx_kl            | 0.00027662353 |
|    clip_fraction        | 0.0067        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.013        |
|    explained_variance   | 0.194         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.888         |
|    n_updates            | 4730          |
|    policy_gradient_loss | -0.000872     |
|    value_loss           | 1.22          |
-------------------------------------------
Progress update from timestep 189500: Updated shield, loss is 0.0614042729139328
Progress update from timestep 189600: Updated shield, loss is 0.05540347099304199
Eval num_timesteps=189600, episode_reward=18.73 +/- 14.23
Episode length: 27.60 +/- 19.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.6     |
|    mean_reward     | 18.7     |
| time/              |          |
|    total_timesteps | 189600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.89     |
|    ep_rew_mean     | 2.41     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 474      |
|    time_elapsed    | 39988    |
|    total_timesteps | 189600   |
---------------------------------
Progress update from timestep 189700: Updated shield, loss is 0.05265951529145241
Progress update from timestep 189800: Updated shield, loss is 0.053961727768182755
Eval num_timesteps=189800, episode_reward=27.95 +/- 13.63
Episode length: 40.40 +/- 19.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 40.4         |
|    mean_reward          | 27.9         |
| time/                   |              |
|    total_timesteps      | 189800       |
| train/                  |              |
|    approx_kl            | 3.993617e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00208     |
|    explained_variance   | 0.188        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.496        |
|    n_updates            | 4740         |
|    policy_gradient_loss | -6.77e-06    |
|    value_loss           | 1.66         |
------------------------------------------
Progress update from timestep 189900: Updated shield, loss is 0.059385281056165695
Progress update from timestep 190000: Updated shield, loss is 0.051936905831098557
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 190000: Saved shield_loss.csv. 
Eval num_timesteps=190000, episode_reward=30.37 +/- 9.37
Episode length: 43.60 +/- 12.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.6     |
|    mean_reward     | 30.4     |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.42     |
|    ep_rew_mean     | 2.81     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 475      |
|    time_elapsed    | 40102    |
|    total_timesteps | 190000   |
---------------------------------
Progress update from timestep 190100: Updated shield, loss is 0.06030076742172241
Progress update from timestep 190200: Updated shield, loss is 0.06019945442676544
Eval num_timesteps=190200, episode_reward=22.37 +/- 15.76
Episode length: 32.20 +/- 21.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.2         |
|    mean_reward          | 22.4         |
| time/                   |              |
|    total_timesteps      | 190200       |
| train/                  |              |
|    approx_kl            | 0.0003015313 |
|    clip_fraction        | 0.0123       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.015       |
|    explained_variance   | 0.255        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.583        |
|    n_updates            | 4750         |
|    policy_gradient_loss | -0.00136     |
|    value_loss           | 1.5          |
------------------------------------------
Progress update from timestep 190300: Updated shield, loss is 0.05827520415186882
Progress update from timestep 190400: Updated shield, loss is 0.054902900010347366
Eval num_timesteps=190400, episode_reward=25.18 +/- 12.24
Episode length: 36.60 +/- 17.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 25.2     |
| time/              |          |
|    total_timesteps | 190400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.89     |
|    ep_rew_mean     | 3.18     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 476      |
|    time_elapsed    | 40208    |
|    total_timesteps | 190400   |
---------------------------------
Progress update from timestep 190500: Updated shield, loss is 0.0629555806517601
Progress update from timestep 190600: Updated shield, loss is 0.061058495193719864
Eval num_timesteps=190600, episode_reward=5.88 +/- 3.45
Episode length: 9.40 +/- 4.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 9.4           |
|    mean_reward          | 5.88          |
| time/                   |               |
|    total_timesteps      | 190600        |
| train/                  |               |
|    approx_kl            | 9.3749375e-05 |
|    clip_fraction        | 0.00469       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00941      |
|    explained_variance   | 0.258         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.37          |
|    n_updates            | 4760          |
|    policy_gradient_loss | -0.000405     |
|    value_loss           | 1.99          |
-------------------------------------------
Progress update from timestep 190700: Updated shield, loss is 0.051687855273485184
Progress update from timestep 190800: Updated shield, loss is 0.06792415678501129
Eval num_timesteps=190800, episode_reward=15.34 +/- 10.72
Episode length: 22.60 +/- 14.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 15.3     |
| time/              |          |
|    total_timesteps | 190800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.86     |
|    ep_rew_mean     | 3.13     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 477      |
|    time_elapsed    | 40283    |
|    total_timesteps | 190800   |
---------------------------------
Progress update from timestep 190900: Updated shield, loss is 0.05659289285540581
Progress update from timestep 191000: Updated shield, loss is 0.055759720504283905
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 191000: Saved shield_loss.csv. 
Eval num_timesteps=191000, episode_reward=11.85 +/- 13.27
Episode length: 17.20 +/- 17.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.2         |
|    mean_reward          | 11.8         |
| time/                   |              |
|    total_timesteps      | 191000       |
| train/                  |              |
|    approx_kl            | 0.0007838237 |
|    clip_fraction        | 0.00804      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0219      |
|    explained_variance   | 0.164        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.14         |
|    n_updates            | 4770         |
|    policy_gradient_loss | -0.00115     |
|    value_loss           | 1.88         |
------------------------------------------
Progress update from timestep 191100: Updated shield, loss is 0.05298067256808281
Progress update from timestep 191200: Updated shield, loss is 0.05923620983958244
Eval num_timesteps=191200, episode_reward=17.65 +/- 15.12
Episode length: 25.00 +/- 20.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | 17.7     |
| time/              |          |
|    total_timesteps | 191200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.62     |
|    ep_rew_mean     | 2.91     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 478      |
|    time_elapsed    | 40372    |
|    total_timesteps | 191200   |
---------------------------------
Progress update from timestep 191300: Updated shield, loss is 0.04687497392296791
Progress update from timestep 191400: Updated shield, loss is 0.05536434426903725
Eval num_timesteps=191400, episode_reward=10.86 +/- 11.87
Episode length: 16.20 +/- 16.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.2         |
|    mean_reward          | 10.9         |
| time/                   |              |
|    total_timesteps      | 191400       |
| train/                  |              |
|    approx_kl            | 6.677273e-05 |
|    clip_fraction        | 0.00268      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00619     |
|    explained_variance   | 0.222        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.757        |
|    n_updates            | 4780         |
|    policy_gradient_loss | -0.000361    |
|    value_loss           | 1.59         |
------------------------------------------
Progress update from timestep 191500: Updated shield, loss is 0.05399031937122345
Progress update from timestep 191600: Updated shield, loss is 0.055661581456661224
Eval num_timesteps=191600, episode_reward=21.14 +/- 15.90
Episode length: 31.40 +/- 22.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.4     |
|    mean_reward     | 21.1     |
| time/              |          |
|    total_timesteps | 191600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.3      |
|    ep_rew_mean     | 2.73     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 479      |
|    time_elapsed    | 40457    |
|    total_timesteps | 191600   |
---------------------------------
Progress update from timestep 191700: Updated shield, loss is 0.05659254267811775
Progress update from timestep 191800: Updated shield, loss is 0.05600711703300476
Eval num_timesteps=191800, episode_reward=9.15 +/- 7.33
Episode length: 14.40 +/- 10.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.4         |
|    mean_reward          | 9.15         |
| time/                   |              |
|    total_timesteps      | 191800       |
| train/                  |              |
|    approx_kl            | 3.571637e-05 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0133      |
|    explained_variance   | 0.0994       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.881        |
|    n_updates            | 4790         |
|    policy_gradient_loss | -0.000204    |
|    value_loss           | 1.79         |
------------------------------------------
Progress update from timestep 191900: Updated shield, loss is 0.055553287267684937
Progress update from timestep 192000: Updated shield, loss is 0.051737673580646515
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 192000: Saved shield_loss.csv. 
Eval num_timesteps=192000, episode_reward=9.29 +/- 12.65
Episode length: 14.20 +/- 17.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.2     |
|    mean_reward     | 9.29     |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.93     |
|    ep_rew_mean     | 3.18     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 480      |
|    time_elapsed    | 40536    |
|    total_timesteps | 192000   |
---------------------------------
Progress update from timestep 192100: Updated shield, loss is 0.054793693125247955
Progress update from timestep 192200: Updated shield, loss is 0.05987987294793129
Eval num_timesteps=192200, episode_reward=23.25 +/- 13.80
Episode length: 34.00 +/- 19.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34            |
|    mean_reward          | 23.3          |
| time/                   |               |
|    total_timesteps      | 192200        |
| train/                  |               |
|    approx_kl            | 0.00012150758 |
|    clip_fraction        | 0.00424       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0102       |
|    explained_variance   | 0.274         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.81          |
|    n_updates            | 4800          |
|    policy_gradient_loss | -0.0007       |
|    value_loss           | 2.27          |
-------------------------------------------
Progress update from timestep 192300: Updated shield, loss is 0.058229293674230576
Progress update from timestep 192400: Updated shield, loss is 0.04851938784122467
Eval num_timesteps=192400, episode_reward=15.43 +/- 15.60
Episode length: 23.00 +/- 22.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 15.4     |
| time/              |          |
|    total_timesteps | 192400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.33     |
|    ep_rew_mean     | 2.75     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 481      |
|    time_elapsed    | 40643    |
|    total_timesteps | 192400   |
---------------------------------
Progress update from timestep 192500: Updated shield, loss is 0.0713406428694725
Progress update from timestep 192600: Updated shield, loss is 0.053033288568258286
Eval num_timesteps=192600, episode_reward=17.74 +/- 14.86
Episode length: 25.60 +/- 20.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.6          |
|    mean_reward          | 17.7          |
| time/                   |               |
|    total_timesteps      | 192600        |
| train/                  |               |
|    approx_kl            | 4.2655924e-05 |
|    clip_fraction        | 0.00179       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00578      |
|    explained_variance   | 0.225         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.05          |
|    n_updates            | 4810          |
|    policy_gradient_loss | -0.000215     |
|    value_loss           | 2.4           |
-------------------------------------------
Progress update from timestep 192700: Updated shield, loss is 0.06327658146619797
Progress update from timestep 192800: Updated shield, loss is 0.051649387925863266
Eval num_timesteps=192800, episode_reward=11.04 +/- 12.55
Episode length: 16.40 +/- 17.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | 11       |
| time/              |          |
|    total_timesteps | 192800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.24     |
|    ep_rew_mean     | 2.65     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 482      |
|    time_elapsed    | 40734    |
|    total_timesteps | 192800   |
---------------------------------
Progress update from timestep 192900: Updated shield, loss is 0.06864060461521149
Progress update from timestep 193000: Updated shield, loss is 0.04552857577800751
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 193000: Saved shield_loss.csv. 
Eval num_timesteps=193000, episode_reward=16.22 +/- 14.91
Episode length: 24.00 +/- 21.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24            |
|    mean_reward          | 16.2          |
| time/                   |               |
|    total_timesteps      | 193000        |
| train/                  |               |
|    approx_kl            | 2.5362915e-05 |
|    clip_fraction        | 0.00134       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00755      |
|    explained_variance   | 0.0599        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.993         |
|    n_updates            | 4820          |
|    policy_gradient_loss | -0.000623     |
|    value_loss           | 1.66          |
-------------------------------------------
Progress update from timestep 193100: Updated shield, loss is 0.06245582923293114
Progress update from timestep 193200: Updated shield, loss is 0.06023157760500908
Eval num_timesteps=193200, episode_reward=3.76 +/- 2.33
Episode length: 6.20 +/- 3.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6.2      |
|    mean_reward     | 3.76     |
| time/              |          |
|    total_timesteps | 193200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.92     |
|    ep_rew_mean     | 3.24     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 483      |
|    time_elapsed    | 40805    |
|    total_timesteps | 193200   |
---------------------------------
Progress update from timestep 193300: Updated shield, loss is 0.04365231469273567
Progress update from timestep 193400: Updated shield, loss is 0.047119833528995514
Eval num_timesteps=193400, episode_reward=11.05 +/- 13.14
Episode length: 15.80 +/- 17.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | 11.1         |
| time/                   |              |
|    total_timesteps      | 193400       |
| train/                  |              |
|    approx_kl            | 0.0001596794 |
|    clip_fraction        | 0.00335      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0178      |
|    explained_variance   | 0.166        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.845        |
|    n_updates            | 4830         |
|    policy_gradient_loss | -0.000516    |
|    value_loss           | 2.79         |
------------------------------------------
Progress update from timestep 193500: Updated shield, loss is 0.05522450804710388
Progress update from timestep 193600: Updated shield, loss is 0.05161423236131668
Eval num_timesteps=193600, episode_reward=10.89 +/- 12.13
Episode length: 17.00 +/- 17.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | 10.9     |
| time/              |          |
|    total_timesteps | 193600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.09     |
|    ep_rew_mean     | 3.33     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 484      |
|    time_elapsed    | 40889    |
|    total_timesteps | 193600   |
---------------------------------
Progress update from timestep 193700: Updated shield, loss is 0.05538490042090416
Progress update from timestep 193800: Updated shield, loss is 0.059187132865190506
Eval num_timesteps=193800, episode_reward=16.49 +/- 14.28
Episode length: 25.00 +/- 21.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25            |
|    mean_reward          | 16.5          |
| time/                   |               |
|    total_timesteps      | 193800        |
| train/                  |               |
|    approx_kl            | 9.8661985e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00259      |
|    explained_variance   | 0.175         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.693         |
|    n_updates            | 4840          |
|    policy_gradient_loss | -7.52e-06     |
|    value_loss           | 1.71          |
-------------------------------------------
Progress update from timestep 193900: Updated shield, loss is 0.050228849053382874
Progress update from timestep 194000: Updated shield, loss is 0.048316508531570435
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 194000: Saved shield_loss.csv. 
Eval num_timesteps=194000, episode_reward=4.56 +/- 1.81
Episode length: 7.80 +/- 2.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7.8      |
|    mean_reward     | 4.56     |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.73     |
|    ep_rew_mean     | 2.98     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 485      |
|    time_elapsed    | 40968    |
|    total_timesteps | 194000   |
---------------------------------
Progress update from timestep 194100: Updated shield, loss is 0.03807390108704567
Progress update from timestep 194200: Updated shield, loss is 0.062307536602020264
Eval num_timesteps=194200, episode_reward=14.38 +/- 10.39
Episode length: 21.40 +/- 15.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 21.4         |
|    mean_reward          | 14.4         |
| time/                   |              |
|    total_timesteps      | 194200       |
| train/                  |              |
|    approx_kl            | 8.421659e-05 |
|    clip_fraction        | 0.0029       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0158      |
|    explained_variance   | 0.0656       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.49         |
|    n_updates            | 4850         |
|    policy_gradient_loss | -0.000302    |
|    value_loss           | 2.48         |
------------------------------------------
Progress update from timestep 194300: Updated shield, loss is 0.049810029566287994
Progress update from timestep 194400: Updated shield, loss is 0.05703389644622803
Eval num_timesteps=194400, episode_reward=10.75 +/- 12.25
Episode length: 16.20 +/- 17.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | 10.8     |
| time/              |          |
|    total_timesteps | 194400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.36     |
|    ep_rew_mean     | 2.74     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 486      |
|    time_elapsed    | 41051    |
|    total_timesteps | 194400   |
---------------------------------
Progress update from timestep 194500: Updated shield, loss is 0.044581349939107895
Progress update from timestep 194600: Updated shield, loss is 0.049193739891052246
Eval num_timesteps=194600, episode_reward=12.14 +/- 12.62
Episode length: 17.80 +/- 16.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.8         |
|    mean_reward          | 12.1         |
| time/                   |              |
|    total_timesteps      | 194600       |
| train/                  |              |
|    approx_kl            | 6.765601e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00273     |
|    explained_variance   | 0.0596       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.565        |
|    n_updates            | 4860         |
|    policy_gradient_loss | -2.36e-06    |
|    value_loss           | 2.29         |
------------------------------------------
Progress update from timestep 194700: Updated shield, loss is 0.05436112359166145
Progress update from timestep 194800: Updated shield, loss is 0.04228891059756279
Eval num_timesteps=194800, episode_reward=9.93 +/- 12.88
Episode length: 14.80 +/- 17.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 9.93     |
| time/              |          |
|    total_timesteps | 194800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5        |
|    ep_rew_mean     | 3.26     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 487      |
|    time_elapsed    | 41137    |
|    total_timesteps | 194800   |
---------------------------------
Progress update from timestep 194900: Updated shield, loss is 0.054850660264492035
Progress update from timestep 195000: Updated shield, loss is 0.0661480575799942
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 195000: Saved shield_loss.csv. 
Eval num_timesteps=195000, episode_reward=8.91 +/- 13.97
Episode length: 13.00 +/- 18.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 13           |
|    mean_reward          | 8.91         |
| time/                   |              |
|    total_timesteps      | 195000       |
| train/                  |              |
|    approx_kl            | 9.132845e-05 |
|    clip_fraction        | 0.00201      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0129      |
|    explained_variance   | 0.18         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.996        |
|    n_updates            | 4870         |
|    policy_gradient_loss | -0.000346    |
|    value_loss           | 2.34         |
------------------------------------------
Progress update from timestep 195100: Updated shield, loss is 0.07489275932312012
Progress update from timestep 195200: Updated shield, loss is 0.05530658736824989
Eval num_timesteps=195200, episode_reward=16.82 +/- 15.33
Episode length: 24.20 +/- 21.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 195200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.22     |
|    ep_rew_mean     | 2.71     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 488      |
|    time_elapsed    | 41221    |
|    total_timesteps | 195200   |
---------------------------------
Progress update from timestep 195300: Updated shield, loss is 0.059505194425582886
Progress update from timestep 195400: Updated shield, loss is 0.05833474174141884
Eval num_timesteps=195400, episode_reward=21.34 +/- 15.18
Episode length: 32.00 +/- 22.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32            |
|    mean_reward          | 21.3          |
| time/                   |               |
|    total_timesteps      | 195400        |
| train/                  |               |
|    approx_kl            | 0.00013290788 |
|    clip_fraction        | 0.00246       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0118       |
|    explained_variance   | 0.182         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.412         |
|    n_updates            | 4880          |
|    policy_gradient_loss | -0.000257     |
|    value_loss           | 1.92          |
-------------------------------------------
Progress update from timestep 195500: Updated shield, loss is 0.06925546377897263
Progress update from timestep 195600: Updated shield, loss is 0.06352797150611877
Eval num_timesteps=195600, episode_reward=18.65 +/- 12.81
Episode length: 28.00 +/- 18.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28       |
|    mean_reward     | 18.6     |
| time/              |          |
|    total_timesteps | 195600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.27     |
|    ep_rew_mean     | 3.39     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 489      |
|    time_elapsed    | 41311    |
|    total_timesteps | 195600   |
---------------------------------
Progress update from timestep 195700: Updated shield, loss is 0.055677466094493866
Progress update from timestep 195800: Updated shield, loss is 0.04471125826239586
Eval num_timesteps=195800, episode_reward=27.94 +/- 13.65
Episode length: 40.40 +/- 19.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 40.4          |
|    mean_reward          | 27.9          |
| time/                   |               |
|    total_timesteps      | 195800        |
| train/                  |               |
|    approx_kl            | 4.6951904e-05 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0189       |
|    explained_variance   | 0.305         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.74          |
|    n_updates            | 4890          |
|    policy_gradient_loss | -0.000639     |
|    value_loss           | 2.97          |
-------------------------------------------
Progress update from timestep 195900: Updated shield, loss is 0.0530860498547554
Progress update from timestep 196000: Updated shield, loss is 0.052172400057315826
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 196000: Saved shield_loss.csv. 
Eval num_timesteps=196000, episode_reward=16.41 +/- 15.29
Episode length: 24.00 +/- 21.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16.4     |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.41     |
|    ep_rew_mean     | 2.88     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 490      |
|    time_elapsed    | 41401    |
|    total_timesteps | 196000   |
---------------------------------
Progress update from timestep 196100: Updated shield, loss is 0.0633876845240593
Progress update from timestep 196200: Updated shield, loss is 0.05476723611354828
Eval num_timesteps=196200, episode_reward=28.34 +/- 12.87
Episode length: 41.00 +/- 18.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41          |
|    mean_reward          | 28.3        |
| time/                   |             |
|    total_timesteps      | 196200      |
| train/                  |             |
|    approx_kl            | 4.11859e-05 |
|    clip_fraction        | 0.00313     |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.00956    |
|    explained_variance   | 0.0581      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.923       |
|    n_updates            | 4900        |
|    policy_gradient_loss | -0.000722   |
|    value_loss           | 2.2         |
-----------------------------------------
Progress update from timestep 196300: Updated shield, loss is 0.05477193742990494
Progress update from timestep 196400: Updated shield, loss is 0.05004240944981575
Eval num_timesteps=196400, episode_reward=29.58 +/- 11.52
Episode length: 42.00 +/- 16.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 29.6     |
| time/              |          |
|    total_timesteps | 196400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.74     |
|    ep_rew_mean     | 2.26     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 491      |
|    time_elapsed    | 41526    |
|    total_timesteps | 196400   |
---------------------------------
Progress update from timestep 196500: Updated shield, loss is 0.06794535368680954
Progress update from timestep 196600: Updated shield, loss is 0.05400977283716202
Eval num_timesteps=196600, episode_reward=16.91 +/- 14.86
Episode length: 24.60 +/- 20.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.6         |
|    mean_reward          | 16.9         |
| time/                   |              |
|    total_timesteps      | 196600       |
| train/                  |              |
|    approx_kl            | 5.550516e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00232     |
|    explained_variance   | 0.215        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.78         |
|    n_updates            | 4910         |
|    policy_gradient_loss | -9.28e-07    |
|    value_loss           | 1.66         |
------------------------------------------
Progress update from timestep 196700: Updated shield, loss is 0.05018484964966774
Progress update from timestep 196800: Updated shield, loss is 0.06570634990930557
Eval num_timesteps=196800, episode_reward=17.08 +/- 16.31
Episode length: 24.00 +/- 21.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 17.1     |
| time/              |          |
|    total_timesteps | 196800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.35     |
|    ep_rew_mean     | 2.77     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 492      |
|    time_elapsed    | 41612    |
|    total_timesteps | 196800   |
---------------------------------
Progress update from timestep 196900: Updated shield, loss is 0.048587411642074585
Progress update from timestep 197000: Updated shield, loss is 0.05161326751112938
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 197000: Saved shield_loss.csv. 
Eval num_timesteps=197000, episode_reward=12.94 +/- 12.15
Episode length: 19.20 +/- 16.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 19.2          |
|    mean_reward          | 12.9          |
| time/                   |               |
|    total_timesteps      | 197000        |
| train/                  |               |
|    approx_kl            | 0.00031673186 |
|    clip_fraction        | 0.00647       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0285       |
|    explained_variance   | 0.00757       |
|    learning_rate        | 0.0001        |
|    loss                 | 0.923         |
|    n_updates            | 4920          |
|    policy_gradient_loss | -0.000942     |
|    value_loss           | 2.17          |
-------------------------------------------
Progress update from timestep 197100: Updated shield, loss is 0.06431984156370163
Progress update from timestep 197200: Updated shield, loss is 0.059082694351673126
Eval num_timesteps=197200, episode_reward=15.83 +/- 16.19
Episode length: 22.80 +/- 22.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 15.8     |
| time/              |          |
|    total_timesteps | 197200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.89     |
|    ep_rew_mean     | 3.15     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 493      |
|    time_elapsed    | 41696    |
|    total_timesteps | 197200   |
---------------------------------
Progress update from timestep 197300: Updated shield, loss is 0.05871867761015892
Progress update from timestep 197400: Updated shield, loss is 0.046996839344501495
Eval num_timesteps=197400, episode_reward=11.88 +/- 13.56
Episode length: 17.40 +/- 18.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17.4          |
|    mean_reward          | 11.9          |
| time/                   |               |
|    total_timesteps      | 197400        |
| train/                  |               |
|    approx_kl            | 4.5156153e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00265      |
|    explained_variance   | 0.28          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.823         |
|    n_updates            | 4930          |
|    policy_gradient_loss | -1.3e-05      |
|    value_loss           | 1.86          |
-------------------------------------------
Progress update from timestep 197500: Updated shield, loss is 0.05839574709534645
Progress update from timestep 197600: Updated shield, loss is 0.05739793926477432
Eval num_timesteps=197600, episode_reward=29.53 +/- 12.68
Episode length: 41.40 +/- 17.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | 29.5     |
| time/              |          |
|    total_timesteps | 197600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.94     |
|    ep_rew_mean     | 3.19     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 494      |
|    time_elapsed    | 41802    |
|    total_timesteps | 197600   |
---------------------------------
Progress update from timestep 197700: Updated shield, loss is 0.046241872012615204
Progress update from timestep 197800: Updated shield, loss is 0.06298331171274185
Eval num_timesteps=197800, episode_reward=11.90 +/- 12.26
Episode length: 18.00 +/- 17.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18           |
|    mean_reward          | 11.9         |
| time/                   |              |
|    total_timesteps      | 197800       |
| train/                  |              |
|    approx_kl            | 2.723287e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00195     |
|    explained_variance   | 0.151        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.09         |
|    n_updates            | 4940         |
|    policy_gradient_loss | -9.4e-07     |
|    value_loss           | 2.19         |
------------------------------------------
Progress update from timestep 197900: Updated shield, loss is 0.05490969121456146
Progress update from timestep 198000: Updated shield, loss is 0.05307431519031525
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 198000: Saved shield_loss.csv. 
Eval num_timesteps=198000, episode_reward=29.10 +/- 10.76
Episode length: 42.40 +/- 15.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 29.1     |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.38     |
|    ep_rew_mean     | 2.74     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 495      |
|    time_elapsed    | 41892    |
|    total_timesteps | 198000   |
---------------------------------
Progress update from timestep 198100: Updated shield, loss is 0.04999922588467598
Progress update from timestep 198200: Updated shield, loss is 0.05270357429981232
Eval num_timesteps=198200, episode_reward=17.15 +/- 15.08
Episode length: 24.60 +/- 20.76
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 24.6           |
|    mean_reward          | 17.2           |
| time/                   |                |
|    total_timesteps      | 198200         |
| train/                  |                |
|    approx_kl            | 0.000108814056 |
|    clip_fraction        | 0.00379        |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.014         |
|    explained_variance   | 0.166          |
|    learning_rate        | 0.0001         |
|    loss                 | 0.596          |
|    n_updates            | 4950           |
|    policy_gradient_loss | -0.000282      |
|    value_loss           | 2.03           |
--------------------------------------------
Progress update from timestep 198300: Updated shield, loss is 0.051636405289173126
Progress update from timestep 198400: Updated shield, loss is 0.05011352524161339
Eval num_timesteps=198400, episode_reward=23.04 +/- 14.48
Episode length: 33.40 +/- 20.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 23       |
| time/              |          |
|    total_timesteps | 198400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.49     |
|    ep_rew_mean     | 2.93     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 496      |
|    time_elapsed    | 41983    |
|    total_timesteps | 198400   |
---------------------------------
Progress update from timestep 198500: Updated shield, loss is 0.059552039951086044
Progress update from timestep 198600: Updated shield, loss is 0.05181162804365158
Eval num_timesteps=198600, episode_reward=18.52 +/- 14.39
Episode length: 26.80 +/- 19.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.8          |
|    mean_reward          | 18.5          |
| time/                   |               |
|    total_timesteps      | 198600        |
| train/                  |               |
|    approx_kl            | 1.7823202e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0133       |
|    explained_variance   | 0.199         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.13          |
|    n_updates            | 4960          |
|    policy_gradient_loss | -0.000202     |
|    value_loss           | 1.75          |
-------------------------------------------
Progress update from timestep 198700: Updated shield, loss is 0.05257663503289223
Progress update from timestep 198800: Updated shield, loss is 0.0565304309129715
Eval num_timesteps=198800, episode_reward=15.40 +/- 16.54
Episode length: 22.20 +/- 22.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 15.4     |
| time/              |          |
|    total_timesteps | 198800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.19     |
|    ep_rew_mean     | 2.61     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 497      |
|    time_elapsed    | 42096    |
|    total_timesteps | 198800   |
---------------------------------
Progress update from timestep 198900: Updated shield, loss is 0.05569722503423691
Progress update from timestep 199000: Updated shield, loss is 0.05058349296450615
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 199000: Saved shield_loss.csv. 
Eval num_timesteps=199000, episode_reward=22.67 +/- 14.98
Episode length: 33.00 +/- 20.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33            |
|    mean_reward          | 22.7          |
| time/                   |               |
|    total_timesteps      | 199000        |
| train/                  |               |
|    approx_kl            | 1.9910138e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00228      |
|    explained_variance   | 0.151         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.444         |
|    n_updates            | 4970          |
|    policy_gradient_loss | -8.27e-07     |
|    value_loss           | 1.48          |
-------------------------------------------
Progress update from timestep 199100: Updated shield, loss is 0.049287181347608566
Progress update from timestep 199200: Updated shield, loss is 0.04903962463140488
Eval num_timesteps=199200, episode_reward=7.60 +/- 4.88
Episode length: 11.60 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 11.6     |
|    mean_reward     | 7.6      |
| time/              |          |
|    total_timesteps | 199200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.52     |
|    ep_rew_mean     | 2.85     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 498      |
|    time_elapsed    | 42169    |
|    total_timesteps | 199200   |
---------------------------------
Progress update from timestep 199300: Updated shield, loss is 0.0556214302778244
Progress update from timestep 199400: Updated shield, loss is 0.04288600757718086
Eval num_timesteps=199400, episode_reward=9.50 +/- 12.25
Episode length: 15.00 +/- 18.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15            |
|    mean_reward          | 9.5           |
| time/                   |               |
|    total_timesteps      | 199400        |
| train/                  |               |
|    approx_kl            | 0.00013725898 |
|    clip_fraction        | 0.00759       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0142       |
|    explained_variance   | 0.138         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.628         |
|    n_updates            | 4980          |
|    policy_gradient_loss | -0.0014       |
|    value_loss           | 1.76          |
-------------------------------------------
Progress update from timestep 199500: Updated shield, loss is 0.05617420747876167
Progress update from timestep 199600: Updated shield, loss is 0.05074240639805794
Eval num_timesteps=199600, episode_reward=15.08 +/- 11.70
Episode length: 21.80 +/- 16.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | 15.1     |
| time/              |          |
|    total_timesteps | 199600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.95     |
|    ep_rew_mean     | 3.21     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 499      |
|    time_elapsed    | 42253    |
|    total_timesteps | 199600   |
---------------------------------
Progress update from timestep 199700: Updated shield, loss is 0.04190068319439888
Progress update from timestep 199800: Updated shield, loss is 0.04667855426669121
Eval num_timesteps=199800, episode_reward=10.13 +/- 12.74
Episode length: 15.20 +/- 17.43
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.2          |
|    mean_reward          | 10.1          |
| time/                   |               |
|    total_timesteps      | 199800        |
| train/                  |               |
|    approx_kl            | 3.0648782e-05 |
|    clip_fraction        | 0.0029        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.017        |
|    explained_variance   | 0.26          |
|    learning_rate        | 0.0001        |
|    loss                 | 1.04          |
|    n_updates            | 4990          |
|    policy_gradient_loss | -0.000222     |
|    value_loss           | 1.77          |
-------------------------------------------
Progress update from timestep 199900: Updated shield, loss is 0.04214102029800415
Progress update from timestep 200000: Updated shield, loss is 0.042389072477817535
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 200000: Saved shield_loss.csv. 
Eval num_timesteps=200000, episode_reward=23.18 +/- 13.41
Episode length: 34.20 +/- 19.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 23.2     |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.85     |
|    ep_rew_mean     | 3.21     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 500      |
|    time_elapsed    | 42358    |
|    total_timesteps | 200000   |
---------------------------------
Progress update from timestep 200100: Updated shield, loss is 0.05259362980723381
Progress update from timestep 200200: Updated shield, loss is 0.05131848528981209
Eval num_timesteps=200200, episode_reward=19.56 +/- 12.96
Episode length: 29.40 +/- 18.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 29.4          |
|    mean_reward          | 19.6          |
| time/                   |               |
|    total_timesteps      | 200200        |
| train/                  |               |
|    approx_kl            | 7.0166956e-05 |
|    clip_fraction        | 0.00313       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00982      |
|    explained_variance   | 0.191         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.544         |
|    n_updates            | 5000          |
|    policy_gradient_loss | -0.000439     |
|    value_loss           | 2.27          |
-------------------------------------------
Progress update from timestep 200300: Updated shield, loss is 0.040261220186948776
Progress update from timestep 200400: Updated shield, loss is 0.06059601530432701
Eval num_timesteps=200400, episode_reward=9.74 +/- 11.99
Episode length: 15.20 +/- 17.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | 9.74     |
| time/              |          |
|    total_timesteps | 200400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.27     |
|    ep_rew_mean     | 2.75     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 501      |
|    time_elapsed    | 42447    |
|    total_timesteps | 200400   |
---------------------------------
Progress update from timestep 200500: Updated shield, loss is 0.056376807391643524
Progress update from timestep 200600: Updated shield, loss is 0.052369628101587296
Eval num_timesteps=200600, episode_reward=8.54 +/- 12.44
Episode length: 13.40 +/- 18.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 13.4          |
|    mean_reward          | 8.54          |
| time/                   |               |
|    total_timesteps      | 200600        |
| train/                  |               |
|    approx_kl            | 4.8361737e-05 |
|    clip_fraction        | 0.00402       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0173       |
|    explained_variance   | 0.194         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.677         |
|    n_updates            | 5010          |
|    policy_gradient_loss | -0.000287     |
|    value_loss           | 1.49          |
-------------------------------------------
Progress update from timestep 200700: Updated shield, loss is 0.050041258335113525
Progress update from timestep 200800: Updated shield, loss is 0.050489626824855804
Eval num_timesteps=200800, episode_reward=11.80 +/- 11.37
Episode length: 17.80 +/- 16.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | 11.8     |
| time/              |          |
|    total_timesteps | 200800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.91     |
|    ep_rew_mean     | 3.24     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 502      |
|    time_elapsed    | 42538    |
|    total_timesteps | 200800   |
---------------------------------
Progress update from timestep 200900: Updated shield, loss is 0.06173420324921608
Progress update from timestep 201000: Updated shield, loss is 0.06257949024438858
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 201000: Saved shield_loss.csv. 
Eval num_timesteps=201000, episode_reward=15.64 +/- 16.30
Episode length: 22.60 +/- 22.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.6          |
|    mean_reward          | 15.6          |
| time/                   |               |
|    total_timesteps      | 201000        |
| train/                  |               |
|    approx_kl            | 1.6526817e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00109      |
|    explained_variance   | 0.389         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.64          |
|    n_updates            | 5020          |
|    policy_gradient_loss | -7.58e-07     |
|    value_loss           | 2.19          |
-------------------------------------------
Progress update from timestep 201100: Updated shield, loss is 0.05717175453901291
Progress update from timestep 201200: Updated shield, loss is 0.0436277836561203
Eval num_timesteps=201200, episode_reward=10.23 +/- 12.20
Episode length: 15.60 +/- 17.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 10.2     |
| time/              |          |
|    total_timesteps | 201200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.48     |
|    ep_rew_mean     | 2.85     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 503      |
|    time_elapsed    | 42628    |
|    total_timesteps | 201200   |
---------------------------------
Progress update from timestep 201300: Updated shield, loss is 0.058508846908807755
Progress update from timestep 201400: Updated shield, loss is 0.056462109088897705
Eval num_timesteps=201400, episode_reward=23.53 +/- 15.24
Episode length: 33.20 +/- 20.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.2          |
|    mean_reward          | 23.5          |
| time/                   |               |
|    total_timesteps      | 201400        |
| train/                  |               |
|    approx_kl            | 0.00014009164 |
|    clip_fraction        | 0.01          |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.015        |
|    explained_variance   | 0.148         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.64          |
|    n_updates            | 5030          |
|    policy_gradient_loss | -0.000616     |
|    value_loss           | 1.53          |
-------------------------------------------
Progress update from timestep 201500: Updated shield, loss is 0.04879199340939522
Progress update from timestep 201600: Updated shield, loss is 0.05248841270804405
Eval num_timesteps=201600, episode_reward=21.71 +/- 15.26
Episode length: 32.20 +/- 21.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 21.7     |
| time/              |          |
|    total_timesteps | 201600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.09     |
|    ep_rew_mean     | 3.29     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 504      |
|    time_elapsed    | 42718    |
|    total_timesteps | 201600   |
---------------------------------
Progress update from timestep 201700: Updated shield, loss is 0.03939548134803772
Progress update from timestep 201800: Updated shield, loss is 0.04757606238126755
Eval num_timesteps=201800, episode_reward=18.80 +/- 14.75
Episode length: 26.60 +/- 19.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.6          |
|    mean_reward          | 18.8          |
| time/                   |               |
|    total_timesteps      | 201800        |
| train/                  |               |
|    approx_kl            | 1.1009563e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00205      |
|    explained_variance   | 0.172         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.89          |
|    n_updates            | 5040          |
|    policy_gradient_loss | -8.33e-06     |
|    value_loss           | 2.2           |
-------------------------------------------
Progress update from timestep 201900: Updated shield, loss is 0.04789024963974953
Progress update from timestep 202000: Updated shield, loss is 0.04979595169425011
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 202000: Saved shield_loss.csv. 
Eval num_timesteps=202000, episode_reward=9.93 +/- 12.34
Episode length: 15.00 +/- 17.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | 9.93     |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.28     |
|    ep_rew_mean     | 3.44     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 505      |
|    time_elapsed    | 42812    |
|    total_timesteps | 202000   |
---------------------------------
Progress update from timestep 202100: Updated shield, loss is 0.04128933697938919
Progress update from timestep 202200: Updated shield, loss is 0.047038283199071884
Eval num_timesteps=202200, episode_reward=18.67 +/- 13.41
Episode length: 27.40 +/- 18.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 27.4          |
|    mean_reward          | 18.7          |
| time/                   |               |
|    total_timesteps      | 202200        |
| train/                  |               |
|    approx_kl            | 3.4768683e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0028       |
|    explained_variance   | 0.0932        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.7           |
|    n_updates            | 5050          |
|    policy_gradient_loss | -4.1e-06      |
|    value_loss           | 2.97          |
-------------------------------------------
Progress update from timestep 202300: Updated shield, loss is 0.0445520244538784
Progress update from timestep 202400: Updated shield, loss is 0.050873760133981705
Eval num_timesteps=202400, episode_reward=24.47 +/- 13.19
Episode length: 35.40 +/- 17.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 24.5     |
| time/              |          |
|    total_timesteps | 202400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.2      |
|    ep_rew_mean     | 2.61     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 506      |
|    time_elapsed    | 42915    |
|    total_timesteps | 202400   |
---------------------------------
Progress update from timestep 202500: Updated shield, loss is 0.05131400376558304
Progress update from timestep 202600: Updated shield, loss is 0.04526231437921524
Eval num_timesteps=202600, episode_reward=17.97 +/- 14.92
Episode length: 25.60 +/- 20.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.6          |
|    mean_reward          | 18            |
| time/                   |               |
|    total_timesteps      | 202600        |
| train/                  |               |
|    approx_kl            | 0.00023002297 |
|    clip_fraction        | 0.0067        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0146       |
|    explained_variance   | 0.105         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.389         |
|    n_updates            | 5060          |
|    policy_gradient_loss | -0.000848     |
|    value_loss           | 2.29          |
-------------------------------------------
Progress update from timestep 202700: Updated shield, loss is 0.0577760748565197
Progress update from timestep 202800: Updated shield, loss is 0.052767880260944366
Eval num_timesteps=202800, episode_reward=22.80 +/- 15.68
Episode length: 32.40 +/- 21.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.4     |
|    mean_reward     | 22.8     |
| time/              |          |
|    total_timesteps | 202800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.54     |
|    ep_rew_mean     | 2.9      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 507      |
|    time_elapsed    | 43029    |
|    total_timesteps | 202800   |
---------------------------------
Progress update from timestep 202900: Updated shield, loss is 0.05082245171070099
Progress update from timestep 203000: Updated shield, loss is 0.0500180721282959
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 203000: Saved shield_loss.csv. 
Eval num_timesteps=203000, episode_reward=16.52 +/- 16.12
Episode length: 23.40 +/- 21.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.4          |
|    mean_reward          | 16.5          |
| time/                   |               |
|    total_timesteps      | 203000        |
| train/                  |               |
|    approx_kl            | 5.2603333e-05 |
|    clip_fraction        | 0.00446       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00895      |
|    explained_variance   | 0.168         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.454         |
|    n_updates            | 5070          |
|    policy_gradient_loss | -0.000348     |
|    value_loss           | 1.59          |
-------------------------------------------
Progress update from timestep 203100: Updated shield, loss is 0.06400195509195328
Progress update from timestep 203200: Updated shield, loss is 0.05230230838060379
Eval num_timesteps=203200, episode_reward=13.73 +/- 11.99
Episode length: 20.80 +/- 17.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.8     |
|    mean_reward     | 13.7     |
| time/              |          |
|    total_timesteps | 203200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.59     |
|    ep_rew_mean     | 2.93     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 508      |
|    time_elapsed    | 43122    |
|    total_timesteps | 203200   |
---------------------------------
Progress update from timestep 203300: Updated shield, loss is 0.043939851224422455
Progress update from timestep 203400: Updated shield, loss is 0.052427101880311966
Eval num_timesteps=203400, episode_reward=12.15 +/- 11.66
Episode length: 18.40 +/- 16.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 18.4          |
|    mean_reward          | 12.2          |
| time/                   |               |
|    total_timesteps      | 203400        |
| train/                  |               |
|    approx_kl            | 1.4675258e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0101       |
|    explained_variance   | 0.112         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.976         |
|    n_updates            | 5080          |
|    policy_gradient_loss | -4.2e-05      |
|    value_loss           | 1.93          |
-------------------------------------------
Progress update from timestep 203500: Updated shield, loss is 0.05246388539671898
Progress update from timestep 203600: Updated shield, loss is 0.055263761430978775
Eval num_timesteps=203600, episode_reward=5.62 +/- 4.51
Episode length: 9.20 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.2      |
|    mean_reward     | 5.62     |
| time/              |          |
|    total_timesteps | 203600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.39     |
|    ep_rew_mean     | 2.75     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 509      |
|    time_elapsed    | 43204    |
|    total_timesteps | 203600   |
---------------------------------
Progress update from timestep 203700: Updated shield, loss is 0.06489149481058121
Progress update from timestep 203800: Updated shield, loss is 0.04975798353552818
Eval num_timesteps=203800, episode_reward=14.20 +/- 10.47
Episode length: 21.20 +/- 14.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 21.2         |
|    mean_reward          | 14.2         |
| time/                   |              |
|    total_timesteps      | 203800       |
| train/                  |              |
|    approx_kl            | 2.847498e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0023      |
|    explained_variance   | 0.253        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.712        |
|    n_updates            | 5090         |
|    policy_gradient_loss | -2.21e-06    |
|    value_loss           | 1.76         |
------------------------------------------
Progress update from timestep 203900: Updated shield, loss is 0.05079018697142601
Progress update from timestep 204000: Updated shield, loss is 0.05874651297926903
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 204000: Saved shield_loss.csv. 
Eval num_timesteps=204000, episode_reward=10.94 +/- 12.02
Episode length: 16.40 +/- 17.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | 10.9     |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.39     |
|    ep_rew_mean     | 2.78     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 510      |
|    time_elapsed    | 43296    |
|    total_timesteps | 204000   |
---------------------------------
Progress update from timestep 204100: Updated shield, loss is 0.051618542522192
Progress update from timestep 204200: Updated shield, loss is 0.05058733746409416
Eval num_timesteps=204200, episode_reward=9.60 +/- 13.20
Episode length: 14.20 +/- 18.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.2          |
|    mean_reward          | 9.6           |
| time/                   |               |
|    total_timesteps      | 204200        |
| train/                  |               |
|    approx_kl            | 0.00018739841 |
|    clip_fraction        | 0.00313       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0126       |
|    explained_variance   | 0.0955        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.728         |
|    n_updates            | 5100          |
|    policy_gradient_loss | -0.000667     |
|    value_loss           | 1.41          |
-------------------------------------------
Progress update from timestep 204300: Updated shield, loss is 0.05216555297374725
Progress update from timestep 204400: Updated shield, loss is 0.042956821620464325
Eval num_timesteps=204400, episode_reward=24.27 +/- 13.02
Episode length: 35.00 +/- 18.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 24.3     |
| time/              |          |
|    total_timesteps | 204400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.68     |
|    ep_rew_mean     | 2.27     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 511      |
|    time_elapsed    | 43409    |
|    total_timesteps | 204400   |
---------------------------------
Progress update from timestep 204500: Updated shield, loss is 0.06007615849375725
Progress update from timestep 204600: Updated shield, loss is 0.05692952498793602
Eval num_timesteps=204600, episode_reward=5.60 +/- 3.87
Episode length: 8.80 +/- 5.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 8.8           |
|    mean_reward          | 5.6           |
| time/                   |               |
|    total_timesteps      | 204600        |
| train/                  |               |
|    approx_kl            | 0.00015274389 |
|    clip_fraction        | 0.00402       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00973      |
|    explained_variance   | 0.0456        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.672         |
|    n_updates            | 5110          |
|    policy_gradient_loss | -0.000616     |
|    value_loss           | 1.6           |
-------------------------------------------
Progress update from timestep 204700: Updated shield, loss is 0.04419003799557686
Progress update from timestep 204800: Updated shield, loss is 0.05012494698166847
Eval num_timesteps=204800, episode_reward=16.92 +/- 14.34
Episode length: 25.20 +/- 20.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 204800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.98     |
|    ep_rew_mean     | 2.48     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 512      |
|    time_elapsed    | 43487    |
|    total_timesteps | 204800   |
---------------------------------
Progress update from timestep 204900: Updated shield, loss is 0.0654744952917099
Progress update from timestep 205000: Updated shield, loss is 0.0509461835026741
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 205000: Saved shield_loss.csv. 
Eval num_timesteps=205000, episode_reward=9.32 +/- 13.27
Episode length: 13.80 +/- 18.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 13.8          |
|    mean_reward          | 9.32          |
| time/                   |               |
|    total_timesteps      | 205000        |
| train/                  |               |
|    approx_kl            | 0.00017612409 |
|    clip_fraction        | 0.00424       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0154       |
|    explained_variance   | 0.232         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.748         |
|    n_updates            | 5120          |
|    policy_gradient_loss | -0.000828     |
|    value_loss           | 1.1           |
-------------------------------------------
Progress update from timestep 205100: Updated shield, loss is 0.057027172297239304
Progress update from timestep 205200: Updated shield, loss is 0.060289643704891205
Eval num_timesteps=205200, episode_reward=14.96 +/- 11.33
Episode length: 21.40 +/- 14.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 15       |
| time/              |          |
|    total_timesteps | 205200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.53     |
|    ep_rew_mean     | 2.91     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 513      |
|    time_elapsed    | 43585    |
|    total_timesteps | 205200   |
---------------------------------
Progress update from timestep 205300: Updated shield, loss is 0.054643724113702774
Progress update from timestep 205400: Updated shield, loss is 0.050428006798028946
Eval num_timesteps=205400, episode_reward=16.88 +/- 14.58
Episode length: 24.80 +/- 20.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.8         |
|    mean_reward          | 16.9         |
| time/                   |              |
|    total_timesteps      | 205400       |
| train/                  |              |
|    approx_kl            | 0.0012384268 |
|    clip_fraction        | 0.00536      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00937     |
|    explained_variance   | 0.131        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.523        |
|    n_updates            | 5130         |
|    policy_gradient_loss | -0.001       |
|    value_loss           | 1.55         |
------------------------------------------
Progress update from timestep 205500: Updated shield, loss is 0.043778181076049805
Progress update from timestep 205600: Updated shield, loss is 0.056195616722106934
Eval num_timesteps=205600, episode_reward=10.85 +/- 12.88
Episode length: 17.00 +/- 19.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | 10.9     |
| time/              |          |
|    total_timesteps | 205600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5        |
|    ep_rew_mean     | 3.3      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 514      |
|    time_elapsed    | 43678    |
|    total_timesteps | 205600   |
---------------------------------
Progress update from timestep 205700: Updated shield, loss is 0.06831920146942139
Progress update from timestep 205800: Updated shield, loss is 0.05473431572318077
Eval num_timesteps=205800, episode_reward=14.32 +/- 10.95
Episode length: 21.00 +/- 15.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 21            |
|    mean_reward          | 14.3          |
| time/                   |               |
|    total_timesteps      | 205800        |
| train/                  |               |
|    approx_kl            | 1.2493342e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0123       |
|    explained_variance   | 0.154         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.6           |
|    n_updates            | 5140          |
|    policy_gradient_loss | -0.000271     |
|    value_loss           | 2.28          |
-------------------------------------------
Progress update from timestep 205900: Updated shield, loss is 0.051134299486875534
Progress update from timestep 206000: Updated shield, loss is 0.06581170111894608
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 206000: Saved shield_loss.csv. 
Eval num_timesteps=206000, episode_reward=8.65 +/- 12.94
Episode length: 13.20 +/- 18.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.2     |
|    mean_reward     | 8.65     |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.34     |
|    ep_rew_mean     | 2.69     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 515      |
|    time_elapsed    | 43771    |
|    total_timesteps | 206000   |
---------------------------------
Progress update from timestep 206100: Updated shield, loss is 0.052357643842697144
Progress update from timestep 206200: Updated shield, loss is 0.057526156306266785
Eval num_timesteps=206200, episode_reward=10.50 +/- 11.45
Episode length: 16.20 +/- 16.93
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 16.2           |
|    mean_reward          | 10.5           |
| time/                   |                |
|    total_timesteps      | 206200         |
| train/                  |                |
|    approx_kl            | 0.000116119205 |
|    clip_fraction        | 0.00625        |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.00905       |
|    explained_variance   | 0.196          |
|    learning_rate        | 0.0001         |
|    loss                 | 0.563          |
|    n_updates            | 5150           |
|    policy_gradient_loss | -0.000562      |
|    value_loss           | 1.82           |
--------------------------------------------
Progress update from timestep 206300: Updated shield, loss is 0.04731381684541702
Progress update from timestep 206400: Updated shield, loss is 0.04713878035545349
Eval num_timesteps=206400, episode_reward=16.34 +/- 15.39
Episode length: 24.00 +/- 21.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16.3     |
| time/              |          |
|    total_timesteps | 206400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.49     |
|    ep_rew_mean     | 2.81     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 516      |
|    time_elapsed    | 43866    |
|    total_timesteps | 206400   |
---------------------------------
Progress update from timestep 206500: Updated shield, loss is 0.04688611254096031
Progress update from timestep 206600: Updated shield, loss is 0.056855663657188416
Eval num_timesteps=206600, episode_reward=23.78 +/- 14.94
Episode length: 33.60 +/- 20.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.6         |
|    mean_reward          | 23.8         |
| time/                   |              |
|    total_timesteps      | 206600       |
| train/                  |              |
|    approx_kl            | 6.962052e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00226     |
|    explained_variance   | 0.15         |
|    learning_rate        | 0.0001       |
|    loss                 | 1.11         |
|    n_updates            | 5160         |
|    policy_gradient_loss | -3.03e-06    |
|    value_loss           | 1.77         |
------------------------------------------
Progress update from timestep 206700: Updated shield, loss is 0.06501463800668716
Progress update from timestep 206800: Updated shield, loss is 0.04543011635541916
Eval num_timesteps=206800, episode_reward=3.31 +/- 2.06
Episode length: 5.80 +/- 3.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5.8      |
|    mean_reward     | 3.31     |
| time/              |          |
|    total_timesteps | 206800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.03     |
|    ep_rew_mean     | 3.27     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 517      |
|    time_elapsed    | 43962    |
|    total_timesteps | 206800   |
---------------------------------
Progress update from timestep 206900: Updated shield, loss is 0.053631410002708435
Progress update from timestep 207000: Updated shield, loss is 0.040113415569067
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 207000: Saved shield_loss.csv. 
Eval num_timesteps=207000, episode_reward=10.37 +/- 12.18
Episode length: 15.60 +/- 17.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.6          |
|    mean_reward          | 10.4          |
| time/                   |               |
|    total_timesteps      | 207000        |
| train/                  |               |
|    approx_kl            | 1.0212326e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0024       |
|    explained_variance   | 0.105         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.839         |
|    n_updates            | 5170          |
|    policy_gradient_loss | -5.45e-06     |
|    value_loss           | 2.2           |
-------------------------------------------
Progress update from timestep 207100: Updated shield, loss is 0.04868650808930397
Progress update from timestep 207200: Updated shield, loss is 0.05637151375412941
Eval num_timesteps=207200, episode_reward=11.04 +/- 12.37
Episode length: 16.40 +/- 16.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | 11       |
| time/              |          |
|    total_timesteps | 207200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.56     |
|    ep_rew_mean     | 2.89     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 518      |
|    time_elapsed    | 44056    |
|    total_timesteps | 207200   |
---------------------------------
Progress update from timestep 207300: Updated shield, loss is 0.043795689940452576
Progress update from timestep 207400: Updated shield, loss is 0.054361749440431595
Eval num_timesteps=207400, episode_reward=27.74 +/- 13.48
Episode length: 40.40 +/- 19.20
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 40.4           |
|    mean_reward          | 27.7           |
| time/                   |                |
|    total_timesteps      | 207400         |
| train/                  |                |
|    approx_kl            | 1.07983524e-07 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.00622       |
|    explained_variance   | 0.193          |
|    learning_rate        | 0.0001         |
|    loss                 | 0.379          |
|    n_updates            | 5180           |
|    policy_gradient_loss | -1.93e-05      |
|    value_loss           | 1.89           |
--------------------------------------------
Progress update from timestep 207500: Updated shield, loss is 0.03468136861920357
Progress update from timestep 207600: Updated shield, loss is 0.05634861811995506
Eval num_timesteps=207600, episode_reward=4.40 +/- 2.45
Episode length: 7.20 +/- 3.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7.2      |
|    mean_reward     | 4.4      |
| time/              |          |
|    total_timesteps | 207600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.73     |
|    ep_rew_mean     | 3.05     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 519      |
|    time_elapsed    | 44132    |
|    total_timesteps | 207600   |
---------------------------------
Progress update from timestep 207700: Updated shield, loss is 0.047972019761800766
Progress update from timestep 207800: Updated shield, loss is 0.05614522099494934
Eval num_timesteps=207800, episode_reward=23.59 +/- 14.72
Episode length: 33.40 +/- 20.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.4         |
|    mean_reward          | 23.6         |
| time/                   |              |
|    total_timesteps      | 207800       |
| train/                  |              |
|    approx_kl            | 7.818536e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00329     |
|    explained_variance   | 0.171        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.597        |
|    n_updates            | 5190         |
|    policy_gradient_loss | -3.27e-06    |
|    value_loss           | 2.65         |
------------------------------------------
Progress update from timestep 207900: Updated shield, loss is 0.051268648356199265
Progress update from timestep 208000: Updated shield, loss is 0.04990282282233238
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 208000: Saved shield_loss.csv. 
Eval num_timesteps=208000, episode_reward=13.30 +/- 12.38
Episode length: 19.60 +/- 17.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 13.3     |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.76     |
|    ep_rew_mean     | 3.05     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 520      |
|    time_elapsed    | 44256    |
|    total_timesteps | 208000   |
---------------------------------
Progress update from timestep 208100: Updated shield, loss is 0.05451911687850952
Progress update from timestep 208200: Updated shield, loss is 0.047912295907735825
Eval num_timesteps=208200, episode_reward=16.10 +/- 15.53
Episode length: 23.40 +/- 21.81
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.4          |
|    mean_reward          | 16.1          |
| time/                   |               |
|    total_timesteps      | 208200        |
| train/                  |               |
|    approx_kl            | 2.7136266e-05 |
|    clip_fraction        | 0.000893      |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00906      |
|    explained_variance   | 0.25          |
|    learning_rate        | 0.0001        |
|    loss                 | 1.05          |
|    n_updates            | 5200          |
|    policy_gradient_loss | -0.000412     |
|    value_loss           | 1.84          |
-------------------------------------------
Progress update from timestep 208300: Updated shield, loss is 0.06537487357854843
Progress update from timestep 208400: Updated shield, loss is 0.05491165444254875
Eval num_timesteps=208400, episode_reward=12.93 +/- 13.14
Episode length: 19.40 +/- 18.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 12.9     |
| time/              |          |
|    total_timesteps | 208400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.02     |
|    ep_rew_mean     | 2.46     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 521      |
|    time_elapsed    | 44348    |
|    total_timesteps | 208400   |
---------------------------------
Progress update from timestep 208500: Updated shield, loss is 0.048063211143016815
Progress update from timestep 208600: Updated shield, loss is 0.05236661806702614
Eval num_timesteps=208600, episode_reward=8.64 +/- 12.62
Episode length: 13.60 +/- 18.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.6        |
|    mean_reward          | 8.64        |
| time/                   |             |
|    total_timesteps      | 208600      |
| train/                  |             |
|    approx_kl            | 0.001451952 |
|    clip_fraction        | 0.025       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0253     |
|    explained_variance   | 0.22        |
|    learning_rate        | 0.0001      |
|    loss                 | 1.04        |
|    n_updates            | 5210        |
|    policy_gradient_loss | -0.00187    |
|    value_loss           | 1.66        |
-----------------------------------------
Progress update from timestep 208700: Updated shield, loss is 0.0512809231877327
Progress update from timestep 208800: Updated shield, loss is 0.04396696016192436
Eval num_timesteps=208800, episode_reward=27.94 +/- 12.52
Episode length: 41.00 +/- 18.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41       |
|    mean_reward     | 27.9     |
| time/              |          |
|    total_timesteps | 208800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.82     |
|    ep_rew_mean     | 3.09     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 522      |
|    time_elapsed    | 44441    |
|    total_timesteps | 208800   |
---------------------------------
Progress update from timestep 208900: Updated shield, loss is 0.04761287197470665
Progress update from timestep 209000: Updated shield, loss is 0.06787487119436264
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 209000: Saved shield_loss.csv. 
Eval num_timesteps=209000, episode_reward=12.47 +/- 11.68
Episode length: 18.60 +/- 16.70
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 18.6       |
|    mean_reward          | 12.5       |
| time/                   |            |
|    total_timesteps      | 209000     |
| train/                  |            |
|    approx_kl            | 7.1637e-08 |
|    clip_fraction        | 0          |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.00521   |
|    explained_variance   | 0.222      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.618      |
|    n_updates            | 5220       |
|    policy_gradient_loss | -9.15e-06  |
|    value_loss           | 2.48       |
----------------------------------------
Progress update from timestep 209100: Updated shield, loss is 0.044130839407444
Progress update from timestep 209200: Updated shield, loss is 0.053803205490112305
Eval num_timesteps=209200, episode_reward=16.65 +/- 15.05
Episode length: 24.40 +/- 20.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 209200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.56     |
|    ep_rew_mean     | 2.93     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 523      |
|    time_elapsed    | 44537    |
|    total_timesteps | 209200   |
---------------------------------
Progress update from timestep 209300: Updated shield, loss is 0.05588536337018013
Progress update from timestep 209400: Updated shield, loss is 0.0493498370051384
Eval num_timesteps=209400, episode_reward=25.99 +/- 12.18
Episode length: 37.20 +/- 16.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 37.2          |
|    mean_reward          | 26            |
| time/                   |               |
|    total_timesteps      | 209400        |
| train/                  |               |
|    approx_kl            | 0.00012243683 |
|    clip_fraction        | 0.00625       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0231       |
|    explained_variance   | 0.268         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.04          |
|    n_updates            | 5230          |
|    policy_gradient_loss | -0.000912     |
|    value_loss           | 2.18          |
-------------------------------------------
Progress update from timestep 209500: Updated shield, loss is 0.057700131088495255
Progress update from timestep 209600: Updated shield, loss is 0.045274700969457626
Eval num_timesteps=209600, episode_reward=15.35 +/- 14.74
Episode length: 23.40 +/- 21.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 15.4     |
| time/              |          |
|    total_timesteps | 209600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.6      |
|    ep_rew_mean     | 2.92     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 524      |
|    time_elapsed    | 44654    |
|    total_timesteps | 209600   |
---------------------------------
Progress update from timestep 209700: Updated shield, loss is 0.05637812614440918
Progress update from timestep 209800: Updated shield, loss is 0.04387269169092178
Eval num_timesteps=209800, episode_reward=16.96 +/- 14.93
Episode length: 24.60 +/- 20.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.6         |
|    mean_reward          | 17           |
| time/                   |              |
|    total_timesteps      | 209800       |
| train/                  |              |
|    approx_kl            | 2.361568e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00231     |
|    explained_variance   | 0.206        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.15         |
|    n_updates            | 5240         |
|    policy_gradient_loss | -2.83e-06    |
|    value_loss           | 1.78         |
------------------------------------------
Progress update from timestep 209900: Updated shield, loss is 0.04438966140151024
Progress update from timestep 210000: Updated shield, loss is 0.049608755856752396
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 210000: Saved shield_loss.csv. 
Eval num_timesteps=210000, episode_reward=18.90 +/- 13.39
Episode length: 27.60 +/- 18.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.6     |
|    mean_reward     | 18.9     |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.81     |
|    ep_rew_mean     | 2.38     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 525      |
|    time_elapsed    | 44769    |
|    total_timesteps | 210000   |
---------------------------------
Progress update from timestep 210100: Updated shield, loss is 0.051698699593544006
Progress update from timestep 210200: Updated shield, loss is 0.05095033720135689
Eval num_timesteps=210200, episode_reward=22.82 +/- 16.12
Episode length: 32.20 +/- 21.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.2         |
|    mean_reward          | 22.8         |
| time/                   |              |
|    total_timesteps      | 210200       |
| train/                  |              |
|    approx_kl            | 1.740433e-05 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0113      |
|    explained_variance   | 0.273        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.728        |
|    n_updates            | 5250         |
|    policy_gradient_loss | -0.000267    |
|    value_loss           | 1.09         |
------------------------------------------
Progress update from timestep 210300: Updated shield, loss is 0.05327326059341431
Progress update from timestep 210400: Updated shield, loss is 0.04725087061524391
Eval num_timesteps=210400, episode_reward=23.40 +/- 14.43
Episode length: 33.40 +/- 19.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 23.4     |
| time/              |          |
|    total_timesteps | 210400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.22     |
|    ep_rew_mean     | 2.75     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 526      |
|    time_elapsed    | 44905    |
|    total_timesteps | 210400   |
---------------------------------
Progress update from timestep 210500: Updated shield, loss is 0.05022510141134262
Progress update from timestep 210600: Updated shield, loss is 0.050472866743803024
Eval num_timesteps=210600, episode_reward=24.84 +/- 12.32
Episode length: 36.00 +/- 17.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36           |
|    mean_reward          | 24.8         |
| time/                   |              |
|    total_timesteps      | 210600       |
| train/                  |              |
|    approx_kl            | 0.0002730048 |
|    clip_fraction        | 0.00513      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0178      |
|    explained_variance   | 0.179        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.2          |
|    n_updates            | 5260         |
|    policy_gradient_loss | -0.000779    |
|    value_loss           | 2.29         |
------------------------------------------
Progress update from timestep 210700: Updated shield, loss is 0.04789656773209572
Progress update from timestep 210800: Updated shield, loss is 0.04662054777145386
Eval num_timesteps=210800, episode_reward=28.21 +/- 11.43
Episode length: 41.80 +/- 16.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 28.2     |
| time/              |          |
|    total_timesteps | 210800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.29     |
|    ep_rew_mean     | 2.72     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 527      |
|    time_elapsed    | 45023    |
|    total_timesteps | 210800   |
---------------------------------
Progress update from timestep 210900: Updated shield, loss is 0.047867774963378906
Progress update from timestep 211000: Updated shield, loss is 0.05158096179366112
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 211000: Saved shield_loss.csv. 
Eval num_timesteps=211000, episode_reward=19.27 +/- 13.04
Episode length: 28.00 +/- 18.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 28          |
|    mean_reward          | 19.3        |
| time/                   |             |
|    total_timesteps      | 211000      |
| train/                  |             |
|    approx_kl            | 7.23799e-05 |
|    clip_fraction        | 0.00246     |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.018      |
|    explained_variance   | 0.225       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.403       |
|    n_updates            | 5270        |
|    policy_gradient_loss | -0.000488   |
|    value_loss           | 1.17        |
-----------------------------------------
Progress update from timestep 211100: Updated shield, loss is 0.04968264326453209
Progress update from timestep 211200: Updated shield, loss is 0.04627755284309387
Eval num_timesteps=211200, episode_reward=23.45 +/- 14.45
Episode length: 33.60 +/- 20.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 23.4     |
| time/              |          |
|    total_timesteps | 211200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.09     |
|    ep_rew_mean     | 2.56     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 528      |
|    time_elapsed    | 45120    |
|    total_timesteps | 211200   |
---------------------------------
Progress update from timestep 211300: Updated shield, loss is 0.05034511163830757
Progress update from timestep 211400: Updated shield, loss is 0.05850424990057945
Eval num_timesteps=211400, episode_reward=9.44 +/- 12.11
Episode length: 14.60 +/- 17.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.6          |
|    mean_reward          | 9.44          |
| time/                   |               |
|    total_timesteps      | 211400        |
| train/                  |               |
|    approx_kl            | 0.00033941003 |
|    clip_fraction        | 0.00558       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0128       |
|    explained_variance   | 0.227         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.05          |
|    n_updates            | 5280          |
|    policy_gradient_loss | -0.00065      |
|    value_loss           | 1.43          |
-------------------------------------------
Progress update from timestep 211500: Updated shield, loss is 0.04097095876932144
Progress update from timestep 211600: Updated shield, loss is 0.06028904765844345
Eval num_timesteps=211600, episode_reward=16.61 +/- 15.08
Episode length: 24.20 +/- 21.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 211600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.3      |
|    ep_rew_mean     | 2.73     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 529      |
|    time_elapsed    | 45212    |
|    total_timesteps | 211600   |
---------------------------------
Progress update from timestep 211700: Updated shield, loss is 0.04564803093671799
Progress update from timestep 211800: Updated shield, loss is 0.059402961283922195
Eval num_timesteps=211800, episode_reward=5.62 +/- 4.67
Episode length: 9.20 +/- 7.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 9.2           |
|    mean_reward          | 5.62          |
| time/                   |               |
|    total_timesteps      | 211800        |
| train/                  |               |
|    approx_kl            | 1.6811828e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00402      |
|    explained_variance   | 0.124         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.68          |
|    n_updates            | 5290          |
|    policy_gradient_loss | -5.2e-06      |
|    value_loss           | 1.7           |
-------------------------------------------
Progress update from timestep 211900: Updated shield, loss is 0.05080109089612961
Progress update from timestep 212000: Updated shield, loss is 0.045992735773324966
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 212000: Saved shield_loss.csv. 
Eval num_timesteps=212000, episode_reward=16.61 +/- 16.13
Episode length: 23.60 +/- 21.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.73     |
|    ep_rew_mean     | 2.27     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 530      |
|    time_elapsed    | 45296    |
|    total_timesteps | 212000   |
---------------------------------
Progress update from timestep 212100: Updated shield, loss is 0.047847360372543335
Progress update from timestep 212200: Updated shield, loss is 0.049232013523578644
Eval num_timesteps=212200, episode_reward=22.72 +/- 15.07
Episode length: 33.00 +/- 21.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33            |
|    mean_reward          | 22.7          |
| time/                   |               |
|    total_timesteps      | 212200        |
| train/                  |               |
|    approx_kl            | 2.0546913e-05 |
|    clip_fraction        | 0.000223      |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00815      |
|    explained_variance   | 0.208         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.322         |
|    n_updates            | 5300          |
|    policy_gradient_loss | -0.000159     |
|    value_loss           | 1.09          |
-------------------------------------------
Progress update from timestep 212300: Updated shield, loss is 0.03962136432528496
Progress update from timestep 212400: Updated shield, loss is 0.047067757695913315
Eval num_timesteps=212400, episode_reward=3.95 +/- 2.61
Episode length: 6.40 +/- 3.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6.4      |
|    mean_reward     | 3.95     |
| time/              |          |
|    total_timesteps | 212400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.63     |
|    ep_rew_mean     | 2.23     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 531      |
|    time_elapsed    | 45374    |
|    total_timesteps | 212400   |
---------------------------------
Progress update from timestep 212500: Updated shield, loss is 0.05143723636865616
Progress update from timestep 212600: Updated shield, loss is 0.0466848760843277
Eval num_timesteps=212600, episode_reward=23.17 +/- 13.58
Episode length: 34.20 +/- 19.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.2          |
|    mean_reward          | 23.2          |
| time/                   |               |
|    total_timesteps      | 212600        |
| train/                  |               |
|    approx_kl            | 6.6476445e-05 |
|    clip_fraction        | 0.00201       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00823      |
|    explained_variance   | 0.137         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.289         |
|    n_updates            | 5310          |
|    policy_gradient_loss | -0.000212     |
|    value_loss           | 1.31          |
-------------------------------------------
Progress update from timestep 212700: Updated shield, loss is 0.04557694122195244
Progress update from timestep 212800: Updated shield, loss is 0.046190857887268066
Eval num_timesteps=212800, episode_reward=15.40 +/- 16.50
Episode length: 22.20 +/- 22.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 15.4     |
| time/              |          |
|    total_timesteps | 212800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.23     |
|    ep_rew_mean     | 2.63     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 532      |
|    time_elapsed    | 45469    |
|    total_timesteps | 212800   |
---------------------------------
Progress update from timestep 212900: Updated shield, loss is 0.047932904213666916
Progress update from timestep 213000: Updated shield, loss is 0.046608250588178635
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 213000: Saved shield_loss.csv. 
Eval num_timesteps=213000, episode_reward=3.85 +/- 1.43
Episode length: 6.40 +/- 2.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 6.4           |
|    mean_reward          | 3.85          |
| time/                   |               |
|    total_timesteps      | 213000        |
| train/                  |               |
|    approx_kl            | 0.00016489932 |
|    clip_fraction        | 0.00737       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0185       |
|    explained_variance   | 0.221         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.575         |
|    n_updates            | 5320          |
|    policy_gradient_loss | -0.00103      |
|    value_loss           | 1.42          |
-------------------------------------------
Progress update from timestep 213100: Updated shield, loss is 0.0467093251645565
Progress update from timestep 213200: Updated shield, loss is 0.050281375646591187
Eval num_timesteps=213200, episode_reward=35.14 +/- 1.13
Episode length: 50.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 35.1     |
| time/              |          |
|    total_timesteps | 213200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.88     |
|    ep_rew_mean     | 2.37     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 533      |
|    time_elapsed    | 45566    |
|    total_timesteps | 213200   |
---------------------------------
Progress update from timestep 213300: Updated shield, loss is 0.048112090677022934
Progress update from timestep 213400: Updated shield, loss is 0.04968807473778725
Eval num_timesteps=213400, episode_reward=17.09 +/- 15.65
Episode length: 24.40 +/- 21.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.4          |
|    mean_reward          | 17.1          |
| time/                   |               |
|    total_timesteps      | 213400        |
| train/                  |               |
|    approx_kl            | 0.00066778297 |
|    clip_fraction        | 0.0221        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0237       |
|    explained_variance   | 0.176         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.65          |
|    n_updates            | 5330          |
|    policy_gradient_loss | -0.00214      |
|    value_loss           | 1.41          |
-------------------------------------------
Progress update from timestep 213500: Updated shield, loss is 0.05292615294456482
Progress update from timestep 213600: Updated shield, loss is 0.05076194554567337
Eval num_timesteps=213600, episode_reward=9.52 +/- 13.69
Episode length: 13.80 +/- 18.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.8     |
|    mean_reward     | 9.52     |
| time/              |          |
|    total_timesteps | 213600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.07     |
|    ep_rew_mean     | 2.54     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 534      |
|    time_elapsed    | 45657    |
|    total_timesteps | 213600   |
---------------------------------
Progress update from timestep 213700: Updated shield, loss is 0.04870634153485298
Progress update from timestep 213800: Updated shield, loss is 0.0440436527132988
Eval num_timesteps=213800, episode_reward=9.26 +/- 12.23
Episode length: 14.40 +/- 18.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.4         |
|    mean_reward          | 9.26         |
| time/                   |              |
|    total_timesteps      | 213800       |
| train/                  |              |
|    approx_kl            | 0.0001782849 |
|    clip_fraction        | 0.00848      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0216      |
|    explained_variance   | 0.174        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.02         |
|    n_updates            | 5340         |
|    policy_gradient_loss | -0.000976    |
|    value_loss           | 1.75         |
------------------------------------------
Progress update from timestep 213900: Updated shield, loss is 0.0653379037976265
Progress update from timestep 214000: Updated shield, loss is 0.05953356996178627
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 214000: Saved shield_loss.csv. 
Eval num_timesteps=214000, episode_reward=8.98 +/- 6.48
Episode length: 13.60 +/- 9.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.6     |
|    mean_reward     | 8.98     |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.42     |
|    ep_rew_mean     | 2.87     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 535      |
|    time_elapsed    | 45741    |
|    total_timesteps | 214000   |
---------------------------------
Progress update from timestep 214100: Updated shield, loss is 0.047562506049871445
Progress update from timestep 214200: Updated shield, loss is 0.03851011022925377
Eval num_timesteps=214200, episode_reward=5.73 +/- 2.07
Episode length: 9.00 +/- 3.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 9            |
|    mean_reward          | 5.73         |
| time/                   |              |
|    total_timesteps      | 214200       |
| train/                  |              |
|    approx_kl            | 3.252353e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00274     |
|    explained_variance   | 0.094        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.68         |
|    n_updates            | 5350         |
|    policy_gradient_loss | -3.78e-06    |
|    value_loss           | 2.54         |
------------------------------------------
Progress update from timestep 214300: Updated shield, loss is 0.0453157015144825
Progress update from timestep 214400: Updated shield, loss is 0.05299415439367294
Eval num_timesteps=214400, episode_reward=23.87 +/- 14.58
Episode length: 34.00 +/- 19.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 23.9     |
| time/              |          |
|    total_timesteps | 214400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.75     |
|    ep_rew_mean     | 3.08     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 536      |
|    time_elapsed    | 45825    |
|    total_timesteps | 214400   |
---------------------------------
Progress update from timestep 214500: Updated shield, loss is 0.04939335584640503
Progress update from timestep 214600: Updated shield, loss is 0.04973415657877922
Eval num_timesteps=214600, episode_reward=29.29 +/- 10.96
Episode length: 42.20 +/- 15.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.2         |
|    mean_reward          | 29.3         |
| time/                   |              |
|    total_timesteps      | 214600       |
| train/                  |              |
|    approx_kl            | 4.420373e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00703     |
|    explained_variance   | 0.233        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.36         |
|    n_updates            | 5360         |
|    policy_gradient_loss | -1.34e-05    |
|    value_loss           | 2.1          |
------------------------------------------
Progress update from timestep 214700: Updated shield, loss is 0.04727581515908241
Progress update from timestep 214800: Updated shield, loss is 0.046004027128219604
Eval num_timesteps=214800, episode_reward=7.40 +/- 3.75
Episode length: 11.40 +/- 5.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 11.4     |
|    mean_reward     | 7.4      |
| time/              |          |
|    total_timesteps | 214800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.66     |
|    ep_rew_mean     | 2.97     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 537      |
|    time_elapsed    | 45924    |
|    total_timesteps | 214800   |
---------------------------------
Progress update from timestep 214900: Updated shield, loss is 0.04707048088312149
Progress update from timestep 215000: Updated shield, loss is 0.04896121472120285
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 215000: Saved shield_loss.csv. 
Eval num_timesteps=215000, episode_reward=22.41 +/- 16.63
Episode length: 31.60 +/- 22.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 31.6         |
|    mean_reward          | 22.4         |
| time/                   |              |
|    total_timesteps      | 215000       |
| train/                  |              |
|    approx_kl            | 4.101839e-05 |
|    clip_fraction        | 0.00134      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0208      |
|    explained_variance   | 0.175        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.589        |
|    n_updates            | 5370         |
|    policy_gradient_loss | -0.000482    |
|    value_loss           | 1.83         |
------------------------------------------
Progress update from timestep 215100: Updated shield, loss is 0.04682167246937752
Progress update from timestep 215200: Updated shield, loss is 0.05558701604604721
Eval num_timesteps=215200, episode_reward=11.18 +/- 13.44
Episode length: 16.60 +/- 18.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | 11.2     |
| time/              |          |
|    total_timesteps | 215200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.45     |
|    ep_rew_mean     | 2.07     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 538      |
|    time_elapsed    | 46049    |
|    total_timesteps | 215200   |
---------------------------------
Progress update from timestep 215300: Updated shield, loss is 0.048591867089271545
Progress update from timestep 215400: Updated shield, loss is 0.04858178645372391
Eval num_timesteps=215400, episode_reward=12.84 +/- 11.64
Episode length: 19.00 +/- 15.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 19           |
|    mean_reward          | 12.8         |
| time/                   |              |
|    total_timesteps      | 215400       |
| train/                  |              |
|    approx_kl            | 3.822996e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00249     |
|    explained_variance   | 0.0122       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.57         |
|    n_updates            | 5380         |
|    policy_gradient_loss | -2.11e-06    |
|    value_loss           | 1.25         |
------------------------------------------
Progress update from timestep 215500: Updated shield, loss is 0.06364014744758606
Progress update from timestep 215600: Updated shield, loss is 0.046400606632232666
Eval num_timesteps=215600, episode_reward=8.45 +/- 5.17
Episode length: 13.40 +/- 7.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.4     |
|    mean_reward     | 8.45     |
| time/              |          |
|    total_timesteps | 215600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.95     |
|    ep_rew_mean     | 2.45     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 539      |
|    time_elapsed    | 46136    |
|    total_timesteps | 215600   |
---------------------------------
Progress update from timestep 215700: Updated shield, loss is 0.053423795849084854
Progress update from timestep 215800: Updated shield, loss is 0.04884685203433037
Eval num_timesteps=215800, episode_reward=25.12 +/- 11.65
Episode length: 36.60 +/- 16.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.6          |
|    mean_reward          | 25.1          |
| time/                   |               |
|    total_timesteps      | 215800        |
| train/                  |               |
|    approx_kl            | 0.00017733114 |
|    clip_fraction        | 0.00491       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00896      |
|    explained_variance   | 0.15          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.902         |
|    n_updates            | 5390          |
|    policy_gradient_loss | -0.000777     |
|    value_loss           | 1.39          |
-------------------------------------------
Progress update from timestep 215900: Updated shield, loss is 0.06059674546122551
Progress update from timestep 216000: Updated shield, loss is 0.049244072288274765
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 216000: Saved shield_loss.csv. 
Eval num_timesteps=216000, episode_reward=10.82 +/- 13.22
Episode length: 15.80 +/- 17.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | 10.8     |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.34     |
|    ep_rew_mean     | 2.77     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 540      |
|    time_elapsed    | 46234    |
|    total_timesteps | 216000   |
---------------------------------
Progress update from timestep 216100: Updated shield, loss is 0.046557437628507614
Progress update from timestep 216200: Updated shield, loss is 0.054890744388103485
Eval num_timesteps=216200, episode_reward=16.05 +/- 15.14
Episode length: 23.60 +/- 21.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.6          |
|    mean_reward          | 16            |
| time/                   |               |
|    total_timesteps      | 216200        |
| train/                  |               |
|    approx_kl            | 0.00019825286 |
|    clip_fraction        | 0.00759       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0224       |
|    explained_variance   | 0.212         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.733         |
|    n_updates            | 5400          |
|    policy_gradient_loss | -0.00107      |
|    value_loss           | 1.69          |
-------------------------------------------
Progress update from timestep 216300: Updated shield, loss is 0.04314735531806946
Progress update from timestep 216400: Updated shield, loss is 0.0553094856441021
Eval num_timesteps=216400, episode_reward=22.26 +/- 15.45
Episode length: 32.20 +/- 21.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 22.3     |
| time/              |          |
|    total_timesteps | 216400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.38     |
|    ep_rew_mean     | 2.76     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 541      |
|    time_elapsed    | 46330    |
|    total_timesteps | 216400   |
---------------------------------
Progress update from timestep 216500: Updated shield, loss is 0.04207903891801834
Progress update from timestep 216600: Updated shield, loss is 0.042544327676296234
Eval num_timesteps=216600, episode_reward=9.02 +/- 12.87
Episode length: 13.80 +/- 18.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 13.8          |
|    mean_reward          | 9.02          |
| time/                   |               |
|    total_timesteps      | 216600        |
| train/                  |               |
|    approx_kl            | 0.00022093687 |
|    clip_fraction        | 0.00201       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00681      |
|    explained_variance   | 0.0658        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.76          |
|    n_updates            | 5410          |
|    policy_gradient_loss | -0.000583     |
|    value_loss           | 2.31          |
-------------------------------------------
Progress update from timestep 216700: Updated shield, loss is 0.053842734545469284
Progress update from timestep 216800: Updated shield, loss is 0.0587134025990963
Eval num_timesteps=216800, episode_reward=6.56 +/- 5.56
Episode length: 10.20 +/- 7.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10.2     |
|    mean_reward     | 6.56     |
| time/              |          |
|    total_timesteps | 216800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.54     |
|    ep_rew_mean     | 2.96     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 542      |
|    time_elapsed    | 46411    |
|    total_timesteps | 216800   |
---------------------------------
Progress update from timestep 216900: Updated shield, loss is 0.044932760298252106
Progress update from timestep 217000: Updated shield, loss is 0.053661491721868515
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 217000: Saved shield_loss.csv. 
Eval num_timesteps=217000, episode_reward=8.05 +/- 6.62
Episode length: 12.20 +/- 9.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 12.2          |
|    mean_reward          | 8.05          |
| time/                   |               |
|    total_timesteps      | 217000        |
| train/                  |               |
|    approx_kl            | 0.00053586916 |
|    clip_fraction        | 0.00536       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00817      |
|    explained_variance   | 0.128         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.577         |
|    n_updates            | 5420          |
|    policy_gradient_loss | -0.000382     |
|    value_loss           | 1.96          |
-------------------------------------------
Progress update from timestep 217100: Updated shield, loss is 0.049411434680223465
Progress update from timestep 217200: Updated shield, loss is 0.048078227788209915
Eval num_timesteps=217200, episode_reward=23.69 +/- 13.22
Episode length: 34.60 +/- 18.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 23.7     |
| time/              |          |
|    total_timesteps | 217200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.87     |
|    ep_rew_mean     | 3.14     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 543      |
|    time_elapsed    | 46497    |
|    total_timesteps | 217200   |
---------------------------------
Progress update from timestep 217300: Updated shield, loss is 0.053794559091329575
Progress update from timestep 217400: Updated shield, loss is 0.056354597210884094
Eval num_timesteps=217400, episode_reward=24.79 +/- 13.18
Episode length: 35.60 +/- 18.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.6          |
|    mean_reward          | 24.8          |
| time/                   |               |
|    total_timesteps      | 217400        |
| train/                  |               |
|    approx_kl            | 4.7080277e-05 |
|    clip_fraction        | 0.00201       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00726      |
|    explained_variance   | 0.137         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.62          |
|    n_updates            | 5430          |
|    policy_gradient_loss | -0.000413     |
|    value_loss           | 2.11          |
-------------------------------------------
Progress update from timestep 217500: Updated shield, loss is 0.0495963878929615
Progress update from timestep 217600: Updated shield, loss is 0.04331662133336067
Eval num_timesteps=217600, episode_reward=3.66 +/- 1.93
Episode length: 6.20 +/- 2.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6.2      |
|    mean_reward     | 3.66     |
| time/              |          |
|    total_timesteps | 217600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.68     |
|    ep_rew_mean     | 2.24     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 544      |
|    time_elapsed    | 46574    |
|    total_timesteps | 217600   |
---------------------------------
Progress update from timestep 217700: Updated shield, loss is 0.048996198922395706
Progress update from timestep 217800: Updated shield, loss is 0.04740195348858833
Eval num_timesteps=217800, episode_reward=2.62 +/- 1.38
Episode length: 4.60 +/- 1.85
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 4.6           |
|    mean_reward          | 2.62          |
| time/                   |               |
|    total_timesteps      | 217800        |
| train/                  |               |
|    approx_kl            | 1.0217523e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00323      |
|    explained_variance   | 0.159         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.649         |
|    n_updates            | 5440          |
|    policy_gradient_loss | -4.04e-06     |
|    value_loss           | 1.72          |
-------------------------------------------
Progress update from timestep 217900: Updated shield, loss is 0.04400591552257538
Progress update from timestep 218000: Updated shield, loss is 0.049171313643455505
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 218000: Saved shield_loss.csv. 
Eval num_timesteps=218000, episode_reward=14.09 +/- 11.30
Episode length: 21.00 +/- 15.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | 14.1     |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.33     |
|    ep_rew_mean     | 2.77     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 545      |
|    time_elapsed    | 46648    |
|    total_timesteps | 218000   |
---------------------------------
Progress update from timestep 218100: Updated shield, loss is 0.058570217341184616
Progress update from timestep 218200: Updated shield, loss is 0.05436372011899948
Eval num_timesteps=218200, episode_reward=22.25 +/- 15.94
Episode length: 32.00 +/- 22.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32            |
|    mean_reward          | 22.2          |
| time/                   |               |
|    total_timesteps      | 218200        |
| train/                  |               |
|    approx_kl            | 1.3553472e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0146       |
|    explained_variance   | 0.106         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.32          |
|    n_updates            | 5450          |
|    policy_gradient_loss | -0.000368     |
|    value_loss           | 2.29          |
-------------------------------------------
Progress update from timestep 218300: Updated shield, loss is 0.04556359350681305
Progress update from timestep 218400: Updated shield, loss is 0.043241847306489944
Eval num_timesteps=218400, episode_reward=5.92 +/- 7.13
Episode length: 9.60 +/- 10.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.6      |
|    mean_reward     | 5.92     |
| time/              |          |
|    total_timesteps | 218400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.23     |
|    ep_rew_mean     | 3.39     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 546      |
|    time_elapsed    | 46730    |
|    total_timesteps | 218400   |
---------------------------------
Progress update from timestep 218500: Updated shield, loss is 0.0519084557890892
Progress update from timestep 218600: Updated shield, loss is 0.049566492438316345
Eval num_timesteps=218600, episode_reward=16.88 +/- 15.76
Episode length: 24.00 +/- 21.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24           |
|    mean_reward          | 16.9         |
| time/                   |              |
|    total_timesteps      | 218600       |
| train/                  |              |
|    approx_kl            | 7.700275e-05 |
|    clip_fraction        | 0.00268      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.017       |
|    explained_variance   | 0.258        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.825        |
|    n_updates            | 5460         |
|    policy_gradient_loss | -0.000616    |
|    value_loss           | 2.42         |
------------------------------------------
Progress update from timestep 218700: Updated shield, loss is 0.06232689693570137
Progress update from timestep 218800: Updated shield, loss is 0.057467445731163025
Eval num_timesteps=218800, episode_reward=24.23 +/- 14.41
Episode length: 34.20 +/- 19.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 24.2     |
| time/              |          |
|    total_timesteps | 218800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.01     |
|    ep_rew_mean     | 3.28     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 547      |
|    time_elapsed    | 46825    |
|    total_timesteps | 218800   |
---------------------------------
Progress update from timestep 218900: Updated shield, loss is 0.05293325334787369
Progress update from timestep 219000: Updated shield, loss is 0.05809204652905464
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 219000: Saved shield_loss.csv. 
Eval num_timesteps=219000, episode_reward=8.40 +/- 12.52
Episode length: 13.20 +/- 18.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 13.2          |
|    mean_reward          | 8.4           |
| time/                   |               |
|    total_timesteps      | 219000        |
| train/                  |               |
|    approx_kl            | 0.00014785891 |
|    clip_fraction        | 0.0109        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0175       |
|    explained_variance   | 0.0687        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.924         |
|    n_updates            | 5470          |
|    policy_gradient_loss | -0.000871     |
|    value_loss           | 2.41          |
-------------------------------------------
Progress update from timestep 219100: Updated shield, loss is 0.05172305554151535
Progress update from timestep 219200: Updated shield, loss is 0.05991292744874954
Eval num_timesteps=219200, episode_reward=11.97 +/- 12.44
Episode length: 17.40 +/- 16.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 12       |
| time/              |          |
|    total_timesteps | 219200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.1      |
|    ep_rew_mean     | 3.38     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 548      |
|    time_elapsed    | 46923    |
|    total_timesteps | 219200   |
---------------------------------
Progress update from timestep 219300: Updated shield, loss is 0.05578307434916496
Progress update from timestep 219400: Updated shield, loss is 0.052414365112781525
Eval num_timesteps=219400, episode_reward=14.03 +/- 11.75
Episode length: 20.80 +/- 16.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 20.8          |
|    mean_reward          | 14            |
| time/                   |               |
|    total_timesteps      | 219400        |
| train/                  |               |
|    approx_kl            | 4.7078565e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00408      |
|    explained_variance   | 0.156         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.2           |
|    n_updates            | 5480          |
|    policy_gradient_loss | -2.71e-05     |
|    value_loss           | 3.08          |
-------------------------------------------
Progress update from timestep 219500: Updated shield, loss is 0.05084765702486038
Progress update from timestep 219600: Updated shield, loss is 0.05630269646644592
Eval num_timesteps=219600, episode_reward=17.14 +/- 14.15
Episode length: 25.20 +/- 20.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 17.1     |
| time/              |          |
|    total_timesteps | 219600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.94     |
|    ep_rew_mean     | 2.4      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 549      |
|    time_elapsed    | 47037    |
|    total_timesteps | 219600   |
---------------------------------
Progress update from timestep 219700: Updated shield, loss is 0.06384432315826416
Progress update from timestep 219800: Updated shield, loss is 0.05842646211385727
Eval num_timesteps=219800, episode_reward=11.24 +/- 13.00
Episode length: 16.40 +/- 17.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.4          |
|    mean_reward          | 11.2          |
| time/                   |               |
|    total_timesteps      | 219800        |
| train/                  |               |
|    approx_kl            | 1.3459648e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00827      |
|    explained_variance   | 0.0425        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.29          |
|    n_updates            | 5490          |
|    policy_gradient_loss | -7.17e-05     |
|    value_loss           | 1.91          |
-------------------------------------------
Progress update from timestep 219900: Updated shield, loss is 0.056670788675546646
Progress update from timestep 220000: Updated shield, loss is 0.04282071813941002
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 220000: Saved shield_loss.csv. 
Eval num_timesteps=220000, episode_reward=10.43 +/- 12.60
Episode length: 15.60 +/- 17.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 10.4     |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.64     |
|    ep_rew_mean     | 3.02     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 550      |
|    time_elapsed    | 47128    |
|    total_timesteps | 220000   |
---------------------------------
Progress update from timestep 220100: Updated shield, loss is 0.05171431228518486
Progress update from timestep 220200: Updated shield, loss is 0.048892855644226074
Eval num_timesteps=220200, episode_reward=21.97 +/- 14.89
Episode length: 32.60 +/- 21.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32.6          |
|    mean_reward          | 22            |
| time/                   |               |
|    total_timesteps      | 220200        |
| train/                  |               |
|    approx_kl            | 3.2861344e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00331      |
|    explained_variance   | 0.133         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.695         |
|    n_updates            | 5500          |
|    policy_gradient_loss | -1.88e-05     |
|    value_loss           | 2.09          |
-------------------------------------------
Progress update from timestep 220300: Updated shield, loss is 0.06121736019849777
Progress update from timestep 220400: Updated shield, loss is 0.05134601145982742
Eval num_timesteps=220400, episode_reward=13.28 +/- 11.91
Episode length: 19.40 +/- 15.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 13.3     |
| time/              |          |
|    total_timesteps | 220400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.75     |
|    ep_rew_mean     | 3.05     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 551      |
|    time_elapsed    | 47243    |
|    total_timesteps | 220400   |
---------------------------------
Progress update from timestep 220500: Updated shield, loss is 0.05115242674946785
Progress update from timestep 220600: Updated shield, loss is 0.046960730105638504
Eval num_timesteps=220600, episode_reward=16.23 +/- 14.51
Episode length: 24.40 +/- 21.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.4          |
|    mean_reward          | 16.2          |
| time/                   |               |
|    total_timesteps      | 220600        |
| train/                  |               |
|    approx_kl            | 0.00026632744 |
|    clip_fraction        | 0.0129        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0334       |
|    explained_variance   | 0.191         |
|    learning_rate        | 0.0001        |
|    loss                 | 1             |
|    n_updates            | 5510          |
|    policy_gradient_loss | -0.0013       |
|    value_loss           | 1.9           |
-------------------------------------------
Progress update from timestep 220700: Updated shield, loss is 0.05354094132781029
Progress update from timestep 220800: Updated shield, loss is 0.04125421866774559
Eval num_timesteps=220800, episode_reward=16.02 +/- 14.70
Episode length: 24.00 +/- 21.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 220800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.12     |
|    ep_rew_mean     | 2.55     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 552      |
|    time_elapsed    | 47337    |
|    total_timesteps | 220800   |
---------------------------------
Progress update from timestep 220900: Updated shield, loss is 0.04805625230073929
Progress update from timestep 221000: Updated shield, loss is 0.057299770414829254
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 221000: Saved shield_loss.csv. 
Eval num_timesteps=221000, episode_reward=10.11 +/- 9.31
Episode length: 15.60 +/- 13.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.6          |
|    mean_reward          | 10.1          |
| time/                   |               |
|    total_timesteps      | 221000        |
| train/                  |               |
|    approx_kl            | 5.0858944e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00448      |
|    explained_variance   | 0.0993        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.878         |
|    n_updates            | 5520          |
|    policy_gradient_loss | -5.62e-06     |
|    value_loss           | 1.9           |
-------------------------------------------
Progress update from timestep 221100: Updated shield, loss is 0.04619651287794113
Progress update from timestep 221200: Updated shield, loss is 0.052929751574993134
Eval num_timesteps=221200, episode_reward=22.74 +/- 15.32
Episode length: 32.80 +/- 21.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 22.7     |
| time/              |          |
|    total_timesteps | 221200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.72     |
|    ep_rew_mean     | 2.97     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 553      |
|    time_elapsed    | 47445    |
|    total_timesteps | 221200   |
---------------------------------
Progress update from timestep 221300: Updated shield, loss is 0.06470447778701782
Progress update from timestep 221400: Updated shield, loss is 0.04657849296927452
Eval num_timesteps=221400, episode_reward=16.08 +/- 15.63
Episode length: 23.40 +/- 21.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.4         |
|    mean_reward          | 16.1         |
| time/                   |              |
|    total_timesteps      | 221400       |
| train/                  |              |
|    approx_kl            | 1.956404e-05 |
|    clip_fraction        | 0.000223     |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0142      |
|    explained_variance   | 0.188        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.548        |
|    n_updates            | 5530         |
|    policy_gradient_loss | -9.49e-05    |
|    value_loss           | 1.58         |
------------------------------------------
Progress update from timestep 221500: Updated shield, loss is 0.05134093016386032
Progress update from timestep 221600: Updated shield, loss is 0.0534893199801445
Eval num_timesteps=221600, episode_reward=22.56 +/- 15.10
Episode length: 32.60 +/- 21.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.6     |
|    mean_reward     | 22.6     |
| time/              |          |
|    total_timesteps | 221600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.73     |
|    ep_rew_mean     | 3.06     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 554      |
|    time_elapsed    | 47539    |
|    total_timesteps | 221600   |
---------------------------------
Progress update from timestep 221700: Updated shield, loss is 0.040384579449892044
Progress update from timestep 221800: Updated shield, loss is 0.04885663092136383
Eval num_timesteps=221800, episode_reward=5.59 +/- 3.15
Episode length: 9.00 +/- 4.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 9             |
|    mean_reward          | 5.59          |
| time/                   |               |
|    total_timesteps      | 221800        |
| train/                  |               |
|    approx_kl            | 3.7523154e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00366      |
|    explained_variance   | 0.146         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.867         |
|    n_updates            | 5540          |
|    policy_gradient_loss | -1.34e-05     |
|    value_loss           | 2.02          |
-------------------------------------------
Progress update from timestep 221900: Updated shield, loss is 0.04945915937423706
Progress update from timestep 222000: Updated shield, loss is 0.03987418860197067
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 222000: Saved shield_loss.csv. 
Eval num_timesteps=222000, episode_reward=12.21 +/- 11.92
Episode length: 18.20 +/- 16.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | 12.2     |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.33     |
|    ep_rew_mean     | 2.78     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 555      |
|    time_elapsed    | 47616    |
|    total_timesteps | 222000   |
---------------------------------
Progress update from timestep 222100: Updated shield, loss is 0.052155639976263046
Progress update from timestep 222200: Updated shield, loss is 0.04424523934721947
Eval num_timesteps=222200, episode_reward=10.27 +/- 12.14
Episode length: 15.60 +/- 17.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.6         |
|    mean_reward          | 10.3         |
| time/                   |              |
|    total_timesteps      | 222200       |
| train/                  |              |
|    approx_kl            | 4.183203e-05 |
|    clip_fraction        | 0.00379      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0221      |
|    explained_variance   | 0.172        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.547        |
|    n_updates            | 5550         |
|    policy_gradient_loss | -0.000397    |
|    value_loss           | 1.98         |
------------------------------------------
Progress update from timestep 222300: Updated shield, loss is 0.05380992591381073
Progress update from timestep 222400: Updated shield, loss is 0.04669155552983284
Eval num_timesteps=222400, episode_reward=21.69 +/- 14.75
Episode length: 32.40 +/- 21.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.4     |
|    mean_reward     | 21.7     |
| time/              |          |
|    total_timesteps | 222400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.26     |
|    ep_rew_mean     | 2.71     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 556      |
|    time_elapsed    | 47730    |
|    total_timesteps | 222400   |
---------------------------------
Progress update from timestep 222500: Updated shield, loss is 0.04311748594045639
Progress update from timestep 222600: Updated shield, loss is 0.04094681516289711
Eval num_timesteps=222600, episode_reward=14.69 +/- 11.09
Episode length: 21.80 +/- 15.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 21.8          |
|    mean_reward          | 14.7          |
| time/                   |               |
|    total_timesteps      | 222600        |
| train/                  |               |
|    approx_kl            | 0.00027985458 |
|    clip_fraction        | 0.00937       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00804      |
|    explained_variance   | 0.164         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.517         |
|    n_updates            | 5560          |
|    policy_gradient_loss | -0.00151      |
|    value_loss           | 1.32          |
-------------------------------------------
Progress update from timestep 222700: Updated shield, loss is 0.04994160309433937
Progress update from timestep 222800: Updated shield, loss is 0.04916737973690033
Eval num_timesteps=222800, episode_reward=10.44 +/- 12.95
Episode length: 15.60 +/- 17.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 10.4     |
| time/              |          |
|    total_timesteps | 222800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.71     |
|    ep_rew_mean     | 3.04     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 557      |
|    time_elapsed    | 47824    |
|    total_timesteps | 222800   |
---------------------------------
Progress update from timestep 222900: Updated shield, loss is 0.04723329097032547
Progress update from timestep 223000: Updated shield, loss is 0.044745512306690216
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 223000: Saved shield_loss.csv. 
Eval num_timesteps=223000, episode_reward=9.17 +/- 12.17
Episode length: 14.20 +/- 18.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.2         |
|    mean_reward          | 9.17         |
| time/                   |              |
|    total_timesteps      | 223000       |
| train/                  |              |
|    approx_kl            | 8.645856e-05 |
|    clip_fraction        | 0.00737      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0182      |
|    explained_variance   | 0.0555       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.821        |
|    n_updates            | 5570         |
|    policy_gradient_loss | -0.00124     |
|    value_loss           | 2.29         |
------------------------------------------
Progress update from timestep 223100: Updated shield, loss is 0.04305655136704445
Progress update from timestep 223200: Updated shield, loss is 0.04169316589832306
Eval num_timesteps=223200, episode_reward=11.46 +/- 12.67
Episode length: 16.60 +/- 16.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | 11.5     |
| time/              |          |
|    total_timesteps | 223200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.47     |
|    ep_rew_mean     | 2.86     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 558      |
|    time_elapsed    | 47918    |
|    total_timesteps | 223200   |
---------------------------------
Progress update from timestep 223300: Updated shield, loss is 0.038581810891628265
Progress update from timestep 223400: Updated shield, loss is 0.04362175613641739
Eval num_timesteps=223400, episode_reward=11.11 +/- 11.65
Episode length: 17.00 +/- 17.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17           |
|    mean_reward          | 11.1         |
| time/                   |              |
|    total_timesteps      | 223400       |
| train/                  |              |
|    approx_kl            | 8.428551e-05 |
|    clip_fraction        | 0.00268      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.016       |
|    explained_variance   | 0.202        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.833        |
|    n_updates            | 5580         |
|    policy_gradient_loss | -0.000531    |
|    value_loss           | 1.76         |
------------------------------------------
Progress update from timestep 223500: Updated shield, loss is 0.04108254238963127
Progress update from timestep 223600: Updated shield, loss is 0.04612725228071213
Eval num_timesteps=223600, episode_reward=18.61 +/- 13.14
Episode length: 27.60 +/- 19.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.6     |
|    mean_reward     | 18.6     |
| time/              |          |
|    total_timesteps | 223600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.16     |
|    ep_rew_mean     | 2.59     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 559      |
|    time_elapsed    | 48013    |
|    total_timesteps | 223600   |
---------------------------------
Progress update from timestep 223700: Updated shield, loss is 0.041694700717926025
Progress update from timestep 223800: Updated shield, loss is 0.046101074665784836
Eval num_timesteps=223800, episode_reward=19.06 +/- 14.28
Episode length: 27.20 +/- 19.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 27.2          |
|    mean_reward          | 19.1          |
| time/                   |               |
|    total_timesteps      | 223800        |
| train/                  |               |
|    approx_kl            | 8.5648416e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00249      |
|    explained_variance   | 0.00218       |
|    learning_rate        | 0.0001        |
|    loss                 | 0.821         |
|    n_updates            | 5590          |
|    policy_gradient_loss | -1.48e-06     |
|    value_loss           | 2.03          |
-------------------------------------------
Progress update from timestep 223900: Updated shield, loss is 0.044050704687833786
Progress update from timestep 224000: Updated shield, loss is 0.046559885144233704
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 224000: Saved shield_loss.csv. 
Eval num_timesteps=224000, episode_reward=23.61 +/- 13.93
Episode length: 34.40 +/- 19.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 23.6     |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.93     |
|    ep_rew_mean     | 2.46     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 560      |
|    time_elapsed    | 48116    |
|    total_timesteps | 224000   |
---------------------------------
Progress update from timestep 224100: Updated shield, loss is 0.03787188604474068
Progress update from timestep 224200: Updated shield, loss is 0.04309892654418945
Eval num_timesteps=224200, episode_reward=24.42 +/- 14.20
Episode length: 34.60 +/- 18.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.6         |
|    mean_reward          | 24.4         |
| time/                   |              |
|    total_timesteps      | 224200       |
| train/                  |              |
|    approx_kl            | 0.0005696314 |
|    clip_fraction        | 0.0192       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0398      |
|    explained_variance   | 0.142        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.298        |
|    n_updates            | 5600         |
|    policy_gradient_loss | -0.00117     |
|    value_loss           | 1.45         |
------------------------------------------
Progress update from timestep 224300: Updated shield, loss is 0.04359425604343414
Progress update from timestep 224400: Updated shield, loss is 0.04590069130063057
Eval num_timesteps=224400, episode_reward=7.00 +/- 3.32
Episode length: 10.80 +/- 4.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10.8     |
|    mean_reward     | 7        |
| time/              |          |
|    total_timesteps | 224400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.06     |
|    ep_rew_mean     | 2.51     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 561      |
|    time_elapsed    | 48196    |
|    total_timesteps | 224400   |
---------------------------------
Progress update from timestep 224500: Updated shield, loss is 0.04635296016931534
Progress update from timestep 224600: Updated shield, loss is 0.037147071212530136
Eval num_timesteps=224600, episode_reward=23.57 +/- 13.54
Episode length: 34.60 +/- 19.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.6         |
|    mean_reward          | 23.6         |
| time/                   |              |
|    total_timesteps      | 224600       |
| train/                  |              |
|    approx_kl            | 3.488059e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0128      |
|    explained_variance   | 0.0294       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.999        |
|    n_updates            | 5610         |
|    policy_gradient_loss | -0.000192    |
|    value_loss           | 1.37         |
------------------------------------------
Progress update from timestep 224700: Updated shield, loss is 0.04371574521064758
Progress update from timestep 224800: Updated shield, loss is 0.04794614017009735
Eval num_timesteps=224800, episode_reward=11.93 +/- 12.25
Episode length: 17.60 +/- 16.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | 11.9     |
| time/              |          |
|    total_timesteps | 224800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.36     |
|    ep_rew_mean     | 2.76     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 562      |
|    time_elapsed    | 48309    |
|    total_timesteps | 224800   |
---------------------------------
Progress update from timestep 224900: Updated shield, loss is 0.05045311897993088
Progress update from timestep 225000: Updated shield, loss is 0.051932476460933685
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 225000: Saved shield_loss.csv. 
Eval num_timesteps=225000, episode_reward=12.14 +/- 11.90
Episode length: 18.20 +/- 16.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 18.2          |
|    mean_reward          | 12.1          |
| time/                   |               |
|    total_timesteps      | 225000        |
| train/                  |               |
|    approx_kl            | 1.2337945e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00366      |
|    explained_variance   | 0.15          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.543         |
|    n_updates            | 5620          |
|    policy_gradient_loss | -5.6e-06      |
|    value_loss           | 1.77          |
-------------------------------------------
Progress update from timestep 225100: Updated shield, loss is 0.036691173911094666
Progress update from timestep 225200: Updated shield, loss is 0.042843885719776154
Eval num_timesteps=225200, episode_reward=18.75 +/- 14.21
Episode length: 27.60 +/- 20.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.6     |
|    mean_reward     | 18.8     |
| time/              |          |
|    total_timesteps | 225200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.14     |
|    ep_rew_mean     | 2.63     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 563      |
|    time_elapsed    | 48402    |
|    total_timesteps | 225200   |
---------------------------------
Progress update from timestep 225300: Updated shield, loss is 0.054092004895210266
Progress update from timestep 225400: Updated shield, loss is 0.05148661136627197
Eval num_timesteps=225400, episode_reward=15.88 +/- 16.18
Episode length: 23.00 +/- 22.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23            |
|    mean_reward          | 15.9          |
| time/                   |               |
|    total_timesteps      | 225400        |
| train/                  |               |
|    approx_kl            | 0.00017153846 |
|    clip_fraction        | 0.00179       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00112      |
|    explained_variance   | 0.17          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.692         |
|    n_updates            | 5630          |
|    policy_gradient_loss | -0.000231     |
|    value_loss           | 1.36          |
-------------------------------------------
Progress update from timestep 225500: Updated shield, loss is 0.04923991113901138
Progress update from timestep 225600: Updated shield, loss is 0.042655762284994125
Eval num_timesteps=225600, episode_reward=12.17 +/- 11.34
Episode length: 18.40 +/- 16.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.4     |
|    mean_reward     | 12.2     |
| time/              |          |
|    total_timesteps | 225600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.09     |
|    ep_rew_mean     | 2.56     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 564      |
|    time_elapsed    | 48514    |
|    total_timesteps | 225600   |
---------------------------------
Progress update from timestep 225700: Updated shield, loss is 0.06092097610235214
Progress update from timestep 225800: Updated shield, loss is 0.04841603338718414
Eval num_timesteps=225800, episode_reward=28.43 +/- 11.56
Episode length: 41.80 +/- 16.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.8         |
|    mean_reward          | 28.4         |
| time/                   |              |
|    total_timesteps      | 225800       |
| train/                  |              |
|    approx_kl            | 0.0001960525 |
|    clip_fraction        | 0.00737      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.025       |
|    explained_variance   | 0.191        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.635        |
|    n_updates            | 5640         |
|    policy_gradient_loss | -0.00151     |
|    value_loss           | 1.52         |
------------------------------------------
Progress update from timestep 225900: Updated shield, loss is 0.04772799834609032
Progress update from timestep 226000: Updated shield, loss is 0.050387874245643616
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 226000: Saved shield_loss.csv. 
Eval num_timesteps=226000, episode_reward=18.35 +/- 13.36
Episode length: 27.00 +/- 19.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27       |
|    mean_reward     | 18.3     |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.29     |
|    ep_rew_mean     | 2.71     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 565      |
|    time_elapsed    | 48608    |
|    total_timesteps | 226000   |
---------------------------------
Progress update from timestep 226100: Updated shield, loss is 0.05236075073480606
Progress update from timestep 226200: Updated shield, loss is 0.05239490047097206
Eval num_timesteps=226200, episode_reward=16.77 +/- 14.99
Episode length: 24.60 +/- 20.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.6          |
|    mean_reward          | 16.8          |
| time/                   |               |
|    total_timesteps      | 226200        |
| train/                  |               |
|    approx_kl            | 0.00014568791 |
|    clip_fraction        | 0.00491       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0114       |
|    explained_variance   | 0.216         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.888         |
|    n_updates            | 5650          |
|    policy_gradient_loss | -0.000339     |
|    value_loss           | 1.71          |
-------------------------------------------
Progress update from timestep 226300: Updated shield, loss is 0.056569941341876984
Progress update from timestep 226400: Updated shield, loss is 0.04683014377951622
Eval num_timesteps=226400, episode_reward=24.03 +/- 13.78
Episode length: 34.80 +/- 19.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 24       |
| time/              |          |
|    total_timesteps | 226400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.82     |
|    ep_rew_mean     | 3.13     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 566      |
|    time_elapsed    | 48743    |
|    total_timesteps | 226400   |
---------------------------------
Progress update from timestep 226500: Updated shield, loss is 0.054759684950113297
Progress update from timestep 226600: Updated shield, loss is 0.05254610627889633
Eval num_timesteps=226600, episode_reward=16.02 +/- 16.17
Episode length: 23.00 +/- 22.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23            |
|    mean_reward          | 16            |
| time/                   |               |
|    total_timesteps      | 226600        |
| train/                  |               |
|    approx_kl            | 0.00021970038 |
|    clip_fraction        | 0.00871       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.032        |
|    explained_variance   | 0.189         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.558         |
|    n_updates            | 5660          |
|    policy_gradient_loss | -0.00207      |
|    value_loss           | 1.84          |
-------------------------------------------
Progress update from timestep 226700: Updated shield, loss is 0.05586947128176689
Progress update from timestep 226800: Updated shield, loss is 0.049739085137844086
Eval num_timesteps=226800, episode_reward=11.30 +/- 11.73
Episode length: 17.20 +/- 17.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | 11.3     |
| time/              |          |
|    total_timesteps | 226800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.79     |
|    ep_rew_mean     | 3.17     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 567      |
|    time_elapsed    | 48837    |
|    total_timesteps | 226800   |
---------------------------------
Progress update from timestep 226900: Updated shield, loss is 0.055591318756341934
Progress update from timestep 227000: Updated shield, loss is 0.050509992986917496
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 227000: Saved shield_loss.csv. 
Eval num_timesteps=227000, episode_reward=16.60 +/- 14.61
Episode length: 24.60 +/- 20.74
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 24.6           |
|    mean_reward          | 16.6           |
| time/                   |                |
|    total_timesteps      | 227000         |
| train/                  |                |
|    approx_kl            | 0.000106994914 |
|    clip_fraction        | 0.00402        |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.0136        |
|    explained_variance   | 0.152          |
|    learning_rate        | 0.0001         |
|    loss                 | 0.687          |
|    n_updates            | 5670           |
|    policy_gradient_loss | -0.000505      |
|    value_loss           | 1.91           |
--------------------------------------------
Progress update from timestep 227100: Updated shield, loss is 0.03902427479624748
Progress update from timestep 227200: Updated shield, loss is 0.049774087965488434
Eval num_timesteps=227200, episode_reward=31.96 +/- 6.82
Episode length: 45.40 +/- 9.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.4     |
|    mean_reward     | 32       |
| time/              |          |
|    total_timesteps | 227200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.13     |
|    ep_rew_mean     | 3.37     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 568      |
|    time_elapsed    | 48940    |
|    total_timesteps | 227200   |
---------------------------------
Progress update from timestep 227300: Updated shield, loss is 0.044412851333618164
Progress update from timestep 227400: Updated shield, loss is 0.05397633835673332
Eval num_timesteps=227400, episode_reward=16.69 +/- 15.45
Episode length: 24.00 +/- 21.25
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24            |
|    mean_reward          | 16.7          |
| time/                   |               |
|    total_timesteps      | 227400        |
| train/                  |               |
|    approx_kl            | 1.3739417e-05 |
|    clip_fraction        | 0.000223      |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0105       |
|    explained_variance   | 0.0617        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.8           |
|    n_updates            | 5680          |
|    policy_gradient_loss | -0.00013      |
|    value_loss           | 2.92          |
-------------------------------------------
Progress update from timestep 227500: Updated shield, loss is 0.04688084498047829
Progress update from timestep 227600: Updated shield, loss is 0.03986193984746933
Eval num_timesteps=227600, episode_reward=12.80 +/- 12.17
Episode length: 18.60 +/- 16.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 12.8     |
| time/              |          |
|    total_timesteps | 227600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.84     |
|    ep_rew_mean     | 3.13     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 569      |
|    time_elapsed    | 49030    |
|    total_timesteps | 227600   |
---------------------------------
Progress update from timestep 227700: Updated shield, loss is 0.036831460893154144
Progress update from timestep 227800: Updated shield, loss is 0.054925791919231415
Eval num_timesteps=227800, episode_reward=18.18 +/- 14.90
Episode length: 26.40 +/- 20.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.4          |
|    mean_reward          | 18.2          |
| time/                   |               |
|    total_timesteps      | 227800        |
| train/                  |               |
|    approx_kl            | 6.8439585e-05 |
|    clip_fraction        | 0.00424       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0231       |
|    explained_variance   | 0.21          |
|    learning_rate        | 0.0001        |
|    loss                 | 1.32          |
|    n_updates            | 5690          |
|    policy_gradient_loss | -0.000881     |
|    value_loss           | 2.22          |
-------------------------------------------
Progress update from timestep 227900: Updated shield, loss is 0.04361553117632866
Progress update from timestep 228000: Updated shield, loss is 0.04498416185379028
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 228000: Saved shield_loss.csv. 
Eval num_timesteps=228000, episode_reward=17.86 +/- 15.11
Episode length: 25.40 +/- 20.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.16     |
|    ep_rew_mean     | 2.61     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 570      |
|    time_elapsed    | 49165    |
|    total_timesteps | 228000   |
---------------------------------
Progress update from timestep 228100: Updated shield, loss is 0.042743511497974396
Progress update from timestep 228200: Updated shield, loss is 0.050079893320798874
Eval num_timesteps=228200, episode_reward=23.85 +/- 14.87
Episode length: 33.60 +/- 20.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.6          |
|    mean_reward          | 23.8          |
| time/                   |               |
|    total_timesteps      | 228200        |
| train/                  |               |
|    approx_kl            | 0.00044752954 |
|    clip_fraction        | 0.00402       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0172       |
|    explained_variance   | 0.278         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.499         |
|    n_updates            | 5700          |
|    policy_gradient_loss | -0.000448     |
|    value_loss           | 1.55          |
-------------------------------------------
Progress update from timestep 228300: Updated shield, loss is 0.049550700932741165
Progress update from timestep 228400: Updated shield, loss is 0.0584205687046051
Eval num_timesteps=228400, episode_reward=17.64 +/- 14.69
Episode length: 25.40 +/- 20.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 228400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.2      |
|    ep_rew_mean     | 2.64     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 571      |
|    time_elapsed    | 49265    |
|    total_timesteps | 228400   |
---------------------------------
Progress update from timestep 228500: Updated shield, loss is 0.06142441928386688
Progress update from timestep 228600: Updated shield, loss is 0.048837896436452866
Eval num_timesteps=228600, episode_reward=19.84 +/- 12.87
Episode length: 29.40 +/- 18.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 29.4          |
|    mean_reward          | 19.8          |
| time/                   |               |
|    total_timesteps      | 228600        |
| train/                  |               |
|    approx_kl            | 0.00012244558 |
|    clip_fraction        | 0.00313       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0129       |
|    explained_variance   | 0.00724       |
|    learning_rate        | 0.0001        |
|    loss                 | 1.62          |
|    n_updates            | 5710          |
|    policy_gradient_loss | -0.000377     |
|    value_loss           | 1.94          |
-------------------------------------------
Progress update from timestep 228700: Updated shield, loss is 0.05341296270489693
Progress update from timestep 228800: Updated shield, loss is 0.06466147303581238
Eval num_timesteps=228800, episode_reward=15.37 +/- 15.61
Episode length: 22.80 +/- 22.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 15.4     |
| time/              |          |
|    total_timesteps | 228800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.08     |
|    ep_rew_mean     | 2.59     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 572      |
|    time_elapsed    | 49368    |
|    total_timesteps | 228800   |
---------------------------------
Progress update from timestep 228900: Updated shield, loss is 0.03853058069944382
Progress update from timestep 229000: Updated shield, loss is 0.04693827033042908
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 229000: Saved shield_loss.csv. 
Eval num_timesteps=229000, episode_reward=13.23 +/- 13.16
Episode length: 18.80 +/- 17.55
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 18.8          |
|    mean_reward          | 13.2          |
| time/                   |               |
|    total_timesteps      | 229000        |
| train/                  |               |
|    approx_kl            | 0.00018220535 |
|    clip_fraction        | 0.00915       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0237       |
|    explained_variance   | 0.0966        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.941         |
|    n_updates            | 5720          |
|    policy_gradient_loss | -0.000861     |
|    value_loss           | 1.37          |
-------------------------------------------
Progress update from timestep 229100: Updated shield, loss is 0.05348823592066765
Progress update from timestep 229200: Updated shield, loss is 0.04576501250267029
Eval num_timesteps=229200, episode_reward=16.08 +/- 15.51
Episode length: 23.60 +/- 21.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 16.1     |
| time/              |          |
|    total_timesteps | 229200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.41     |
|    ep_rew_mean     | 2.83     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 573      |
|    time_elapsed    | 49463    |
|    total_timesteps | 229200   |
---------------------------------
Progress update from timestep 229300: Updated shield, loss is 0.04455779120326042
Progress update from timestep 229400: Updated shield, loss is 0.04328208416700363
Eval num_timesteps=229400, episode_reward=11.68 +/- 12.76
Episode length: 17.40 +/- 18.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17.4          |
|    mean_reward          | 11.7          |
| time/                   |               |
|    total_timesteps      | 229400        |
| train/                  |               |
|    approx_kl            | 0.00013031934 |
|    clip_fraction        | 0.0058        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0202       |
|    explained_variance   | 0.131         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.21          |
|    n_updates            | 5730          |
|    policy_gradient_loss | -0.000797     |
|    value_loss           | 2.12          |
-------------------------------------------
Progress update from timestep 229500: Updated shield, loss is 0.05269366875290871
Progress update from timestep 229600: Updated shield, loss is 0.05035456269979477
Eval num_timesteps=229600, episode_reward=24.65 +/- 13.35
Episode length: 35.40 +/- 18.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 24.6     |
| time/              |          |
|    total_timesteps | 229600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.54     |
|    ep_rew_mean     | 2.95     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 574      |
|    time_elapsed    | 49565    |
|    total_timesteps | 229600   |
---------------------------------
Progress update from timestep 229700: Updated shield, loss is 0.04136604815721512
Progress update from timestep 229800: Updated shield, loss is 0.04778091982007027
Eval num_timesteps=229800, episode_reward=17.88 +/- 13.64
Episode length: 26.60 +/- 19.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.6          |
|    mean_reward          | 17.9          |
| time/                   |               |
|    total_timesteps      | 229800        |
| train/                  |               |
|    approx_kl            | 3.1303014e-05 |
|    clip_fraction        | 0.00112       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0113       |
|    explained_variance   | 0.243         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.594         |
|    n_updates            | 5740          |
|    policy_gradient_loss | -0.000239     |
|    value_loss           | 2.21          |
-------------------------------------------
Progress update from timestep 229900: Updated shield, loss is 0.044297970831394196
Progress update from timestep 230000: Updated shield, loss is 0.04156108200550079
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 230000: Saved shield_loss.csv. 
Eval num_timesteps=230000, episode_reward=18.08 +/- 14.07
Episode length: 27.20 +/- 20.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.2     |
|    mean_reward     | 18.1     |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.83     |
|    ep_rew_mean     | 3.11     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 575      |
|    time_elapsed    | 49656    |
|    total_timesteps | 230000   |
---------------------------------
Progress update from timestep 230100: Updated shield, loss is 0.04468444362282753
Progress update from timestep 230200: Updated shield, loss is 0.04555616155266762
Eval num_timesteps=230200, episode_reward=15.15 +/- 15.84
Episode length: 22.60 +/- 22.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.6          |
|    mean_reward          | 15.1          |
| time/                   |               |
|    total_timesteps      | 230200        |
| train/                  |               |
|    approx_kl            | 0.00021799783 |
|    clip_fraction        | 0.00268       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00626      |
|    explained_variance   | 0.304         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.993         |
|    n_updates            | 5750          |
|    policy_gradient_loss | -0.000192     |
|    value_loss           | 2.27          |
-------------------------------------------
Progress update from timestep 230300: Updated shield, loss is 0.04326590895652771
Progress update from timestep 230400: Updated shield, loss is 0.036070745438337326
Eval num_timesteps=230400, episode_reward=16.31 +/- 14.98
Episode length: 24.00 +/- 21.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16.3     |
| time/              |          |
|    total_timesteps | 230400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.99     |
|    ep_rew_mean     | 2.49     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 576      |
|    time_elapsed    | 49751    |
|    total_timesteps | 230400   |
---------------------------------
Progress update from timestep 230500: Updated shield, loss is 0.05071651190519333
Progress update from timestep 230600: Updated shield, loss is 0.05020439997315407
Eval num_timesteps=230600, episode_reward=18.75 +/- 13.02
Episode length: 28.00 +/- 18.79
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 28            |
|    mean_reward          | 18.7          |
| time/                   |               |
|    total_timesteps      | 230600        |
| train/                  |               |
|    approx_kl            | 0.00017219654 |
|    clip_fraction        | 0.0143        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.042        |
|    explained_variance   | 0.195         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.478         |
|    n_updates            | 5760          |
|    policy_gradient_loss | -0.00185      |
|    value_loss           | 1.32          |
-------------------------------------------
Progress update from timestep 230700: Updated shield, loss is 0.03657805174589157
Progress update from timestep 230800: Updated shield, loss is 0.041333556175231934
Eval num_timesteps=230800, episode_reward=11.59 +/- 11.00
Episode length: 17.60 +/- 16.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | 11.6     |
| time/              |          |
|    total_timesteps | 230800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.92     |
|    ep_rew_mean     | 2.38     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 577      |
|    time_elapsed    | 49847    |
|    total_timesteps | 230800   |
---------------------------------
Progress update from timestep 230900: Updated shield, loss is 0.05065065994858742
Progress update from timestep 231000: Updated shield, loss is 0.0481087751686573
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 231000: Saved shield_loss.csv. 
Eval num_timesteps=231000, episode_reward=16.59 +/- 14.67
Episode length: 24.40 +/- 20.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.4          |
|    mean_reward          | 16.6          |
| time/                   |               |
|    total_timesteps      | 231000        |
| train/                  |               |
|    approx_kl            | 0.00046827082 |
|    clip_fraction        | 0.00201       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0117       |
|    explained_variance   | 0.255         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.518         |
|    n_updates            | 5770          |
|    policy_gradient_loss | -0.000552     |
|    value_loss           | 1.64          |
-------------------------------------------
Progress update from timestep 231100: Updated shield, loss is 0.0506150983273983
Progress update from timestep 231200: Updated shield, loss is 0.047831203788518906
Eval num_timesteps=231200, episode_reward=35.14 +/- 1.33
Episode length: 50.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 35.1     |
| time/              |          |
|    total_timesteps | 231200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.99     |
|    ep_rew_mean     | 3.25     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 578      |
|    time_elapsed    | 49960    |
|    total_timesteps | 231200   |
---------------------------------
Progress update from timestep 231300: Updated shield, loss is 0.047492362558841705
Progress update from timestep 231400: Updated shield, loss is 0.05227680504322052
Eval num_timesteps=231400, episode_reward=24.27 +/- 13.63
Episode length: 35.00 +/- 18.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35           |
|    mean_reward          | 24.3         |
| time/                   |              |
|    total_timesteps      | 231400       |
| train/                  |              |
|    approx_kl            | 4.029516e-05 |
|    clip_fraction        | 0.00067      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0182      |
|    explained_variance   | 0.0962       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.439        |
|    n_updates            | 5780         |
|    policy_gradient_loss | -0.000405    |
|    value_loss           | 2.2          |
------------------------------------------
Progress update from timestep 231500: Updated shield, loss is 0.04954901710152626
Progress update from timestep 231600: Updated shield, loss is 0.03631870821118355
Eval num_timesteps=231600, episode_reward=29.95 +/- 11.31
Episode length: 42.20 +/- 15.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 30       |
| time/              |          |
|    total_timesteps | 231600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.84     |
|    ep_rew_mean     | 3.07     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 579      |
|    time_elapsed    | 50078    |
|    total_timesteps | 231600   |
---------------------------------
Progress update from timestep 231700: Updated shield, loss is 0.050364959985017776
Progress update from timestep 231800: Updated shield, loss is 0.04822404310107231
Eval num_timesteps=231800, episode_reward=24.92 +/- 12.06
Episode length: 36.40 +/- 17.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.4          |
|    mean_reward          | 24.9          |
| time/                   |               |
|    total_timesteps      | 231800        |
| train/                  |               |
|    approx_kl            | 0.00034700168 |
|    clip_fraction        | 0.00491       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0133       |
|    explained_variance   | 0.111         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.13          |
|    n_updates            | 5790          |
|    policy_gradient_loss | -0.000585     |
|    value_loss           | 1.89          |
-------------------------------------------
Progress update from timestep 231900: Updated shield, loss is 0.04620645195245743
Progress update from timestep 232000: Updated shield, loss is 0.045459017157554626
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 232000: Saved shield_loss.csv. 
Eval num_timesteps=232000, episode_reward=15.93 +/- 15.20
Episode length: 23.60 +/- 21.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 15.9     |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.04     |
|    ep_rew_mean     | 2.54     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 580      |
|    time_elapsed    | 50177    |
|    total_timesteps | 232000   |
---------------------------------
Progress update from timestep 232100: Updated shield, loss is 0.051017649471759796
Progress update from timestep 232200: Updated shield, loss is 0.05641911178827286
Eval num_timesteps=232200, episode_reward=11.37 +/- 12.59
Episode length: 17.00 +/- 17.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17           |
|    mean_reward          | 11.4         |
| time/                   |              |
|    total_timesteps      | 232200       |
| train/                  |              |
|    approx_kl            | 8.925546e-05 |
|    clip_fraction        | 0.00179      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00554     |
|    explained_variance   | 0.154        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.8          |
|    n_updates            | 5800         |
|    policy_gradient_loss | -0.000202    |
|    value_loss           | 1.57         |
------------------------------------------
Progress update from timestep 232300: Updated shield, loss is 0.0395515151321888
Progress update from timestep 232400: Updated shield, loss is 0.04189232364296913
Eval num_timesteps=232400, episode_reward=11.47 +/- 12.11
Episode length: 16.80 +/- 16.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | 11.5     |
| time/              |          |
|    total_timesteps | 232400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.6      |
|    ep_rew_mean     | 3.01     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 581      |
|    time_elapsed    | 50269    |
|    total_timesteps | 232400   |
---------------------------------
Progress update from timestep 232500: Updated shield, loss is 0.05501195415854454
Progress update from timestep 232600: Updated shield, loss is 0.05455111712217331
Eval num_timesteps=232600, episode_reward=18.03 +/- 14.81
Episode length: 25.60 +/- 19.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.6          |
|    mean_reward          | 18            |
| time/                   |               |
|    total_timesteps      | 232600        |
| train/                  |               |
|    approx_kl            | 0.00020908176 |
|    clip_fraction        | 0.00536       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0183       |
|    explained_variance   | 0.317         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.692         |
|    n_updates            | 5810          |
|    policy_gradient_loss | -0.001        |
|    value_loss           | 2.87          |
-------------------------------------------
Progress update from timestep 232700: Updated shield, loss is 0.045518361032009125
Progress update from timestep 232800: Updated shield, loss is 0.05305371806025505
Eval num_timesteps=232800, episode_reward=16.59 +/- 15.13
Episode length: 24.40 +/- 21.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 232800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.36     |
|    ep_rew_mean     | 2.77     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 582      |
|    time_elapsed    | 50365    |
|    total_timesteps | 232800   |
---------------------------------
Progress update from timestep 232900: Updated shield, loss is 0.045655105262994766
Progress update from timestep 233000: Updated shield, loss is 0.04256194084882736
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 233000: Saved shield_loss.csv. 
Eval num_timesteps=233000, episode_reward=9.28 +/- 13.72
Episode length: 13.40 +/- 18.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 13.4          |
|    mean_reward          | 9.28          |
| time/                   |               |
|    total_timesteps      | 233000        |
| train/                  |               |
|    approx_kl            | 0.00043220576 |
|    clip_fraction        | 0.0067        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0204       |
|    explained_variance   | 0.147         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.685         |
|    n_updates            | 5820          |
|    policy_gradient_loss | -0.000748     |
|    value_loss           | 1.86          |
-------------------------------------------
Progress update from timestep 233100: Updated shield, loss is 0.04668112471699715
Progress update from timestep 233200: Updated shield, loss is 0.04075181484222412
Eval num_timesteps=233200, episode_reward=9.99 +/- 12.92
Episode length: 14.80 +/- 17.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 9.99     |
| time/              |          |
|    total_timesteps | 233200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.57     |
|    ep_rew_mean     | 2.91     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 583      |
|    time_elapsed    | 50463    |
|    total_timesteps | 233200   |
---------------------------------
Progress update from timestep 233300: Updated shield, loss is 0.037755005061626434
Progress update from timestep 233400: Updated shield, loss is 0.06402719020843506
Eval num_timesteps=233400, episode_reward=9.67 +/- 12.56
Episode length: 14.80 +/- 17.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.8          |
|    mean_reward          | 9.67          |
| time/                   |               |
|    total_timesteps      | 233400        |
| train/                  |               |
|    approx_kl            | 0.00021128119 |
|    clip_fraction        | 0.01          |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0186       |
|    explained_variance   | 0.149         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.18          |
|    n_updates            | 5830          |
|    policy_gradient_loss | -0.00142      |
|    value_loss           | 2.24          |
-------------------------------------------
Progress update from timestep 233500: Updated shield, loss is 0.05156029015779495
Progress update from timestep 233600: Updated shield, loss is 0.06130754575133324
Eval num_timesteps=233600, episode_reward=14.09 +/- 11.67
Episode length: 20.60 +/- 15.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 14.1     |
| time/              |          |
|    total_timesteps | 233600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.35     |
|    ep_rew_mean     | 2.71     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 584      |
|    time_elapsed    | 50560    |
|    total_timesteps | 233600   |
---------------------------------
Progress update from timestep 233700: Updated shield, loss is 0.04232996702194214
Progress update from timestep 233800: Updated shield, loss is 0.04737074300646782
Eval num_timesteps=233800, episode_reward=30.46 +/- 9.73
Episode length: 43.40 +/- 13.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 43.4         |
|    mean_reward          | 30.5         |
| time/                   |              |
|    total_timesteps      | 233800       |
| train/                  |              |
|    approx_kl            | 6.231338e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00275     |
|    explained_variance   | 0.142        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.523        |
|    n_updates            | 5840         |
|    policy_gradient_loss | -1.82e-06    |
|    value_loss           | 1.49         |
------------------------------------------
Progress update from timestep 233900: Updated shield, loss is 0.06216958537697792
Progress update from timestep 234000: Updated shield, loss is 0.033960599452257156
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 234000: Saved shield_loss.csv. 
Eval num_timesteps=234000, episode_reward=21.76 +/- 16.95
Episode length: 31.00 +/- 23.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31       |
|    mean_reward     | 21.8     |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.3      |
|    ep_rew_mean     | 2.68     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 585      |
|    time_elapsed    | 50659    |
|    total_timesteps | 234000   |
---------------------------------
Progress update from timestep 234100: Updated shield, loss is 0.05509592220187187
Progress update from timestep 234200: Updated shield, loss is 0.05659104883670807
Eval num_timesteps=234200, episode_reward=10.37 +/- 13.66
Episode length: 15.00 +/- 18.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15           |
|    mean_reward          | 10.4         |
| time/                   |              |
|    total_timesteps      | 234200       |
| train/                  |              |
|    approx_kl            | 7.605721e-05 |
|    clip_fraction        | 0.0029       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0157      |
|    explained_variance   | 0.17         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.858        |
|    n_updates            | 5850         |
|    policy_gradient_loss | -0.00051     |
|    value_loss           | 1.59         |
------------------------------------------
Progress update from timestep 234300: Updated shield, loss is 0.05549434944987297
Progress update from timestep 234400: Updated shield, loss is 0.056285321712493896
Eval num_timesteps=234400, episode_reward=12.64 +/- 12.03
Episode length: 18.80 +/- 16.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.8     |
|    mean_reward     | 12.6     |
| time/              |          |
|    total_timesteps | 234400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.62     |
|    ep_rew_mean     | 2.94     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 586      |
|    time_elapsed    | 50752    |
|    total_timesteps | 234400   |
---------------------------------
Progress update from timestep 234500: Updated shield, loss is 0.04871869459748268
Progress update from timestep 234600: Updated shield, loss is 0.0487009733915329
Eval num_timesteps=234600, episode_reward=12.34 +/- 12.06
Episode length: 18.00 +/- 16.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18           |
|    mean_reward          | 12.3         |
| time/                   |              |
|    total_timesteps      | 234600       |
| train/                  |              |
|    approx_kl            | 0.0003918619 |
|    clip_fraction        | 0.00603      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0312      |
|    explained_variance   | 0.275        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.18         |
|    n_updates            | 5860         |
|    policy_gradient_loss | -0.00133     |
|    value_loss           | 2.12         |
------------------------------------------
Progress update from timestep 234700: Updated shield, loss is 0.0691058337688446
Progress update from timestep 234800: Updated shield, loss is 0.060013916343450546
Eval num_timesteps=234800, episode_reward=17.46 +/- 13.93
Episode length: 25.80 +/- 19.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | 17.5     |
| time/              |          |
|    total_timesteps | 234800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.03     |
|    ep_rew_mean     | 3.23     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 587      |
|    time_elapsed    | 50847    |
|    total_timesteps | 234800   |
---------------------------------
Progress update from timestep 234900: Updated shield, loss is 0.04931517317891121
Progress update from timestep 235000: Updated shield, loss is 0.05380920693278313
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 235000: Saved shield_loss.csv. 
Eval num_timesteps=235000, episode_reward=10.92 +/- 11.65
Episode length: 16.80 +/- 17.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.8          |
|    mean_reward          | 10.9          |
| time/                   |               |
|    total_timesteps      | 235000        |
| train/                  |               |
|    approx_kl            | 0.00015556646 |
|    clip_fraction        | 0.0058        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0308       |
|    explained_variance   | 0.206         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.1           |
|    n_updates            | 5870          |
|    policy_gradient_loss | -0.000733     |
|    value_loss           | 2.03          |
-------------------------------------------
Progress update from timestep 235100: Updated shield, loss is 0.04312083497643471
Progress update from timestep 235200: Updated shield, loss is 0.04544134438037872
Eval num_timesteps=235200, episode_reward=16.72 +/- 15.13
Episode length: 24.60 +/- 21.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 235200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.5      |
|    ep_rew_mean     | 3.57     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 588      |
|    time_elapsed    | 50945    |
|    total_timesteps | 235200   |
---------------------------------
Progress update from timestep 235300: Updated shield, loss is 0.05116695165634155
Progress update from timestep 235400: Updated shield, loss is 0.05315767973661423
Eval num_timesteps=235400, episode_reward=17.22 +/- 15.99
Episode length: 24.20 +/- 21.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 24.2        |
|    mean_reward          | 17.2        |
| time/                   |             |
|    total_timesteps      | 235400      |
| train/                  |             |
|    approx_kl            | 0.000664352 |
|    clip_fraction        | 0.00714     |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0111     |
|    explained_variance   | 0.101       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.68        |
|    n_updates            | 5880        |
|    policy_gradient_loss | -0.000891   |
|    value_loss           | 3.14        |
-----------------------------------------
Progress update from timestep 235500: Updated shield, loss is 0.045617952942848206
Progress update from timestep 235600: Updated shield, loss is 0.03891807794570923
Eval num_timesteps=235600, episode_reward=18.70 +/- 14.31
Episode length: 26.60 +/- 19.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.6     |
|    mean_reward     | 18.7     |
| time/              |          |
|    total_timesteps | 235600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.31     |
|    ep_rew_mean     | 2.69     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 589      |
|    time_elapsed    | 51037    |
|    total_timesteps | 235600   |
---------------------------------
Progress update from timestep 235700: Updated shield, loss is 0.05711276829242706
Progress update from timestep 235800: Updated shield, loss is 0.04818940535187721
Eval num_timesteps=235800, episode_reward=22.38 +/- 14.89
Episode length: 32.80 +/- 21.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32.8          |
|    mean_reward          | 22.4          |
| time/                   |               |
|    total_timesteps      | 235800        |
| train/                  |               |
|    approx_kl            | 1.7313621e-05 |
|    clip_fraction        | 0.00134       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00891      |
|    explained_variance   | 0.0825        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.887         |
|    n_updates            | 5890          |
|    policy_gradient_loss | -0.000233     |
|    value_loss           | 1.55          |
-------------------------------------------
Progress update from timestep 235900: Updated shield, loss is 0.05891723930835724
Progress update from timestep 236000: Updated shield, loss is 0.058072302490472794
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 236000: Saved shield_loss.csv. 
Eval num_timesteps=236000, episode_reward=12.35 +/- 12.16
Episode length: 18.00 +/- 16.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | 12.3     |
| time/              |          |
|    total_timesteps | 236000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.25     |
|    ep_rew_mean     | 2.7      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 590      |
|    time_elapsed    | 51135    |
|    total_timesteps | 236000   |
---------------------------------
Progress update from timestep 236100: Updated shield, loss is 0.047338344156742096
Progress update from timestep 236200: Updated shield, loss is 0.04624125361442566
Eval num_timesteps=236200, episode_reward=17.10 +/- 15.65
Episode length: 24.20 +/- 21.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.2          |
|    mean_reward          | 17.1          |
| time/                   |               |
|    total_timesteps      | 236200        |
| train/                  |               |
|    approx_kl            | 0.00016525507 |
|    clip_fraction        | 0.00379       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0111       |
|    explained_variance   | 0.166         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.481         |
|    n_updates            | 5900          |
|    policy_gradient_loss | -0.000207     |
|    value_loss           | 1.64          |
-------------------------------------------
Progress update from timestep 236300: Updated shield, loss is 0.05482151359319687
Progress update from timestep 236400: Updated shield, loss is 0.04366312175989151
Eval num_timesteps=236400, episode_reward=6.97 +/- 3.39
Episode length: 10.60 +/- 4.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10.6     |
|    mean_reward     | 6.97     |
| time/              |          |
|    total_timesteps | 236400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.43     |
|    ep_rew_mean     | 2.8      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 591      |
|    time_elapsed    | 51216    |
|    total_timesteps | 236400   |
---------------------------------
Progress update from timestep 236500: Updated shield, loss is 0.04179772734642029
Progress update from timestep 236600: Updated shield, loss is 0.04959183186292648
Eval num_timesteps=236600, episode_reward=16.39 +/- 15.26
Episode length: 24.00 +/- 21.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24            |
|    mean_reward          | 16.4          |
| time/                   |               |
|    total_timesteps      | 236600        |
| train/                  |               |
|    approx_kl            | 0.00012396637 |
|    clip_fraction        | 0.00491       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00928      |
|    explained_variance   | 0.251         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.17          |
|    n_updates            | 5910          |
|    policy_gradient_loss | -0.000462     |
|    value_loss           | 1.92          |
-------------------------------------------
Progress update from timestep 236700: Updated shield, loss is 0.043231215327978134
Progress update from timestep 236800: Updated shield, loss is 0.04743020609021187
Eval num_timesteps=236800, episode_reward=29.95 +/- 11.28
Episode length: 42.20 +/- 15.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 30       |
| time/              |          |
|    total_timesteps | 236800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.53     |
|    ep_rew_mean     | 2.9      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 592      |
|    time_elapsed    | 51316    |
|    total_timesteps | 236800   |
---------------------------------
Progress update from timestep 236900: Updated shield, loss is 0.04836707562208176
Progress update from timestep 237000: Updated shield, loss is 0.042518459260463715
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 237000: Saved shield_loss.csv. 
Eval num_timesteps=237000, episode_reward=23.45 +/- 14.00
Episode length: 34.00 +/- 19.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34            |
|    mean_reward          | 23.4          |
| time/                   |               |
|    total_timesteps      | 237000        |
| train/                  |               |
|    approx_kl            | 9.6861804e-05 |
|    clip_fraction        | 0.00313       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0201       |
|    explained_variance   | 0.17          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.602         |
|    n_updates            | 5920          |
|    policy_gradient_loss | -0.000741     |
|    value_loss           | 1.69          |
-------------------------------------------
Progress update from timestep 237100: Updated shield, loss is 0.03940106928348541
Progress update from timestep 237200: Updated shield, loss is 0.03856329992413521
Eval num_timesteps=237200, episode_reward=13.06 +/- 10.46
Episode length: 19.60 +/- 15.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 13.1     |
| time/              |          |
|    total_timesteps | 237200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.13     |
|    ep_rew_mean     | 2.66     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 593      |
|    time_elapsed    | 51416    |
|    total_timesteps | 237200   |
---------------------------------
Progress update from timestep 237300: Updated shield, loss is 0.04804665595293045
Progress update from timestep 237400: Updated shield, loss is 0.04754269868135452
Eval num_timesteps=237400, episode_reward=20.30 +/- 12.50
Episode length: 30.00 +/- 17.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 30            |
|    mean_reward          | 20.3          |
| time/                   |               |
|    total_timesteps      | 237400        |
| train/                  |               |
|    approx_kl            | 1.8548495e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0019       |
|    explained_variance   | 0.221         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.818         |
|    n_updates            | 5930          |
|    policy_gradient_loss | -1.19e-06     |
|    value_loss           | 1.74          |
-------------------------------------------
Progress update from timestep 237500: Updated shield, loss is 0.04672415927052498
Progress update from timestep 237600: Updated shield, loss is 0.04533539339900017
Eval num_timesteps=237600, episode_reward=17.98 +/- 14.84
Episode length: 25.60 +/- 19.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 18       |
| time/              |          |
|    total_timesteps | 237600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.34     |
|    ep_rew_mean     | 2.74     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 594      |
|    time_elapsed    | 51512    |
|    total_timesteps | 237600   |
---------------------------------
Progress update from timestep 237700: Updated shield, loss is 0.0410340279340744
Progress update from timestep 237800: Updated shield, loss is 0.045587264001369476
Eval num_timesteps=237800, episode_reward=5.39 +/- 3.11
Episode length: 8.60 +/- 4.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 8.6          |
|    mean_reward          | 5.39         |
| time/                   |              |
|    total_timesteps      | 237800       |
| train/                  |              |
|    approx_kl            | 7.568118e-05 |
|    clip_fraction        | 0.00246      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0187      |
|    explained_variance   | 0.122        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.838        |
|    n_updates            | 5940         |
|    policy_gradient_loss | -0.000282    |
|    value_loss           | 1.68         |
------------------------------------------
Progress update from timestep 237900: Updated shield, loss is 0.04652918875217438
Progress update from timestep 238000: Updated shield, loss is 0.046313609927892685
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 238000: Saved shield_loss.csv. 
Eval num_timesteps=238000, episode_reward=11.34 +/- 12.80
Episode length: 16.40 +/- 17.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | 11.3     |
| time/              |          |
|    total_timesteps | 238000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.9      |
|    ep_rew_mean     | 2.44     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 595      |
|    time_elapsed    | 51588    |
|    total_timesteps | 238000   |
---------------------------------
Progress update from timestep 238100: Updated shield, loss is 0.042405594140291214
Progress update from timestep 238200: Updated shield, loss is 0.04662942886352539
Eval num_timesteps=238200, episode_reward=22.76 +/- 16.65
Episode length: 31.80 +/- 22.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 31.8         |
|    mean_reward          | 22.8         |
| time/                   |              |
|    total_timesteps      | 238200       |
| train/                  |              |
|    approx_kl            | 7.992719e-05 |
|    clip_fraction        | 0.00179      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0165      |
|    explained_variance   | 0.123        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.43         |
|    n_updates            | 5950         |
|    policy_gradient_loss | -0.000474    |
|    value_loss           | 1.48         |
------------------------------------------
Progress update from timestep 238300: Updated shield, loss is 0.04895642772316933
Progress update from timestep 238400: Updated shield, loss is 0.042972318828105927
Eval num_timesteps=238400, episode_reward=12.68 +/- 11.81
Episode length: 18.80 +/- 16.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.8     |
|    mean_reward     | 12.7     |
| time/              |          |
|    total_timesteps | 238400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.76     |
|    ep_rew_mean     | 3.06     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 596      |
|    time_elapsed    | 51680    |
|    total_timesteps | 238400   |
---------------------------------
Progress update from timestep 238500: Updated shield, loss is 0.05599034205079079
Progress update from timestep 238600: Updated shield, loss is 0.04309336096048355
Eval num_timesteps=238600, episode_reward=4.57 +/- 4.60
Episode length: 7.20 +/- 6.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 7.2           |
|    mean_reward          | 4.57          |
| time/                   |               |
|    total_timesteps      | 238600        |
| train/                  |               |
|    approx_kl            | 9.8618846e-05 |
|    clip_fraction        | 0.00246       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0102       |
|    explained_variance   | 0.173         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.71          |
|    n_updates            | 5960          |
|    policy_gradient_loss | -0.000493     |
|    value_loss           | 2.15          |
-------------------------------------------
Progress update from timestep 238700: Updated shield, loss is 0.04439777880907059
Progress update from timestep 238800: Updated shield, loss is 0.045900750905275345
Eval num_timesteps=238800, episode_reward=25.19 +/- 12.59
Episode length: 36.20 +/- 17.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 25.2     |
| time/              |          |
|    total_timesteps | 238800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4        |
|    ep_rew_mean     | 2.47     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 597      |
|    time_elapsed    | 51770    |
|    total_timesteps | 238800   |
---------------------------------
Progress update from timestep 238900: Updated shield, loss is 0.0466441884636879
Progress update from timestep 239000: Updated shield, loss is 0.05159749090671539
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 239000: Saved shield_loss.csv. 
Eval num_timesteps=239000, episode_reward=24.25 +/- 15.24
Episode length: 33.40 +/- 20.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.4          |
|    mean_reward          | 24.3          |
| time/                   |               |
|    total_timesteps      | 239000        |
| train/                  |               |
|    approx_kl            | 0.00010084177 |
|    clip_fraction        | 0.00915       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0114       |
|    explained_variance   | 0.221         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.02          |
|    n_updates            | 5970          |
|    policy_gradient_loss | -0.00107      |
|    value_loss           | 1.69          |
-------------------------------------------
Progress update from timestep 239100: Updated shield, loss is 0.04498827084898949
Progress update from timestep 239200: Updated shield, loss is 0.04327907785773277
Eval num_timesteps=239200, episode_reward=23.58 +/- 13.84
Episode length: 34.00 +/- 19.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 23.6     |
| time/              |          |
|    total_timesteps | 239200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.34     |
|    ep_rew_mean     | 2.75     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 598      |
|    time_elapsed    | 51867    |
|    total_timesteps | 239200   |
---------------------------------
Progress update from timestep 239300: Updated shield, loss is 0.05295099318027496
Progress update from timestep 239400: Updated shield, loss is 0.04835328459739685
Eval num_timesteps=239400, episode_reward=23.58 +/- 14.06
Episode length: 34.40 +/- 19.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.4          |
|    mean_reward          | 23.6          |
| time/                   |               |
|    total_timesteps      | 239400        |
| train/                  |               |
|    approx_kl            | 5.9893722e-05 |
|    clip_fraction        | 0.0029        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0141       |
|    explained_variance   | 0.18          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.541         |
|    n_updates            | 5980          |
|    policy_gradient_loss | -0.000323     |
|    value_loss           | 1.77          |
-------------------------------------------
Progress update from timestep 239500: Updated shield, loss is 0.043053533881902695
Progress update from timestep 239600: Updated shield, loss is 0.043832968920469284
Eval num_timesteps=239600, episode_reward=4.09 +/- 2.81
Episode length: 7.00 +/- 4.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7        |
|    mean_reward     | 4.09     |
| time/              |          |
|    total_timesteps | 239600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.57     |
|    ep_rew_mean     | 2.92     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 599      |
|    time_elapsed    | 51967    |
|    total_timesteps | 239600   |
---------------------------------
Progress update from timestep 239700: Updated shield, loss is 0.039847131818532944
Progress update from timestep 239800: Updated shield, loss is 0.05476491153240204
Eval num_timesteps=239800, episode_reward=10.53 +/- 13.21
Episode length: 15.20 +/- 17.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.2          |
|    mean_reward          | 10.5          |
| time/                   |               |
|    total_timesteps      | 239800        |
| train/                  |               |
|    approx_kl            | 0.00024158237 |
|    clip_fraction        | 0.00915       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0217       |
|    explained_variance   | 0.146         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.624         |
|    n_updates            | 5990          |
|    policy_gradient_loss | -0.00126      |
|    value_loss           | 1.49          |
-------------------------------------------
Progress update from timestep 239900: Updated shield, loss is 0.03907279670238495
Progress update from timestep 240000: Updated shield, loss is 0.04321977123618126
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 240000: Saved shield_loss.csv. 
Eval num_timesteps=240000, episode_reward=24.10 +/- 14.32
Episode length: 34.40 +/- 19.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 24.1     |
| time/              |          |
|    total_timesteps | 240000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.05     |
|    ep_rew_mean     | 3.27     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 600      |
|    time_elapsed    | 52065    |
|    total_timesteps | 240000   |
---------------------------------
Progress update from timestep 240100: Updated shield, loss is 0.050114043056964874
Progress update from timestep 240200: Updated shield, loss is 0.04274226352572441
Eval num_timesteps=240200, episode_reward=23.18 +/- 15.74
Episode length: 32.60 +/- 21.43
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32.6          |
|    mean_reward          | 23.2          |
| time/                   |               |
|    total_timesteps      | 240200        |
| train/                  |               |
|    approx_kl            | 1.9910283e-05 |
|    clip_fraction        | 0.00134       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00344      |
|    explained_variance   | 0.21          |
|    learning_rate        | 0.0001        |
|    loss                 | 1.02          |
|    n_updates            | 6000          |
|    policy_gradient_loss | -8.98e-05     |
|    value_loss           | 2.14          |
-------------------------------------------
Progress update from timestep 240300: Updated shield, loss is 0.040151145309209824
Progress update from timestep 240400: Updated shield, loss is 0.03965042531490326
Eval num_timesteps=240400, episode_reward=7.89 +/- 5.91
Episode length: 12.00 +/- 8.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 12       |
|    mean_reward     | 7.89     |
| time/              |          |
|    total_timesteps | 240400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.45     |
|    ep_rew_mean     | 2.79     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 601      |
|    time_elapsed    | 52150    |
|    total_timesteps | 240400   |
---------------------------------
Progress update from timestep 240500: Updated shield, loss is 0.044487785547971725
Progress update from timestep 240600: Updated shield, loss is 0.03861905261874199
Eval num_timesteps=240600, episode_reward=10.36 +/- 12.74
Episode length: 15.20 +/- 17.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.2         |
|    mean_reward          | 10.4         |
| time/                   |              |
|    total_timesteps      | 240600       |
| train/                  |              |
|    approx_kl            | 0.0007144272 |
|    clip_fraction        | 0.0136       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0284      |
|    explained_variance   | 0.195        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.02         |
|    n_updates            | 6010         |
|    policy_gradient_loss | -0.0014      |
|    value_loss           | 2.12         |
------------------------------------------
Progress update from timestep 240700: Updated shield, loss is 0.04882054030895233
Progress update from timestep 240800: Updated shield, loss is 0.03753967583179474
Eval num_timesteps=240800, episode_reward=13.31 +/- 10.28
Episode length: 20.20 +/- 15.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.2     |
|    mean_reward     | 13.3     |
| time/              |          |
|    total_timesteps | 240800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.07     |
|    ep_rew_mean     | 2.59     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 602      |
|    time_elapsed    | 52245    |
|    total_timesteps | 240800   |
---------------------------------
Progress update from timestep 240900: Updated shield, loss is 0.048946116119623184
Progress update from timestep 241000: Updated shield, loss is 0.04771742969751358
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 241000: Saved shield_loss.csv. 
Eval num_timesteps=241000, episode_reward=14.22 +/- 11.00
Episode length: 21.20 +/- 15.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 21.2          |
|    mean_reward          | 14.2          |
| time/                   |               |
|    total_timesteps      | 241000        |
| train/                  |               |
|    approx_kl            | 0.00011754851 |
|    clip_fraction        | 0.00402       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0096       |
|    explained_variance   | 0.139         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.652         |
|    n_updates            | 6020          |
|    policy_gradient_loss | -0.000402     |
|    value_loss           | 2.11          |
-------------------------------------------
Progress update from timestep 241100: Updated shield, loss is 0.048002343624830246
Progress update from timestep 241200: Updated shield, loss is 0.045481376349925995
Eval num_timesteps=241200, episode_reward=10.95 +/- 12.01
Episode length: 16.40 +/- 17.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | 11       |
| time/              |          |
|    total_timesteps | 241200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.01     |
|    ep_rew_mean     | 2.46     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 603      |
|    time_elapsed    | 52343    |
|    total_timesteps | 241200   |
---------------------------------
Progress update from timestep 241300: Updated shield, loss is 0.043378472328186035
Progress update from timestep 241400: Updated shield, loss is 0.03948212042450905
Eval num_timesteps=241400, episode_reward=7.98 +/- 3.15
Episode length: 12.40 +/- 4.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 12.4          |
|    mean_reward          | 7.98          |
| time/                   |               |
|    total_timesteps      | 241400        |
| train/                  |               |
|    approx_kl            | 0.00023907509 |
|    clip_fraction        | 0.00647       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.018        |
|    explained_variance   | 0.145         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.397         |
|    n_updates            | 6030          |
|    policy_gradient_loss | -0.000974     |
|    value_loss           | 1.38          |
-------------------------------------------
Progress update from timestep 241500: Updated shield, loss is 0.0469057522714138
Progress update from timestep 241600: Updated shield, loss is 0.046129871159791946
Eval num_timesteps=241600, episode_reward=23.53 +/- 15.25
Episode length: 33.20 +/- 20.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | 23.5     |
| time/              |          |
|    total_timesteps | 241600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.1      |
|    ep_rew_mean     | 2.58     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 604      |
|    time_elapsed    | 52426    |
|    total_timesteps | 241600   |
---------------------------------
Progress update from timestep 241700: Updated shield, loss is 0.04218033701181412
Progress update from timestep 241800: Updated shield, loss is 0.041909635066986084
Eval num_timesteps=241800, episode_reward=10.13 +/- 13.53
Episode length: 14.80 +/- 17.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.8          |
|    mean_reward          | 10.1          |
| time/                   |               |
|    total_timesteps      | 241800        |
| train/                  |               |
|    approx_kl            | 0.00011964548 |
|    clip_fraction        | 0.00558       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0287       |
|    explained_variance   | 0.166         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.622         |
|    n_updates            | 6040          |
|    policy_gradient_loss | -0.00107      |
|    value_loss           | 1.22          |
-------------------------------------------
Progress update from timestep 241900: Updated shield, loss is 0.0515398308634758
Progress update from timestep 242000: Updated shield, loss is 0.055327728390693665
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 242000: Saved shield_loss.csv. 
Eval num_timesteps=242000, episode_reward=24.13 +/- 14.05
Episode length: 34.40 +/- 19.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 24.1     |
| time/              |          |
|    total_timesteps | 242000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.42     |
|    ep_rew_mean     | 2.8      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 605      |
|    time_elapsed    | 52523    |
|    total_timesteps | 242000   |
---------------------------------
Progress update from timestep 242100: Updated shield, loss is 0.048372525721788406
Progress update from timestep 242200: Updated shield, loss is 0.04546133056282997
Eval num_timesteps=242200, episode_reward=20.43 +/- 12.96
Episode length: 29.40 +/- 17.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 29.4          |
|    mean_reward          | 20.4          |
| time/                   |               |
|    total_timesteps      | 242200        |
| train/                  |               |
|    approx_kl            | 3.4377386e-05 |
|    clip_fraction        | 0.00223       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0165       |
|    explained_variance   | 0.181         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.325         |
|    n_updates            | 6050          |
|    policy_gradient_loss | -0.000379     |
|    value_loss           | 1.6           |
-------------------------------------------
Progress update from timestep 242300: Updated shield, loss is 0.046476587653160095
Progress update from timestep 242400: Updated shield, loss is 0.04224514216184616
Eval num_timesteps=242400, episode_reward=15.77 +/- 15.82
Episode length: 23.00 +/- 22.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 15.8     |
| time/              |          |
|    total_timesteps | 242400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.46     |
|    ep_rew_mean     | 2.88     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 606      |
|    time_elapsed    | 52617    |
|    total_timesteps | 242400   |
---------------------------------
Progress update from timestep 242500: Updated shield, loss is 0.05794462189078331
Progress update from timestep 242600: Updated shield, loss is 0.05910396948456764
Eval num_timesteps=242600, episode_reward=28.74 +/- 12.09
Episode length: 41.60 +/- 16.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 41.6          |
|    mean_reward          | 28.7          |
| time/                   |               |
|    total_timesteps      | 242600        |
| train/                  |               |
|    approx_kl            | 0.00030162281 |
|    clip_fraction        | 0.00536       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0397       |
|    explained_variance   | 0.23          |
|    learning_rate        | 0.0001        |
|    loss                 | 1.33          |
|    n_updates            | 6060          |
|    policy_gradient_loss | -0.000939     |
|    value_loss           | 1.82          |
-------------------------------------------
Progress update from timestep 242700: Updated shield, loss is 0.05777689814567566
Progress update from timestep 242800: Updated shield, loss is 0.04524432495236397
Eval num_timesteps=242800, episode_reward=16.79 +/- 14.48
Episode length: 24.80 +/- 20.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 242800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.44     |
|    ep_rew_mean     | 2.81     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 607      |
|    time_elapsed    | 52732    |
|    total_timesteps | 242800   |
---------------------------------
Progress update from timestep 242900: Updated shield, loss is 0.0578334741294384
Progress update from timestep 243000: Updated shield, loss is 0.049313876777887344
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 243000: Saved shield_loss.csv. 
Eval num_timesteps=243000, episode_reward=29.17 +/- 12.85
Episode length: 41.20 +/- 17.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.2         |
|    mean_reward          | 29.2         |
| time/                   |              |
|    total_timesteps      | 243000       |
| train/                  |              |
|    approx_kl            | 0.0003177866 |
|    clip_fraction        | 0.00647      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0229      |
|    explained_variance   | 0.13         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.632        |
|    n_updates            | 6070         |
|    policy_gradient_loss | -0.00071     |
|    value_loss           | 1.57         |
------------------------------------------
Progress update from timestep 243100: Updated shield, loss is 0.044285647571086884
Progress update from timestep 243200: Updated shield, loss is 0.048282213509082794
Eval num_timesteps=243200, episode_reward=22.25 +/- 15.97
Episode length: 32.00 +/- 22.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32       |
|    mean_reward     | 22.2     |
| time/              |          |
|    total_timesteps | 243200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.27     |
|    ep_rew_mean     | 2.7      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 608      |
|    time_elapsed    | 52848    |
|    total_timesteps | 243200   |
---------------------------------
Progress update from timestep 243300: Updated shield, loss is 0.04647773504257202
Progress update from timestep 243400: Updated shield, loss is 0.04913643002510071
Eval num_timesteps=243400, episode_reward=12.72 +/- 12.46
Episode length: 18.60 +/- 17.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 18.6          |
|    mean_reward          | 12.7          |
| time/                   |               |
|    total_timesteps      | 243400        |
| train/                  |               |
|    approx_kl            | 0.00013506498 |
|    clip_fraction        | 0.00357       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0204       |
|    explained_variance   | 0.151         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.587         |
|    n_updates            | 6080          |
|    policy_gradient_loss | -0.000846     |
|    value_loss           | 2.02          |
-------------------------------------------
Progress update from timestep 243500: Updated shield, loss is 0.050400976091623306
Progress update from timestep 243600: Updated shield, loss is 0.05041498690843582
Eval num_timesteps=243600, episode_reward=22.25 +/- 15.91
Episode length: 32.00 +/- 22.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32       |
|    mean_reward     | 22.2     |
| time/              |          |
|    total_timesteps | 243600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.09     |
|    ep_rew_mean     | 2.55     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 609      |
|    time_elapsed    | 52961    |
|    total_timesteps | 243600   |
---------------------------------
Progress update from timestep 243700: Updated shield, loss is 0.048165060579776764
Progress update from timestep 243800: Updated shield, loss is 0.05441412702202797
Eval num_timesteps=243800, episode_reward=18.42 +/- 13.73
Episode length: 26.60 +/- 19.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.6          |
|    mean_reward          | 18.4          |
| time/                   |               |
|    total_timesteps      | 243800        |
| train/                  |               |
|    approx_kl            | 0.00012077346 |
|    clip_fraction        | 0.00536       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0238       |
|    explained_variance   | 0.2           |
|    learning_rate        | 0.0001        |
|    loss                 | 0.366         |
|    n_updates            | 6090          |
|    policy_gradient_loss | -0.000898     |
|    value_loss           | 1.34          |
-------------------------------------------
Progress update from timestep 243900: Updated shield, loss is 0.04821595922112465
Progress update from timestep 244000: Updated shield, loss is 0.0526762530207634
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 244000: Saved shield_loss.csv. 
Eval num_timesteps=244000, episode_reward=22.40 +/- 15.79
Episode length: 32.20 +/- 21.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 22.4     |
| time/              |          |
|    total_timesteps | 244000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.23     |
|    ep_rew_mean     | 2.69     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 610      |
|    time_elapsed    | 53058    |
|    total_timesteps | 244000   |
---------------------------------
Progress update from timestep 244100: Updated shield, loss is 0.03941316902637482
Progress update from timestep 244200: Updated shield, loss is 0.052622850984334946
Eval num_timesteps=244200, episode_reward=11.84 +/- 10.90
Episode length: 18.20 +/- 16.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 18.2          |
|    mean_reward          | 11.8          |
| time/                   |               |
|    total_timesteps      | 244200        |
| train/                  |               |
|    approx_kl            | 0.00023553478 |
|    clip_fraction        | 0.00647       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0316       |
|    explained_variance   | 0.265         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.514         |
|    n_updates            | 6100          |
|    policy_gradient_loss | -0.000684     |
|    value_loss           | 1.71          |
-------------------------------------------
Progress update from timestep 244300: Updated shield, loss is 0.047060877084732056
Progress update from timestep 244400: Updated shield, loss is 0.04880501702427864
Eval num_timesteps=244400, episode_reward=27.72 +/- 13.53
Episode length: 40.40 +/- 19.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.4     |
|    mean_reward     | 27.7     |
| time/              |          |
|    total_timesteps | 244400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.69     |
|    ep_rew_mean     | 2.93     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 611      |
|    time_elapsed    | 53153    |
|    total_timesteps | 244400   |
---------------------------------
Progress update from timestep 244500: Updated shield, loss is 0.04832536354660988
Progress update from timestep 244600: Updated shield, loss is 0.05201663821935654
Eval num_timesteps=244600, episode_reward=9.89 +/- 12.95
Episode length: 14.60 +/- 17.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.6         |
|    mean_reward          | 9.89         |
| time/                   |              |
|    total_timesteps      | 244600       |
| train/                  |              |
|    approx_kl            | 7.419398e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00636     |
|    explained_variance   | 0.318        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.56         |
|    n_updates            | 6110         |
|    policy_gradient_loss | -4.61e-06    |
|    value_loss           | 2.3          |
------------------------------------------
Progress update from timestep 244700: Updated shield, loss is 0.03879346698522568
Progress update from timestep 244800: Updated shield, loss is 0.055116403847932816
Eval num_timesteps=244800, episode_reward=23.97 +/- 12.68
Episode length: 35.40 +/- 18.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 24       |
| time/              |          |
|    total_timesteps | 244800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.02     |
|    ep_rew_mean     | 2.6      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 612      |
|    time_elapsed    | 53243    |
|    total_timesteps | 244800   |
---------------------------------
Progress update from timestep 244900: Updated shield, loss is 0.06127322465181351
Progress update from timestep 245000: Updated shield, loss is 0.05020589381456375
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 245000: Saved shield_loss.csv. 
Eval num_timesteps=245000, episode_reward=22.54 +/- 16.08
Episode length: 32.00 +/- 22.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32            |
|    mean_reward          | 22.5          |
| time/                   |               |
|    total_timesteps      | 245000        |
| train/                  |               |
|    approx_kl            | 0.00045478396 |
|    clip_fraction        | 0.00982       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0161       |
|    explained_variance   | 0.198         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.794         |
|    n_updates            | 6120          |
|    policy_gradient_loss | -0.000949     |
|    value_loss           | 1.71          |
-------------------------------------------
Progress update from timestep 245100: Updated shield, loss is 0.045927029103040695
Progress update from timestep 245200: Updated shield, loss is 0.05772183835506439
Eval num_timesteps=245200, episode_reward=8.79 +/- 12.33
Episode length: 13.80 +/- 18.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.8     |
|    mean_reward     | 8.79     |
| time/              |          |
|    total_timesteps | 245200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.23     |
|    ep_rew_mean     | 2.67     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 613      |
|    time_elapsed    | 53335    |
|    total_timesteps | 245200   |
---------------------------------
Progress update from timestep 245300: Updated shield, loss is 0.05336024612188339
Progress update from timestep 245400: Updated shield, loss is 0.05017473176121712
Eval num_timesteps=245400, episode_reward=29.60 +/- 10.89
Episode length: 42.40 +/- 15.20
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 42.4           |
|    mean_reward          | 29.6           |
| time/                   |                |
|    total_timesteps      | 245400         |
| train/                  |                |
|    approx_kl            | 0.000102813414 |
|    clip_fraction        | 0.00335        |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.0253        |
|    explained_variance   | 0.301          |
|    learning_rate        | 0.0001         |
|    loss                 | 0.634          |
|    n_updates            | 6130           |
|    policy_gradient_loss | -0.000383      |
|    value_loss           | 1.29           |
--------------------------------------------
Progress update from timestep 245500: Updated shield, loss is 0.04674907028675079
Progress update from timestep 245600: Updated shield, loss is 0.04862121865153313
Eval num_timesteps=245600, episode_reward=11.34 +/- 11.98
Episode length: 17.20 +/- 17.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | 11.3     |
| time/              |          |
|    total_timesteps | 245600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.2      |
|    ep_rew_mean     | 2.66     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 614      |
|    time_elapsed    | 53445    |
|    total_timesteps | 245600   |
---------------------------------
Progress update from timestep 245700: Updated shield, loss is 0.05594133585691452
Progress update from timestep 245800: Updated shield, loss is 0.05226843059062958
Eval num_timesteps=245800, episode_reward=16.62 +/- 14.64
Episode length: 24.60 +/- 20.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 24.6        |
|    mean_reward          | 16.6        |
| time/                   |             |
|    total_timesteps      | 245800      |
| train/                  |             |
|    approx_kl            | 3.69389e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0151     |
|    explained_variance   | -0.0195     |
|    learning_rate        | 0.0001      |
|    loss                 | 0.796       |
|    n_updates            | 6140        |
|    policy_gradient_loss | -0.000141   |
|    value_loss           | 1.87        |
-----------------------------------------
Progress update from timestep 245900: Updated shield, loss is 0.04709802567958832
Progress update from timestep 246000: Updated shield, loss is 0.046072326600551605
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 246000: Saved shield_loss.csv. 
Eval num_timesteps=246000, episode_reward=28.87 +/- 11.27
Episode length: 42.00 +/- 16.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 28.9     |
| time/              |          |
|    total_timesteps | 246000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.91     |
|    ep_rew_mean     | 3.24     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 615      |
|    time_elapsed    | 53558    |
|    total_timesteps | 246000   |
---------------------------------
Progress update from timestep 246100: Updated shield, loss is 0.042820099741220474
Progress update from timestep 246200: Updated shield, loss is 0.05062965303659439
Eval num_timesteps=246200, episode_reward=10.77 +/- 11.72
Episode length: 16.60 +/- 17.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.6          |
|    mean_reward          | 10.8          |
| time/                   |               |
|    total_timesteps      | 246200        |
| train/                  |               |
|    approx_kl            | 1.7077711e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00219      |
|    explained_variance   | 0.119         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.2           |
|    n_updates            | 6150          |
|    policy_gradient_loss | -1.51e-06     |
|    value_loss           | 2.48          |
-------------------------------------------
Progress update from timestep 246300: Updated shield, loss is 0.051992133259773254
Progress update from timestep 246400: Updated shield, loss is 0.053319867700338364
Eval num_timesteps=246400, episode_reward=15.82 +/- 10.24
Episode length: 24.00 +/- 15.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 15.8     |
| time/              |          |
|    total_timesteps | 246400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.48     |
|    ep_rew_mean     | 2.83     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 616      |
|    time_elapsed    | 53663    |
|    total_timesteps | 246400   |
---------------------------------
Progress update from timestep 246500: Updated shield, loss is 0.05969324707984924
Progress update from timestep 246600: Updated shield, loss is 0.05535568296909332
Eval num_timesteps=246600, episode_reward=5.68 +/- 2.51
Episode length: 9.00 +/- 3.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 9         |
|    mean_reward          | 5.68      |
| time/                   |           |
|    total_timesteps      | 246600    |
| train/                  |           |
|    approx_kl            | 9.309e-05 |
|    clip_fraction        | 0.00379   |
|    clip_range           | 0.1       |
|    entropy_loss         | -0.0298   |
|    explained_variance   | 0.175     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.788     |
|    n_updates            | 6160      |
|    policy_gradient_loss | -0.000914 |
|    value_loss           | 1.6       |
---------------------------------------
Progress update from timestep 246700: Updated shield, loss is 0.0521424226462841
Progress update from timestep 246800: Updated shield, loss is 0.05285613238811493
Eval num_timesteps=246800, episode_reward=23.17 +/- 15.21
Episode length: 33.00 +/- 20.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 23.2     |
| time/              |          |
|    total_timesteps | 246800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.83     |
|    ep_rew_mean     | 3.08     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 617      |
|    time_elapsed    | 53735    |
|    total_timesteps | 246800   |
---------------------------------
Progress update from timestep 246900: Updated shield, loss is 0.05122142285108566
Progress update from timestep 247000: Updated shield, loss is 0.04661920294165611
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 247000: Saved shield_loss.csv. 
Eval num_timesteps=247000, episode_reward=6.92 +/- 3.83
Episode length: 10.80 +/- 5.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 10.8         |
|    mean_reward          | 6.92         |
| time/                   |              |
|    total_timesteps      | 247000       |
| train/                  |              |
|    approx_kl            | 6.822499e-05 |
|    clip_fraction        | 0.00201      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0198      |
|    explained_variance   | 0.169        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.02         |
|    n_updates            | 6170         |
|    policy_gradient_loss | -0.00046     |
|    value_loss           | 1.88         |
------------------------------------------
Progress update from timestep 247100: Updated shield, loss is 0.04484789818525314
Progress update from timestep 247200: Updated shield, loss is 0.044009409844875336
Eval num_timesteps=247200, episode_reward=17.61 +/- 14.26
Episode length: 25.60 +/- 19.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 247200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.58     |
|    ep_rew_mean     | 2.91     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 618      |
|    time_elapsed    | 53815    |
|    total_timesteps | 247200   |
---------------------------------
Progress update from timestep 247300: Updated shield, loss is 0.049489155411720276
Progress update from timestep 247400: Updated shield, loss is 0.03699302300810814
Eval num_timesteps=247400, episode_reward=23.85 +/- 14.39
Episode length: 34.00 +/- 19.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34            |
|    mean_reward          | 23.8          |
| time/                   |               |
|    total_timesteps      | 247400        |
| train/                  |               |
|    approx_kl            | 0.00014642013 |
|    clip_fraction        | 0.00536       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0185       |
|    explained_variance   | 0.113         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.818         |
|    n_updates            | 6180          |
|    policy_gradient_loss | -0.000864     |
|    value_loss           | 1.72          |
-------------------------------------------
Progress update from timestep 247500: Updated shield, loss is 0.04358585178852081
Progress update from timestep 247600: Updated shield, loss is 0.03923577442765236
Eval num_timesteps=247600, episode_reward=5.62 +/- 3.32
Episode length: 9.00 +/- 4.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 5.62     |
| time/              |          |
|    total_timesteps | 247600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.21     |
|    ep_rew_mean     | 2.65     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 619      |
|    time_elapsed    | 53897    |
|    total_timesteps | 247600   |
---------------------------------
Progress update from timestep 247700: Updated shield, loss is 0.045523546636104584
Progress update from timestep 247800: Updated shield, loss is 0.052312396466732025
Eval num_timesteps=247800, episode_reward=16.51 +/- 15.19
Episode length: 24.20 +/- 21.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.2          |
|    mean_reward          | 16.5          |
| time/                   |               |
|    total_timesteps      | 247800        |
| train/                  |               |
|    approx_kl            | 0.00034405995 |
|    clip_fraction        | 0.00469       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00747      |
|    explained_variance   | 0.179         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.266         |
|    n_updates            | 6190          |
|    policy_gradient_loss | -0.000797     |
|    value_loss           | 1.61          |
-------------------------------------------
Progress update from timestep 247900: Updated shield, loss is 0.03846950829029083
Progress update from timestep 248000: Updated shield, loss is 0.05666119605302811
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 248000: Saved shield_loss.csv. 
Eval num_timesteps=248000, episode_reward=17.70 +/- 15.08
Episode length: 25.20 +/- 20.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 17.7     |
| time/              |          |
|    total_timesteps | 248000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.91     |
|    ep_rew_mean     | 2.43     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 620      |
|    time_elapsed    | 53995    |
|    total_timesteps | 248000   |
---------------------------------
Progress update from timestep 248100: Updated shield, loss is 0.04076923057436943
Progress update from timestep 248200: Updated shield, loss is 0.044893551617860794
Eval num_timesteps=248200, episode_reward=29.01 +/- 11.52
Episode length: 42.00 +/- 16.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42            |
|    mean_reward          | 29            |
| time/                   |               |
|    total_timesteps      | 248200        |
| train/                  |               |
|    approx_kl            | 0.00017624612 |
|    clip_fraction        | 0.00201       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0039       |
|    explained_variance   | 0.144         |
|    learning_rate        | 0.0001        |
|    loss                 | 1             |
|    n_updates            | 6200          |
|    policy_gradient_loss | -0.000198     |
|    value_loss           | 1.87          |
-------------------------------------------
Progress update from timestep 248300: Updated shield, loss is 0.03970417380332947
Progress update from timestep 248400: Updated shield, loss is 0.04398789629340172
Eval num_timesteps=248400, episode_reward=18.77 +/- 14.11
Episode length: 27.20 +/- 19.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.2     |
|    mean_reward     | 18.8     |
| time/              |          |
|    total_timesteps | 248400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.63     |
|    ep_rew_mean     | 2.99     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 621      |
|    time_elapsed    | 54109    |
|    total_timesteps | 248400   |
---------------------------------
Progress update from timestep 248500: Updated shield, loss is 0.04685908555984497
Progress update from timestep 248600: Updated shield, loss is 0.04012429714202881
Eval num_timesteps=248600, episode_reward=11.53 +/- 11.90
Episode length: 17.60 +/- 17.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17.6          |
|    mean_reward          | 11.5          |
| time/                   |               |
|    total_timesteps      | 248600        |
| train/                  |               |
|    approx_kl            | 0.00011948323 |
|    clip_fraction        | 0.00714       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.01         |
|    explained_variance   | 0.122         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.901         |
|    n_updates            | 6210          |
|    policy_gradient_loss | -0.000652     |
|    value_loss           | 1.81          |
-------------------------------------------
Progress update from timestep 248700: Updated shield, loss is 0.04908129945397377
Progress update from timestep 248800: Updated shield, loss is 0.041596394032239914
Eval num_timesteps=248800, episode_reward=11.26 +/- 12.30
Episode length: 16.60 +/- 16.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | 11.3     |
| time/              |          |
|    total_timesteps | 248800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.26     |
|    ep_rew_mean     | 2.72     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 622      |
|    time_elapsed    | 54213    |
|    total_timesteps | 248800   |
---------------------------------
Progress update from timestep 248900: Updated shield, loss is 0.044386353343725204
Progress update from timestep 249000: Updated shield, loss is 0.03920084983110428
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 249000: Saved shield_loss.csv. 
Eval num_timesteps=249000, episode_reward=22.42 +/- 14.79
Episode length: 32.80 +/- 21.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.8         |
|    mean_reward          | 22.4         |
| time/                   |              |
|    total_timesteps      | 249000       |
| train/                  |              |
|    approx_kl            | 0.0002060091 |
|    clip_fraction        | 0.00848      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0265      |
|    explained_variance   | 0.161        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.611        |
|    n_updates            | 6220         |
|    policy_gradient_loss | -0.000768    |
|    value_loss           | 2.49         |
------------------------------------------
Progress update from timestep 249100: Updated shield, loss is 0.03405843675136566
Progress update from timestep 249200: Updated shield, loss is 0.04562566429376602
Eval num_timesteps=249200, episode_reward=19.96 +/- 13.14
Episode length: 28.60 +/- 17.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.6     |
|    mean_reward     | 20       |
| time/              |          |
|    total_timesteps | 249200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.09     |
|    ep_rew_mean     | 2.56     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 623      |
|    time_elapsed    | 54324    |
|    total_timesteps | 249200   |
---------------------------------
Progress update from timestep 249300: Updated shield, loss is 0.05218655988574028
Progress update from timestep 249400: Updated shield, loss is 0.04428118094801903
Eval num_timesteps=249400, episode_reward=13.37 +/- 11.42
Episode length: 19.80 +/- 16.18
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 19.8           |
|    mean_reward          | 13.4           |
| time/                   |                |
|    total_timesteps      | 249400         |
| train/                  |                |
|    approx_kl            | 0.000119338256 |
|    clip_fraction        | 0.00558        |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.0171        |
|    explained_variance   | 0.0593         |
|    learning_rate        | 0.0001         |
|    loss                 | 0.526          |
|    n_updates            | 6230           |
|    policy_gradient_loss | -0.00127       |
|    value_loss           | 1.86           |
--------------------------------------------
Progress update from timestep 249500: Updated shield, loss is 0.04726404696702957
Progress update from timestep 249600: Updated shield, loss is 0.045930828899145126
Eval num_timesteps=249600, episode_reward=18.93 +/- 14.29
Episode length: 27.20 +/- 19.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.2     |
|    mean_reward     | 18.9     |
| time/              |          |
|    total_timesteps | 249600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.1      |
|    ep_rew_mean     | 2.56     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 624      |
|    time_elapsed    | 54414    |
|    total_timesteps | 249600   |
---------------------------------
Progress update from timestep 249700: Updated shield, loss is 0.03810379281640053
Progress update from timestep 249800: Updated shield, loss is 0.03835327550768852
Eval num_timesteps=249800, episode_reward=22.50 +/- 14.85
Episode length: 33.00 +/- 21.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33            |
|    mean_reward          | 22.5          |
| time/                   |               |
|    total_timesteps      | 249800        |
| train/                  |               |
|    approx_kl            | 0.00023125336 |
|    clip_fraction        | 0.00714       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0209       |
|    explained_variance   | 0.172         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.784         |
|    n_updates            | 6240          |
|    policy_gradient_loss | -0.000622     |
|    value_loss           | 1.72          |
-------------------------------------------
Progress update from timestep 249900: Updated shield, loss is 0.053260527551174164
Progress update from timestep 250000: Updated shield, loss is 0.04486642777919769
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 250000: Saved shield_loss.csv. 
Eval num_timesteps=250000, episode_reward=22.46 +/- 15.30
Episode length: 32.60 +/- 21.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.6     |
|    mean_reward     | 22.5     |
| time/              |          |
|    total_timesteps | 250000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.7      |
|    ep_rew_mean     | 3.07     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 625      |
|    time_elapsed    | 54547    |
|    total_timesteps | 250000   |
---------------------------------
Progress update from timestep 250100: Updated shield, loss is 0.04805995523929596
Progress update from timestep 250200: Updated shield, loss is 0.0430479422211647
Eval num_timesteps=250200, episode_reward=11.50 +/- 12.34
Episode length: 17.40 +/- 17.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.4         |
|    mean_reward          | 11.5         |
| time/                   |              |
|    total_timesteps      | 250200       |
| train/                  |              |
|    approx_kl            | 5.020411e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00107     |
|    explained_variance   | 0.142        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.74         |
|    n_updates            | 6250         |
|    policy_gradient_loss | -8.1e-07     |
|    value_loss           | 2.39         |
------------------------------------------
Progress update from timestep 250300: Updated shield, loss is 0.03883391618728638
Progress update from timestep 250400: Updated shield, loss is 0.05476396903395653
Eval num_timesteps=250400, episode_reward=16.75 +/- 13.64
Episode length: 25.40 +/- 20.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 250400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.88     |
|    ep_rew_mean     | 2.43     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 626      |
|    time_elapsed    | 54636    |
|    total_timesteps | 250400   |
---------------------------------
Progress update from timestep 250500: Updated shield, loss is 0.0485813207924366
Progress update from timestep 250600: Updated shield, loss is 0.04331827536225319
Eval num_timesteps=250600, episode_reward=17.01 +/- 14.77
Episode length: 25.00 +/- 20.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25           |
|    mean_reward          | 17           |
| time/                   |              |
|    total_timesteps      | 250600       |
| train/                  |              |
|    approx_kl            | 9.735591e-05 |
|    clip_fraction        | 0.00379      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0308      |
|    explained_variance   | 0.145        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.593        |
|    n_updates            | 6260         |
|    policy_gradient_loss | -0.000727    |
|    value_loss           | 1.64         |
------------------------------------------
Progress update from timestep 250700: Updated shield, loss is 0.04566231742501259
Progress update from timestep 250800: Updated shield, loss is 0.04242318496108055
Eval num_timesteps=250800, episode_reward=23.96 +/- 15.24
Episode length: 33.60 +/- 20.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 24       |
| time/              |          |
|    total_timesteps | 250800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.27     |
|    ep_rew_mean     | 2.65     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 627      |
|    time_elapsed    | 54754    |
|    total_timesteps | 250800   |
---------------------------------
Progress update from timestep 250900: Updated shield, loss is 0.041328173130750656
Progress update from timestep 251000: Updated shield, loss is 0.04828164353966713
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 251000: Saved shield_loss.csv. 
Eval num_timesteps=251000, episode_reward=10.47 +/- 11.55
Episode length: 16.20 +/- 17.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.2          |
|    mean_reward          | 10.5          |
| time/                   |               |
|    total_timesteps      | 251000        |
| train/                  |               |
|    approx_kl            | 0.00017732408 |
|    clip_fraction        | 0.00469       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0155       |
|    explained_variance   | 0.186         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.401         |
|    n_updates            | 6270          |
|    policy_gradient_loss | -0.000706     |
|    value_loss           | 1.31          |
-------------------------------------------
Progress update from timestep 251100: Updated shield, loss is 0.04563712328672409
Progress update from timestep 251200: Updated shield, loss is 0.05387542396783829
Eval num_timesteps=251200, episode_reward=16.48 +/- 14.30
Episode length: 24.60 +/- 20.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 251200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.42     |
|    ep_rew_mean     | 2.83     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 628      |
|    time_elapsed    | 54851    |
|    total_timesteps | 251200   |
---------------------------------
Progress update from timestep 251300: Updated shield, loss is 0.040120385587215424
Progress update from timestep 251400: Updated shield, loss is 0.04963807761669159
Eval num_timesteps=251400, episode_reward=15.40 +/- 15.62
Episode length: 22.80 +/- 22.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.8          |
|    mean_reward          | 15.4          |
| time/                   |               |
|    total_timesteps      | 251400        |
| train/                  |               |
|    approx_kl            | 0.00023207278 |
|    clip_fraction        | 0.00759       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0176       |
|    explained_variance   | 0.178         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.06          |
|    n_updates            | 6280          |
|    policy_gradient_loss | -0.000771     |
|    value_loss           | 1.67          |
-------------------------------------------
Progress update from timestep 251500: Updated shield, loss is 0.05226872116327286
Progress update from timestep 251600: Updated shield, loss is 0.04324369877576828
Eval num_timesteps=251600, episode_reward=9.90 +/- 12.92
Episode length: 14.80 +/- 17.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 9.9      |
| time/              |          |
|    total_timesteps | 251600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.24     |
|    ep_rew_mean     | 2.7      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 629      |
|    time_elapsed    | 54947    |
|    total_timesteps | 251600   |
---------------------------------
Progress update from timestep 251700: Updated shield, loss is 0.04374943673610687
Progress update from timestep 251800: Updated shield, loss is 0.046242184937000275
Eval num_timesteps=251800, episode_reward=6.59 +/- 4.52
Episode length: 10.40 +/- 6.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 10.4          |
|    mean_reward          | 6.59          |
| time/                   |               |
|    total_timesteps      | 251800        |
| train/                  |               |
|    approx_kl            | 9.6261254e-05 |
|    clip_fraction        | 0.00402       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0107       |
|    explained_variance   | 0.00328       |
|    learning_rate        | 0.0001        |
|    loss                 | 1.37          |
|    n_updates            | 6290          |
|    policy_gradient_loss | -0.000514     |
|    value_loss           | 2.39          |
-------------------------------------------
Progress update from timestep 251900: Updated shield, loss is 0.052777525037527084
Progress update from timestep 252000: Updated shield, loss is 0.042568646371364594
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 252000: Saved shield_loss.csv. 
Eval num_timesteps=252000, episode_reward=29.81 +/- 9.90
Episode length: 43.00 +/- 14.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43       |
|    mean_reward     | 29.8     |
| time/              |          |
|    total_timesteps | 252000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.49     |
|    ep_rew_mean     | 2.85     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 630      |
|    time_elapsed    | 55046    |
|    total_timesteps | 252000   |
---------------------------------
Progress update from timestep 252100: Updated shield, loss is 0.04472873732447624
Progress update from timestep 252200: Updated shield, loss is 0.052865128964185715
Eval num_timesteps=252200, episode_reward=22.17 +/- 15.55
Episode length: 32.20 +/- 21.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32.2          |
|    mean_reward          | 22.2          |
| time/                   |               |
|    total_timesteps      | 252200        |
| train/                  |               |
|    approx_kl            | 0.00034888298 |
|    clip_fraction        | 0.0058        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0127       |
|    explained_variance   | 0.117         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.13          |
|    n_updates            | 6300          |
|    policy_gradient_loss | -0.000786     |
|    value_loss           | 2.01          |
-------------------------------------------
Progress update from timestep 252300: Updated shield, loss is 0.0522627979516983
Progress update from timestep 252400: Updated shield, loss is 0.052115026861429214
Eval num_timesteps=252400, episode_reward=28.70 +/- 12.17
Episode length: 41.40 +/- 17.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | 28.7     |
| time/              |          |
|    total_timesteps | 252400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.91     |
|    ep_rew_mean     | 2.4      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 631      |
|    time_elapsed    | 55160    |
|    total_timesteps | 252400   |
---------------------------------
Progress update from timestep 252500: Updated shield, loss is 0.0503559410572052
Progress update from timestep 252600: Updated shield, loss is 0.04235323891043663
Eval num_timesteps=252600, episode_reward=23.00 +/- 14.56
Episode length: 33.20 +/- 20.61
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 33.2           |
|    mean_reward          | 23             |
| time/                   |                |
|    total_timesteps      | 252600         |
| train/                  |                |
|    approx_kl            | 0.000103829836 |
|    clip_fraction        | 0.00179        |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.00302       |
|    explained_variance   | 0.211          |
|    learning_rate        | 0.0001         |
|    loss                 | 0.531          |
|    n_updates            | 6310           |
|    policy_gradient_loss | -0.000252      |
|    value_loss           | 1.2            |
--------------------------------------------
Progress update from timestep 252700: Updated shield, loss is 0.05522031709551811
Progress update from timestep 252800: Updated shield, loss is 0.04323043301701546
Eval num_timesteps=252800, episode_reward=23.23 +/- 13.83
Episode length: 34.00 +/- 19.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 23.2     |
| time/              |          |
|    total_timesteps | 252800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.36     |
|    ep_rew_mean     | 2.78     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 632      |
|    time_elapsed    | 55254    |
|    total_timesteps | 252800   |
---------------------------------
Progress update from timestep 252900: Updated shield, loss is 0.04560716822743416
Progress update from timestep 253000: Updated shield, loss is 0.05902418494224548
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 253000: Saved shield_loss.csv. 
Eval num_timesteps=253000, episode_reward=14.49 +/- 14.30
Episode length: 21.20 +/- 19.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 21.2          |
|    mean_reward          | 14.5          |
| time/                   |               |
|    total_timesteps      | 253000        |
| train/                  |               |
|    approx_kl            | 7.5118483e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00796      |
|    explained_variance   | 0.16          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.91          |
|    n_updates            | 6320          |
|    policy_gradient_loss | -3.58e-05     |
|    value_loss           | 1.38          |
-------------------------------------------
Progress update from timestep 253100: Updated shield, loss is 0.037100303918123245
Progress update from timestep 253200: Updated shield, loss is 0.05156322568655014
Eval num_timesteps=253200, episode_reward=21.62 +/- 15.36
Episode length: 32.00 +/- 22.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32       |
|    mean_reward     | 21.6     |
| time/              |          |
|    total_timesteps | 253200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4        |
|    ep_rew_mean     | 2.44     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 633      |
|    time_elapsed    | 55346    |
|    total_timesteps | 253200   |
---------------------------------
Progress update from timestep 253300: Updated shield, loss is 0.047351911664009094
Progress update from timestep 253400: Updated shield, loss is 0.05448960140347481
Eval num_timesteps=253400, episode_reward=22.33 +/- 14.44
Episode length: 33.00 +/- 20.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33            |
|    mean_reward          | 22.3          |
| time/                   |               |
|    total_timesteps      | 253400        |
| train/                  |               |
|    approx_kl            | 0.00017298861 |
|    clip_fraction        | 0.00759       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0183       |
|    explained_variance   | 0.162         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.599         |
|    n_updates            | 6330          |
|    policy_gradient_loss | -0.000589     |
|    value_loss           | 1.56          |
-------------------------------------------
Progress update from timestep 253500: Updated shield, loss is 0.04214343801140785
Progress update from timestep 253600: Updated shield, loss is 0.05008512735366821
Eval num_timesteps=253600, episode_reward=16.86 +/- 15.76
Episode length: 24.00 +/- 21.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 253600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.15     |
|    ep_rew_mean     | 2.57     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 634      |
|    time_elapsed    | 55437    |
|    total_timesteps | 253600   |
---------------------------------
Progress update from timestep 253700: Updated shield, loss is 0.03842249512672424
Progress update from timestep 253800: Updated shield, loss is 0.04836037755012512
Eval num_timesteps=253800, episode_reward=3.78 +/- 2.49
Episode length: 6.20 +/- 3.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 6.2           |
|    mean_reward          | 3.78          |
| time/                   |               |
|    total_timesteps      | 253800        |
| train/                  |               |
|    approx_kl            | 0.00012052294 |
|    clip_fraction        | 0.00536       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0177       |
|    explained_variance   | 0.106         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.25          |
|    n_updates            | 6340          |
|    policy_gradient_loss | -0.000608     |
|    value_loss           | 1.69          |
-------------------------------------------
Progress update from timestep 253900: Updated shield, loss is 0.04568631947040558
Progress update from timestep 254000: Updated shield, loss is 0.03960898891091347
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 254000: Saved shield_loss.csv. 
Eval num_timesteps=254000, episode_reward=29.27 +/- 14.30
Episode length: 40.40 +/- 19.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.4     |
|    mean_reward     | 29.3     |
| time/              |          |
|    total_timesteps | 254000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.72     |
|    ep_rew_mean     | 3.04     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 635      |
|    time_elapsed    | 55511    |
|    total_timesteps | 254000   |
---------------------------------
Progress update from timestep 254100: Updated shield, loss is 0.04220319911837578
Progress update from timestep 254200: Updated shield, loss is 0.05038022994995117
Eval num_timesteps=254200, episode_reward=7.52 +/- 4.18
Episode length: 11.80 +/- 6.34
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 11.8           |
|    mean_reward          | 7.52           |
| time/                   |                |
|    total_timesteps      | 254200         |
| train/                  |                |
|    approx_kl            | 0.000115793264 |
|    clip_fraction        | 0.00536        |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.0152        |
|    explained_variance   | 0.0779         |
|    learning_rate        | 0.0001         |
|    loss                 | 1.26           |
|    n_updates            | 6350           |
|    policy_gradient_loss | -0.000903      |
|    value_loss           | 2.19           |
--------------------------------------------
Progress update from timestep 254300: Updated shield, loss is 0.040096916258335114
Progress update from timestep 254400: Updated shield, loss is 0.048269666731357574
Eval num_timesteps=254400, episode_reward=29.51 +/- 11.07
Episode length: 42.20 +/- 15.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 29.5     |
| time/              |          |
|    total_timesteps | 254400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.51     |
|    ep_rew_mean     | 2.86     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 636      |
|    time_elapsed    | 55610    |
|    total_timesteps | 254400   |
---------------------------------
Progress update from timestep 254500: Updated shield, loss is 0.050094667822122574
Progress update from timestep 254600: Updated shield, loss is 0.04418171942234039
Eval num_timesteps=254600, episode_reward=4.91 +/- 2.57
Episode length: 8.00 +/- 3.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 8            |
|    mean_reward          | 4.91         |
| time/                   |              |
|    total_timesteps      | 254600       |
| train/                  |              |
|    approx_kl            | 6.184564e-11 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00195     |
|    explained_variance   | 0.173        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.29         |
|    n_updates            | 6360         |
|    policy_gradient_loss | -1.28e-06    |
|    value_loss           | 2.21         |
------------------------------------------
Progress update from timestep 254700: Updated shield, loss is 0.03416094556450844
Progress update from timestep 254800: Updated shield, loss is 0.042976029217243195
Eval num_timesteps=254800, episode_reward=6.12 +/- 2.39
Episode length: 9.60 +/- 3.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.6      |
|    mean_reward     | 6.12     |
| time/              |          |
|    total_timesteps | 254800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.21     |
|    ep_rew_mean     | 3.45     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 637      |
|    time_elapsed    | 55672    |
|    total_timesteps | 254800   |
---------------------------------
Progress update from timestep 254900: Updated shield, loss is 0.041787076741456985
Progress update from timestep 255000: Updated shield, loss is 0.04708261042833328
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 255000: Saved shield_loss.csv. 
Eval num_timesteps=255000, episode_reward=17.28 +/- 15.38
Episode length: 24.80 +/- 21.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.8          |
|    mean_reward          | 17.3          |
| time/                   |               |
|    total_timesteps      | 255000        |
| train/                  |               |
|    approx_kl            | 3.6645088e-05 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0116       |
|    explained_variance   | 0.101         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.05          |
|    n_updates            | 6370          |
|    policy_gradient_loss | -0.000306     |
|    value_loss           | 2.22          |
-------------------------------------------
Progress update from timestep 255100: Updated shield, loss is 0.03274780139327049
Progress update from timestep 255200: Updated shield, loss is 0.04487726837396622
Eval num_timesteps=255200, episode_reward=23.00 +/- 14.55
Episode length: 33.40 +/- 20.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 23       |
| time/              |          |
|    total_timesteps | 255200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.18     |
|    ep_rew_mean     | 2.65     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 638      |
|    time_elapsed    | 55786    |
|    total_timesteps | 255200   |
---------------------------------
Progress update from timestep 255300: Updated shield, loss is 0.05941491946578026
Progress update from timestep 255400: Updated shield, loss is 0.04577038437128067
Eval num_timesteps=255400, episode_reward=15.08 +/- 12.58
Episode length: 22.00 +/- 17.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22           |
|    mean_reward          | 15.1         |
| time/                   |              |
|    total_timesteps      | 255400       |
| train/                  |              |
|    approx_kl            | 0.0001255425 |
|    clip_fraction        | 0.00714      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0236      |
|    explained_variance   | 0.197        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.477        |
|    n_updates            | 6380         |
|    policy_gradient_loss | -0.00102     |
|    value_loss           | 1.48         |
------------------------------------------
Progress update from timestep 255500: Updated shield, loss is 0.04551951587200165
Progress update from timestep 255600: Updated shield, loss is 0.047413378953933716
Eval num_timesteps=255600, episode_reward=10.32 +/- 13.33
Episode length: 15.00 +/- 17.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | 10.3     |
| time/              |          |
|    total_timesteps | 255600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.01     |
|    ep_rew_mean     | 2.49     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 639      |
|    time_elapsed    | 55877    |
|    total_timesteps | 255600   |
---------------------------------
Progress update from timestep 255700: Updated shield, loss is 0.05108380317687988
Progress update from timestep 255800: Updated shield, loss is 0.054420556873083115
Eval num_timesteps=255800, episode_reward=28.09 +/- 13.35
Episode length: 40.60 +/- 18.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 40.6          |
|    mean_reward          | 28.1          |
| time/                   |               |
|    total_timesteps      | 255800        |
| train/                  |               |
|    approx_kl            | 0.00020212108 |
|    clip_fraction        | 0.00759       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0265       |
|    explained_variance   | 0.0968        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.908         |
|    n_updates            | 6390          |
|    policy_gradient_loss | -0.00144      |
|    value_loss           | 1.87          |
-------------------------------------------
Progress update from timestep 255900: Updated shield, loss is 0.046939484775066376
Progress update from timestep 256000: Updated shield, loss is 0.039595529437065125
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 256000: Saved shield_loss.csv. 
Eval num_timesteps=256000, episode_reward=22.80 +/- 16.17
Episode length: 32.20 +/- 21.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 22.8     |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.25     |
|    ep_rew_mean     | 2.56     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 640      |
|    time_elapsed    | 55989    |
|    total_timesteps | 256000   |
---------------------------------
Progress update from timestep 256100: Updated shield, loss is 0.04247342795133591
Progress update from timestep 256200: Updated shield, loss is 0.05242437496781349
Eval num_timesteps=256200, episode_reward=7.90 +/- 6.41
Episode length: 12.40 +/- 9.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 12.4          |
|    mean_reward          | 7.9           |
| time/                   |               |
|    total_timesteps      | 256200        |
| train/                  |               |
|    approx_kl            | 0.00071658794 |
|    clip_fraction        | 0.0214        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0341       |
|    explained_variance   | 0.373         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.521         |
|    n_updates            | 6400          |
|    policy_gradient_loss | -0.00204      |
|    value_loss           | 1.51          |
-------------------------------------------
Progress update from timestep 256300: Updated shield, loss is 0.0442790612578392
Progress update from timestep 256400: Updated shield, loss is 0.04674837738275528
Eval num_timesteps=256400, episode_reward=10.69 +/- 12.16
Episode length: 16.00 +/- 17.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | 10.7     |
| time/              |          |
|    total_timesteps | 256400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.13     |
|    ep_rew_mean     | 2.57     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 641      |
|    time_elapsed    | 56071    |
|    total_timesteps | 256400   |
---------------------------------
Progress update from timestep 256500: Updated shield, loss is 0.046459607779979706
Progress update from timestep 256600: Updated shield, loss is 0.05012430623173714
Eval num_timesteps=256600, episode_reward=17.24 +/- 15.08
Episode length: 24.80 +/- 20.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.8          |
|    mean_reward          | 17.2          |
| time/                   |               |
|    total_timesteps      | 256600        |
| train/                  |               |
|    approx_kl            | 2.1671713e-05 |
|    clip_fraction        | 0.000893      |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00697      |
|    explained_variance   | -0.0383       |
|    learning_rate        | 0.0001        |
|    loss                 | 0.41          |
|    n_updates            | 6410          |
|    policy_gradient_loss | -0.000128     |
|    value_loss           | 1.83          |
-------------------------------------------
Progress update from timestep 256700: Updated shield, loss is 0.05336863920092583
Progress update from timestep 256800: Updated shield, loss is 0.05328208580613136
Eval num_timesteps=256800, episode_reward=5.23 +/- 2.54
Episode length: 8.20 +/- 3.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8.2      |
|    mean_reward     | 5.23     |
| time/              |          |
|    total_timesteps | 256800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.39     |
|    ep_rew_mean     | 2.77     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 642      |
|    time_elapsed    | 56147    |
|    total_timesteps | 256800   |
---------------------------------
Progress update from timestep 256900: Updated shield, loss is 0.04013253375887871
Progress update from timestep 257000: Updated shield, loss is 0.03981340676546097
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 257000: Saved shield_loss.csv. 
Eval num_timesteps=257000, episode_reward=17.33 +/- 14.27
Episode length: 25.60 +/- 20.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.6          |
|    mean_reward          | 17.3          |
| time/                   |               |
|    total_timesteps      | 257000        |
| train/                  |               |
|    approx_kl            | 2.9883399e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.000731     |
|    explained_variance   | 0.196         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.712         |
|    n_updates            | 6420          |
|    policy_gradient_loss | -3.08e-07     |
|    value_loss           | 1.42          |
-------------------------------------------
Progress update from timestep 257100: Updated shield, loss is 0.045830726623535156
Progress update from timestep 257200: Updated shield, loss is 0.04379940778017044
Eval num_timesteps=257200, episode_reward=5.24 +/- 4.38
Episode length: 8.20 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8.2      |
|    mean_reward     | 5.24     |
| time/              |          |
|    total_timesteps | 257200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.11     |
|    ep_rew_mean     | 2.55     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 643      |
|    time_elapsed    | 56222    |
|    total_timesteps | 257200   |
---------------------------------
Progress update from timestep 257300: Updated shield, loss is 0.04110565781593323
Progress update from timestep 257400: Updated shield, loss is 0.04169090837240219
Eval num_timesteps=257400, episode_reward=4.32 +/- 3.08
Episode length: 7.00 +/- 4.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 7             |
|    mean_reward          | 4.32          |
| time/                   |               |
|    total_timesteps      | 257400        |
| train/                  |               |
|    approx_kl            | 1.4466163e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0022       |
|    explained_variance   | 0.0534        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.752         |
|    n_updates            | 6430          |
|    policy_gradient_loss | -1.71e-06     |
|    value_loss           | 1.76          |
-------------------------------------------
Progress update from timestep 257500: Updated shield, loss is 0.05163005366921425
Progress update from timestep 257600: Updated shield, loss is 0.03767601400613785
Eval num_timesteps=257600, episode_reward=12.56 +/- 10.86
Episode length: 19.40 +/- 15.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 12.6     |
| time/              |          |
|    total_timesteps | 257600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.93     |
|    ep_rew_mean     | 2.38     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 644      |
|    time_elapsed    | 56296    |
|    total_timesteps | 257600   |
---------------------------------
Progress update from timestep 257700: Updated shield, loss is 0.04374769702553749
Progress update from timestep 257800: Updated shield, loss is 0.04044274985790253
Eval num_timesteps=257800, episode_reward=13.10 +/- 10.94
Episode length: 19.80 +/- 16.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 19.8         |
|    mean_reward          | 13.1         |
| time/                   |              |
|    total_timesteps      | 257800       |
| train/                  |              |
|    approx_kl            | 5.717619e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00649     |
|    explained_variance   | 0.0732       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.08         |
|    n_updates            | 6440         |
|    policy_gradient_loss | -4.42e-06    |
|    value_loss           | 1.72         |
------------------------------------------
Progress update from timestep 257900: Updated shield, loss is 0.044447656720876694
Progress update from timestep 258000: Updated shield, loss is 0.05182195082306862
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 258000: Saved shield_loss.csv. 
Eval num_timesteps=258000, episode_reward=9.07 +/- 12.22
Episode length: 14.20 +/- 18.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.2     |
|    mean_reward     | 9.07     |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.49     |
|    ep_rew_mean     | 2.9      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 645      |
|    time_elapsed    | 56390    |
|    total_timesteps | 258000   |
---------------------------------
Progress update from timestep 258100: Updated shield, loss is 0.04076355695724487
Progress update from timestep 258200: Updated shield, loss is 0.03903578594326973
Eval num_timesteps=258200, episode_reward=17.21 +/- 13.70
Episode length: 26.00 +/- 19.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26            |
|    mean_reward          | 17.2          |
| time/                   |               |
|    total_timesteps      | 258200        |
| train/                  |               |
|    approx_kl            | 8.7733046e-05 |
|    clip_fraction        | 0.00469       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0163       |
|    explained_variance   | 0.151         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.685         |
|    n_updates            | 6450          |
|    policy_gradient_loss | -0.000576     |
|    value_loss           | 1.97          |
-------------------------------------------
Progress update from timestep 258300: Updated shield, loss is 0.038046594709157944
Progress update from timestep 258400: Updated shield, loss is 0.04600008204579353
Eval num_timesteps=258400, episode_reward=18.30 +/- 14.87
Episode length: 26.60 +/- 20.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.6     |
|    mean_reward     | 18.3     |
| time/              |          |
|    total_timesteps | 258400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.08     |
|    ep_rew_mean     | 2.52     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 646      |
|    time_elapsed    | 56495    |
|    total_timesteps | 258400   |
---------------------------------
Progress update from timestep 258500: Updated shield, loss is 0.039370279759168625
Progress update from timestep 258600: Updated shield, loss is 0.04961840808391571
Eval num_timesteps=258600, episode_reward=16.92 +/- 15.74
Episode length: 24.20 +/- 21.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.2          |
|    mean_reward          | 16.9          |
| time/                   |               |
|    total_timesteps      | 258600        |
| train/                  |               |
|    approx_kl            | 0.00020731361 |
|    clip_fraction        | 0.00737       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0203       |
|    explained_variance   | 0.106         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.612         |
|    n_updates            | 6460          |
|    policy_gradient_loss | -0.00122      |
|    value_loss           | 1.37          |
-------------------------------------------
Progress update from timestep 258700: Updated shield, loss is 0.03595750778913498
Progress update from timestep 258800: Updated shield, loss is 0.04740513116121292
Eval num_timesteps=258800, episode_reward=7.90 +/- 6.22
Episode length: 12.20 +/- 8.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 12.2     |
|    mean_reward     | 7.9      |
| time/              |          |
|    total_timesteps | 258800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.03     |
|    ep_rew_mean     | 2.5      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 647      |
|    time_elapsed    | 56585    |
|    total_timesteps | 258800   |
---------------------------------
Progress update from timestep 258900: Updated shield, loss is 0.05044766888022423
Progress update from timestep 259000: Updated shield, loss is 0.054148826748132706
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 259000: Saved shield_loss.csv. 
Eval num_timesteps=259000, episode_reward=11.90 +/- 11.50
Episode length: 18.20 +/- 16.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 18.2          |
|    mean_reward          | 11.9          |
| time/                   |               |
|    total_timesteps      | 259000        |
| train/                  |               |
|    approx_kl            | 0.00020797653 |
|    clip_fraction        | 0.0125        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0241       |
|    explained_variance   | 0.157         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.786         |
|    n_updates            | 6470          |
|    policy_gradient_loss | -0.00112      |
|    value_loss           | 1.32          |
-------------------------------------------
Progress update from timestep 259100: Updated shield, loss is 0.048739414662122726
Progress update from timestep 259200: Updated shield, loss is 0.049792081117630005
Eval num_timesteps=259200, episode_reward=8.26 +/- 12.58
Episode length: 13.00 +/- 18.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13       |
|    mean_reward     | 8.26     |
| time/              |          |
|    total_timesteps | 259200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.29     |
|    ep_rew_mean     | 2.72     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 648      |
|    time_elapsed    | 56676    |
|    total_timesteps | 259200   |
---------------------------------
Progress update from timestep 259300: Updated shield, loss is 0.04209223389625549
Progress update from timestep 259400: Updated shield, loss is 0.03519149497151375
Eval num_timesteps=259400, episode_reward=13.17 +/- 12.30
Episode length: 18.80 +/- 16.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 18.8          |
|    mean_reward          | 13.2          |
| time/                   |               |
|    total_timesteps      | 259400        |
| train/                  |               |
|    approx_kl            | 4.3567394e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00239      |
|    explained_variance   | 0.265         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.802         |
|    n_updates            | 6480          |
|    policy_gradient_loss | -5.17e-06     |
|    value_loss           | 1.36          |
-------------------------------------------
Progress update from timestep 259500: Updated shield, loss is 0.04801681265234947
Progress update from timestep 259600: Updated shield, loss is 0.043321095407009125
Eval num_timesteps=259600, episode_reward=11.50 +/- 13.30
Episode length: 17.40 +/- 18.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 11.5     |
| time/              |          |
|    total_timesteps | 259600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.18     |
|    ep_rew_mean     | 2.62     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 649      |
|    time_elapsed    | 56770    |
|    total_timesteps | 259600   |
---------------------------------
Progress update from timestep 259700: Updated shield, loss is 0.04675431549549103
Progress update from timestep 259800: Updated shield, loss is 0.051352497190237045
Eval num_timesteps=259800, episode_reward=10.51 +/- 12.63
Episode length: 15.60 +/- 17.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.6          |
|    mean_reward          | 10.5          |
| time/                   |               |
|    total_timesteps      | 259800        |
| train/                  |               |
|    approx_kl            | 0.00014828886 |
|    clip_fraction        | 0.0029        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00815      |
|    explained_variance   | 0.0264        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.772         |
|    n_updates            | 6490          |
|    policy_gradient_loss | -0.000338     |
|    value_loss           | 1.57          |
-------------------------------------------
Progress update from timestep 259900: Updated shield, loss is 0.05645104870200157
Progress update from timestep 260000: Updated shield, loss is 0.039146486669778824
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 260000: Saved shield_loss.csv. 
Eval num_timesteps=260000, episode_reward=9.69 +/- 12.41
Episode length: 14.80 +/- 17.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 9.69     |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.93     |
|    ep_rew_mean     | 3.15     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 650      |
|    time_elapsed    | 56860    |
|    total_timesteps | 260000   |
---------------------------------
Progress update from timestep 260100: Updated shield, loss is 0.04244716465473175
Progress update from timestep 260200: Updated shield, loss is 0.04521836340427399
Eval num_timesteps=260200, episode_reward=10.30 +/- 12.64
Episode length: 15.20 +/- 17.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.2          |
|    mean_reward          | 10.3          |
| time/                   |               |
|    total_timesteps      | 260200        |
| train/                  |               |
|    approx_kl            | 0.00036790638 |
|    clip_fraction        | 0.00826       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.023        |
|    explained_variance   | 0.124         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.998         |
|    n_updates            | 6500          |
|    policy_gradient_loss | -0.00151      |
|    value_loss           | 2.33          |
-------------------------------------------
Progress update from timestep 260300: Updated shield, loss is 0.051283061504364014
Progress update from timestep 260400: Updated shield, loss is 0.046040162444114685
Eval num_timesteps=260400, episode_reward=22.45 +/- 16.10
Episode length: 32.00 +/- 22.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32       |
|    mean_reward     | 22.5     |
| time/              |          |
|    total_timesteps | 260400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.13     |
|    ep_rew_mean     | 2.59     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 651      |
|    time_elapsed    | 56974    |
|    total_timesteps | 260400   |
---------------------------------
Progress update from timestep 260500: Updated shield, loss is 0.0418497771024704
Progress update from timestep 260600: Updated shield, loss is 0.03646567091345787
Eval num_timesteps=260600, episode_reward=12.52 +/- 11.37
Episode length: 18.60 +/- 16.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.6         |
|    mean_reward          | 12.5         |
| time/                   |              |
|    total_timesteps      | 260600       |
| train/                  |              |
|    approx_kl            | 0.0001499995 |
|    clip_fraction        | 0.00424      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.012       |
|    explained_variance   | 0.204        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.752        |
|    n_updates            | 6510         |
|    policy_gradient_loss | -0.000483    |
|    value_loss           | 1.67         |
------------------------------------------
Progress update from timestep 260700: Updated shield, loss is 0.04546617344021797
Progress update from timestep 260800: Updated shield, loss is 0.04586473107337952
Eval num_timesteps=260800, episode_reward=13.68 +/- 11.76
Episode length: 19.60 +/- 15.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 13.7     |
| time/              |          |
|    total_timesteps | 260800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.16     |
|    ep_rew_mean     | 2.66     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 652      |
|    time_elapsed    | 57070    |
|    total_timesteps | 260800   |
---------------------------------
Progress update from timestep 260900: Updated shield, loss is 0.04653087258338928
Progress update from timestep 261000: Updated shield, loss is 0.04658806324005127
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 261000: Saved shield_loss.csv. 
Eval num_timesteps=261000, episode_reward=16.26 +/- 14.78
Episode length: 24.20 +/- 21.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.2          |
|    mean_reward          | 16.3          |
| time/                   |               |
|    total_timesteps      | 261000        |
| train/                  |               |
|    approx_kl            | 5.2828134e-05 |
|    clip_fraction        | 0.00402       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00861      |
|    explained_variance   | 0.159         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.956         |
|    n_updates            | 6520          |
|    policy_gradient_loss | -0.000711     |
|    value_loss           | 2.04          |
-------------------------------------------
Progress update from timestep 261100: Updated shield, loss is 0.051156844943761826
Progress update from timestep 261200: Updated shield, loss is 0.05253196135163307
Eval num_timesteps=261200, episode_reward=11.71 +/- 12.83
Episode length: 17.00 +/- 16.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | 11.7     |
| time/              |          |
|    total_timesteps | 261200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.64     |
|    ep_rew_mean     | 3        |
| time/              |          |
|    fps             | 4        |
|    iterations      | 653      |
|    time_elapsed    | 57184    |
|    total_timesteps | 261200   |
---------------------------------
Progress update from timestep 261300: Updated shield, loss is 0.03835982084274292
Progress update from timestep 261400: Updated shield, loss is 0.04587335139513016
Eval num_timesteps=261400, episode_reward=15.97 +/- 15.13
Episode length: 23.60 +/- 21.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.6          |
|    mean_reward          | 16            |
| time/                   |               |
|    total_timesteps      | 261400        |
| train/                  |               |
|    approx_kl            | 0.00028065094 |
|    clip_fraction        | 0.0158        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.036        |
|    explained_variance   | 0.29          |
|    learning_rate        | 0.0001        |
|    loss                 | 1.17          |
|    n_updates            | 6530          |
|    policy_gradient_loss | -0.0014       |
|    value_loss           | 1.69          |
-------------------------------------------
Progress update from timestep 261500: Updated shield, loss is 0.041998375207185745
Progress update from timestep 261600: Updated shield, loss is 0.03757070004940033
Eval num_timesteps=261600, episode_reward=11.35 +/- 11.86
Episode length: 17.20 +/- 16.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | 11.3     |
| time/              |          |
|    total_timesteps | 261600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.11     |
|    ep_rew_mean     | 2.58     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 654      |
|    time_elapsed    | 57277    |
|    total_timesteps | 261600   |
---------------------------------
Progress update from timestep 261700: Updated shield, loss is 0.03718728944659233
Progress update from timestep 261800: Updated shield, loss is 0.04294880852103233
Eval num_timesteps=261800, episode_reward=23.46 +/- 14.95
Episode length: 33.40 +/- 20.39
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 33.4           |
|    mean_reward          | 23.5           |
| time/                   |                |
|    total_timesteps      | 261800         |
| train/                  |                |
|    approx_kl            | 0.000116282434 |
|    clip_fraction        | 0.00402        |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.0267        |
|    explained_variance   | 0.073          |
|    learning_rate        | 0.0001         |
|    loss                 | 0.645          |
|    n_updates            | 6540           |
|    policy_gradient_loss | -0.000553      |
|    value_loss           | 1.68           |
--------------------------------------------
Progress update from timestep 261900: Updated shield, loss is 0.040593221783638
Progress update from timestep 262000: Updated shield, loss is 0.04183291643857956
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 262000: Saved shield_loss.csv. 
Eval num_timesteps=262000, episode_reward=22.39 +/- 16.22
Episode length: 31.80 +/- 22.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.8     |
|    mean_reward     | 22.4     |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.79     |
|    ep_rew_mean     | 3.12     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 655      |
|    time_elapsed    | 57390    |
|    total_timesteps | 262000   |
---------------------------------
Progress update from timestep 262100: Updated shield, loss is 0.04482819885015488
Progress update from timestep 262200: Updated shield, loss is 0.03647065535187721
Eval num_timesteps=262200, episode_reward=15.53 +/- 15.12
Episode length: 23.40 +/- 21.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.4          |
|    mean_reward          | 15.5          |
| time/                   |               |
|    total_timesteps      | 262200        |
| train/                  |               |
|    approx_kl            | 5.2819083e-05 |
|    clip_fraction        | 0.00268       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00681      |
|    explained_variance   | 0.162         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.04          |
|    n_updates            | 6550          |
|    policy_gradient_loss | -0.000254     |
|    value_loss           | 2.68          |
-------------------------------------------
Progress update from timestep 262300: Updated shield, loss is 0.04779376834630966
Progress update from timestep 262400: Updated shield, loss is 0.04405767098069191
Eval num_timesteps=262400, episode_reward=24.17 +/- 13.39
Episode length: 35.20 +/- 18.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 24.2     |
| time/              |          |
|    total_timesteps | 262400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.34     |
|    ep_rew_mean     | 2.73     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 656      |
|    time_elapsed    | 57491    |
|    total_timesteps | 262400   |
---------------------------------
Progress update from timestep 262500: Updated shield, loss is 0.050355926156044006
Progress update from timestep 262600: Updated shield, loss is 0.051586270332336426
Eval num_timesteps=262600, episode_reward=17.88 +/- 14.45
Episode length: 26.20 +/- 20.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.2          |
|    mean_reward          | 17.9          |
| time/                   |               |
|    total_timesteps      | 262600        |
| train/                  |               |
|    approx_kl            | 9.9970195e-05 |
|    clip_fraction        | 0.00246       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0193       |
|    explained_variance   | 0.255         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.77          |
|    n_updates            | 6560          |
|    policy_gradient_loss | -0.000384     |
|    value_loss           | 2.06          |
-------------------------------------------
Progress update from timestep 262700: Updated shield, loss is 0.04076269641518593
Progress update from timestep 262800: Updated shield, loss is 0.052008409053087234
Eval num_timesteps=262800, episode_reward=11.08 +/- 12.94
Episode length: 16.40 +/- 17.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | 11.1     |
| time/              |          |
|    total_timesteps | 262800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.03     |
|    ep_rew_mean     | 2.44     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 657      |
|    time_elapsed    | 57583    |
|    total_timesteps | 262800   |
---------------------------------
Progress update from timestep 262900: Updated shield, loss is 0.036863576620817184
Progress update from timestep 263000: Updated shield, loss is 0.04269726574420929
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 263000: Saved shield_loss.csv. 
Eval num_timesteps=263000, episode_reward=24.76 +/- 13.74
Episode length: 35.00 +/- 18.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35          |
|    mean_reward          | 24.8        |
| time/                   |             |
|    total_timesteps      | 263000      |
| train/                  |             |
|    approx_kl            | 3.53709e-05 |
|    clip_fraction        | 0.000223    |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0179     |
|    explained_variance   | 0.267       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.93        |
|    n_updates            | 6570        |
|    policy_gradient_loss | -0.000227   |
|    value_loss           | 2.48        |
-----------------------------------------
Progress update from timestep 263100: Updated shield, loss is 0.051239676773548126
Progress update from timestep 263200: Updated shield, loss is 0.04008660838007927
Eval num_timesteps=263200, episode_reward=17.86 +/- 14.46
Episode length: 26.00 +/- 20.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 263200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.21     |
|    ep_rew_mean     | 3.42     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 658      |
|    time_elapsed    | 57715    |
|    total_timesteps | 263200   |
---------------------------------
Progress update from timestep 263300: Updated shield, loss is 0.04366403445601463
Progress update from timestep 263400: Updated shield, loss is 0.05071480572223663
Eval num_timesteps=263400, episode_reward=11.34 +/- 11.52
Episode length: 17.60 +/- 16.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17.6          |
|    mean_reward          | 11.3          |
| time/                   |               |
|    total_timesteps      | 263400        |
| train/                  |               |
|    approx_kl            | 3.2177984e-05 |
|    clip_fraction        | 0.00335       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0143       |
|    explained_variance   | 0.25          |
|    learning_rate        | 0.0001        |
|    loss                 | 1             |
|    n_updates            | 6580          |
|    policy_gradient_loss | -0.000292     |
|    value_loss           | 2.21          |
-------------------------------------------
Progress update from timestep 263500: Updated shield, loss is 0.05082029849290848
Progress update from timestep 263600: Updated shield, loss is 0.058358293026685715
Eval num_timesteps=263600, episode_reward=22.72 +/- 15.05
Episode length: 33.00 +/- 21.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 22.7     |
| time/              |          |
|    total_timesteps | 263600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.61     |
|    ep_rew_mean     | 2.96     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 659      |
|    time_elapsed    | 57827    |
|    total_timesteps | 263600   |
---------------------------------
Progress update from timestep 263700: Updated shield, loss is 0.05049560219049454
Progress update from timestep 263800: Updated shield, loss is 0.06883230060338974
Eval num_timesteps=263800, episode_reward=16.61 +/- 15.60
Episode length: 24.00 +/- 21.38
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24            |
|    mean_reward          | 16.6          |
| time/                   |               |
|    total_timesteps      | 263800        |
| train/                  |               |
|    approx_kl            | 0.00021147904 |
|    clip_fraction        | 0.00714       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.014        |
|    explained_variance   | 0.16          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.816         |
|    n_updates            | 6590          |
|    policy_gradient_loss | -0.00136      |
|    value_loss           | 1.45          |
-------------------------------------------
Progress update from timestep 263900: Updated shield, loss is 0.056795958429574966
Progress update from timestep 264000: Updated shield, loss is 0.06709437072277069
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 264000: Saved shield_loss.csv. 
Eval num_timesteps=264000, episode_reward=5.33 +/- 2.30
Episode length: 8.60 +/- 3.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8.6      |
|    mean_reward     | 5.33     |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.32     |
|    ep_rew_mean     | 3.45     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 660      |
|    time_elapsed    | 57902    |
|    total_timesteps | 264000   |
---------------------------------
Progress update from timestep 264100: Updated shield, loss is 0.03607890009880066
Progress update from timestep 264200: Updated shield, loss is 0.049321386963129044
Eval num_timesteps=264200, episode_reward=17.78 +/- 15.00
Episode length: 25.40 +/- 20.10
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.4          |
|    mean_reward          | 17.8          |
| time/                   |               |
|    total_timesteps      | 264200        |
| train/                  |               |
|    approx_kl            | 0.00024834066 |
|    clip_fraction        | 0.00647       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00821      |
|    explained_variance   | 0.289         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.09          |
|    n_updates            | 6600          |
|    policy_gradient_loss | -0.000659     |
|    value_loss           | 3.21          |
-------------------------------------------
Progress update from timestep 264300: Updated shield, loss is 0.05447421595454216
Progress update from timestep 264400: Updated shield, loss is 0.058345429599285126
Eval num_timesteps=264400, episode_reward=14.85 +/- 12.09
Episode length: 22.40 +/- 17.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 14.8     |
| time/              |          |
|    total_timesteps | 264400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.65     |
|    ep_rew_mean     | 2.99     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 661      |
|    time_elapsed    | 57998    |
|    total_timesteps | 264400   |
---------------------------------
Progress update from timestep 264500: Updated shield, loss is 0.049388397485017776
Progress update from timestep 264600: Updated shield, loss is 0.06475821137428284
Eval num_timesteps=264600, episode_reward=22.92 +/- 15.54
Episode length: 32.60 +/- 21.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32.6          |
|    mean_reward          | 22.9          |
| time/                   |               |
|    total_timesteps      | 264600        |
| train/                  |               |
|    approx_kl            | 4.5830988e-05 |
|    clip_fraction        | 0.00201       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0101       |
|    explained_variance   | 0.132         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.778         |
|    n_updates            | 6610          |
|    policy_gradient_loss | -0.000631     |
|    value_loss           | 1.78          |
-------------------------------------------
Progress update from timestep 264700: Updated shield, loss is 0.05222417041659355
Progress update from timestep 264800: Updated shield, loss is 0.06186797842383385
Eval num_timesteps=264800, episode_reward=15.43 +/- 16.03
Episode length: 22.60 +/- 22.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 15.4     |
| time/              |          |
|    total_timesteps | 264800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.45     |
|    ep_rew_mean     | 2.84     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 662      |
|    time_elapsed    | 58113    |
|    total_timesteps | 264800   |
---------------------------------
Progress update from timestep 264900: Updated shield, loss is 0.05352339148521423
Progress update from timestep 265000: Updated shield, loss is 0.05032757669687271
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 265000: Saved shield_loss.csv. 
Eval num_timesteps=265000, episode_reward=19.59 +/- 14.07
Episode length: 27.60 +/- 19.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 27.6          |
|    mean_reward          | 19.6          |
| time/                   |               |
|    total_timesteps      | 265000        |
| train/                  |               |
|    approx_kl            | 8.6253414e-05 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00778      |
|    explained_variance   | 0.109         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.733         |
|    n_updates            | 6620          |
|    policy_gradient_loss | -0.00032      |
|    value_loss           | 1.72          |
-------------------------------------------
Progress update from timestep 265100: Updated shield, loss is 0.0500544011592865
Progress update from timestep 265200: Updated shield, loss is 0.05381081625819206
Eval num_timesteps=265200, episode_reward=16.04 +/- 14.74
Episode length: 24.00 +/- 21.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 265200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.17     |
|    ep_rew_mean     | 3.29     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 663      |
|    time_elapsed    | 58207    |
|    total_timesteps | 265200   |
---------------------------------
Progress update from timestep 265300: Updated shield, loss is 0.053378645330667496
Progress update from timestep 265400: Updated shield, loss is 0.050104331225156784
Eval num_timesteps=265400, episode_reward=20.55 +/- 16.15
Episode length: 30.80 +/- 23.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30.8         |
|    mean_reward          | 20.6         |
| time/                   |              |
|    total_timesteps      | 265400       |
| train/                  |              |
|    approx_kl            | 4.363287e-05 |
|    clip_fraction        | 0.00223      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00666     |
|    explained_variance   | 0.166        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.744        |
|    n_updates            | 6630         |
|    policy_gradient_loss | -0.000267    |
|    value_loss           | 1.91         |
------------------------------------------
Progress update from timestep 265500: Updated shield, loss is 0.052828382700681686
Progress update from timestep 265600: Updated shield, loss is 0.0515398308634758
Eval num_timesteps=265600, episode_reward=3.16 +/- 2.35
Episode length: 5.40 +/- 3.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5.4      |
|    mean_reward     | 3.16     |
| time/              |          |
|    total_timesteps | 265600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.14     |
|    ep_rew_mean     | 2.64     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 664      |
|    time_elapsed    | 58302    |
|    total_timesteps | 265600   |
---------------------------------
Progress update from timestep 265700: Updated shield, loss is 0.05048232525587082
Progress update from timestep 265800: Updated shield, loss is 0.05724471062421799
Eval num_timesteps=265800, episode_reward=17.63 +/- 13.10
Episode length: 26.60 +/- 19.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.6         |
|    mean_reward          | 17.6         |
| time/                   |              |
|    total_timesteps      | 265800       |
| train/                  |              |
|    approx_kl            | -3.68995e-11 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.000312    |
|    explained_variance   | 0.16         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.986        |
|    n_updates            | 6640         |
|    policy_gradient_loss | -2.65e-08    |
|    value_loss           | 1.7          |
------------------------------------------
Progress update from timestep 265900: Updated shield, loss is 0.04120975732803345
Progress update from timestep 266000: Updated shield, loss is 0.041647769510746
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 266000: Saved shield_loss.csv. 
Eval num_timesteps=266000, episode_reward=16.94 +/- 14.35
Episode length: 25.00 +/- 20.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.26     |
|    ep_rew_mean     | 3.44     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 665      |
|    time_elapsed    | 58397    |
|    total_timesteps | 266000   |
---------------------------------
Progress update from timestep 266100: Updated shield, loss is 0.04914379119873047
Progress update from timestep 266200: Updated shield, loss is 0.048166461288928986
Eval num_timesteps=266200, episode_reward=15.46 +/- 15.55
Episode length: 23.00 +/- 22.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23            |
|    mean_reward          | 15.5          |
| time/                   |               |
|    total_timesteps      | 266200        |
| train/                  |               |
|    approx_kl            | 2.8064408e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00131      |
|    explained_variance   | 0.225         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.19          |
|    n_updates            | 6650          |
|    policy_gradient_loss | -8.82e-07     |
|    value_loss           | 2.08          |
-------------------------------------------
Progress update from timestep 266300: Updated shield, loss is 0.04537693038582802
Progress update from timestep 266400: Updated shield, loss is 0.05291220173239708
Eval num_timesteps=266400, episode_reward=10.17 +/- 12.92
Episode length: 15.20 +/- 18.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | 10.2     |
| time/              |          |
|    total_timesteps | 266400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.73     |
|    ep_rew_mean     | 3.02     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 666      |
|    time_elapsed    | 58487    |
|    total_timesteps | 266400   |
---------------------------------
Progress update from timestep 266500: Updated shield, loss is 0.04332234710454941
Progress update from timestep 266600: Updated shield, loss is 0.03959358111023903
Eval num_timesteps=266600, episode_reward=29.27 +/- 13.20
Episode length: 41.00 +/- 18.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41           |
|    mean_reward          | 29.3         |
| time/                   |              |
|    total_timesteps      | 266600       |
| train/                  |              |
|    approx_kl            | 6.596758e-05 |
|    clip_fraction        | 0.00223      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00786     |
|    explained_variance   | 0.224        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.768        |
|    n_updates            | 6660         |
|    policy_gradient_loss | -0.00024     |
|    value_loss           | 1.52         |
------------------------------------------
Progress update from timestep 266700: Updated shield, loss is 0.04281522333621979
Progress update from timestep 266800: Updated shield, loss is 0.0489504374563694
Eval num_timesteps=266800, episode_reward=9.38 +/- 12.60
Episode length: 14.20 +/- 17.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.2     |
|    mean_reward     | 9.38     |
| time/              |          |
|    total_timesteps | 266800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.49     |
|    ep_rew_mean     | 2.86     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 667      |
|    time_elapsed    | 58579    |
|    total_timesteps | 266800   |
---------------------------------
Progress update from timestep 266900: Updated shield, loss is 0.051824670284986496
Progress update from timestep 267000: Updated shield, loss is 0.04326266795396805
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 267000: Saved shield_loss.csv. 
Eval num_timesteps=267000, episode_reward=9.79 +/- 13.51
Episode length: 14.40 +/- 17.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.4         |
|    mean_reward          | 9.79         |
| time/                   |              |
|    total_timesteps      | 267000       |
| train/                  |              |
|    approx_kl            | 9.973669e-05 |
|    clip_fraction        | 0.00179      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0126      |
|    explained_variance   | 0.256        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.1          |
|    n_updates            | 6670         |
|    policy_gradient_loss | -0.000224    |
|    value_loss           | 1.83         |
------------------------------------------
Progress update from timestep 267100: Updated shield, loss is 0.037806883454322815
Progress update from timestep 267200: Updated shield, loss is 0.043775688856840134
Eval num_timesteps=267200, episode_reward=6.06 +/- 2.73
Episode length: 9.60 +/- 3.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.6      |
|    mean_reward     | 6.06     |
| time/              |          |
|    total_timesteps | 267200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.42     |
|    ep_rew_mean     | 2.76     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 668      |
|    time_elapsed    | 58659    |
|    total_timesteps | 267200   |
---------------------------------
Progress update from timestep 267300: Updated shield, loss is 0.03699161112308502
Progress update from timestep 267400: Updated shield, loss is 0.04188746586441994
Eval num_timesteps=267400, episode_reward=25.07 +/- 12.53
Episode length: 35.80 +/- 17.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.8         |
|    mean_reward          | 25.1         |
| time/                   |              |
|    total_timesteps      | 267400       |
| train/                  |              |
|    approx_kl            | 7.867648e-05 |
|    clip_fraction        | 0.00246      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00836     |
|    explained_variance   | 0.197        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.04         |
|    n_updates            | 6680         |
|    policy_gradient_loss | -0.00039     |
|    value_loss           | 1.68         |
------------------------------------------
Progress update from timestep 267500: Updated shield, loss is 0.04293934628367424
Progress update from timestep 267600: Updated shield, loss is 0.03556487709283829
Eval num_timesteps=267600, episode_reward=17.41 +/- 15.34
Episode length: 24.80 +/- 20.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 267600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.18     |
|    ep_rew_mean     | 2.54     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 669      |
|    time_elapsed    | 58756    |
|    total_timesteps | 267600   |
---------------------------------
Progress update from timestep 267700: Updated shield, loss is 0.03552790358662605
Progress update from timestep 267800: Updated shield, loss is 0.03791056573390961
Eval num_timesteps=267800, episode_reward=10.23 +/- 12.47
Episode length: 15.60 +/- 17.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.6          |
|    mean_reward          | 10.2          |
| time/                   |               |
|    total_timesteps      | 267800        |
| train/                  |               |
|    approx_kl            | 0.00012995453 |
|    clip_fraction        | 0.00781       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0337       |
|    explained_variance   | 0.126         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.495         |
|    n_updates            | 6690          |
|    policy_gradient_loss | -0.000861     |
|    value_loss           | 1.48          |
-------------------------------------------
Progress update from timestep 267900: Updated shield, loss is 0.04589444771409035
Progress update from timestep 268000: Updated shield, loss is 0.041854377835989
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 268000: Saved shield_loss.csv. 
Eval num_timesteps=268000, episode_reward=20.03 +/- 13.24
Episode length: 29.20 +/- 17.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.2     |
|    mean_reward     | 20       |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.82     |
|    ep_rew_mean     | 2.34     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 670      |
|    time_elapsed    | 58862    |
|    total_timesteps | 268000   |
---------------------------------
Progress update from timestep 268100: Updated shield, loss is 0.04254448041319847
Progress update from timestep 268200: Updated shield, loss is 0.04575727880001068
Eval num_timesteps=268200, episode_reward=12.49 +/- 12.52
Episode length: 17.80 +/- 16.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17.8          |
|    mean_reward          | 12.5          |
| time/                   |               |
|    total_timesteps      | 268200        |
| train/                  |               |
|    approx_kl            | 2.6770326e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00107      |
|    explained_variance   | 0.28          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.411         |
|    n_updates            | 6700          |
|    policy_gradient_loss | -3.13e-06     |
|    value_loss           | 1.04          |
-------------------------------------------
Progress update from timestep 268300: Updated shield, loss is 0.04314403235912323
Progress update from timestep 268400: Updated shield, loss is 0.04110965132713318
Eval num_timesteps=268400, episode_reward=10.49 +/- 13.16
Episode length: 15.20 +/- 17.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | 10.5     |
| time/              |          |
|    total_timesteps | 268400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.78     |
|    ep_rew_mean     | 2.34     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 671      |
|    time_elapsed    | 58962    |
|    total_timesteps | 268400   |
---------------------------------
Progress update from timestep 268500: Updated shield, loss is 0.04746412858366966
Progress update from timestep 268600: Updated shield, loss is 0.0383923277258873
Eval num_timesteps=268600, episode_reward=23.15 +/- 15.46
Episode length: 33.00 +/- 21.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33            |
|    mean_reward          | 23.2          |
| time/                   |               |
|    total_timesteps      | 268600        |
| train/                  |               |
|    approx_kl            | 5.3504273e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00103      |
|    explained_variance   | 0.0858        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.356         |
|    n_updates            | 6710          |
|    policy_gradient_loss | -2.5e-07      |
|    value_loss           | 1.84          |
-------------------------------------------
Progress update from timestep 268700: Updated shield, loss is 0.05067745968699455
Progress update from timestep 268800: Updated shield, loss is 0.04702727496623993
Eval num_timesteps=268800, episode_reward=12.14 +/- 11.40
Episode length: 18.80 +/- 16.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.8     |
|    mean_reward     | 12.1     |
| time/              |          |
|    total_timesteps | 268800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.85     |
|    ep_rew_mean     | 2.38     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 672      |
|    time_elapsed    | 59057    |
|    total_timesteps | 268800   |
---------------------------------
Progress update from timestep 268900: Updated shield, loss is 0.03914845362305641
Progress update from timestep 269000: Updated shield, loss is 0.04567782208323479
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 269000: Saved shield_loss.csv. 
Eval num_timesteps=269000, episode_reward=15.79 +/- 15.76
Episode length: 23.20 +/- 21.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.2         |
|    mean_reward          | 15.8         |
| time/                   |              |
|    total_timesteps      | 269000       |
| train/                  |              |
|    approx_kl            | 0.0002979803 |
|    clip_fraction        | 0.0132       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0418      |
|    explained_variance   | 0.205        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.473        |
|    n_updates            | 6720         |
|    policy_gradient_loss | -0.00154     |
|    value_loss           | 1.23         |
------------------------------------------
Progress update from timestep 269100: Updated shield, loss is 0.0382651686668396
Progress update from timestep 269200: Updated shield, loss is 0.04695193096995354
Eval num_timesteps=269200, episode_reward=17.93 +/- 14.52
Episode length: 26.20 +/- 20.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 269200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.02     |
|    ep_rew_mean     | 2.54     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 673      |
|    time_elapsed    | 59169    |
|    total_timesteps | 269200   |
---------------------------------
Progress update from timestep 269300: Updated shield, loss is 0.050458140671253204
Progress update from timestep 269400: Updated shield, loss is 0.03995857387781143
Eval num_timesteps=269400, episode_reward=24.29 +/- 13.11
Episode length: 35.80 +/- 18.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.8          |
|    mean_reward          | 24.3          |
| time/                   |               |
|    total_timesteps      | 269400        |
| train/                  |               |
|    approx_kl            | 0.00014431858 |
|    clip_fraction        | 0.00558       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0219       |
|    explained_variance   | 0.145         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.12          |
|    n_updates            | 6730          |
|    policy_gradient_loss | -0.000741     |
|    value_loss           | 1.86          |
-------------------------------------------
Progress update from timestep 269500: Updated shield, loss is 0.05312180891633034
Progress update from timestep 269600: Updated shield, loss is 0.0502876415848732
Eval num_timesteps=269600, episode_reward=10.22 +/- 12.27
Episode length: 15.60 +/- 17.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 10.2     |
| time/              |          |
|    total_timesteps | 269600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.72     |
|    ep_rew_mean     | 3.09     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 674      |
|    time_elapsed    | 59283    |
|    total_timesteps | 269600   |
---------------------------------
Progress update from timestep 269700: Updated shield, loss is 0.04328891634941101
Progress update from timestep 269800: Updated shield, loss is 0.03885938227176666
Eval num_timesteps=269800, episode_reward=21.02 +/- 16.05
Episode length: 31.20 +/- 23.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 31.2          |
|    mean_reward          | 21            |
| time/                   |               |
|    total_timesteps      | 269800        |
| train/                  |               |
|    approx_kl            | 0.00027394822 |
|    clip_fraction        | 0.0127        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0417       |
|    explained_variance   | 0.187         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.679         |
|    n_updates            | 6740          |
|    policy_gradient_loss | -0.00186      |
|    value_loss           | 2.14          |
-------------------------------------------
Progress update from timestep 269900: Updated shield, loss is 0.04353632777929306
Progress update from timestep 270000: Updated shield, loss is 0.04181699827313423
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 270000: Saved shield_loss.csv. 
Eval num_timesteps=270000, episode_reward=9.69 +/- 12.51
Episode length: 14.60 +/- 17.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 9.69     |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.92     |
|    ep_rew_mean     | 2.42     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 675      |
|    time_elapsed    | 59394    |
|    total_timesteps | 270000   |
---------------------------------
Progress update from timestep 270100: Updated shield, loss is 0.04888095334172249
Progress update from timestep 270200: Updated shield, loss is 0.049469057470560074
Eval num_timesteps=270200, episode_reward=10.96 +/- 12.55
Episode length: 16.00 +/- 17.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | 11           |
| time/                   |              |
|    total_timesteps      | 270200       |
| train/                  |              |
|    approx_kl            | 0.0004220221 |
|    clip_fraction        | 0.0163       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0298      |
|    explained_variance   | 0.107        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.11         |
|    n_updates            | 6750         |
|    policy_gradient_loss | -0.00168     |
|    value_loss           | 1.79         |
------------------------------------------
Progress update from timestep 270300: Updated shield, loss is 0.04599984362721443
Progress update from timestep 270400: Updated shield, loss is 0.04247310385107994
Eval num_timesteps=270400, episode_reward=16.60 +/- 14.51
Episode length: 24.80 +/- 21.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 270400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.25     |
|    ep_rew_mean     | 2.71     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 676      |
|    time_elapsed    | 59484    |
|    total_timesteps | 270400   |
---------------------------------
Progress update from timestep 270500: Updated shield, loss is 0.04656974971294403
Progress update from timestep 270600: Updated shield, loss is 0.04168429970741272
Eval num_timesteps=270600, episode_reward=10.40 +/- 12.12
Episode length: 15.80 +/- 17.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.8          |
|    mean_reward          | 10.4          |
| time/                   |               |
|    total_timesteps      | 270600        |
| train/                  |               |
|    approx_kl            | 0.00015086269 |
|    clip_fraction        | 0.00737       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0235       |
|    explained_variance   | 0.131         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.673         |
|    n_updates            | 6760          |
|    policy_gradient_loss | -0.00101      |
|    value_loss           | 1.93          |
-------------------------------------------
Progress update from timestep 270700: Updated shield, loss is 0.04875053092837334
Progress update from timestep 270800: Updated shield, loss is 0.050302766263484955
Eval num_timesteps=270800, episode_reward=17.64 +/- 14.14
Episode length: 26.20 +/- 20.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 270800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.52     |
|    ep_rew_mean     | 2.95     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 677      |
|    time_elapsed    | 59574    |
|    total_timesteps | 270800   |
---------------------------------
Progress update from timestep 270900: Updated shield, loss is 0.0399935357272625
Progress update from timestep 271000: Updated shield, loss is 0.0385960228741169
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 271000: Saved shield_loss.csv. 
Eval num_timesteps=271000, episode_reward=19.54 +/- 13.48
Episode length: 29.20 +/- 19.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29.2         |
|    mean_reward          | 19.5         |
| time/                   |              |
|    total_timesteps      | 271000       |
| train/                  |              |
|    approx_kl            | 0.0005846803 |
|    clip_fraction        | 0.0154       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0415      |
|    explained_variance   | 0.314        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.602        |
|    n_updates            | 6770         |
|    policy_gradient_loss | -0.00183     |
|    value_loss           | 1.36         |
------------------------------------------
Progress update from timestep 271100: Updated shield, loss is 0.06196410581469536
Progress update from timestep 271200: Updated shield, loss is 0.05258183181285858
Eval num_timesteps=271200, episode_reward=8.92 +/- 12.95
Episode length: 13.60 +/- 18.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.6     |
|    mean_reward     | 8.92     |
| time/              |          |
|    total_timesteps | 271200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.99     |
|    ep_rew_mean     | 2.42     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 678      |
|    time_elapsed    | 59668    |
|    total_timesteps | 271200   |
---------------------------------
Progress update from timestep 271300: Updated shield, loss is 0.04603280872106552
Progress update from timestep 271400: Updated shield, loss is 0.05908391624689102
Eval num_timesteps=271400, episode_reward=23.78 +/- 14.15
Episode length: 34.40 +/- 19.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.4          |
|    mean_reward          | 23.8          |
| time/                   |               |
|    total_timesteps      | 271400        |
| train/                  |               |
|    approx_kl            | 2.7126373e-05 |
|    clip_fraction        | 0.00246       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00768      |
|    explained_variance   | 0.186         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.494         |
|    n_updates            | 6780          |
|    policy_gradient_loss | -0.000372     |
|    value_loss           | 2.06          |
-------------------------------------------
Progress update from timestep 271500: Updated shield, loss is 0.042924072593450546
Progress update from timestep 271600: Updated shield, loss is 0.05331344157457352
Eval num_timesteps=271600, episode_reward=22.60 +/- 15.95
Episode length: 32.20 +/- 21.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 22.6     |
| time/              |          |
|    total_timesteps | 271600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.77     |
|    ep_rew_mean     | 3.03     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 679      |
|    time_elapsed    | 59762    |
|    total_timesteps | 271600   |
---------------------------------
Progress update from timestep 271700: Updated shield, loss is 0.04195219650864601
Progress update from timestep 271800: Updated shield, loss is 0.039892006665468216
Eval num_timesteps=271800, episode_reward=28.53 +/- 13.58
Episode length: 40.60 +/- 18.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 40.6          |
|    mean_reward          | 28.5          |
| time/                   |               |
|    total_timesteps      | 271800        |
| train/                  |               |
|    approx_kl            | 0.00016806174 |
|    clip_fraction        | 0.00446       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0275       |
|    explained_variance   | 0.105         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.16          |
|    n_updates            | 6790          |
|    policy_gradient_loss | -0.000462     |
|    value_loss           | 1.69          |
-------------------------------------------
Progress update from timestep 271900: Updated shield, loss is 0.05493084341287613
Progress update from timestep 272000: Updated shield, loss is 0.04392404481768608
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 272000: Saved shield_loss.csv. 
Eval num_timesteps=272000, episode_reward=16.16 +/- 15.51
Episode length: 23.60 +/- 21.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 16.2     |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.12     |
|    ep_rew_mean     | 2.56     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 680      |
|    time_elapsed    | 59875    |
|    total_timesteps | 272000   |
---------------------------------
Progress update from timestep 272100: Updated shield, loss is 0.043234191834926605
Progress update from timestep 272200: Updated shield, loss is 0.04936697706580162
Eval num_timesteps=272200, episode_reward=23.63 +/- 14.73
Episode length: 33.80 +/- 19.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.8         |
|    mean_reward          | 23.6         |
| time/                   |              |
|    total_timesteps      | 272200       |
| train/                  |              |
|    approx_kl            | 0.0003634325 |
|    clip_fraction        | 0.0103       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0359      |
|    explained_variance   | 0.168        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.664        |
|    n_updates            | 6800         |
|    policy_gradient_loss | -0.00122     |
|    value_loss           | 1.56         |
------------------------------------------
Progress update from timestep 272300: Updated shield, loss is 0.05083196610212326
Progress update from timestep 272400: Updated shield, loss is 0.04589264094829559
Eval num_timesteps=272400, episode_reward=21.16 +/- 16.32
Episode length: 31.00 +/- 23.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31       |
|    mean_reward     | 21.2     |
| time/              |          |
|    total_timesteps | 272400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.95     |
|    ep_rew_mean     | 2.5      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 681      |
|    time_elapsed    | 60008    |
|    total_timesteps | 272400   |
---------------------------------
Progress update from timestep 272500: Updated shield, loss is 0.044604357331991196
Progress update from timestep 272600: Updated shield, loss is 0.04378989711403847
Eval num_timesteps=272600, episode_reward=18.60 +/- 14.37
Episode length: 27.00 +/- 19.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 27           |
|    mean_reward          | 18.6         |
| time/                   |              |
|    total_timesteps      | 272600       |
| train/                  |              |
|    approx_kl            | 0.0004552146 |
|    clip_fraction        | 0.015        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0251      |
|    explained_variance   | 0.163        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.487        |
|    n_updates            | 6810         |
|    policy_gradient_loss | -0.00144     |
|    value_loss           | 1.57         |
------------------------------------------
Progress update from timestep 272700: Updated shield, loss is 0.052090518176555634
Progress update from timestep 272800: Updated shield, loss is 0.04585017263889313
Eval num_timesteps=272800, episode_reward=26.58 +/- 11.09
Episode length: 38.20 +/- 15.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 38.2     |
|    mean_reward     | 26.6     |
| time/              |          |
|    total_timesteps | 272800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.24     |
|    ep_rew_mean     | 2.6      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 682      |
|    time_elapsed    | 60118    |
|    total_timesteps | 272800   |
---------------------------------
Progress update from timestep 272900: Updated shield, loss is 0.047024358063936234
Progress update from timestep 273000: Updated shield, loss is 0.048206329345703125
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 273000: Saved shield_loss.csv. 
Eval num_timesteps=273000, episode_reward=3.83 +/- 2.71
Episode length: 6.40 +/- 3.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 6.4          |
|    mean_reward          | 3.83         |
| time/                   |              |
|    total_timesteps      | 273000       |
| train/                  |              |
|    approx_kl            | 4.986007e-05 |
|    clip_fraction        | 0.00357      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00779     |
|    explained_variance   | 0.151        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.763        |
|    n_updates            | 6820         |
|    policy_gradient_loss | -0.000357    |
|    value_loss           | 1.84         |
------------------------------------------
Progress update from timestep 273100: Updated shield, loss is 0.05069122835993767
Progress update from timestep 273200: Updated shield, loss is 0.04907210171222687
Eval num_timesteps=273200, episode_reward=18.88 +/- 12.92
Episode length: 28.00 +/- 18.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28       |
|    mean_reward     | 18.9     |
| time/              |          |
|    total_timesteps | 273200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.77     |
|    ep_rew_mean     | 3.06     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 683      |
|    time_elapsed    | 60194    |
|    total_timesteps | 273200   |
---------------------------------
Progress update from timestep 273300: Updated shield, loss is 0.04206612706184387
Progress update from timestep 273400: Updated shield, loss is 0.04890168830752373
Eval num_timesteps=273400, episode_reward=17.93 +/- 14.91
Episode length: 25.60 +/- 19.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.6         |
|    mean_reward          | 17.9         |
| time/                   |              |
|    total_timesteps      | 273400       |
| train/                  |              |
|    approx_kl            | 8.705164e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00152     |
|    explained_variance   | 0.247        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.812        |
|    n_updates            | 6830         |
|    policy_gradient_loss | -4.38e-07    |
|    value_loss           | 2.06         |
------------------------------------------
Progress update from timestep 273500: Updated shield, loss is 0.04721373692154884
Progress update from timestep 273600: Updated shield, loss is 0.04328858107328415
Eval num_timesteps=273600, episode_reward=21.71 +/- 15.20
Episode length: 32.20 +/- 21.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 21.7     |
| time/              |          |
|    total_timesteps | 273600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.36     |
|    ep_rew_mean     | 2.77     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 684      |
|    time_elapsed    | 60285    |
|    total_timesteps | 273600   |
---------------------------------
Progress update from timestep 273700: Updated shield, loss is 0.04432080313563347
Progress update from timestep 273800: Updated shield, loss is 0.03718137741088867
Eval num_timesteps=273800, episode_reward=15.78 +/- 16.67
Episode length: 22.40 +/- 22.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.4          |
|    mean_reward          | 15.8          |
| time/                   |               |
|    total_timesteps      | 273800        |
| train/                  |               |
|    approx_kl            | 1.2244398e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00211      |
|    explained_variance   | 0.032         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.835         |
|    n_updates            | 6840          |
|    policy_gradient_loss | -1.72e-06     |
|    value_loss           | 1.64          |
-------------------------------------------
Progress update from timestep 273900: Updated shield, loss is 0.034659940749406815
Progress update from timestep 274000: Updated shield, loss is 0.04263777658343315
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 274000: Saved shield_loss.csv. 
Eval num_timesteps=274000, episode_reward=3.98 +/- 3.22
Episode length: 6.60 +/- 4.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6.6      |
|    mean_reward     | 3.98     |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.31     |
|    ep_rew_mean     | 2.71     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 685      |
|    time_elapsed    | 60361    |
|    total_timesteps | 274000   |
---------------------------------
Progress update from timestep 274100: Updated shield, loss is 0.046624138951301575
Progress update from timestep 274200: Updated shield, loss is 0.04607338458299637
Eval num_timesteps=274200, episode_reward=12.05 +/- 10.82
Episode length: 18.20 +/- 16.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.2         |
|    mean_reward          | 12.1         |
| time/                   |              |
|    total_timesteps      | 274200       |
| train/                  |              |
|    approx_kl            | 9.331453e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0112      |
|    explained_variance   | 0.199        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.774        |
|    n_updates            | 6850         |
|    policy_gradient_loss | -9.68e-05    |
|    value_loss           | 2.05         |
------------------------------------------
Progress update from timestep 274300: Updated shield, loss is 0.04866793751716614
Progress update from timestep 274400: Updated shield, loss is 0.05024583265185356
Eval num_timesteps=274400, episode_reward=21.01 +/- 16.00
Episode length: 30.00 +/- 21.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 21       |
| time/              |          |
|    total_timesteps | 274400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.76     |
|    ep_rew_mean     | 3.08     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 686      |
|    time_elapsed    | 60451    |
|    total_timesteps | 274400   |
---------------------------------
Progress update from timestep 274500: Updated shield, loss is 0.03902376815676689
Progress update from timestep 274600: Updated shield, loss is 0.04050439968705177
Eval num_timesteps=274600, episode_reward=10.76 +/- 12.14
Episode length: 16.20 +/- 17.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.2         |
|    mean_reward          | 10.8         |
| time/                   |              |
|    total_timesteps      | 274600       |
| train/                  |              |
|    approx_kl            | 0.0002470836 |
|    clip_fraction        | 0.00915      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0265      |
|    explained_variance   | 0.243        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.84         |
|    n_updates            | 6860         |
|    policy_gradient_loss | -0.000963    |
|    value_loss           | 1.38         |
------------------------------------------
Progress update from timestep 274700: Updated shield, loss is 0.05022609978914261
Progress update from timestep 274800: Updated shield, loss is 0.04100622236728668
Eval num_timesteps=274800, episode_reward=23.59 +/- 14.30
Episode length: 33.80 +/- 19.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 23.6     |
| time/              |          |
|    total_timesteps | 274800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.4      |
|    ep_rew_mean     | 2.82     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 687      |
|    time_elapsed    | 60543    |
|    total_timesteps | 274800   |
---------------------------------
Progress update from timestep 274900: Updated shield, loss is 0.04264678806066513
Progress update from timestep 275000: Updated shield, loss is 0.055497270077466965
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 275000: Saved shield_loss.csv. 
Eval num_timesteps=275000, episode_reward=11.44 +/- 12.16
Episode length: 16.80 +/- 16.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.8          |
|    mean_reward          | 11.4          |
| time/                   |               |
|    total_timesteps      | 275000        |
| train/                  |               |
|    approx_kl            | 0.00025799187 |
|    clip_fraction        | 0.0147        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0534       |
|    explained_variance   | 0.0674        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.847         |
|    n_updates            | 6870          |
|    policy_gradient_loss | -0.00229      |
|    value_loss           | 2.47          |
-------------------------------------------
Progress update from timestep 275100: Updated shield, loss is 0.03464258462190628
Progress update from timestep 275200: Updated shield, loss is 0.04403131082653999
Eval num_timesteps=275200, episode_reward=17.22 +/- 14.64
Episode length: 25.20 +/- 20.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 17.2     |
| time/              |          |
|    total_timesteps | 275200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.94     |
|    ep_rew_mean     | 2.47     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 688      |
|    time_elapsed    | 60634    |
|    total_timesteps | 275200   |
---------------------------------
Progress update from timestep 275300: Updated shield, loss is 0.05798419937491417
Progress update from timestep 275400: Updated shield, loss is 0.04270542040467262
Eval num_timesteps=275400, episode_reward=16.72 +/- 15.23
Episode length: 24.40 +/- 21.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.4          |
|    mean_reward          | 16.7          |
| time/                   |               |
|    total_timesteps      | 275400        |
| train/                  |               |
|    approx_kl            | 0.00012669861 |
|    clip_fraction        | 0.00201       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0196       |
|    explained_variance   | 0.135         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.726         |
|    n_updates            | 6880          |
|    policy_gradient_loss | -0.000449     |
|    value_loss           | 1.52          |
-------------------------------------------
Progress update from timestep 275500: Updated shield, loss is 0.05004660785198212
Progress update from timestep 275600: Updated shield, loss is 0.039470333606004715
Eval num_timesteps=275600, episode_reward=24.17 +/- 14.01
Episode length: 34.20 +/- 19.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 24.2     |
| time/              |          |
|    total_timesteps | 275600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.45     |
|    ep_rew_mean     | 3.63     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 689      |
|    time_elapsed    | 60734    |
|    total_timesteps | 275600   |
---------------------------------
Progress update from timestep 275700: Updated shield, loss is 0.04533165320754051
Progress update from timestep 275800: Updated shield, loss is 0.04576081037521362
Eval num_timesteps=275800, episode_reward=15.01 +/- 15.07
Episode length: 23.00 +/- 22.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23           |
|    mean_reward          | 15           |
| time/                   |              |
|    total_timesteps      | 275800       |
| train/                  |              |
|    approx_kl            | 0.0001071366 |
|    clip_fraction        | 0.00513      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0434      |
|    explained_variance   | 0.18         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.567        |
|    n_updates            | 6890         |
|    policy_gradient_loss | -0.000844    |
|    value_loss           | 3.43         |
------------------------------------------
Progress update from timestep 275900: Updated shield, loss is 0.05014223977923393
Progress update from timestep 276000: Updated shield, loss is 0.044253624975681305
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 276000: Saved shield_loss.csv. 
Eval num_timesteps=276000, episode_reward=11.63 +/- 11.62
Episode length: 17.80 +/- 17.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | 11.6     |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.99     |
|    ep_rew_mean     | 2.54     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 690      |
|    time_elapsed    | 60832    |
|    total_timesteps | 276000   |
---------------------------------
Progress update from timestep 276100: Updated shield, loss is 0.05789104849100113
Progress update from timestep 276200: Updated shield, loss is 0.04373770207166672
Eval num_timesteps=276200, episode_reward=16.79 +/- 14.90
Episode length: 24.40 +/- 20.91
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.4          |
|    mean_reward          | 16.8          |
| time/                   |               |
|    total_timesteps      | 276200        |
| train/                  |               |
|    approx_kl            | 0.00016276745 |
|    clip_fraction        | 0.00759       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0146       |
|    explained_variance   | 0.211         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.341         |
|    n_updates            | 6900          |
|    policy_gradient_loss | -0.00122      |
|    value_loss           | 1.5           |
-------------------------------------------
Progress update from timestep 276300: Updated shield, loss is 0.044849149882793427
Progress update from timestep 276400: Updated shield, loss is 0.05066073685884476
Eval num_timesteps=276400, episode_reward=24.13 +/- 13.63
Episode length: 34.60 +/- 18.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 24.1     |
| time/              |          |
|    total_timesteps | 276400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.07     |
|    ep_rew_mean     | 2.57     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 691      |
|    time_elapsed    | 60942    |
|    total_timesteps | 276400   |
---------------------------------
Progress update from timestep 276500: Updated shield, loss is 0.0356217622756958
Progress update from timestep 276600: Updated shield, loss is 0.03546220064163208
Eval num_timesteps=276600, episode_reward=15.99 +/- 15.20
Episode length: 23.60 +/- 21.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.6          |
|    mean_reward          | 16            |
| time/                   |               |
|    total_timesteps      | 276600        |
| train/                  |               |
|    approx_kl            | 4.0551087e-05 |
|    clip_fraction        | 0.00424       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00806      |
|    explained_variance   | 0.12          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.606         |
|    n_updates            | 6910          |
|    policy_gradient_loss | -0.000495     |
|    value_loss           | 1.28          |
-------------------------------------------
Progress update from timestep 276700: Updated shield, loss is 0.04706091806292534
Progress update from timestep 276800: Updated shield, loss is 0.04974381625652313
Eval num_timesteps=276800, episode_reward=23.08 +/- 14.01
Episode length: 33.80 +/- 19.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 23.1     |
| time/              |          |
|    total_timesteps | 276800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.12     |
|    ep_rew_mean     | 2.56     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 692      |
|    time_elapsed    | 61041    |
|    total_timesteps | 276800   |
---------------------------------
Progress update from timestep 276900: Updated shield, loss is 0.042326848953962326
Progress update from timestep 277000: Updated shield, loss is 0.035551246255636215
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 277000: Saved shield_loss.csv. 
Eval num_timesteps=277000, episode_reward=17.22 +/- 15.47
Episode length: 24.40 +/- 20.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.4          |
|    mean_reward          | 17.2          |
| time/                   |               |
|    total_timesteps      | 277000        |
| train/                  |               |
|    approx_kl            | 1.1766262e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00142      |
|    explained_variance   | 0.125         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.379         |
|    n_updates            | 6920          |
|    policy_gradient_loss | -5.82e-07     |
|    value_loss           | 1.48          |
-------------------------------------------
Progress update from timestep 277100: Updated shield, loss is 0.04382702708244324
Progress update from timestep 277200: Updated shield, loss is 0.03942292183637619
Eval num_timesteps=277200, episode_reward=17.01 +/- 14.07
Episode length: 25.60 +/- 20.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 277200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.19     |
|    ep_rew_mean     | 2.67     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 693      |
|    time_elapsed    | 61135    |
|    total_timesteps | 277200   |
---------------------------------
Progress update from timestep 277300: Updated shield, loss is 0.0454375296831131
Progress update from timestep 277400: Updated shield, loss is 0.04363921657204628
Eval num_timesteps=277400, episode_reward=10.63 +/- 12.20
Episode length: 16.20 +/- 17.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.2          |
|    mean_reward          | 10.6          |
| time/                   |               |
|    total_timesteps      | 277400        |
| train/                  |               |
|    approx_kl            | 0.00010114574 |
|    clip_fraction        | 0.00536       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0104       |
|    explained_variance   | 0.171         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.07          |
|    n_updates            | 6930          |
|    policy_gradient_loss | -0.000707     |
|    value_loss           | 2.01          |
-------------------------------------------
Progress update from timestep 277500: Updated shield, loss is 0.03858448565006256
Progress update from timestep 277600: Updated shield, loss is 0.03791835159063339
Eval num_timesteps=277600, episode_reward=23.40 +/- 15.42
Episode length: 33.00 +/- 20.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 23.4     |
| time/              |          |
|    total_timesteps | 277600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.08     |
|    ep_rew_mean     | 2.55     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 694      |
|    time_elapsed    | 61229    |
|    total_timesteps | 277600   |
---------------------------------
Progress update from timestep 277700: Updated shield, loss is 0.042303163558244705
Progress update from timestep 277800: Updated shield, loss is 0.04502413421869278
Eval num_timesteps=277800, episode_reward=23.58 +/- 11.54
Episode length: 34.60 +/- 15.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.6          |
|    mean_reward          | 23.6          |
| time/                   |               |
|    total_timesteps      | 277800        |
| train/                  |               |
|    approx_kl            | 0.00015735273 |
|    clip_fraction        | 0.0107        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.023        |
|    explained_variance   | 0.0338        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.953         |
|    n_updates            | 6940          |
|    policy_gradient_loss | -0.00117      |
|    value_loss           | 1.38          |
-------------------------------------------
Progress update from timestep 277900: Updated shield, loss is 0.0463225431740284
Progress update from timestep 278000: Updated shield, loss is 0.04182558134198189
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 278000: Saved shield_loss.csv. 
Eval num_timesteps=278000, episode_reward=17.25 +/- 14.65
Episode length: 25.20 +/- 20.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 17.3     |
| time/              |          |
|    total_timesteps | 278000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.63     |
|    ep_rew_mean     | 3.03     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 695      |
|    time_elapsed    | 61339    |
|    total_timesteps | 278000   |
---------------------------------
Progress update from timestep 278100: Updated shield, loss is 0.05467510223388672
Progress update from timestep 278200: Updated shield, loss is 0.04744211211800575
Eval num_timesteps=278200, episode_reward=23.42 +/- 14.03
Episode length: 34.00 +/- 19.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 34         |
|    mean_reward          | 23.4       |
| time/                   |            |
|    total_timesteps      | 278200     |
| train/                  |            |
|    approx_kl            | 7.6752e-05 |
|    clip_fraction        | 0.00268    |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.0105    |
|    explained_variance   | 0.213      |
|    learning_rate        | 0.0001     |
|    loss                 | 1.28       |
|    n_updates            | 6950       |
|    policy_gradient_loss | -0.000648  |
|    value_loss           | 1.51       |
----------------------------------------
Progress update from timestep 278300: Updated shield, loss is 0.04451394081115723
Progress update from timestep 278400: Updated shield, loss is 0.043001241981983185
Eval num_timesteps=278400, episode_reward=23.53 +/- 13.90
Episode length: 34.00 +/- 19.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 23.5     |
| time/              |          |
|    total_timesteps | 278400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.47     |
|    ep_rew_mean     | 2.82     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 696      |
|    time_elapsed    | 61457    |
|    total_timesteps | 278400   |
---------------------------------
Progress update from timestep 278500: Updated shield, loss is 0.0469539538025856
Progress update from timestep 278600: Updated shield, loss is 0.03945724666118622
Eval num_timesteps=278600, episode_reward=16.40 +/- 15.73
Episode length: 23.80 +/- 21.49
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.8          |
|    mean_reward          | 16.4          |
| time/                   |               |
|    total_timesteps      | 278600        |
| train/                  |               |
|    approx_kl            | 0.00016673526 |
|    clip_fraction        | 0.00603       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0182       |
|    explained_variance   | 0.13          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.671         |
|    n_updates            | 6960          |
|    policy_gradient_loss | -0.00136      |
|    value_loss           | 1.8           |
-------------------------------------------
Progress update from timestep 278700: Updated shield, loss is 0.039361853152513504
Progress update from timestep 278800: Updated shield, loss is 0.04467581585049629
Eval num_timesteps=278800, episode_reward=20.09 +/- 13.65
Episode length: 28.40 +/- 18.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.4     |
|    mean_reward     | 20.1     |
| time/              |          |
|    total_timesteps | 278800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.44     |
|    ep_rew_mean     | 2.83     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 697      |
|    time_elapsed    | 61552    |
|    total_timesteps | 278800   |
---------------------------------
Progress update from timestep 278900: Updated shield, loss is 0.043802011758089066
Progress update from timestep 279000: Updated shield, loss is 0.05217625945806503
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 279000: Saved shield_loss.csv. 
Eval num_timesteps=279000, episode_reward=22.52 +/- 14.69
Episode length: 33.00 +/- 20.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33            |
|    mean_reward          | 22.5          |
| time/                   |               |
|    total_timesteps      | 279000        |
| train/                  |               |
|    approx_kl            | 0.00028185136 |
|    clip_fraction        | 0.00313       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00473      |
|    explained_variance   | 0.198         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.895         |
|    n_updates            | 6970          |
|    policy_gradient_loss | -0.000368     |
|    value_loss           | 2.1           |
-------------------------------------------
Progress update from timestep 279100: Updated shield, loss is 0.05976778268814087
Progress update from timestep 279200: Updated shield, loss is 0.04296153783798218
Eval num_timesteps=279200, episode_reward=19.03 +/- 12.91
Episode length: 28.00 +/- 18.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28       |
|    mean_reward     | 19       |
| time/              |          |
|    total_timesteps | 279200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.68     |
|    ep_rew_mean     | 2.98     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 698      |
|    time_elapsed    | 61654    |
|    total_timesteps | 279200   |
---------------------------------
Progress update from timestep 279300: Updated shield, loss is 0.044529594480991364
Progress update from timestep 279400: Updated shield, loss is 0.03889523074030876
Eval num_timesteps=279400, episode_reward=22.44 +/- 16.59
Episode length: 31.60 +/- 22.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 31.6         |
|    mean_reward          | 22.4         |
| time/                   |              |
|    total_timesteps      | 279400       |
| train/                  |              |
|    approx_kl            | 5.171127e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.000733    |
|    explained_variance   | 0.186        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.996        |
|    n_updates            | 6980         |
|    policy_gradient_loss | -3.33e-07    |
|    value_loss           | 1.89         |
------------------------------------------
Progress update from timestep 279500: Updated shield, loss is 0.03969509154558182
Progress update from timestep 279600: Updated shield, loss is 0.037074264138936996
Eval num_timesteps=279600, episode_reward=16.72 +/- 13.74
Episode length: 25.40 +/- 20.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 279600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.67     |
|    ep_rew_mean     | 2.97     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 699      |
|    time_elapsed    | 61749    |
|    total_timesteps | 279600   |
---------------------------------
Progress update from timestep 279700: Updated shield, loss is 0.040487416088581085
Progress update from timestep 279800: Updated shield, loss is 0.045731864869594574
Eval num_timesteps=279800, episode_reward=15.85 +/- 14.76
Episode length: 23.80 +/- 21.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.8          |
|    mean_reward          | 15.8          |
| time/                   |               |
|    total_timesteps      | 279800        |
| train/                  |               |
|    approx_kl            | 2.3049194e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.000499     |
|    explained_variance   | 0.125         |
|    learning_rate        | 0.0001        |
|    loss                 | 1             |
|    n_updates            | 6990          |
|    policy_gradient_loss | -1.19e-07     |
|    value_loss           | 1.57          |
-------------------------------------------
Progress update from timestep 279900: Updated shield, loss is 0.04551710560917854
Progress update from timestep 280000: Updated shield, loss is 0.046269841492176056
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 280000: Saved shield_loss.csv. 
Eval num_timesteps=280000, episode_reward=12.66 +/- 11.35
Episode length: 19.60 +/- 16.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 12.7     |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.76     |
|    ep_rew_mean     | 2.29     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 700      |
|    time_elapsed    | 61842    |
|    total_timesteps | 280000   |
---------------------------------
Progress update from timestep 280100: Updated shield, loss is 0.049548421055078506
Progress update from timestep 280200: Updated shield, loss is 0.045656055212020874
Eval num_timesteps=280200, episode_reward=17.83 +/- 13.30
Episode length: 26.60 +/- 19.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.6          |
|    mean_reward          | 17.8          |
| time/                   |               |
|    total_timesteps      | 280200        |
| train/                  |               |
|    approx_kl            | 4.7364992e-05 |
|    clip_fraction        | 0.00134       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0126       |
|    explained_variance   | 0.256         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.731         |
|    n_updates            | 7000          |
|    policy_gradient_loss | -0.000395     |
|    value_loss           | 1.06          |
-------------------------------------------
Progress update from timestep 280300: Updated shield, loss is 0.04867364838719368
Progress update from timestep 280400: Updated shield, loss is 0.037142086774110794
Eval num_timesteps=280400, episode_reward=22.37 +/- 15.77
Episode length: 32.20 +/- 21.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 22.4     |
| time/              |          |
|    total_timesteps | 280400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.75     |
|    ep_rew_mean     | 2.27     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 701      |
|    time_elapsed    | 61938    |
|    total_timesteps | 280400   |
---------------------------------
Progress update from timestep 280500: Updated shield, loss is 0.03943999111652374
Progress update from timestep 280600: Updated shield, loss is 0.04561943560838699
Eval num_timesteps=280600, episode_reward=16.99 +/- 15.71
Episode length: 24.20 +/- 21.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.2          |
|    mean_reward          | 17            |
| time/                   |               |
|    total_timesteps      | 280600        |
| train/                  |               |
|    approx_kl            | 2.8225662e-05 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0079       |
|    explained_variance   | 0.129         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.02          |
|    n_updates            | 7010          |
|    policy_gradient_loss | -0.000313     |
|    value_loss           | 1.38          |
-------------------------------------------
Progress update from timestep 280700: Updated shield, loss is 0.03349149599671364
Progress update from timestep 280800: Updated shield, loss is 0.0444805733859539
Eval num_timesteps=280800, episode_reward=22.14 +/- 15.66
Episode length: 32.20 +/- 21.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 22.1     |
| time/              |          |
|    total_timesteps | 280800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.56     |
|    ep_rew_mean     | 2.84     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 702      |
|    time_elapsed    | 62055    |
|    total_timesteps | 280800   |
---------------------------------
Progress update from timestep 280900: Updated shield, loss is 0.037998929619789124
Progress update from timestep 281000: Updated shield, loss is 0.03920732066035271
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 281000: Saved shield_loss.csv. 
Eval num_timesteps=281000, episode_reward=8.40 +/- 13.11
Episode length: 12.80 +/- 18.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 12.8          |
|    mean_reward          | 8.4           |
| time/                   |               |
|    total_timesteps      | 281000        |
| train/                  |               |
|    approx_kl            | 0.00014304825 |
|    clip_fraction        | 0.00647       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0176       |
|    explained_variance   | 0.143         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.735         |
|    n_updates            | 7020          |
|    policy_gradient_loss | -0.00087      |
|    value_loss           | 1.68          |
-------------------------------------------
Progress update from timestep 281100: Updated shield, loss is 0.040964264422655106
Progress update from timestep 281200: Updated shield, loss is 0.047759830951690674
Eval num_timesteps=281200, episode_reward=23.33 +/- 14.68
Episode length: 33.60 +/- 20.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 23.3     |
| time/              |          |
|    total_timesteps | 281200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.82     |
|    ep_rew_mean     | 3.06     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 703      |
|    time_elapsed    | 62168    |
|    total_timesteps | 281200   |
---------------------------------
Progress update from timestep 281300: Updated shield, loss is 0.04656456038355827
Progress update from timestep 281400: Updated shield, loss is 0.048492107540369034
Eval num_timesteps=281400, episode_reward=12.70 +/- 11.51
Episode length: 19.20 +/- 16.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19.2        |
|    mean_reward          | 12.7        |
| time/                   |             |
|    total_timesteps      | 281400      |
| train/                  |             |
|    approx_kl            | 0.000598139 |
|    clip_fraction        | 0.01        |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0219     |
|    explained_variance   | 0.142       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.32        |
|    n_updates            | 7030        |
|    policy_gradient_loss | -0.00175    |
|    value_loss           | 2.78        |
-----------------------------------------
Progress update from timestep 281500: Updated shield, loss is 0.0429031066596508
Progress update from timestep 281600: Updated shield, loss is 0.03885131701827049
Eval num_timesteps=281600, episode_reward=23.74 +/- 13.21
Episode length: 34.60 +/- 18.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 23.7     |
| time/              |          |
|    total_timesteps | 281600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.47     |
|    ep_rew_mean     | 2.85     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 704      |
|    time_elapsed    | 62264    |
|    total_timesteps | 281600   |
---------------------------------
Progress update from timestep 281700: Updated shield, loss is 0.03738337755203247
Progress update from timestep 281800: Updated shield, loss is 0.05087962746620178
Eval num_timesteps=281800, episode_reward=9.65 +/- 12.52
Episode length: 14.60 +/- 17.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.6         |
|    mean_reward          | 9.65         |
| time/                   |              |
|    total_timesteps      | 281800       |
| train/                  |              |
|    approx_kl            | 3.145809e-05 |
|    clip_fraction        | 0.000223     |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00705     |
|    explained_variance   | 0.283        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.1          |
|    n_updates            | 7040         |
|    policy_gradient_loss | -0.000118    |
|    value_loss           | 1.81         |
------------------------------------------
Progress update from timestep 281900: Updated shield, loss is 0.04982847720384598
Progress update from timestep 282000: Updated shield, loss is 0.043716996908187866
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 282000: Saved shield_loss.csv. 
Eval num_timesteps=282000, episode_reward=11.34 +/- 12.31
Episode length: 17.40 +/- 18.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 11.3     |
| time/              |          |
|    total_timesteps | 282000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.01     |
|    ep_rew_mean     | 3.22     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 705      |
|    time_elapsed    | 62366    |
|    total_timesteps | 282000   |
---------------------------------
Progress update from timestep 282100: Updated shield, loss is 0.046977102756500244
Progress update from timestep 282200: Updated shield, loss is 0.04730050265789032
Eval num_timesteps=282200, episode_reward=21.94 +/- 16.30
Episode length: 31.60 +/- 22.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 31.6          |
|    mean_reward          | 21.9          |
| time/                   |               |
|    total_timesteps      | 282200        |
| train/                  |               |
|    approx_kl            | 5.5695964e-05 |
|    clip_fraction        | 0.00201       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.01         |
|    explained_variance   | 0.265         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.3           |
|    n_updates            | 7050          |
|    policy_gradient_loss | -0.000584     |
|    value_loss           | 2.4           |
-------------------------------------------
Progress update from timestep 282300: Updated shield, loss is 0.0450001023709774
Progress update from timestep 282400: Updated shield, loss is 0.052207376807928085
Eval num_timesteps=282400, episode_reward=17.89 +/- 14.43
Episode length: 26.00 +/- 20.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 282400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.62     |
|    ep_rew_mean     | 2.17     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 706      |
|    time_elapsed    | 62466    |
|    total_timesteps | 282400   |
---------------------------------
Progress update from timestep 282500: Updated shield, loss is 0.029303966090083122
Progress update from timestep 282600: Updated shield, loss is 0.04246944934129715
Eval num_timesteps=282600, episode_reward=16.34 +/- 15.77
Episode length: 23.60 +/- 21.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.6          |
|    mean_reward          | 16.3          |
| time/                   |               |
|    total_timesteps      | 282600        |
| train/                  |               |
|    approx_kl            | 3.8020982e-05 |
|    clip_fraction        | 0.00246       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0143       |
|    explained_variance   | 0.136         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.568         |
|    n_updates            | 7060          |
|    policy_gradient_loss | -0.000269     |
|    value_loss           | 1.26          |
-------------------------------------------
Progress update from timestep 282700: Updated shield, loss is 0.03401721641421318
Progress update from timestep 282800: Updated shield, loss is 0.05582554265856743
Eval num_timesteps=282800, episode_reward=16.71 +/- 14.10
Episode length: 25.20 +/- 20.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 282800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.45     |
|    ep_rew_mean     | 2.84     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 707      |
|    time_elapsed    | 62560    |
|    total_timesteps | 282800   |
---------------------------------
Progress update from timestep 282900: Updated shield, loss is 0.04265422374010086
Progress update from timestep 283000: Updated shield, loss is 0.043489087373018265
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 283000: Saved shield_loss.csv. 
Eval num_timesteps=283000, episode_reward=15.90 +/- 14.90
Episode length: 24.00 +/- 21.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24            |
|    mean_reward          | 15.9          |
| time/                   |               |
|    total_timesteps      | 283000        |
| train/                  |               |
|    approx_kl            | -4.217457e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.000693     |
|    explained_variance   | 0.278         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.465         |
|    n_updates            | 7070          |
|    policy_gradient_loss | -1.2e-07      |
|    value_loss           | 1.32          |
-------------------------------------------
Progress update from timestep 283100: Updated shield, loss is 0.041973892599344254
Progress update from timestep 283200: Updated shield, loss is 0.05475997179746628
Eval num_timesteps=283200, episode_reward=35.58 +/- 0.70
Episode length: 50.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 35.6     |
| time/              |          |
|    total_timesteps | 283200   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.1      |
|    ep_rew_mean     | 2.55     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 708      |
|    time_elapsed    | 62694    |
|    total_timesteps | 283200   |
---------------------------------
Progress update from timestep 283300: Updated shield, loss is 0.04040622338652611
Progress update from timestep 283400: Updated shield, loss is 0.05102017894387245
Eval num_timesteps=283400, episode_reward=10.20 +/- 11.67
Episode length: 15.80 +/- 17.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | 10.2         |
| time/                   |              |
|    total_timesteps      | 283400       |
| train/                  |              |
|    approx_kl            | 0.0001264255 |
|    clip_fraction        | 0.00313      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0134      |
|    explained_variance   | 0.247        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.88         |
|    n_updates            | 7080         |
|    policy_gradient_loss | -0.000469    |
|    value_loss           | 1.69         |
------------------------------------------
Progress update from timestep 283500: Updated shield, loss is 0.04146178439259529
Progress update from timestep 283600: Updated shield, loss is 0.039373330771923065
Eval num_timesteps=283600, episode_reward=5.47 +/- 2.76
Episode length: 9.00 +/- 4.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 5.47     |
| time/              |          |
|    total_timesteps | 283600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.03     |
|    ep_rew_mean     | 2.57     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 709      |
|    time_elapsed    | 62769    |
|    total_timesteps | 283600   |
---------------------------------
Progress update from timestep 283700: Updated shield, loss is 0.048824843019247055
Progress update from timestep 283800: Updated shield, loss is 0.04242754355072975
Eval num_timesteps=283800, episode_reward=21.48 +/- 13.67
Episode length: 30.80 +/- 18.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 30.8          |
|    mean_reward          | 21.5          |
| time/                   |               |
|    total_timesteps      | 283800        |
| train/                  |               |
|    approx_kl            | 0.00030224054 |
|    clip_fraction        | 0.0096        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0211       |
|    explained_variance   | 0.0524        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.712         |
|    n_updates            | 7090          |
|    policy_gradient_loss | -0.0011       |
|    value_loss           | 1.57          |
-------------------------------------------
Progress update from timestep 283900: Updated shield, loss is 0.03627926483750343
Progress update from timestep 284000: Updated shield, loss is 0.04500441625714302
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 284000: Saved shield_loss.csv. 
Eval num_timesteps=284000, episode_reward=23.06 +/- 15.38
Episode length: 32.80 +/- 21.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 23.1     |
| time/              |          |
|    total_timesteps | 284000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.98     |
|    ep_rew_mean     | 2.46     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 710      |
|    time_elapsed    | 62863    |
|    total_timesteps | 284000   |
---------------------------------
Progress update from timestep 284100: Updated shield, loss is 0.03305358812212944
Progress update from timestep 284200: Updated shield, loss is 0.03974016383290291
Eval num_timesteps=284200, episode_reward=17.63 +/- 13.40
Episode length: 26.40 +/- 19.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.4          |
|    mean_reward          | 17.6          |
| time/                   |               |
|    total_timesteps      | 284200        |
| train/                  |               |
|    approx_kl            | 2.9805441e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0012       |
|    explained_variance   | 0.165         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.87          |
|    n_updates            | 7100          |
|    policy_gradient_loss | -7.34e-07     |
|    value_loss           | 1.71          |
-------------------------------------------
Progress update from timestep 284300: Updated shield, loss is 0.043042175471782684
Progress update from timestep 284400: Updated shield, loss is 0.04788646847009659
Eval num_timesteps=284400, episode_reward=15.99 +/- 15.15
Episode length: 23.60 +/- 21.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 284400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.15     |
|    ep_rew_mean     | 3.36     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 711      |
|    time_elapsed    | 62960    |
|    total_timesteps | 284400   |
---------------------------------
Progress update from timestep 284500: Updated shield, loss is 0.04007328301668167
Progress update from timestep 284600: Updated shield, loss is 0.04013293981552124
Eval num_timesteps=284600, episode_reward=16.92 +/- 14.29
Episode length: 25.40 +/- 20.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.4          |
|    mean_reward          | 16.9          |
| time/                   |               |
|    total_timesteps      | 284600        |
| train/                  |               |
|    approx_kl            | 2.2716693e-05 |
|    clip_fraction        | 0.00067       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0162       |
|    explained_variance   | 0.0447        |
|    learning_rate        | 0.0001        |
|    loss                 | 2.17          |
|    n_updates            | 7110          |
|    policy_gradient_loss | -0.000464     |
|    value_loss           | 3.53          |
-------------------------------------------
Progress update from timestep 284700: Updated shield, loss is 0.047340936958789825
Progress update from timestep 284800: Updated shield, loss is 0.03997941315174103
Eval num_timesteps=284800, episode_reward=25.18 +/- 12.89
Episode length: 35.80 +/- 17.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 25.2     |
| time/              |          |
|    total_timesteps | 284800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.41     |
|    ep_rew_mean     | 3.52     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 712      |
|    time_elapsed    | 63075    |
|    total_timesteps | 284800   |
---------------------------------
Progress update from timestep 284900: Updated shield, loss is 0.04180413857102394
Progress update from timestep 285000: Updated shield, loss is 0.046348195523023605
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 285000: Saved shield_loss.csv. 
Eval num_timesteps=285000, episode_reward=10.26 +/- 12.74
Episode length: 15.20 +/- 17.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.2          |
|    mean_reward          | 10.3          |
| time/                   |               |
|    total_timesteps      | 285000        |
| train/                  |               |
|    approx_kl            | 0.00010088272 |
|    clip_fraction        | 0.00246       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0221       |
|    explained_variance   | 0.177         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.83          |
|    n_updates            | 7120          |
|    policy_gradient_loss | -0.000255     |
|    value_loss           | 2.36          |
-------------------------------------------
Progress update from timestep 285100: Updated shield, loss is 0.04357657581567764
Progress update from timestep 285200: Updated shield, loss is 0.039761535823345184
Eval num_timesteps=285200, episode_reward=9.89 +/- 12.60
Episode length: 15.00 +/- 17.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | 9.89     |
| time/              |          |
|    total_timesteps | 285200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.82     |
|    ep_rew_mean     | 3.09     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 713      |
|    time_elapsed    | 63175    |
|    total_timesteps | 285200   |
---------------------------------
Progress update from timestep 285300: Updated shield, loss is 0.05090925469994545
Progress update from timestep 285400: Updated shield, loss is 0.04292501509189606
Eval num_timesteps=285400, episode_reward=24.13 +/- 13.82
Episode length: 35.20 +/- 19.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.2          |
|    mean_reward          | 24.1          |
| time/                   |               |
|    total_timesteps      | 285400        |
| train/                  |               |
|    approx_kl            | 5.3361354e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00184      |
|    explained_variance   | 0.162         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.956         |
|    n_updates            | 7130          |
|    policy_gradient_loss | -7.77e-07     |
|    value_loss           | 2.06          |
-------------------------------------------
Progress update from timestep 285500: Updated shield, loss is 0.04677005484700203
Progress update from timestep 285600: Updated shield, loss is 0.041102394461631775
Eval num_timesteps=285600, episode_reward=17.15 +/- 14.03
Episode length: 25.60 +/- 20.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 17.1     |
| time/              |          |
|    total_timesteps | 285600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.15     |
|    ep_rew_mean     | 3.31     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 714      |
|    time_elapsed    | 63265    |
|    total_timesteps | 285600   |
---------------------------------
Progress update from timestep 285700: Updated shield, loss is 0.04842424392700195
Progress update from timestep 285800: Updated shield, loss is 0.04283170402050018
Eval num_timesteps=285800, episode_reward=11.14 +/- 13.52
Episode length: 16.20 +/- 18.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.2          |
|    mean_reward          | 11.1          |
| time/                   |               |
|    total_timesteps      | 285800        |
| train/                  |               |
|    approx_kl            | 4.0038958e-05 |
|    clip_fraction        | 0.00134       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00821      |
|    explained_variance   | 0.18          |
|    learning_rate        | 0.0001        |
|    loss                 | 1.09          |
|    n_updates            | 7140          |
|    policy_gradient_loss | -0.000217     |
|    value_loss           | 2.38          |
-------------------------------------------
Progress update from timestep 285900: Updated shield, loss is 0.035563427954912186
Progress update from timestep 286000: Updated shield, loss is 0.04335881024599075
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 286000: Saved shield_loss.csv. 
Eval num_timesteps=286000, episode_reward=15.98 +/- 15.58
Episode length: 23.40 +/- 21.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 286000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.87     |
|    ep_rew_mean     | 3.04     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 715      |
|    time_elapsed    | 63357    |
|    total_timesteps | 286000   |
---------------------------------
Progress update from timestep 286100: Updated shield, loss is 0.034165240824222565
Progress update from timestep 286200: Updated shield, loss is 0.04280715808272362
Eval num_timesteps=286200, episode_reward=18.21 +/- 14.23
Episode length: 26.40 +/- 19.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.4          |
|    mean_reward          | 18.2          |
| time/                   |               |
|    total_timesteps      | 286200        |
| train/                  |               |
|    approx_kl            | 0.00020047424 |
|    clip_fraction        | 0.00826       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0211       |
|    explained_variance   | 0.169         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.885         |
|    n_updates            | 7150          |
|    policy_gradient_loss | -0.000972     |
|    value_loss           | 2.46          |
-------------------------------------------
Progress update from timestep 286300: Updated shield, loss is 0.03771308809518814
Progress update from timestep 286400: Updated shield, loss is 0.038161925971508026
Eval num_timesteps=286400, episode_reward=17.15 +/- 16.22
Episode length: 24.00 +/- 21.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 17.1     |
| time/              |          |
|    total_timesteps | 286400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.32     |
|    ep_rew_mean     | 2.66     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 716      |
|    time_elapsed    | 63455    |
|    total_timesteps | 286400   |
---------------------------------
Progress update from timestep 286500: Updated shield, loss is 0.04864097014069557
Progress update from timestep 286600: Updated shield, loss is 0.03989943489432335
Eval num_timesteps=286600, episode_reward=14.69 +/- 17.07
Episode length: 21.20 +/- 23.52
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 21.2           |
|    mean_reward          | 14.7           |
| time/                   |                |
|    total_timesteps      | 286600         |
| train/                  |                |
|    approx_kl            | -1.1368684e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.00111       |
|    explained_variance   | 0.126          |
|    learning_rate        | 0.0001         |
|    loss                 | 0.256          |
|    n_updates            | 7160           |
|    policy_gradient_loss | -1.91e-07      |
|    value_loss           | 1.59           |
--------------------------------------------
Progress update from timestep 286700: Updated shield, loss is 0.043351396918296814
Progress update from timestep 286800: Updated shield, loss is 0.04626869037747383
Eval num_timesteps=286800, episode_reward=16.41 +/- 15.27
Episode length: 24.00 +/- 21.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16.4     |
| time/              |          |
|    total_timesteps | 286800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.58     |
|    ep_rew_mean     | 2.9      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 717      |
|    time_elapsed    | 63550    |
|    total_timesteps | 286800   |
---------------------------------
Progress update from timestep 286900: Updated shield, loss is 0.03384625166654587
Progress update from timestep 287000: Updated shield, loss is 0.04299001768231392
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 287000: Saved shield_loss.csv. 
Eval num_timesteps=287000, episode_reward=18.35 +/- 14.77
Episode length: 27.00 +/- 20.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 27            |
|    mean_reward          | 18.3          |
| time/                   |               |
|    total_timesteps      | 287000        |
| train/                  |               |
|    approx_kl            | 0.00041841547 |
|    clip_fraction        | 0.00781       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0224       |
|    explained_variance   | 0.0893        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.91          |
|    n_updates            | 7170          |
|    policy_gradient_loss | -0.0012       |
|    value_loss           | 2.37          |
-------------------------------------------
Progress update from timestep 287100: Updated shield, loss is 0.04238438233733177
Progress update from timestep 287200: Updated shield, loss is 0.04576488584280014
Eval num_timesteps=287200, episode_reward=12.66 +/- 11.80
Episode length: 18.80 +/- 16.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.8     |
|    mean_reward     | 12.7     |
| time/              |          |
|    total_timesteps | 287200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.86     |
|    ep_rew_mean     | 2.35     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 718      |
|    time_elapsed    | 63642    |
|    total_timesteps | 287200   |
---------------------------------
Progress update from timestep 287300: Updated shield, loss is 0.04911744222044945
Progress update from timestep 287400: Updated shield, loss is 0.040388815104961395
Eval num_timesteps=287400, episode_reward=16.83 +/- 12.70
Episode length: 24.20 +/- 16.93
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.2          |
|    mean_reward          | 16.8          |
| time/                   |               |
|    total_timesteps      | 287400        |
| train/                  |               |
|    approx_kl            | 0.00013319799 |
|    clip_fraction        | 0.00536       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.023        |
|    explained_variance   | 0.0229        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.04          |
|    n_updates            | 7180          |
|    policy_gradient_loss | -0.00102      |
|    value_loss           | 1.95          |
-------------------------------------------
Progress update from timestep 287500: Updated shield, loss is 0.04509499669075012
Progress update from timestep 287600: Updated shield, loss is 0.04171448573470116
Eval num_timesteps=287600, episode_reward=10.93 +/- 11.84
Episode length: 16.60 +/- 16.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | 10.9     |
| time/              |          |
|    total_timesteps | 287600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.87     |
|    ep_rew_mean     | 2.39     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 719      |
|    time_elapsed    | 63735    |
|    total_timesteps | 287600   |
---------------------------------
Progress update from timestep 287700: Updated shield, loss is 0.04773611202836037
Progress update from timestep 287800: Updated shield, loss is 0.04466142877936363
Eval num_timesteps=287800, episode_reward=12.52 +/- 11.02
Episode length: 18.80 +/- 15.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.8         |
|    mean_reward          | 12.5         |
| time/                   |              |
|    total_timesteps      | 287800       |
| train/                  |              |
|    approx_kl            | 7.584621e-05 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00749     |
|    explained_variance   | 0.195        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.468        |
|    n_updates            | 7190         |
|    policy_gradient_loss | -0.000373    |
|    value_loss           | 1.48         |
------------------------------------------
Progress update from timestep 287900: Updated shield, loss is 0.053570706397295
Progress update from timestep 288000: Updated shield, loss is 0.05031660944223404
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 288000: Saved shield_loss.csv. 
Eval num_timesteps=288000, episode_reward=15.01 +/- 11.82
Episode length: 22.20 +/- 16.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 15       |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.98     |
|    ep_rew_mean     | 2.47     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 720      |
|    time_elapsed    | 63834    |
|    total_timesteps | 288000   |
---------------------------------
Progress update from timestep 288100: Updated shield, loss is 0.04706254601478577
Progress update from timestep 288200: Updated shield, loss is 0.0503007173538208
Eval num_timesteps=288200, episode_reward=9.25 +/- 12.73
Episode length: 14.20 +/- 18.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.2          |
|    mean_reward          | 9.25          |
| time/                   |               |
|    total_timesteps      | 288200        |
| train/                  |               |
|    approx_kl            | 3.4903776e-05 |
|    clip_fraction        | 0.00134       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0118       |
|    explained_variance   | 0.148         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.853         |
|    n_updates            | 7200          |
|    policy_gradient_loss | -0.000308     |
|    value_loss           | 1.34          |
-------------------------------------------
Progress update from timestep 288300: Updated shield, loss is 0.047883257269859314
Progress update from timestep 288400: Updated shield, loss is 0.0432201623916626
Eval num_timesteps=288400, episode_reward=23.00 +/- 15.00
Episode length: 33.00 +/- 20.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 23       |
| time/              |          |
|    total_timesteps | 288400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.83     |
|    ep_rew_mean     | 3.12     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 721      |
|    time_elapsed    | 63946    |
|    total_timesteps | 288400   |
---------------------------------
Progress update from timestep 288500: Updated shield, loss is 0.042446210980415344
Progress update from timestep 288600: Updated shield, loss is 0.04188738763332367
Eval num_timesteps=288600, episode_reward=29.91 +/- 10.85
Episode length: 42.60 +/- 14.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.6          |
|    mean_reward          | 29.9          |
| time/                   |               |
|    total_timesteps      | 288600        |
| train/                  |               |
|    approx_kl            | 0.00023116797 |
|    clip_fraction        | 0.00804       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0254       |
|    explained_variance   | 0.118         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.846         |
|    n_updates            | 7210          |
|    policy_gradient_loss | -0.0011       |
|    value_loss           | 2.98          |
-------------------------------------------
Progress update from timestep 288700: Updated shield, loss is 0.04671574756503105
Progress update from timestep 288800: Updated shield, loss is 0.03782196715474129
Eval num_timesteps=288800, episode_reward=23.10 +/- 13.54
Episode length: 34.20 +/- 19.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 23.1     |
| time/              |          |
|    total_timesteps | 288800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.13     |
|    ep_rew_mean     | 2.56     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 722      |
|    time_elapsed    | 64064    |
|    total_timesteps | 288800   |
---------------------------------
Progress update from timestep 288900: Updated shield, loss is 0.04249357804656029
Progress update from timestep 289000: Updated shield, loss is 0.033945441246032715
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 289000: Saved shield_loss.csv. 
Eval num_timesteps=289000, episode_reward=16.81 +/- 15.87
Episode length: 23.80 +/- 21.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.8         |
|    mean_reward          | 16.8         |
| time/                   |              |
|    total_timesteps      | 289000       |
| train/                  |              |
|    approx_kl            | 6.571256e-05 |
|    clip_fraction        | 0.00179      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0142      |
|    explained_variance   | 0.0774       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.569        |
|    n_updates            | 7220         |
|    policy_gradient_loss | -0.000114    |
|    value_loss           | 1.87         |
------------------------------------------
Progress update from timestep 289100: Updated shield, loss is 0.052433788776397705
Progress update from timestep 289200: Updated shield, loss is 0.04010036587715149
Eval num_timesteps=289200, episode_reward=6.21 +/- 2.18
Episode length: 9.40 +/- 3.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.4      |
|    mean_reward     | 6.21     |
| time/              |          |
|    total_timesteps | 289200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.75     |
|    ep_rew_mean     | 2.31     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 723      |
|    time_elapsed    | 64143    |
|    total_timesteps | 289200   |
---------------------------------
Progress update from timestep 289300: Updated shield, loss is 0.04121199995279312
Progress update from timestep 289400: Updated shield, loss is 0.03913874924182892
Eval num_timesteps=289400, episode_reward=17.87 +/- 14.48
Episode length: 25.80 +/- 19.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.8          |
|    mean_reward          | 17.9          |
| time/                   |               |
|    total_timesteps      | 289400        |
| train/                  |               |
|    approx_kl            | 6.4346495e-05 |
|    clip_fraction        | 0.00223       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0111       |
|    explained_variance   | 0.0748        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.695         |
|    n_updates            | 7230          |
|    policy_gradient_loss | -0.000161     |
|    value_loss           | 1.52          |
-------------------------------------------
Progress update from timestep 289500: Updated shield, loss is 0.04485715180635452
Progress update from timestep 289600: Updated shield, loss is 0.04287314414978027
Eval num_timesteps=289600, episode_reward=24.82 +/- 12.35
Episode length: 36.40 +/- 17.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 24.8     |
| time/              |          |
|    total_timesteps | 289600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.84     |
|    ep_rew_mean     | 2.32     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 724      |
|    time_elapsed    | 64245    |
|    total_timesteps | 289600   |
---------------------------------
Progress update from timestep 289700: Updated shield, loss is 0.04975903034210205
Progress update from timestep 289800: Updated shield, loss is 0.03876439109444618
Eval num_timesteps=289800, episode_reward=23.24 +/- 13.85
Episode length: 34.00 +/- 19.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34           |
|    mean_reward          | 23.2         |
| time/                   |              |
|    total_timesteps      | 289800       |
| train/                  |              |
|    approx_kl            | 7.535813e-12 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0015      |
|    explained_variance   | 0.0973       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.58         |
|    n_updates            | 7240         |
|    policy_gradient_loss | -3.51e-07    |
|    value_loss           | 1.91         |
------------------------------------------
Progress update from timestep 289900: Updated shield, loss is 0.04127741977572441
Progress update from timestep 290000: Updated shield, loss is 0.04604610428214073
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 290000: Saved shield_loss.csv. 
Eval num_timesteps=290000, episode_reward=17.11 +/- 14.37
Episode length: 25.20 +/- 20.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 17.1     |
| time/              |          |
|    total_timesteps | 290000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.06     |
|    ep_rew_mean     | 2.49     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 725      |
|    time_elapsed    | 64344    |
|    total_timesteps | 290000   |
---------------------------------
Progress update from timestep 290100: Updated shield, loss is 0.037981677800416946
Progress update from timestep 290200: Updated shield, loss is 0.04352691397070885
Eval num_timesteps=290200, episode_reward=9.95 +/- 13.45
Episode length: 14.40 +/- 17.91
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.4          |
|    mean_reward          | 9.95          |
| time/                   |               |
|    total_timesteps      | 290200        |
| train/                  |               |
|    approx_kl            | 0.00031588465 |
|    clip_fraction        | 0.0121        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0345       |
|    explained_variance   | 0.217         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.726         |
|    n_updates            | 7250          |
|    policy_gradient_loss | -0.000924     |
|    value_loss           | 1.38          |
-------------------------------------------
Progress update from timestep 290300: Updated shield, loss is 0.045235525816679
Progress update from timestep 290400: Updated shield, loss is 0.03749733790755272
Eval num_timesteps=290400, episode_reward=21.28 +/- 16.18
Episode length: 31.20 +/- 23.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.2     |
|    mean_reward     | 21.3     |
| time/              |          |
|    total_timesteps | 290400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.33     |
|    ep_rew_mean     | 2.65     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 726      |
|    time_elapsed    | 64435    |
|    total_timesteps | 290400   |
---------------------------------
Progress update from timestep 290500: Updated shield, loss is 0.04436367377638817
Progress update from timestep 290600: Updated shield, loss is 0.04427724331617355
Eval num_timesteps=290600, episode_reward=11.55 +/- 12.26
Episode length: 17.00 +/- 16.86
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17            |
|    mean_reward          | 11.5          |
| time/                   |               |
|    total_timesteps      | 290600        |
| train/                  |               |
|    approx_kl            | 0.00016105185 |
|    clip_fraction        | 0.00491       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0234       |
|    explained_variance   | 0.145         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.727         |
|    n_updates            | 7260          |
|    policy_gradient_loss | -0.000758     |
|    value_loss           | 1.86          |
-------------------------------------------
Progress update from timestep 290700: Updated shield, loss is 0.04527231305837631
Progress update from timestep 290800: Updated shield, loss is 0.03978214040398598
Eval num_timesteps=290800, episode_reward=24.08 +/- 14.56
Episode length: 34.00 +/- 19.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 24.1     |
| time/              |          |
|    total_timesteps | 290800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.76     |
|    ep_rew_mean     | 2.38     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 727      |
|    time_elapsed    | 64550    |
|    total_timesteps | 290800   |
---------------------------------
Progress update from timestep 290900: Updated shield, loss is 0.038296107202768326
Progress update from timestep 291000: Updated shield, loss is 0.035656340420246124
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 291000: Saved shield_loss.csv. 
Eval num_timesteps=291000, episode_reward=15.59 +/- 16.42
Episode length: 22.40 +/- 22.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.4          |
|    mean_reward          | 15.6          |
| time/                   |               |
|    total_timesteps      | 291000        |
| train/                  |               |
|    approx_kl            | 0.00022810676 |
|    clip_fraction        | 0.00335       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00649      |
|    explained_variance   | 0.0764        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.795         |
|    n_updates            | 7270          |
|    policy_gradient_loss | -0.000261     |
|    value_loss           | 2.16          |
-------------------------------------------
Progress update from timestep 291100: Updated shield, loss is 0.04099242761731148
Progress update from timestep 291200: Updated shield, loss is 0.04264449328184128
Eval num_timesteps=291200, episode_reward=15.68 +/- 16.72
Episode length: 22.20 +/- 22.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 15.7     |
| time/              |          |
|    total_timesteps | 291200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.39     |
|    ep_rew_mean     | 2.81     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 728      |
|    time_elapsed    | 64641    |
|    total_timesteps | 291200   |
---------------------------------
Progress update from timestep 291300: Updated shield, loss is 0.039653755724430084
Progress update from timestep 291400: Updated shield, loss is 0.0369250550866127
Eval num_timesteps=291400, episode_reward=17.07 +/- 15.92
Episode length: 24.20 +/- 21.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.2         |
|    mean_reward          | 17.1         |
| time/                   |              |
|    total_timesteps      | 291400       |
| train/                  |              |
|    approx_kl            | 4.023643e-05 |
|    clip_fraction        | 0.00268      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00709     |
|    explained_variance   | 0.115        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.72         |
|    n_updates            | 7280         |
|    policy_gradient_loss | -0.000402    |
|    value_loss           | 2.04         |
------------------------------------------
Progress update from timestep 291500: Updated shield, loss is 0.03934381902217865
Progress update from timestep 291600: Updated shield, loss is 0.04921422153711319
Eval num_timesteps=291600, episode_reward=28.40 +/- 13.84
Episode length: 40.40 +/- 19.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.4     |
|    mean_reward     | 28.4     |
| time/              |          |
|    total_timesteps | 291600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.7      |
|    ep_rew_mean     | 2.3      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 729      |
|    time_elapsed    | 64753    |
|    total_timesteps | 291600   |
---------------------------------
Progress update from timestep 291700: Updated shield, loss is 0.046391990035772324
Progress update from timestep 291800: Updated shield, loss is 0.04185190796852112
Eval num_timesteps=291800, episode_reward=15.72 +/- 14.44
Episode length: 24.00 +/- 21.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24            |
|    mean_reward          | 15.7          |
| time/                   |               |
|    total_timesteps      | 291800        |
| train/                  |               |
|    approx_kl            | 0.00034387477 |
|    clip_fraction        | 0.0114        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0317       |
|    explained_variance   | 0.152         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.441         |
|    n_updates            | 7290          |
|    policy_gradient_loss | -0.00123      |
|    value_loss           | 1.32          |
-------------------------------------------
Progress update from timestep 291900: Updated shield, loss is 0.04878651350736618
Progress update from timestep 292000: Updated shield, loss is 0.04561310261487961
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 292000: Saved shield_loss.csv. 
Eval num_timesteps=292000, episode_reward=16.11 +/- 15.04
Episode length: 24.00 +/- 21.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16.1     |
| time/              |          |
|    total_timesteps | 292000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.37     |
|    ep_rew_mean     | 2.78     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 730      |
|    time_elapsed    | 64852    |
|    total_timesteps | 292000   |
---------------------------------
Progress update from timestep 292100: Updated shield, loss is 0.052192967385053635
Progress update from timestep 292200: Updated shield, loss is 0.052090588957071304
Eval num_timesteps=292200, episode_reward=11.08 +/- 13.22
Episode length: 16.20 +/- 17.55
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.2          |
|    mean_reward          | 11.1          |
| time/                   |               |
|    total_timesteps      | 292200        |
| train/                  |               |
|    approx_kl            | 0.00014580566 |
|    clip_fraction        | 0.00759       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0316       |
|    explained_variance   | 0.128         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.03          |
|    n_updates            | 7300          |
|    policy_gradient_loss | -0.00107      |
|    value_loss           | 1.68          |
-------------------------------------------
Progress update from timestep 292300: Updated shield, loss is 0.04343102127313614
Progress update from timestep 292400: Updated shield, loss is 0.04346510022878647
Eval num_timesteps=292400, episode_reward=24.25 +/- 13.46
Episode length: 35.20 +/- 18.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 24.3     |
| time/              |          |
|    total_timesteps | 292400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.94     |
|    ep_rew_mean     | 2.48     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 731      |
|    time_elapsed    | 64964    |
|    total_timesteps | 292400   |
---------------------------------
Progress update from timestep 292500: Updated shield, loss is 0.03435882553458214
Progress update from timestep 292600: Updated shield, loss is 0.048572346568107605
Eval num_timesteps=292600, episode_reward=10.56 +/- 13.26
Episode length: 15.20 +/- 17.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.2          |
|    mean_reward          | 10.6          |
| time/                   |               |
|    total_timesteps      | 292600        |
| train/                  |               |
|    approx_kl            | 0.00020707617 |
|    clip_fraction        | 0.00513       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0168       |
|    explained_variance   | 0.227         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.715         |
|    n_updates            | 7310          |
|    policy_gradient_loss | -0.000987     |
|    value_loss           | 1.14          |
-------------------------------------------
Progress update from timestep 292700: Updated shield, loss is 0.045294858515262604
Progress update from timestep 292800: Updated shield, loss is 0.03496278077363968
Eval num_timesteps=292800, episode_reward=11.26 +/- 12.27
Episode length: 16.80 +/- 16.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | 11.3     |
| time/              |          |
|    total_timesteps | 292800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.3      |
|    ep_rew_mean     | 2.69     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 732      |
|    time_elapsed    | 65055    |
|    total_timesteps | 292800   |
---------------------------------
Progress update from timestep 292900: Updated shield, loss is 0.040955353528261185
Progress update from timestep 293000: Updated shield, loss is 0.04822959750890732
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 293000: Saved shield_loss.csv. 
Eval num_timesteps=293000, episode_reward=4.58 +/- 2.35
Episode length: 7.40 +/- 3.38
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 7.4           |
|    mean_reward          | 4.58          |
| time/                   |               |
|    total_timesteps      | 293000        |
| train/                  |               |
|    approx_kl            | 7.6933815e-05 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0203       |
|    explained_variance   | 0.113         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.845         |
|    n_updates            | 7320          |
|    policy_gradient_loss | -0.000748     |
|    value_loss           | 2.13          |
-------------------------------------------
Progress update from timestep 293100: Updated shield, loss is 0.040311332792043686
Progress update from timestep 293200: Updated shield, loss is 0.03987467288970947
Eval num_timesteps=293200, episode_reward=15.84 +/- 11.43
Episode length: 23.60 +/- 16.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 15.8     |
| time/              |          |
|    total_timesteps | 293200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.07     |
|    ep_rew_mean     | 2.56     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 733      |
|    time_elapsed    | 65130    |
|    total_timesteps | 293200   |
---------------------------------
Progress update from timestep 293300: Updated shield, loss is 0.041987258940935135
Progress update from timestep 293400: Updated shield, loss is 0.04852771759033203
Eval num_timesteps=293400, episode_reward=28.26 +/- 13.05
Episode length: 40.80 +/- 18.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 40.8         |
|    mean_reward          | 28.3         |
| time/                   |              |
|    total_timesteps      | 293400       |
| train/                  |              |
|    approx_kl            | 6.210721e-05 |
|    clip_fraction        | 0.00179      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.013       |
|    explained_variance   | 0.125        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.585        |
|    n_updates            | 7330         |
|    policy_gradient_loss | -0.000504    |
|    value_loss           | 1.94         |
------------------------------------------
Progress update from timestep 293500: Updated shield, loss is 0.04741045832633972
Progress update from timestep 293600: Updated shield, loss is 0.044988829642534256
Eval num_timesteps=293600, episode_reward=28.12 +/- 12.75
Episode length: 41.00 +/- 18.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41       |
|    mean_reward     | 28.1     |
| time/              |          |
|    total_timesteps | 293600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.52     |
|    ep_rew_mean     | 2.85     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 734      |
|    time_elapsed    | 65265    |
|    total_timesteps | 293600   |
---------------------------------
Progress update from timestep 293700: Updated shield, loss is 0.04331289231777191
Progress update from timestep 293800: Updated shield, loss is 0.043864697217941284
Eval num_timesteps=293800, episode_reward=23.03 +/- 14.47
Episode length: 33.40 +/- 20.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.4          |
|    mean_reward          | 23            |
| time/                   |               |
|    total_timesteps      | 293800        |
| train/                  |               |
|    approx_kl            | 0.00014187828 |
|    clip_fraction        | 0.00871       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0133       |
|    explained_variance   | 0.109         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.611         |
|    n_updates            | 7340          |
|    policy_gradient_loss | -0.000978     |
|    value_loss           | 1.93          |
-------------------------------------------
Progress update from timestep 293900: Updated shield, loss is 0.035968370735645294
Progress update from timestep 294000: Updated shield, loss is 0.05182728171348572
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 294000: Saved shield_loss.csv. 
Eval num_timesteps=294000, episode_reward=23.48 +/- 13.50
Episode length: 34.40 +/- 19.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 23.5     |
| time/              |          |
|    total_timesteps | 294000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.19     |
|    ep_rew_mean     | 2.62     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 735      |
|    time_elapsed    | 65384    |
|    total_timesteps | 294000   |
---------------------------------
Progress update from timestep 294100: Updated shield, loss is 0.045243509113788605
Progress update from timestep 294200: Updated shield, loss is 0.049952421337366104
Eval num_timesteps=294200, episode_reward=4.03 +/- 3.16
Episode length: 6.80 +/- 4.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 6.8          |
|    mean_reward          | 4.03         |
| time/                   |              |
|    total_timesteps      | 294200       |
| train/                  |              |
|    approx_kl            | 0.0001087627 |
|    clip_fraction        | 0.00335      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00767     |
|    explained_variance   | 0.141        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.765        |
|    n_updates            | 7350         |
|    policy_gradient_loss | -0.000469    |
|    value_loss           | 1.24         |
------------------------------------------
Progress update from timestep 294300: Updated shield, loss is 0.04203002154827118
Progress update from timestep 294400: Updated shield, loss is 0.039351992309093475
Eval num_timesteps=294400, episode_reward=30.03 +/- 11.67
Episode length: 42.20 +/- 15.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 30       |
| time/              |          |
|    total_timesteps | 294400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.5      |
|    ep_rew_mean     | 2.91     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 736      |
|    time_elapsed    | 65481    |
|    total_timesteps | 294400   |
---------------------------------
Progress update from timestep 294500: Updated shield, loss is 0.0425691083073616
Progress update from timestep 294600: Updated shield, loss is 0.051164280623197556
Eval num_timesteps=294600, episode_reward=7.06 +/- 2.35
Episode length: 11.00 +/- 3.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 11            |
|    mean_reward          | 7.06          |
| time/                   |               |
|    total_timesteps      | 294600        |
| train/                  |               |
|    approx_kl            | 0.00010217038 |
|    clip_fraction        | 0.0067        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0132       |
|    explained_variance   | 0.195         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.488         |
|    n_updates            | 7360          |
|    policy_gradient_loss | -0.000749     |
|    value_loss           | 2.02          |
-------------------------------------------
Progress update from timestep 294700: Updated shield, loss is 0.04014906659722328
Progress update from timestep 294800: Updated shield, loss is 0.03710780292749405
Eval num_timesteps=294800, episode_reward=2.64 +/- 1.61
Episode length: 4.60 +/- 2.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.6      |
|    mean_reward     | 2.64     |
| time/              |          |
|    total_timesteps | 294800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.28     |
|    ep_rew_mean     | 2.74     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 737      |
|    time_elapsed    | 65544    |
|    total_timesteps | 294800   |
---------------------------------
Progress update from timestep 294900: Updated shield, loss is 0.036268748342990875
Progress update from timestep 295000: Updated shield, loss is 0.04134124517440796
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 295000: Saved shield_loss.csv. 
Eval num_timesteps=295000, episode_reward=16.65 +/- 14.79
Episode length: 24.80 +/- 20.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.8          |
|    mean_reward          | 16.6          |
| time/                   |               |
|    total_timesteps      | 295000        |
| train/                  |               |
|    approx_kl            | 4.0082732e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00142      |
|    explained_variance   | 0.253         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.11          |
|    n_updates            | 7370          |
|    policy_gradient_loss | -3.56e-07     |
|    value_loss           | 2.39          |
-------------------------------------------
Progress update from timestep 295100: Updated shield, loss is 0.04076284542679787
Progress update from timestep 295200: Updated shield, loss is 0.04108327627182007
Eval num_timesteps=295200, episode_reward=15.67 +/- 15.86
Episode length: 23.00 +/- 22.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 15.7     |
| time/              |          |
|    total_timesteps | 295200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.3      |
|    ep_rew_mean     | 2.68     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 738      |
|    time_elapsed    | 65637    |
|    total_timesteps | 295200   |
---------------------------------
Progress update from timestep 295300: Updated shield, loss is 0.031869858503341675
Progress update from timestep 295400: Updated shield, loss is 0.036783117800951004
Eval num_timesteps=295400, episode_reward=18.82 +/- 14.02
Episode length: 27.20 +/- 19.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 27.2         |
|    mean_reward          | 18.8         |
| time/                   |              |
|    total_timesteps      | 295400       |
| train/                  |              |
|    approx_kl            | 0.0006288696 |
|    clip_fraction        | 0.0221       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0399      |
|    explained_variance   | 0.373        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.767        |
|    n_updates            | 7380         |
|    policy_gradient_loss | -0.00217     |
|    value_loss           | 2.16         |
------------------------------------------
Progress update from timestep 295500: Updated shield, loss is 0.044889796525239944
Progress update from timestep 295600: Updated shield, loss is 0.04504551365971565
Eval num_timesteps=295600, episode_reward=22.73 +/- 16.27
Episode length: 32.00 +/- 22.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32       |
|    mean_reward     | 22.7     |
| time/              |          |
|    total_timesteps | 295600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.92     |
|    ep_rew_mean     | 2.46     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 739      |
|    time_elapsed    | 65754    |
|    total_timesteps | 295600   |
---------------------------------
Progress update from timestep 295700: Updated shield, loss is 0.040619172155857086
Progress update from timestep 295800: Updated shield, loss is 0.03963835537433624
Eval num_timesteps=295800, episode_reward=23.53 +/- 13.87
Episode length: 34.60 +/- 19.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.6         |
|    mean_reward          | 23.5         |
| time/                   |              |
|    total_timesteps      | 295800       |
| train/                  |              |
|    approx_kl            | 0.0002019652 |
|    clip_fraction        | 0.00781      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0314      |
|    explained_variance   | 0.115        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.948        |
|    n_updates            | 7390         |
|    policy_gradient_loss | -0.00123     |
|    value_loss           | 2.2          |
------------------------------------------
Progress update from timestep 295900: Updated shield, loss is 0.03191711753606796
Progress update from timestep 296000: Updated shield, loss is 0.04272155091166496
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 296000: Saved shield_loss.csv. 
Eval num_timesteps=296000, episode_reward=22.16 +/- 15.14
Episode length: 32.40 +/- 21.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.4     |
|    mean_reward     | 22.2     |
| time/              |          |
|    total_timesteps | 296000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.98     |
|    ep_rew_mean     | 2.49     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 740      |
|    time_elapsed    | 65855    |
|    total_timesteps | 296000   |
---------------------------------
Progress update from timestep 296100: Updated shield, loss is 0.03893914818763733
Progress update from timestep 296200: Updated shield, loss is 0.038871366530656815
Eval num_timesteps=296200, episode_reward=23.25 +/- 14.23
Episode length: 33.80 +/- 19.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.8         |
|    mean_reward          | 23.3         |
| time/                   |              |
|    total_timesteps      | 296200       |
| train/                  |              |
|    approx_kl            | 1.362683e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00132     |
|    explained_variance   | 0.0766       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.59         |
|    n_updates            | 7400         |
|    policy_gradient_loss | 1.59e-06     |
|    value_loss           | 1.36         |
------------------------------------------
Progress update from timestep 296300: Updated shield, loss is 0.03927633538842201
Progress update from timestep 296400: Updated shield, loss is 0.04709459841251373
Eval num_timesteps=296400, episode_reward=10.08 +/- 12.24
Episode length: 15.40 +/- 17.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | 10.1     |
| time/              |          |
|    total_timesteps | 296400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.54     |
|    ep_rew_mean     | 2.86     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 741      |
|    time_elapsed    | 65949    |
|    total_timesteps | 296400   |
---------------------------------
Progress update from timestep 296500: Updated shield, loss is 0.039891816675662994
Progress update from timestep 296600: Updated shield, loss is 0.04209532588720322
Eval num_timesteps=296600, episode_reward=29.60 +/- 10.86
Episode length: 42.40 +/- 15.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.4         |
|    mean_reward          | 29.6         |
| time/                   |              |
|    total_timesteps      | 296600       |
| train/                  |              |
|    approx_kl            | 4.380596e-05 |
|    clip_fraction        | 0.00134      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00707     |
|    explained_variance   | 0.136        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.721        |
|    n_updates            | 7410         |
|    policy_gradient_loss | -0.000192    |
|    value_loss           | 1.9          |
------------------------------------------
Progress update from timestep 296700: Updated shield, loss is 0.03776099532842636
Progress update from timestep 296800: Updated shield, loss is 0.044880468398332596
Eval num_timesteps=296800, episode_reward=24.87 +/- 13.50
Episode length: 35.40 +/- 18.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 24.9     |
| time/              |          |
|    total_timesteps | 296800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.67     |
|    ep_rew_mean     | 2.25     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 742      |
|    time_elapsed    | 66062    |
|    total_timesteps | 296800   |
---------------------------------
Progress update from timestep 296900: Updated shield, loss is 0.03983553126454353
Progress update from timestep 297000: Updated shield, loss is 0.03595273196697235
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 297000: Saved shield_loss.csv. 
Eval num_timesteps=297000, episode_reward=16.50 +/- 15.59
Episode length: 23.80 +/- 21.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.8          |
|    mean_reward          | 16.5          |
| time/                   |               |
|    total_timesteps      | 297000        |
| train/                  |               |
|    approx_kl            | 0.00034415416 |
|    clip_fraction        | 0.00915       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0308       |
|    explained_variance   | 0.156         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.507         |
|    n_updates            | 7420          |
|    policy_gradient_loss | -0.00138      |
|    value_loss           | 1.36          |
-------------------------------------------
Progress update from timestep 297100: Updated shield, loss is 0.041445232927799225
Progress update from timestep 297200: Updated shield, loss is 0.04479563981294632
Eval num_timesteps=297200, episode_reward=19.75 +/- 12.59
Episode length: 29.20 +/- 17.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.2     |
|    mean_reward     | 19.7     |
| time/              |          |
|    total_timesteps | 297200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.39     |
|    ep_rew_mean     | 2.79     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 743      |
|    time_elapsed    | 66152    |
|    total_timesteps | 297200   |
---------------------------------
Progress update from timestep 297300: Updated shield, loss is 0.041467443108558655
Progress update from timestep 297400: Updated shield, loss is 0.040810082107782364
Eval num_timesteps=297400, episode_reward=7.32 +/- 3.70
Episode length: 11.40 +/- 5.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 7.32          |
| time/                   |               |
|    total_timesteps      | 297400        |
| train/                  |               |
|    approx_kl            | 0.00047053845 |
|    clip_fraction        | 0.0109        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0144       |
|    explained_variance   | 0.256         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.54          |
|    n_updates            | 7430          |
|    policy_gradient_loss | -0.00161      |
|    value_loss           | 1.18          |
-------------------------------------------
Progress update from timestep 297500: Updated shield, loss is 0.04827573522925377
Progress update from timestep 297600: Updated shield, loss is 0.04169377312064171
Eval num_timesteps=297600, episode_reward=29.17 +/- 12.85
Episode length: 41.20 +/- 17.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | 29.2     |
| time/              |          |
|    total_timesteps | 297600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.99     |
|    ep_rew_mean     | 2.44     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 744      |
|    time_elapsed    | 66233    |
|    total_timesteps | 297600   |
---------------------------------
Progress update from timestep 297700: Updated shield, loss is 0.03195028007030487
Progress update from timestep 297800: Updated shield, loss is 0.03752214461565018
Eval num_timesteps=297800, episode_reward=17.75 +/- 13.31
Episode length: 26.60 +/- 19.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.6         |
|    mean_reward          | 17.8         |
| time/                   |              |
|    total_timesteps      | 297800       |
| train/                  |              |
|    approx_kl            | 4.331884e-05 |
|    clip_fraction        | 0.00134      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00759     |
|    explained_variance   | 0.214        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.57         |
|    n_updates            | 7440         |
|    policy_gradient_loss | -0.000181    |
|    value_loss           | 1.65         |
------------------------------------------
Progress update from timestep 297900: Updated shield, loss is 0.05170543119311333
Progress update from timestep 298000: Updated shield, loss is 0.04722827672958374
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 298000: Saved shield_loss.csv. 
Eval num_timesteps=298000, episode_reward=10.08 +/- 12.09
Episode length: 15.80 +/- 17.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | 10.1     |
| time/              |          |
|    total_timesteps | 298000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.79     |
|    ep_rew_mean     | 3.11     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 745      |
|    time_elapsed    | 66329    |
|    total_timesteps | 298000   |
---------------------------------
Progress update from timestep 298100: Updated shield, loss is 0.05028814449906349
Progress update from timestep 298200: Updated shield, loss is 0.03857308253645897
Eval num_timesteps=298200, episode_reward=9.92 +/- 12.94
Episode length: 14.80 +/- 17.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.8          |
|    mean_reward          | 9.92          |
| time/                   |               |
|    total_timesteps      | 298200        |
| train/                  |               |
|    approx_kl            | 9.2473965e-05 |
|    clip_fraction        | 0.00313       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00633      |
|    explained_variance   | 0.385         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.879         |
|    n_updates            | 7450          |
|    policy_gradient_loss | -0.000546     |
|    value_loss           | 2.12          |
-------------------------------------------
Progress update from timestep 298300: Updated shield, loss is 0.04545595124363899
Progress update from timestep 298400: Updated shield, loss is 0.046089187264442444
Eval num_timesteps=298400, episode_reward=24.17 +/- 15.33
Episode length: 33.40 +/- 20.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 24.2     |
| time/              |          |
|    total_timesteps | 298400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.38     |
|    ep_rew_mean     | 2.76     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 746      |
|    time_elapsed    | 66427    |
|    total_timesteps | 298400   |
---------------------------------
Progress update from timestep 298500: Updated shield, loss is 0.05386869236826897
Progress update from timestep 298600: Updated shield, loss is 0.03859804570674896
Eval num_timesteps=298600, episode_reward=10.17 +/- 11.83
Episode length: 16.00 +/- 17.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16            |
|    mean_reward          | 10.2          |
| time/                   |               |
|    total_timesteps      | 298600        |
| train/                  |               |
|    approx_kl            | 2.2849776e-05 |
|    clip_fraction        | 0.00067       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0198       |
|    explained_variance   | 0.174         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.607         |
|    n_updates            | 7460          |
|    policy_gradient_loss | -0.000269     |
|    value_loss           | 1.69          |
-------------------------------------------
Progress update from timestep 298700: Updated shield, loss is 0.04017262905836105
Progress update from timestep 298800: Updated shield, loss is 0.04717111587524414
Eval num_timesteps=298800, episode_reward=11.44 +/- 11.68
Episode length: 17.40 +/- 16.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 11.4     |
| time/              |          |
|    total_timesteps | 298800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.44     |
|    ep_rew_mean     | 2.84     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 747      |
|    time_elapsed    | 66517    |
|    total_timesteps | 298800   |
---------------------------------
Progress update from timestep 298900: Updated shield, loss is 0.04050728306174278
Progress update from timestep 299000: Updated shield, loss is 0.04084315523505211
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 299000: Saved shield_loss.csv. 
Eval num_timesteps=299000, episode_reward=10.39 +/- 11.71
Episode length: 16.00 +/- 17.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16            |
|    mean_reward          | 10.4          |
| time/                   |               |
|    total_timesteps      | 299000        |
| train/                  |               |
|    approx_kl            | 0.00011340536 |
|    clip_fraction        | 0.00179       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0164       |
|    explained_variance   | 0.275         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.73          |
|    n_updates            | 7470          |
|    policy_gradient_loss | -0.001        |
|    value_loss           | 1.97          |
-------------------------------------------
Progress update from timestep 299100: Updated shield, loss is 0.042988553643226624
Progress update from timestep 299200: Updated shield, loss is 0.04263181611895561
Eval num_timesteps=299200, episode_reward=23.67 +/- 13.48
Episode length: 34.60 +/- 19.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 23.7     |
| time/              |          |
|    total_timesteps | 299200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.27     |
|    ep_rew_mean     | 2.72     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 748      |
|    time_elapsed    | 66610    |
|    total_timesteps | 299200   |
---------------------------------
Progress update from timestep 299300: Updated shield, loss is 0.046428509056568146
Progress update from timestep 299400: Updated shield, loss is 0.035893168300390244
Eval num_timesteps=299400, episode_reward=17.17 +/- 15.08
Episode length: 24.80 +/- 20.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.8          |
|    mean_reward          | 17.2          |
| time/                   |               |
|    total_timesteps      | 299400        |
| train/                  |               |
|    approx_kl            | 5.7209872e-05 |
|    clip_fraction        | 0.00223       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0133       |
|    explained_variance   | 0.0857        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.6           |
|    n_updates            | 7480          |
|    policy_gradient_loss | -0.000309     |
|    value_loss           | 2.53          |
-------------------------------------------
Progress update from timestep 299500: Updated shield, loss is 0.04584969952702522
Progress update from timestep 299600: Updated shield, loss is 0.0402010902762413
Eval num_timesteps=299600, episode_reward=13.53 +/- 11.74
Episode length: 20.20 +/- 16.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.2     |
|    mean_reward     | 13.5     |
| time/              |          |
|    total_timesteps | 299600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.67     |
|    ep_rew_mean     | 3.02     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 749      |
|    time_elapsed    | 66711    |
|    total_timesteps | 299600   |
---------------------------------
Progress update from timestep 299700: Updated shield, loss is 0.04241196811199188
Progress update from timestep 299800: Updated shield, loss is 0.04418293759226799
Eval num_timesteps=299800, episode_reward=10.57 +/- 12.30
Episode length: 16.00 +/- 17.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | 10.6        |
| time/                   |             |
|    total_timesteps      | 299800      |
| train/                  |             |
|    approx_kl            | 4.36159e-05 |
|    clip_fraction        | 0.00268     |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0178     |
|    explained_variance   | 0.118       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.01        |
|    n_updates            | 7490        |
|    policy_gradient_loss | -0.000468   |
|    value_loss           | 2.02        |
-----------------------------------------
Progress update from timestep 299900: Updated shield, loss is 0.04993078485131264
Progress update from timestep 300000: Updated shield, loss is 0.03693234920501709
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 300000: Saved shield_loss.csv. 
Eval num_timesteps=300000, episode_reward=29.14 +/- 12.95
Episode length: 41.20 +/- 17.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | 29.1     |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.11     |
|    ep_rew_mean     | 2.61     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 750      |
|    time_elapsed    | 66825    |
|    total_timesteps | 300000   |
---------------------------------
Progress update from timestep 300100: Updated shield, loss is 0.0375879742205143
Progress update from timestep 300200: Updated shield, loss is 0.03874468803405762
Eval num_timesteps=300200, episode_reward=11.31 +/- 12.88
Episode length: 16.60 +/- 17.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.6         |
|    mean_reward          | 11.3         |
| time/                   |              |
|    total_timesteps      | 300200       |
| train/                  |              |
|    approx_kl            | 0.0006692639 |
|    clip_fraction        | 0.0174       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0499      |
|    explained_variance   | 0.247        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.52         |
|    n_updates            | 7500         |
|    policy_gradient_loss | -0.0015      |
|    value_loss           | 1.63         |
------------------------------------------
Progress update from timestep 300300: Updated shield, loss is 0.03834544122219086
Progress update from timestep 300400: Updated shield, loss is 0.035689178854227066
Eval num_timesteps=300400, episode_reward=15.54 +/- 14.35
Episode length: 22.60 +/- 19.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 15.5     |
| time/              |          |
|    total_timesteps | 300400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.62     |
|    ep_rew_mean     | 2.93     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 751      |
|    time_elapsed    | 66923    |
|    total_timesteps | 300400   |
---------------------------------
Progress update from timestep 300500: Updated shield, loss is 0.03332340344786644
Progress update from timestep 300600: Updated shield, loss is 0.05398918688297272
Eval num_timesteps=300600, episode_reward=18.00 +/- 13.56
Episode length: 26.80 +/- 19.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.8         |
|    mean_reward          | 18           |
| time/                   |              |
|    total_timesteps      | 300600       |
| train/                  |              |
|    approx_kl            | 0.0003107589 |
|    clip_fraction        | 0.00893      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0183      |
|    explained_variance   | 0.218        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.02         |
|    n_updates            | 7510         |
|    policy_gradient_loss | -0.000695    |
|    value_loss           | 2.67         |
------------------------------------------
Progress update from timestep 300700: Updated shield, loss is 0.035800427198410034
Progress update from timestep 300800: Updated shield, loss is 0.037498462945222855
Eval num_timesteps=300800, episode_reward=17.39 +/- 15.48
Episode length: 24.60 +/- 20.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 300800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.23     |
|    ep_rew_mean     | 2.63     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 752      |
|    time_elapsed    | 67020    |
|    total_timesteps | 300800   |
---------------------------------
Progress update from timestep 300900: Updated shield, loss is 0.04233607277274132
Progress update from timestep 301000: Updated shield, loss is 0.03760237619280815
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 301000: Saved shield_loss.csv. 
Eval num_timesteps=301000, episode_reward=16.68 +/- 14.73
Episode length: 24.80 +/- 20.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.8         |
|    mean_reward          | 16.7         |
| time/                   |              |
|    total_timesteps      | 301000       |
| train/                  |              |
|    approx_kl            | 0.0005158605 |
|    clip_fraction        | 0.0116       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0142      |
|    explained_variance   | 0.195        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.834        |
|    n_updates            | 7520         |
|    policy_gradient_loss | -0.00104     |
|    value_loss           | 1.59         |
------------------------------------------
Progress update from timestep 301100: Updated shield, loss is 0.04309101030230522
Progress update from timestep 301200: Updated shield, loss is 0.0423433855175972
Eval num_timesteps=301200, episode_reward=23.12 +/- 16.19
Episode length: 32.20 +/- 21.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 23.1     |
| time/              |          |
|    total_timesteps | 301200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.8      |
|    ep_rew_mean     | 2.39     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 753      |
|    time_elapsed    | 67115    |
|    total_timesteps | 301200   |
---------------------------------
Progress update from timestep 301300: Updated shield, loss is 0.03677193075418472
Progress update from timestep 301400: Updated shield, loss is 0.037543028593063354
Eval num_timesteps=301400, episode_reward=11.73 +/- 11.99
Episode length: 17.20 +/- 16.49
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 17.2           |
|    mean_reward          | 11.7           |
| time/                   |                |
|    total_timesteps      | 301400         |
| train/                  |                |
|    approx_kl            | -1.1953359e-11 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.000766      |
|    explained_variance   | 0.0779         |
|    learning_rate        | 0.0001         |
|    loss                 | 0.465          |
|    n_updates            | 7530           |
|    policy_gradient_loss | -3.08e-07      |
|    value_loss           | 1.88           |
--------------------------------------------
Progress update from timestep 301500: Updated shield, loss is 0.039153505116701126
Progress update from timestep 301600: Updated shield, loss is 0.03339777886867523
Eval num_timesteps=301600, episode_reward=17.21 +/- 15.28
Episode length: 24.80 +/- 20.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 17.2     |
| time/              |          |
|    total_timesteps | 301600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.19     |
|    ep_rew_mean     | 2.69     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 754      |
|    time_elapsed    | 67211    |
|    total_timesteps | 301600   |
---------------------------------
Progress update from timestep 301700: Updated shield, loss is 0.04104528948664665
Progress update from timestep 301800: Updated shield, loss is 0.041921548545360565
Eval num_timesteps=301800, episode_reward=9.26 +/- 12.67
Episode length: 14.00 +/- 18.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14           |
|    mean_reward          | 9.26         |
| time/                   |              |
|    total_timesteps      | 301800       |
| train/                  |              |
|    approx_kl            | 0.0004041431 |
|    clip_fraction        | 0.0123       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0325      |
|    explained_variance   | 0.0636       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.367        |
|    n_updates            | 7540         |
|    policy_gradient_loss | -0.00164     |
|    value_loss           | 1.96         |
------------------------------------------
Progress update from timestep 301900: Updated shield, loss is 0.042246971279382706
Progress update from timestep 302000: Updated shield, loss is 0.03724908083677292
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 302000: Saved shield_loss.csv. 
Eval num_timesteps=302000, episode_reward=9.93 +/- 13.45
Episode length: 14.60 +/- 17.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 9.93     |
| time/              |          |
|    total_timesteps | 302000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.64     |
|    ep_rew_mean     | 2.99     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 755      |
|    time_elapsed    | 67303    |
|    total_timesteps | 302000   |
---------------------------------
Progress update from timestep 302100: Updated shield, loss is 0.04422978311777115
Progress update from timestep 302200: Updated shield, loss is 0.0416436493396759
Eval num_timesteps=302200, episode_reward=2.27 +/- 1.69
Episode length: 4.00 +/- 2.28
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 4             |
|    mean_reward          | 2.27          |
| time/                   |               |
|    total_timesteps      | 302200        |
| train/                  |               |
|    approx_kl            | 0.00011071657 |
|    clip_fraction        | 0.00357       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0112       |
|    explained_variance   | 0.347         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.22          |
|    n_updates            | 7550          |
|    policy_gradient_loss | -0.000432     |
|    value_loss           | 2.2           |
-------------------------------------------
Progress update from timestep 302300: Updated shield, loss is 0.050404272973537445
Progress update from timestep 302400: Updated shield, loss is 0.03867126628756523
Eval num_timesteps=302400, episode_reward=17.84 +/- 14.97
Episode length: 25.40 +/- 20.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | 17.8     |
| time/              |          |
|    total_timesteps | 302400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.36     |
|    ep_rew_mean     | 2.74     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 756      |
|    time_elapsed    | 67378    |
|    total_timesteps | 302400   |
---------------------------------
Progress update from timestep 302500: Updated shield, loss is 0.045103032141923904
Progress update from timestep 302600: Updated shield, loss is 0.0351770743727684
Eval num_timesteps=302600, episode_reward=13.26 +/- 11.69
Episode length: 19.20 +/- 16.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 19.2          |
|    mean_reward          | 13.3          |
| time/                   |               |
|    total_timesteps      | 302600        |
| train/                  |               |
|    approx_kl            | 5.7268328e-05 |
|    clip_fraction        | 0.00067       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0128       |
|    explained_variance   | 0.108         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.424         |
|    n_updates            | 7560          |
|    policy_gradient_loss | -0.000197     |
|    value_loss           | 1.88          |
-------------------------------------------
Progress update from timestep 302700: Updated shield, loss is 0.043066877871751785
Progress update from timestep 302800: Updated shield, loss is 0.03827619552612305
Eval num_timesteps=302800, episode_reward=10.81 +/- 12.05
Episode length: 16.40 +/- 17.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | 10.8     |
| time/              |          |
|    total_timesteps | 302800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.52     |
|    ep_rew_mean     | 2.88     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 757      |
|    time_elapsed    | 67474    |
|    total_timesteps | 302800   |
---------------------------------
Progress update from timestep 302900: Updated shield, loss is 0.037344399839639664
Progress update from timestep 303000: Updated shield, loss is 0.04078611731529236
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 303000: Saved shield_loss.csv. 
Eval num_timesteps=303000, episode_reward=29.38 +/- 10.82
Episode length: 42.40 +/- 15.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.4          |
|    mean_reward          | 29.4          |
| time/                   |               |
|    total_timesteps      | 303000        |
| train/                  |               |
|    approx_kl            | 0.00010244313 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00376      |
|    explained_variance   | 0.131         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.23          |
|    n_updates            | 7570          |
|    policy_gradient_loss | -0.000255     |
|    value_loss           | 2.29          |
-------------------------------------------
Progress update from timestep 303100: Updated shield, loss is 0.032394640147686005
Progress update from timestep 303200: Updated shield, loss is 0.039092399179935455
Eval num_timesteps=303200, episode_reward=28.47 +/- 11.48
Episode length: 41.80 +/- 16.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 28.5     |
| time/              |          |
|    total_timesteps | 303200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.06     |
|    ep_rew_mean     | 2.5      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 758      |
|    time_elapsed    | 67608    |
|    total_timesteps | 303200   |
---------------------------------
Progress update from timestep 303300: Updated shield, loss is 0.043088871985673904
Progress update from timestep 303400: Updated shield, loss is 0.03456345573067665
Eval num_timesteps=303400, episode_reward=14.93 +/- 12.12
Episode length: 22.00 +/- 16.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22            |
|    mean_reward          | 14.9          |
| time/                   |               |
|    total_timesteps      | 303400        |
| train/                  |               |
|    approx_kl            | 1.2517921e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00476      |
|    explained_variance   | 0.139         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.83          |
|    n_updates            | 7580          |
|    policy_gradient_loss | -1.16e-05     |
|    value_loss           | 1.7           |
-------------------------------------------
Progress update from timestep 303500: Updated shield, loss is 0.040101345628499985
Progress update from timestep 303600: Updated shield, loss is 0.031162424013018608
Eval num_timesteps=303600, episode_reward=7.67 +/- 4.19
Episode length: 11.80 +/- 5.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 11.8     |
|    mean_reward     | 7.67     |
| time/              |          |
|    total_timesteps | 303600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.22     |
|    ep_rew_mean     | 2.61     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 759      |
|    time_elapsed    | 67684    |
|    total_timesteps | 303600   |
---------------------------------
Progress update from timestep 303700: Updated shield, loss is 0.03760118782520294
Progress update from timestep 303800: Updated shield, loss is 0.033418744802474976
Eval num_timesteps=303800, episode_reward=18.50 +/- 13.57
Episode length: 27.20 +/- 18.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 27.2         |
|    mean_reward          | 18.5         |
| time/                   |              |
|    total_timesteps      | 303800       |
| train/                  |              |
|    approx_kl            | 3.949498e-05 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00525     |
|    explained_variance   | 0.286        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.14         |
|    n_updates            | 7590         |
|    policy_gradient_loss | -0.000185    |
|    value_loss           | 2.34         |
------------------------------------------
Progress update from timestep 303900: Updated shield, loss is 0.04121309518814087
Progress update from timestep 304000: Updated shield, loss is 0.047404322773218155
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 304000: Saved shield_loss.csv. 
Eval num_timesteps=304000, episode_reward=14.91 +/- 15.08
Episode length: 22.80 +/- 22.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 14.9     |
| time/              |          |
|    total_timesteps | 304000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.58     |
|    ep_rew_mean     | 2.86     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 760      |
|    time_elapsed    | 67776    |
|    total_timesteps | 304000   |
---------------------------------
Progress update from timestep 304100: Updated shield, loss is 0.04506821185350418
Progress update from timestep 304200: Updated shield, loss is 0.04340309649705887
Eval num_timesteps=304200, episode_reward=16.96 +/- 14.13
Episode length: 25.40 +/- 20.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.4          |
|    mean_reward          | 17            |
| time/                   |               |
|    total_timesteps      | 304200        |
| train/                  |               |
|    approx_kl            | 0.00037459057 |
|    clip_fraction        | 0.00558       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0142       |
|    explained_variance   | 0.394         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.723         |
|    n_updates            | 7600          |
|    policy_gradient_loss | -0.000855     |
|    value_loss           | 2.1           |
-------------------------------------------
Progress update from timestep 304300: Updated shield, loss is 0.03721655532717705
Progress update from timestep 304400: Updated shield, loss is 0.04294513538479805
Eval num_timesteps=304400, episode_reward=23.41 +/- 14.84
Episode length: 33.80 +/- 20.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 23.4     |
| time/              |          |
|    total_timesteps | 304400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.55     |
|    ep_rew_mean     | 2.85     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 761      |
|    time_elapsed    | 67896    |
|    total_timesteps | 304400   |
---------------------------------
Progress update from timestep 304500: Updated shield, loss is 0.0474226251244545
Progress update from timestep 304600: Updated shield, loss is 0.04908042401075363
Eval num_timesteps=304600, episode_reward=21.39 +/- 16.05
Episode length: 31.40 +/- 22.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 31.4          |
|    mean_reward          | 21.4          |
| time/                   |               |
|    total_timesteps      | 304600        |
| train/                  |               |
|    approx_kl            | 0.00014679297 |
|    clip_fraction        | 0.00826       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.028        |
|    explained_variance   | 0.306         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.532         |
|    n_updates            | 7610          |
|    policy_gradient_loss | -0.00143      |
|    value_loss           | 1.97          |
-------------------------------------------
Progress update from timestep 304700: Updated shield, loss is 0.039435554295778275
Progress update from timestep 304800: Updated shield, loss is 0.043417807668447495
Eval num_timesteps=304800, episode_reward=16.76 +/- 15.92
Episode length: 23.80 +/- 21.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 304800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.67     |
|    ep_rew_mean     | 2.27     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 762      |
|    time_elapsed    | 68013    |
|    total_timesteps | 304800   |
---------------------------------
Progress update from timestep 304900: Updated shield, loss is 0.04368998855352402
Progress update from timestep 305000: Updated shield, loss is 0.04863576218485832
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 305000: Saved shield_loss.csv. 
Eval num_timesteps=305000, episode_reward=23.73 +/- 14.58
Episode length: 33.60 +/- 20.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.6          |
|    mean_reward          | 23.7          |
| time/                   |               |
|    total_timesteps      | 305000        |
| train/                  |               |
|    approx_kl            | 0.00030084993 |
|    clip_fraction        | 0.00179       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00346      |
|    explained_variance   | 0.0357        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.696         |
|    n_updates            | 7620          |
|    policy_gradient_loss | -0.000199     |
|    value_loss           | 1.44          |
-------------------------------------------
Progress update from timestep 305100: Updated shield, loss is 0.04182467609643936
Progress update from timestep 305200: Updated shield, loss is 0.0353202298283577
Eval num_timesteps=305200, episode_reward=9.54 +/- 12.11
Episode length: 15.00 +/- 17.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | 9.54     |
| time/              |          |
|    total_timesteps | 305200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.23     |
|    ep_rew_mean     | 2.65     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 763      |
|    time_elapsed    | 68112    |
|    total_timesteps | 305200   |
---------------------------------
Progress update from timestep 305300: Updated shield, loss is 0.033861495554447174
Progress update from timestep 305400: Updated shield, loss is 0.03904411941766739
Eval num_timesteps=305400, episode_reward=21.93 +/- 15.89
Episode length: 31.80 +/- 22.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 31.8        |
|    mean_reward          | 21.9        |
| time/                   |             |
|    total_timesteps      | 305400      |
| train/                  |             |
|    approx_kl            | 0.000170683 |
|    clip_fraction        | 0.00848     |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0226     |
|    explained_variance   | 0.176       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.52        |
|    n_updates            | 7630        |
|    policy_gradient_loss | -0.00062    |
|    value_loss           | 1.9         |
-----------------------------------------
Progress update from timestep 305500: Updated shield, loss is 0.04103374108672142
Progress update from timestep 305600: Updated shield, loss is 0.04802074655890465
Eval num_timesteps=305600, episode_reward=10.53 +/- 13.21
Episode length: 15.20 +/- 17.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | 10.5     |
| time/              |          |
|    total_timesteps | 305600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.21     |
|    ep_rew_mean     | 2.66     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 764      |
|    time_elapsed    | 68226    |
|    total_timesteps | 305600   |
---------------------------------
Progress update from timestep 305700: Updated shield, loss is 0.04162733629345894
Progress update from timestep 305800: Updated shield, loss is 0.03779253363609314
Eval num_timesteps=305800, episode_reward=11.77 +/- 11.00
Episode length: 18.20 +/- 16.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 18.2          |
|    mean_reward          | 11.8          |
| time/                   |               |
|    total_timesteps      | 305800        |
| train/                  |               |
|    approx_kl            | 6.7892026e-05 |
|    clip_fraction        | 0.00402       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00913      |
|    explained_variance   | 0.112         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.718         |
|    n_updates            | 7640          |
|    policy_gradient_loss | -0.000401     |
|    value_loss           | 1.78          |
-------------------------------------------
Progress update from timestep 305900: Updated shield, loss is 0.04183511435985565
Progress update from timestep 306000: Updated shield, loss is 0.04769246280193329
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 306000: Saved shield_loss.csv. 
Eval num_timesteps=306000, episode_reward=9.45 +/- 13.65
Episode length: 13.80 +/- 18.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.8     |
|    mean_reward     | 9.45     |
| time/              |          |
|    total_timesteps | 306000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.81     |
|    ep_rew_mean     | 3.05     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 765      |
|    time_elapsed    | 68321    |
|    total_timesteps | 306000   |
---------------------------------
Progress update from timestep 306100: Updated shield, loss is 0.037258196622133255
Progress update from timestep 306200: Updated shield, loss is 0.03893616050481796
Eval num_timesteps=306200, episode_reward=9.52 +/- 12.10
Episode length: 15.00 +/- 17.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15            |
|    mean_reward          | 9.52          |
| time/                   |               |
|    total_timesteps      | 306200        |
| train/                  |               |
|    approx_kl            | 0.00030952395 |
|    clip_fraction        | 0.01          |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0391       |
|    explained_variance   | 0.177         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.995         |
|    n_updates            | 7650          |
|    policy_gradient_loss | -0.00131      |
|    value_loss           | 1.98          |
-------------------------------------------
Progress update from timestep 306300: Updated shield, loss is 0.04650060459971428
Progress update from timestep 306400: Updated shield, loss is 0.040831662714481354
Eval num_timesteps=306400, episode_reward=6.12 +/- 4.14
Episode length: 10.00 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 6.12     |
| time/              |          |
|    total_timesteps | 306400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.42     |
|    ep_rew_mean     | 2.76     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 766      |
|    time_elapsed    | 68400    |
|    total_timesteps | 306400   |
---------------------------------
Progress update from timestep 306500: Updated shield, loss is 0.0432422049343586
Progress update from timestep 306600: Updated shield, loss is 0.039349161088466644
Eval num_timesteps=306600, episode_reward=28.48 +/- 13.14
Episode length: 40.80 +/- 18.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 40.8         |
|    mean_reward          | 28.5         |
| time/                   |              |
|    total_timesteps      | 306600       |
| train/                  |              |
|    approx_kl            | 0.0009433396 |
|    clip_fraction        | 0.0167       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0343      |
|    explained_variance   | 0.187        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.456        |
|    n_updates            | 7660         |
|    policy_gradient_loss | -0.00192     |
|    value_loss           | 1.41         |
------------------------------------------
Progress update from timestep 306700: Updated shield, loss is 0.04658469185233116
Progress update from timestep 306800: Updated shield, loss is 0.040926743298769
Eval num_timesteps=306800, episode_reward=11.22 +/- 11.91
Episode length: 16.80 +/- 16.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | 11.2     |
| time/              |          |
|    total_timesteps | 306800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.38     |
|    ep_rew_mean     | 2.82     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 767      |
|    time_elapsed    | 68498    |
|    total_timesteps | 306800   |
---------------------------------
Progress update from timestep 306900: Updated shield, loss is 0.042447615414857864
Progress update from timestep 307000: Updated shield, loss is 0.032308708876371384
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 307000: Saved shield_loss.csv. 
Eval num_timesteps=307000, episode_reward=9.44 +/- 8.15
Episode length: 14.60 +/- 11.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.6         |
|    mean_reward          | 9.44         |
| time/                   |              |
|    total_timesteps      | 307000       |
| train/                  |              |
|    approx_kl            | 0.0004978696 |
|    clip_fraction        | 0.0129       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0298      |
|    explained_variance   | 0.166        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.828        |
|    n_updates            | 7670         |
|    policy_gradient_loss | -0.00146     |
|    value_loss           | 1.71         |
------------------------------------------
Progress update from timestep 307100: Updated shield, loss is 0.04335763677954674
Progress update from timestep 307200: Updated shield, loss is 0.037543244659900665
Eval num_timesteps=307200, episode_reward=16.34 +/- 15.45
Episode length: 23.80 +/- 21.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | 16.3     |
| time/              |          |
|    total_timesteps | 307200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.69     |
|    ep_rew_mean     | 2.23     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 768      |
|    time_elapsed    | 68585    |
|    total_timesteps | 307200   |
---------------------------------
Progress update from timestep 307300: Updated shield, loss is 0.04094875603914261
Progress update from timestep 307400: Updated shield, loss is 0.03810840845108032
Eval num_timesteps=307400, episode_reward=10.50 +/- 6.64
Episode length: 15.80 +/- 9.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.8          |
|    mean_reward          | 10.5          |
| time/                   |               |
|    total_timesteps      | 307400        |
| train/                  |               |
|    approx_kl            | 0.00011962657 |
|    clip_fraction        | 0.00446       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00723      |
|    explained_variance   | 0.215         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.33          |
|    n_updates            | 7680          |
|    policy_gradient_loss | -0.000716     |
|    value_loss           | 1.1           |
-------------------------------------------
Progress update from timestep 307500: Updated shield, loss is 0.05077553167939186
Progress update from timestep 307600: Updated shield, loss is 0.0370820015668869
Eval num_timesteps=307600, episode_reward=19.00 +/- 14.68
Episode length: 26.60 +/- 19.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.6     |
|    mean_reward     | 19       |
| time/              |          |
|    total_timesteps | 307600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.86     |
|    ep_rew_mean     | 2.4      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 769      |
|    time_elapsed    | 68670    |
|    total_timesteps | 307600   |
---------------------------------
Progress update from timestep 307700: Updated shield, loss is 0.045114804059267044
Progress update from timestep 307800: Updated shield, loss is 0.04053500294685364
Eval num_timesteps=307800, episode_reward=17.56 +/- 14.65
Episode length: 25.60 +/- 20.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.6          |
|    mean_reward          | 17.6          |
| time/                   |               |
|    total_timesteps      | 307800        |
| train/                  |               |
|    approx_kl            | 0.00016491736 |
|    clip_fraction        | 0.00513       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0141       |
|    explained_variance   | 0.169         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.733         |
|    n_updates            | 7690          |
|    policy_gradient_loss | -0.000421     |
|    value_loss           | 1.31          |
-------------------------------------------
Progress update from timestep 307900: Updated shield, loss is 0.050560999661684036
Progress update from timestep 308000: Updated shield, loss is 0.04564625769853592
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 308000: Saved shield_loss.csv. 
Eval num_timesteps=308000, episode_reward=12.05 +/- 12.67
Episode length: 17.40 +/- 16.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 12.1     |
| time/              |          |
|    total_timesteps | 308000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.01     |
|    ep_rew_mean     | 2.45     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 770      |
|    time_elapsed    | 68767    |
|    total_timesteps | 308000   |
---------------------------------
Progress update from timestep 308100: Updated shield, loss is 0.04378806799650192
Progress update from timestep 308200: Updated shield, loss is 0.044894009828567505
Eval num_timesteps=308200, episode_reward=17.50 +/- 10.97
Episode length: 25.00 +/- 14.49
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25            |
|    mean_reward          | 17.5          |
| time/                   |               |
|    total_timesteps      | 308200        |
| train/                  |               |
|    approx_kl            | 0.00022892165 |
|    clip_fraction        | 0.00714       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0192       |
|    explained_variance   | 0.192         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.531         |
|    n_updates            | 7700          |
|    policy_gradient_loss | -0.00139      |
|    value_loss           | 0.988         |
-------------------------------------------
Progress update from timestep 308300: Updated shield, loss is 0.042257439345121384
Progress update from timestep 308400: Updated shield, loss is 0.03781287372112274
Eval num_timesteps=308400, episode_reward=21.74 +/- 16.54
Episode length: 31.20 +/- 23.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.2     |
|    mean_reward     | 21.7     |
| time/              |          |
|    total_timesteps | 308400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.17     |
|    ep_rew_mean     | 2.61     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 771      |
|    time_elapsed    | 68857    |
|    total_timesteps | 308400   |
---------------------------------
Progress update from timestep 308500: Updated shield, loss is 0.045140597969293594
Progress update from timestep 308600: Updated shield, loss is 0.047682806849479675
Eval num_timesteps=308600, episode_reward=17.64 +/- 14.02
Episode length: 26.20 +/- 20.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.2          |
|    mean_reward          | 17.6          |
| time/                   |               |
|    total_timesteps      | 308600        |
| train/                  |               |
|    approx_kl            | 3.1284646e-05 |
|    clip_fraction        | 0.00134       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00683      |
|    explained_variance   | 0.151         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.6           |
|    n_updates            | 7710          |
|    policy_gradient_loss | -0.000299     |
|    value_loss           | 2.05          |
-------------------------------------------
Progress update from timestep 308700: Updated shield, loss is 0.03932224586606026
Progress update from timestep 308800: Updated shield, loss is 0.04145185276865959
Eval num_timesteps=308800, episode_reward=16.74 +/- 15.12
Episode length: 24.40 +/- 21.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 308800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.5      |
|    ep_rew_mean     | 2.12     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 772      |
|    time_elapsed    | 68954    |
|    total_timesteps | 308800   |
---------------------------------
Progress update from timestep 308900: Updated shield, loss is 0.041860032826662064
Progress update from timestep 309000: Updated shield, loss is 0.03666458651423454
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 309000: Saved shield_loss.csv. 
Eval num_timesteps=309000, episode_reward=16.48 +/- 16.55
Episode length: 23.20 +/- 21.96
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.2          |
|    mean_reward          | 16.5          |
| time/                   |               |
|    total_timesteps      | 309000        |
| train/                  |               |
|    approx_kl            | 2.8340084e-05 |
|    clip_fraction        | 0.00067       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00784      |
|    explained_variance   | 0.18          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.504         |
|    n_updates            | 7720          |
|    policy_gradient_loss | -0.000237     |
|    value_loss           | 1.31          |
-------------------------------------------
Progress update from timestep 309100: Updated shield, loss is 0.03722672909498215
Progress update from timestep 309200: Updated shield, loss is 0.04796181619167328
Eval num_timesteps=309200, episode_reward=17.70 +/- 15.23
Episode length: 25.00 +/- 20.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | 17.7     |
| time/              |          |
|    total_timesteps | 309200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.84     |
|    ep_rew_mean     | 2.4      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 773      |
|    time_elapsed    | 69046    |
|    total_timesteps | 309200   |
---------------------------------
Progress update from timestep 309300: Updated shield, loss is 0.04017217457294464
Progress update from timestep 309400: Updated shield, loss is 0.033134475350379944
Eval num_timesteps=309400, episode_reward=15.72 +/- 14.95
Episode length: 23.60 +/- 21.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.6          |
|    mean_reward          | 15.7          |
| time/                   |               |
|    total_timesteps      | 309400        |
| train/                  |               |
|    approx_kl            | 0.00032306995 |
|    clip_fraction        | 0.0129        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0238       |
|    explained_variance   | 0.189         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.801         |
|    n_updates            | 7730          |
|    policy_gradient_loss | -0.00167      |
|    value_loss           | 1.38          |
-------------------------------------------
Progress update from timestep 309500: Updated shield, loss is 0.03862201049923897
Progress update from timestep 309600: Updated shield, loss is 0.03897976130247116
Eval num_timesteps=309600, episode_reward=6.29 +/- 2.77
Episode length: 9.80 +/- 4.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.8      |
|    mean_reward     | 6.29     |
| time/              |          |
|    total_timesteps | 309600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4        |
|    ep_rew_mean     | 2.48     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 774      |
|    time_elapsed    | 69124    |
|    total_timesteps | 309600   |
---------------------------------
Progress update from timestep 309700: Updated shield, loss is 0.03640933334827423
Progress update from timestep 309800: Updated shield, loss is 0.04359433054924011
Eval num_timesteps=309800, episode_reward=21.78 +/- 16.04
Episode length: 31.60 +/- 22.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 31.6          |
|    mean_reward          | 21.8          |
| time/                   |               |
|    total_timesteps      | 309800        |
| train/                  |               |
|    approx_kl            | 1.5877179e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.000282     |
|    explained_variance   | 0.0717        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.346         |
|    n_updates            | 7740          |
|    policy_gradient_loss | -2.93e-08     |
|    value_loss           | 1.54          |
-------------------------------------------
Progress update from timestep 309900: Updated shield, loss is 0.035167478024959564
Progress update from timestep 310000: Updated shield, loss is 0.04603330045938492
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 310000: Saved shield_loss.csv. 
Eval num_timesteps=310000, episode_reward=15.51 +/- 16.41
Episode length: 22.40 +/- 22.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 15.5     |
| time/              |          |
|    total_timesteps | 310000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.59     |
|    ep_rew_mean     | 2.9      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 775      |
|    time_elapsed    | 69218    |
|    total_timesteps | 310000   |
---------------------------------
Progress update from timestep 310100: Updated shield, loss is 0.03643301874399185
Progress update from timestep 310200: Updated shield, loss is 0.03422468528151512
Eval num_timesteps=310200, episode_reward=4.37 +/- 0.88
Episode length: 7.20 +/- 1.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 7.2          |
|    mean_reward          | 4.37         |
| time/                   |              |
|    total_timesteps      | 310200       |
| train/                  |              |
|    approx_kl            | 5.612234e-05 |
|    clip_fraction        | 0.00246      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0063      |
|    explained_variance   | 0.283        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.793        |
|    n_updates            | 7750         |
|    policy_gradient_loss | -0.000539    |
|    value_loss           | 1.7          |
------------------------------------------
Progress update from timestep 310300: Updated shield, loss is 0.04648134484887123
Progress update from timestep 310400: Updated shield, loss is 0.03740144520998001
Eval num_timesteps=310400, episode_reward=3.29 +/- 0.80
Episode length: 5.60 +/- 1.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5.6      |
|    mean_reward     | 3.29     |
| time/              |          |
|    total_timesteps | 310400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.96     |
|    ep_rew_mean     | 2.47     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 776      |
|    time_elapsed    | 69275    |
|    total_timesteps | 310400   |
---------------------------------
Progress update from timestep 310500: Updated shield, loss is 0.035009369254112244
Progress update from timestep 310600: Updated shield, loss is 0.036856431514024734
Eval num_timesteps=310600, episode_reward=6.65 +/- 4.94
Episode length: 10.20 +/- 6.85
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 10.2          |
|    mean_reward          | 6.65          |
| time/                   |               |
|    total_timesteps      | 310600        |
| train/                  |               |
|    approx_kl            | 0.00011550605 |
|    clip_fraction        | 0.00424       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00755      |
|    explained_variance   | 0.23          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.777         |
|    n_updates            | 7760          |
|    policy_gradient_loss | -0.00063      |
|    value_loss           | 1.57          |
-------------------------------------------
Progress update from timestep 310700: Updated shield, loss is 0.042946625500917435
Progress update from timestep 310800: Updated shield, loss is 0.04875721409916878
Eval num_timesteps=310800, episode_reward=6.87 +/- 0.78
Episode length: 10.60 +/- 1.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10.6     |
|    mean_reward     | 6.87     |
| time/              |          |
|    total_timesteps | 310800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.71     |
|    ep_rew_mean     | 2.23     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 777      |
|    time_elapsed    | 69340    |
|    total_timesteps | 310800   |
---------------------------------
Progress update from timestep 310900: Updated shield, loss is 0.035777390003204346
Progress update from timestep 311000: Updated shield, loss is 0.03785714507102966
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 311000: Saved shield_loss.csv. 
Eval num_timesteps=311000, episode_reward=23.15 +/- 11.53
Episode length: 33.80 +/- 16.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.8          |
|    mean_reward          | 23.2          |
| time/                   |               |
|    total_timesteps      | 311000        |
| train/                  |               |
|    approx_kl            | 0.00013066588 |
|    clip_fraction        | 0.00469       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00876      |
|    explained_variance   | 0.259         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.3           |
|    n_updates            | 7770          |
|    policy_gradient_loss | -0.00052      |
|    value_loss           | 2.13          |
-------------------------------------------
Progress update from timestep 311100: Updated shield, loss is 0.0366688035428524
Progress update from timestep 311200: Updated shield, loss is 0.035770632326602936
Eval num_timesteps=311200, episode_reward=10.21 +/- 11.61
Episode length: 15.80 +/- 17.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | 10.2     |
| time/              |          |
|    total_timesteps | 311200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.59     |
|    ep_rew_mean     | 2.96     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 778      |
|    time_elapsed    | 69439    |
|    total_timesteps | 311200   |
---------------------------------
Progress update from timestep 311300: Updated shield, loss is 0.03855224326252937
Progress update from timestep 311400: Updated shield, loss is 0.032716743648052216
Eval num_timesteps=311400, episode_reward=23.37 +/- 14.97
Episode length: 33.20 +/- 20.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.2          |
|    mean_reward          | 23.4          |
| time/                   |               |
|    total_timesteps      | 311400        |
| train/                  |               |
|    approx_kl            | 0.00022430035 |
|    clip_fraction        | 0.00692       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0179       |
|    explained_variance   | 0.332         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.362         |
|    n_updates            | 7780          |
|    policy_gradient_loss | -0.00118      |
|    value_loss           | 1.2           |
-------------------------------------------
Progress update from timestep 311500: Updated shield, loss is 0.046335361897945404
Progress update from timestep 311600: Updated shield, loss is 0.032924503087997437
Eval num_timesteps=311600, episode_reward=11.46 +/- 12.66
Episode length: 17.40 +/- 18.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 11.5     |
| time/              |          |
|    total_timesteps | 311600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.75     |
|    ep_rew_mean     | 2.28     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 779      |
|    time_elapsed    | 69544    |
|    total_timesteps | 311600   |
---------------------------------
Progress update from timestep 311700: Updated shield, loss is 0.04764093458652496
Progress update from timestep 311800: Updated shield, loss is 0.0323919951915741
Eval num_timesteps=311800, episode_reward=17.70 +/- 14.15
Episode length: 26.20 +/- 20.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.2          |
|    mean_reward          | 17.7          |
| time/                   |               |
|    total_timesteps      | 311800        |
| train/                  |               |
|    approx_kl            | 0.00025781608 |
|    clip_fraction        | 0.00402       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0196       |
|    explained_variance   | 0.204         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.212         |
|    n_updates            | 7790          |
|    policy_gradient_loss | -0.00059      |
|    value_loss           | 1.33          |
-------------------------------------------
Progress update from timestep 311900: Updated shield, loss is 0.05006273090839386
Progress update from timestep 312000: Updated shield, loss is 0.03963300585746765
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 312000: Saved shield_loss.csv. 
Eval num_timesteps=312000, episode_reward=9.86 +/- 12.89
Episode length: 14.80 +/- 18.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 9.86     |
| time/              |          |
|    total_timesteps | 312000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.88     |
|    ep_rew_mean     | 2.35     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 780      |
|    time_elapsed    | 69642    |
|    total_timesteps | 312000   |
---------------------------------
Progress update from timestep 312100: Updated shield, loss is 0.049415137618780136
Progress update from timestep 312200: Updated shield, loss is 0.03754757344722748
Eval num_timesteps=312200, episode_reward=7.15 +/- 3.14
Episode length: 11.20 +/- 4.71
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 11.2           |
|    mean_reward          | 7.15           |
| time/                   |                |
|    total_timesteps      | 312200         |
| train/                  |                |
|    approx_kl            | 0.000113402006 |
|    clip_fraction        | 0.00692        |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.0125        |
|    explained_variance   | 0.237          |
|    learning_rate        | 0.0001         |
|    loss                 | 0.496          |
|    n_updates            | 7800           |
|    policy_gradient_loss | -0.00123       |
|    value_loss           | 2.07           |
--------------------------------------------
Progress update from timestep 312300: Updated shield, loss is 0.053360216319561005
Progress update from timestep 312400: Updated shield, loss is 0.03772794455289841
Eval num_timesteps=312400, episode_reward=16.74 +/- 15.09
Episode length: 24.40 +/- 21.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 312400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.69     |
|    ep_rew_mean     | 3.01     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 781      |
|    time_elapsed    | 69731    |
|    total_timesteps | 312400   |
---------------------------------
Progress update from timestep 312500: Updated shield, loss is 0.041974086314439774
Progress update from timestep 312600: Updated shield, loss is 0.03657102957367897
Eval num_timesteps=312600, episode_reward=22.13 +/- 12.50
Episode length: 32.00 +/- 17.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32           |
|    mean_reward          | 22.1         |
| time/                   |              |
|    total_timesteps      | 312600       |
| train/                  |              |
|    approx_kl            | 3.344342e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.000664    |
|    explained_variance   | 0.336        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.667        |
|    n_updates            | 7810         |
|    policy_gradient_loss | 1.97e-07     |
|    value_loss           | 2.07         |
------------------------------------------
Progress update from timestep 312700: Updated shield, loss is 0.04695366695523262
Progress update from timestep 312800: Updated shield, loss is 0.037206511944532394
Eval num_timesteps=312800, episode_reward=21.28 +/- 16.20
Episode length: 31.20 +/- 23.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.2     |
|    mean_reward     | 21.3     |
| time/              |          |
|    total_timesteps | 312800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.32     |
|    ep_rew_mean     | 2.73     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 782      |
|    time_elapsed    | 69822    |
|    total_timesteps | 312800   |
---------------------------------
Progress update from timestep 312900: Updated shield, loss is 0.04125828295946121
Progress update from timestep 313000: Updated shield, loss is 0.04428469017148018
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 313000: Saved shield_loss.csv. 
Eval num_timesteps=313000, episode_reward=18.12 +/- 14.33
Episode length: 26.00 +/- 19.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26            |
|    mean_reward          | 18.1          |
| time/                   |               |
|    total_timesteps      | 313000        |
| train/                  |               |
|    approx_kl            | 0.00066209206 |
|    clip_fraction        | 0.0147        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0355       |
|    explained_variance   | 0.224         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.83          |
|    n_updates            | 7820          |
|    policy_gradient_loss | -0.00178      |
|    value_loss           | 1.79          |
-------------------------------------------
Progress update from timestep 313100: Updated shield, loss is 0.04587440565228462
Progress update from timestep 313200: Updated shield, loss is 0.04131696745753288
Eval num_timesteps=313200, episode_reward=14.77 +/- 10.69
Episode length: 21.60 +/- 14.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 14.8     |
| time/              |          |
|    total_timesteps | 313200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.17     |
|    ep_rew_mean     | 2.68     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 783      |
|    time_elapsed    | 69925    |
|    total_timesteps | 313200   |
---------------------------------
Progress update from timestep 313300: Updated shield, loss is 0.04214358702301979
Progress update from timestep 313400: Updated shield, loss is 0.04420473054051399
Eval num_timesteps=313400, episode_reward=9.65 +/- 12.56
Episode length: 14.80 +/- 17.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.8         |
|    mean_reward          | 9.65         |
| time/                   |              |
|    total_timesteps      | 313400       |
| train/                  |              |
|    approx_kl            | 7.767195e-05 |
|    clip_fraction        | 0.00335      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0277      |
|    explained_variance   | 0.229        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.24         |
|    n_updates            | 7830         |
|    policy_gradient_loss | -0.000523    |
|    value_loss           | 2.12         |
------------------------------------------
Progress update from timestep 313500: Updated shield, loss is 0.04143472760915756
Progress update from timestep 313600: Updated shield, loss is 0.04921070858836174
Eval num_timesteps=313600, episode_reward=11.59 +/- 13.30
Episode length: 16.80 +/- 17.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | 11.6     |
| time/              |          |
|    total_timesteps | 313600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.85     |
|    ep_rew_mean     | 2.37     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 784      |
|    time_elapsed    | 70015    |
|    total_timesteps | 313600   |
---------------------------------
Progress update from timestep 313700: Updated shield, loss is 0.03333607316017151
Progress update from timestep 313800: Updated shield, loss is 0.07604679465293884
Eval num_timesteps=313800, episode_reward=28.74 +/- 12.07
Episode length: 41.60 +/- 16.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.6         |
|    mean_reward          | 28.7         |
| time/                   |              |
|    total_timesteps      | 313800       |
| train/                  |              |
|    approx_kl            | 0.0003820366 |
|    clip_fraction        | 0.01         |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0291      |
|    explained_variance   | 0.161        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.31         |
|    n_updates            | 7840         |
|    policy_gradient_loss | -0.00184     |
|    value_loss           | 1.59         |
------------------------------------------
Progress update from timestep 313900: Updated shield, loss is 0.043964289128780365
Progress update from timestep 314000: Updated shield, loss is 0.06832047551870346
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 314000: Saved shield_loss.csv. 
Eval num_timesteps=314000, episode_reward=13.76 +/- 11.75
Episode length: 20.00 +/- 15.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20       |
|    mean_reward     | 13.8     |
| time/              |          |
|    total_timesteps | 314000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.22     |
|    ep_rew_mean     | 2.69     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 785      |
|    time_elapsed    | 70128    |
|    total_timesteps | 314000   |
---------------------------------
Progress update from timestep 314100: Updated shield, loss is 0.04010298475623131
Progress update from timestep 314200: Updated shield, loss is 0.05537039786577225
Eval num_timesteps=314200, episode_reward=22.41 +/- 14.36
Episode length: 33.00 +/- 20.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33            |
|    mean_reward          | 22.4          |
| time/                   |               |
|    total_timesteps      | 314200        |
| train/                  |               |
|    approx_kl            | 0.00011793648 |
|    clip_fraction        | 0.00335       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0116       |
|    explained_variance   | 0.111         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.548         |
|    n_updates            | 7850          |
|    policy_gradient_loss | -0.000728     |
|    value_loss           | 1.76          |
-------------------------------------------
Progress update from timestep 314300: Updated shield, loss is 0.03788941353559494
Progress update from timestep 314400: Updated shield, loss is 0.04998048394918442
Eval num_timesteps=314400, episode_reward=15.25 +/- 10.36
Episode length: 22.80 +/- 14.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 15.2     |
| time/              |          |
|    total_timesteps | 314400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.97     |
|    ep_rew_mean     | 3.22     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 786      |
|    time_elapsed    | 70222    |
|    total_timesteps | 314400   |
---------------------------------
Progress update from timestep 314500: Updated shield, loss is 0.046485401690006256
Progress update from timestep 314600: Updated shield, loss is 0.04126158729195595
Eval num_timesteps=314600, episode_reward=17.51 +/- 15.28
Episode length: 25.00 +/- 20.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25            |
|    mean_reward          | 17.5          |
| time/                   |               |
|    total_timesteps      | 314600        |
| train/                  |               |
|    approx_kl            | 3.8958457e-05 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.000811     |
|    explained_variance   | 0.207         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.65          |
|    n_updates            | 7860          |
|    policy_gradient_loss | -0.000111     |
|    value_loss           | 2.71          |
-------------------------------------------
Progress update from timestep 314700: Updated shield, loss is 0.036263223737478256
Progress update from timestep 314800: Updated shield, loss is 0.046954717487096786
Eval num_timesteps=314800, episode_reward=22.29 +/- 15.83
Episode length: 32.00 +/- 22.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32       |
|    mean_reward     | 22.3     |
| time/              |          |
|    total_timesteps | 314800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.86     |
|    ep_rew_mean     | 2.37     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 787      |
|    time_elapsed    | 70315    |
|    total_timesteps | 314800   |
---------------------------------
Progress update from timestep 314900: Updated shield, loss is 0.037539053708314896
Progress update from timestep 315000: Updated shield, loss is 0.036333177238702774
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 315000: Saved shield_loss.csv. 
Eval num_timesteps=315000, episode_reward=11.40 +/- 12.27
Episode length: 17.00 +/- 16.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17           |
|    mean_reward          | 11.4         |
| time/                   |              |
|    total_timesteps      | 315000       |
| train/                  |              |
|    approx_kl            | 8.448244e-05 |
|    clip_fraction        | 0.00357      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00751     |
|    explained_variance   | 0.0526       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.442        |
|    n_updates            | 7870         |
|    policy_gradient_loss | -0.000338    |
|    value_loss           | 1.69         |
------------------------------------------
Progress update from timestep 315100: Updated shield, loss is 0.03721708431839943
Progress update from timestep 315200: Updated shield, loss is 0.03760622441768646
Eval num_timesteps=315200, episode_reward=11.55 +/- 10.96
Episode length: 17.40 +/- 16.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 11.5     |
| time/              |          |
|    total_timesteps | 315200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.27     |
|    ep_rew_mean     | 2.7      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 788      |
|    time_elapsed    | 70409    |
|    total_timesteps | 315200   |
---------------------------------
Progress update from timestep 315300: Updated shield, loss is 0.04756910726428032
Progress update from timestep 315400: Updated shield, loss is 0.032231032848358154
Eval num_timesteps=315400, episode_reward=12.53 +/- 11.72
Episode length: 18.40 +/- 16.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 18.4          |
|    mean_reward          | 12.5          |
| time/                   |               |
|    total_timesteps      | 315400        |
| train/                  |               |
|    approx_kl            | 0.00015220039 |
|    clip_fraction        | 0.0058        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0233       |
|    explained_variance   | 0.133         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.04          |
|    n_updates            | 7880          |
|    policy_gradient_loss | -0.00103      |
|    value_loss           | 1.8           |
-------------------------------------------
Progress update from timestep 315500: Updated shield, loss is 0.03669990599155426
Progress update from timestep 315600: Updated shield, loss is 0.04472741112112999
Eval num_timesteps=315600, episode_reward=12.31 +/- 11.18
Episode length: 18.80 +/- 15.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.8     |
|    mean_reward     | 12.3     |
| time/              |          |
|    total_timesteps | 315600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.41     |
|    ep_rew_mean     | 2.78     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 789      |
|    time_elapsed    | 70500    |
|    total_timesteps | 315600   |
---------------------------------
Progress update from timestep 315700: Updated shield, loss is 0.04518060386180878
Progress update from timestep 315800: Updated shield, loss is 0.036718420684337616
Eval num_timesteps=315800, episode_reward=28.14 +/- 12.71
Episode length: 41.00 +/- 18.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 41            |
|    mean_reward          | 28.1          |
| time/                   |               |
|    total_timesteps      | 315800        |
| train/                  |               |
|    approx_kl            | 0.00039112978 |
|    clip_fraction        | 0.00848       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0184       |
|    explained_variance   | 0.18          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.843         |
|    n_updates            | 7890          |
|    policy_gradient_loss | -0.00136      |
|    value_loss           | 2.4           |
-------------------------------------------
Progress update from timestep 315900: Updated shield, loss is 0.027166591957211494
Progress update from timestep 316000: Updated shield, loss is 0.039103660732507706
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 316000: Saved shield_loss.csv. 
Eval num_timesteps=316000, episode_reward=12.33 +/- 11.45
Episode length: 18.60 +/- 16.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 12.3     |
| time/              |          |
|    total_timesteps | 316000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.61     |
|    ep_rew_mean     | 3.75     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 790      |
|    time_elapsed    | 70617    |
|    total_timesteps | 316000   |
---------------------------------
Progress update from timestep 316100: Updated shield, loss is 0.04389970004558563
Progress update from timestep 316200: Updated shield, loss is 0.04334487393498421
Eval num_timesteps=316200, episode_reward=13.14 +/- 10.62
Episode length: 20.00 +/- 15.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 20            |
|    mean_reward          | 13.1          |
| time/                   |               |
|    total_timesteps      | 316200        |
| train/                  |               |
|    approx_kl            | 3.0327097e-05 |
|    clip_fraction        | 0.00067       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00507      |
|    explained_variance   | 0.311         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.74          |
|    n_updates            | 7900          |
|    policy_gradient_loss | -0.000344     |
|    value_loss           | 3.4           |
-------------------------------------------
Progress update from timestep 316300: Updated shield, loss is 0.037521377205848694
Progress update from timestep 316400: Updated shield, loss is 0.03751019015908241
Eval num_timesteps=316400, episode_reward=22.87 +/- 16.04
Episode length: 32.20 +/- 21.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 22.9     |
| time/              |          |
|    total_timesteps | 316400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.35     |
|    ep_rew_mean     | 2.76     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 791      |
|    time_elapsed    | 70730    |
|    total_timesteps | 316400   |
---------------------------------
Progress update from timestep 316500: Updated shield, loss is 0.04274703189730644
Progress update from timestep 316600: Updated shield, loss is 0.030319325625896454
Eval num_timesteps=316600, episode_reward=17.36 +/- 15.42
Episode length: 24.80 +/- 20.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.8         |
|    mean_reward          | 17.4         |
| time/                   |              |
|    total_timesteps      | 316600       |
| train/                  |              |
|    approx_kl            | 0.0005046863 |
|    clip_fraction        | 0.0121       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0201      |
|    explained_variance   | 0.194        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.733        |
|    n_updates            | 7910         |
|    policy_gradient_loss | -0.00156     |
|    value_loss           | 1.92         |
------------------------------------------
Progress update from timestep 316700: Updated shield, loss is 0.036278847604990005
Progress update from timestep 316800: Updated shield, loss is 0.047482751309871674
Eval num_timesteps=316800, episode_reward=22.32 +/- 14.26
Episode length: 33.40 +/- 20.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 22.3     |
| time/              |          |
|    total_timesteps | 316800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.21     |
|    ep_rew_mean     | 2.63     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 792      |
|    time_elapsed    | 70845    |
|    total_timesteps | 316800   |
---------------------------------
Progress update from timestep 316900: Updated shield, loss is 0.03974615037441254
Progress update from timestep 317000: Updated shield, loss is 0.036540187895298004
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 317000: Saved shield_loss.csv. 
Eval num_timesteps=317000, episode_reward=18.97 +/- 13.83
Episode length: 27.40 +/- 19.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 27.4          |
|    mean_reward          | 19            |
| time/                   |               |
|    total_timesteps      | 317000        |
| train/                  |               |
|    approx_kl            | 7.6061115e-05 |
|    clip_fraction        | 0.00446       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0113       |
|    explained_variance   | 0.0985        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.85          |
|    n_updates            | 7920          |
|    policy_gradient_loss | -0.000503     |
|    value_loss           | 1.57          |
-------------------------------------------
Progress update from timestep 317100: Updated shield, loss is 0.04585626348853111
Progress update from timestep 317200: Updated shield, loss is 0.0440947450697422
Eval num_timesteps=317200, episode_reward=22.45 +/- 16.58
Episode length: 31.60 +/- 22.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.6     |
|    mean_reward     | 22.5     |
| time/              |          |
|    total_timesteps | 317200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.83     |
|    ep_rew_mean     | 2.39     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 793      |
|    time_elapsed    | 70962    |
|    total_timesteps | 317200   |
---------------------------------
Progress update from timestep 317300: Updated shield, loss is 0.046911850571632385
Progress update from timestep 317400: Updated shield, loss is 0.04069502651691437
Eval num_timesteps=317400, episode_reward=4.28 +/- 1.21
Episode length: 7.00 +/- 1.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 7             |
|    mean_reward          | 4.28          |
| time/                   |               |
|    total_timesteps      | 317400        |
| train/                  |               |
|    approx_kl            | 4.3881842e-05 |
|    clip_fraction        | 0.000893      |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0105       |
|    explained_variance   | 0.252         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.431         |
|    n_updates            | 7930          |
|    policy_gradient_loss | -8.44e-05     |
|    value_loss           | 1.13          |
-------------------------------------------
Progress update from timestep 317500: Updated shield, loss is 0.050414469093084335
Progress update from timestep 317600: Updated shield, loss is 0.0520932637155056
Eval num_timesteps=317600, episode_reward=20.13 +/- 12.70
Episode length: 29.40 +/- 17.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.4     |
|    mean_reward     | 20.1     |
| time/              |          |
|    total_timesteps | 317600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.23     |
|    ep_rew_mean     | 2.67     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 794      |
|    time_elapsed    | 71045    |
|    total_timesteps | 317600   |
---------------------------------
Progress update from timestep 317700: Updated shield, loss is 0.03844008594751358
Progress update from timestep 317800: Updated shield, loss is 0.027370832860469818
Eval num_timesteps=317800, episode_reward=19.89 +/- 12.89
Episode length: 28.80 +/- 17.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 28.8         |
|    mean_reward          | 19.9         |
| time/                   |              |
|    total_timesteps      | 317800       |
| train/                  |              |
|    approx_kl            | 0.0001272548 |
|    clip_fraction        | 0.00424      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.016       |
|    explained_variance   | 0.235        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.57         |
|    n_updates            | 7940         |
|    policy_gradient_loss | -0.000677    |
|    value_loss           | 1.79         |
------------------------------------------
Progress update from timestep 317900: Updated shield, loss is 0.04766178876161575
Progress update from timestep 318000: Updated shield, loss is 0.05309830605983734
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 318000: Saved shield_loss.csv. 
Eval num_timesteps=318000, episode_reward=18.38 +/- 13.74
Episode length: 27.40 +/- 19.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.4     |
|    mean_reward     | 18.4     |
| time/              |          |
|    total_timesteps | 318000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.37     |
|    ep_rew_mean     | 2.77     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 795      |
|    time_elapsed    | 71149    |
|    total_timesteps | 318000   |
---------------------------------
Progress update from timestep 318100: Updated shield, loss is 0.05255855992436409
Progress update from timestep 318200: Updated shield, loss is 0.04412900656461716
Eval num_timesteps=318200, episode_reward=16.50 +/- 12.02
Episode length: 24.40 +/- 16.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.4          |
|    mean_reward          | 16.5          |
| time/                   |               |
|    total_timesteps      | 318200        |
| train/                  |               |
|    approx_kl            | 0.00016203124 |
|    clip_fraction        | 0.00558       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0137       |
|    explained_variance   | 0.209         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.33          |
|    n_updates            | 7950          |
|    policy_gradient_loss | -0.00079      |
|    value_loss           | 2.51          |
-------------------------------------------
Progress update from timestep 318300: Updated shield, loss is 0.03729410097002983
Progress update from timestep 318400: Updated shield, loss is 0.03919592127203941
Eval num_timesteps=318400, episode_reward=7.16 +/- 3.86
Episode length: 11.20 +/- 5.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 11.2     |
|    mean_reward     | 7.16     |
| time/              |          |
|    total_timesteps | 318400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.53     |
|    ep_rew_mean     | 2.86     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 796      |
|    time_elapsed    | 71231    |
|    total_timesteps | 318400   |
---------------------------------
Progress update from timestep 318500: Updated shield, loss is 0.05215379223227501
Progress update from timestep 318600: Updated shield, loss is 0.046084243804216385
Eval num_timesteps=318600, episode_reward=16.53 +/- 14.25
Episode length: 24.80 +/- 20.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.8         |
|    mean_reward          | 16.5         |
| time/                   |              |
|    total_timesteps      | 318600       |
| train/                  |              |
|    approx_kl            | 0.0004931511 |
|    clip_fraction        | 0.0154       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0246      |
|    explained_variance   | 0.121        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.24         |
|    n_updates            | 7960         |
|    policy_gradient_loss | -0.00142     |
|    value_loss           | 2.48         |
------------------------------------------
Progress update from timestep 318700: Updated shield, loss is 0.042380400002002716
Progress update from timestep 318800: Updated shield, loss is 0.04898405075073242
Eval num_timesteps=318800, episode_reward=25.24 +/- 12.65
Episode length: 36.40 +/- 17.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 25.2     |
| time/              |          |
|    total_timesteps | 318800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.15     |
|    ep_rew_mean     | 2.68     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 797      |
|    time_elapsed    | 71348    |
|    total_timesteps | 318800   |
---------------------------------
Progress update from timestep 318900: Updated shield, loss is 0.03482367843389511
Progress update from timestep 319000: Updated shield, loss is 0.06039628013968468
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 319000: Saved shield_loss.csv. 
Eval num_timesteps=319000, episode_reward=10.15 +/- 12.86
Episode length: 15.00 +/- 17.73
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15            |
|    mean_reward          | 10.1          |
| time/                   |               |
|    total_timesteps      | 319000        |
| train/                  |               |
|    approx_kl            | 3.0469275e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00661      |
|    explained_variance   | 0.111         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.874         |
|    n_updates            | 7970          |
|    policy_gradient_loss | -4.32e-05     |
|    value_loss           | 1.76          |
-------------------------------------------
Progress update from timestep 319100: Updated shield, loss is 0.04170665144920349
Progress update from timestep 319200: Updated shield, loss is 0.05083141475915909
Eval num_timesteps=319200, episode_reward=16.48 +/- 16.26
Episode length: 23.60 +/- 21.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 319200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.04     |
|    ep_rew_mean     | 2.53     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 798      |
|    time_elapsed    | 71438    |
|    total_timesteps | 319200   |
---------------------------------
Progress update from timestep 319300: Updated shield, loss is 0.04929845780134201
Progress update from timestep 319400: Updated shield, loss is 0.04736139252781868
Eval num_timesteps=319400, episode_reward=29.49 +/- 11.08
Episode length: 42.40 +/- 15.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.4          |
|    mean_reward          | 29.5          |
| time/                   |               |
|    total_timesteps      | 319400        |
| train/                  |               |
|    approx_kl            | 0.00038806023 |
|    clip_fraction        | 0.0138        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0257       |
|    explained_variance   | 0.203         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.461         |
|    n_updates            | 7980          |
|    policy_gradient_loss | -0.00141      |
|    value_loss           | 1.11          |
-------------------------------------------
Progress update from timestep 319500: Updated shield, loss is 0.0404035784304142
Progress update from timestep 319600: Updated shield, loss is 0.04684710130095482
Eval num_timesteps=319600, episode_reward=23.52 +/- 13.53
Episode length: 34.40 +/- 19.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 23.5     |
| time/              |          |
|    total_timesteps | 319600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.78     |
|    ep_rew_mean     | 2.35     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 799      |
|    time_elapsed    | 71555    |
|    total_timesteps | 319600   |
---------------------------------
Progress update from timestep 319700: Updated shield, loss is 0.04665173590183258
Progress update from timestep 319800: Updated shield, loss is 0.03952180594205856
Eval num_timesteps=319800, episode_reward=19.04 +/- 13.29
Episode length: 27.80 +/- 18.43
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 27.8          |
|    mean_reward          | 19            |
| time/                   |               |
|    total_timesteps      | 319800        |
| train/                  |               |
|    approx_kl            | 0.00012484679 |
|    clip_fraction        | 0.00335       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.017        |
|    explained_variance   | 0.0664        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.25          |
|    n_updates            | 7990          |
|    policy_gradient_loss | -0.000362     |
|    value_loss           | 1.79          |
-------------------------------------------
Progress update from timestep 319900: Updated shield, loss is 0.04486524313688278
Progress update from timestep 320000: Updated shield, loss is 0.0336926206946373
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 320000: Saved shield_loss.csv. 
Eval num_timesteps=320000, episode_reward=16.67 +/- 15.55
Episode length: 24.00 +/- 21.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 320000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.28     |
|    ep_rew_mean     | 2.66     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 800      |
|    time_elapsed    | 71667    |
|    total_timesteps | 320000   |
---------------------------------
Progress update from timestep 320100: Updated shield, loss is 0.05331708863377571
Progress update from timestep 320200: Updated shield, loss is 0.04517079517245293
Eval num_timesteps=320200, episode_reward=16.85 +/- 14.91
Episode length: 24.40 +/- 20.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.4          |
|    mean_reward          | 16.9          |
| time/                   |               |
|    total_timesteps      | 320200        |
| train/                  |               |
|    approx_kl            | 0.00017538297 |
|    clip_fraction        | 0.00402       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00686      |
|    explained_variance   | 0.155         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.509         |
|    n_updates            | 8000          |
|    policy_gradient_loss | -0.000338     |
|    value_loss           | 1.83          |
-------------------------------------------
Progress update from timestep 320300: Updated shield, loss is 0.04271402582526207
Progress update from timestep 320400: Updated shield, loss is 0.04187871515750885
Eval num_timesteps=320400, episode_reward=8.77 +/- 6.39
Episode length: 13.20 +/- 8.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.2     |
|    mean_reward     | 8.77     |
| time/              |          |
|    total_timesteps | 320400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.67     |
|    ep_rew_mean     | 2.22     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 801      |
|    time_elapsed    | 71748    |
|    total_timesteps | 320400   |
---------------------------------
Progress update from timestep 320500: Updated shield, loss is 0.04076273366808891
Progress update from timestep 320600: Updated shield, loss is 0.04809228330850601
Eval num_timesteps=320600, episode_reward=23.52 +/- 14.41
Episode length: 33.80 +/- 19.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.8         |
|    mean_reward          | 23.5         |
| time/                   |              |
|    total_timesteps      | 320600       |
| train/                  |              |
|    approx_kl            | 3.870824e-05 |
|    clip_fraction        | 0.000893     |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00689     |
|    explained_variance   | 0.0148       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.571        |
|    n_updates            | 8010         |
|    policy_gradient_loss | -0.000136    |
|    value_loss           | 1.71         |
------------------------------------------
Progress update from timestep 320700: Updated shield, loss is 0.041525065898895264
Progress update from timestep 320800: Updated shield, loss is 0.037152789533138275
Eval num_timesteps=320800, episode_reward=23.91 +/- 13.89
Episode length: 34.20 +/- 19.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 23.9     |
| time/              |          |
|    total_timesteps | 320800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.64     |
|    ep_rew_mean     | 3        |
| time/              |          |
|    fps             | 4        |
|    iterations      | 802      |
|    time_elapsed    | 71846    |
|    total_timesteps | 320800   |
---------------------------------
Progress update from timestep 320900: Updated shield, loss is 0.04526546224951744
Progress update from timestep 321000: Updated shield, loss is 0.04578184336423874
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 321000: Saved shield_loss.csv. 
Eval num_timesteps=321000, episode_reward=16.63 +/- 15.08
Episode length: 24.40 +/- 20.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.4         |
|    mean_reward          | 16.6         |
| time/                   |              |
|    total_timesteps      | 321000       |
| train/                  |              |
|    approx_kl            | 4.904775e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0011      |
|    explained_variance   | 0.214        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.827        |
|    n_updates            | 8020         |
|    policy_gradient_loss | -2.01e-07    |
|    value_loss           | 2.07         |
------------------------------------------
Progress update from timestep 321100: Updated shield, loss is 0.04653279483318329
Progress update from timestep 321200: Updated shield, loss is 0.05144842714071274
Eval num_timesteps=321200, episode_reward=11.20 +/- 13.01
Episode length: 16.20 +/- 17.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | 11.2     |
| time/              |          |
|    total_timesteps | 321200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.09     |
|    ep_rew_mean     | 2.59     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 803      |
|    time_elapsed    | 71943    |
|    total_timesteps | 321200   |
---------------------------------
Progress update from timestep 321300: Updated shield, loss is 0.042955152690410614
Progress update from timestep 321400: Updated shield, loss is 0.05443628132343292
Eval num_timesteps=321400, episode_reward=17.53 +/- 14.32
Episode length: 25.60 +/- 19.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.6          |
|    mean_reward          | 17.5          |
| time/                   |               |
|    total_timesteps      | 321400        |
| train/                  |               |
|    approx_kl            | 2.7433154e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00232      |
|    explained_variance   | 0.118         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.384         |
|    n_updates            | 8030          |
|    policy_gradient_loss | -1.47e-05     |
|    value_loss           | 1.37          |
-------------------------------------------
Progress update from timestep 321500: Updated shield, loss is 0.044811610132455826
Progress update from timestep 321600: Updated shield, loss is 0.048679716885089874
Eval num_timesteps=321600, episode_reward=23.15 +/- 15.31
Episode length: 32.80 +/- 21.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 23.1     |
| time/              |          |
|    total_timesteps | 321600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.82     |
|    ep_rew_mean     | 2.35     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 804      |
|    time_elapsed    | 72041    |
|    total_timesteps | 321600   |
---------------------------------
Progress update from timestep 321700: Updated shield, loss is 0.04371348395943642
Progress update from timestep 321800: Updated shield, loss is 0.04020716995000839
Eval num_timesteps=321800, episode_reward=18.83 +/- 13.76
Episode length: 27.20 +/- 18.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 27.2          |
|    mean_reward          | 18.8          |
| time/                   |               |
|    total_timesteps      | 321800        |
| train/                  |               |
|    approx_kl            | 0.00069047837 |
|    clip_fraction        | 0.0145        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.033        |
|    explained_variance   | 0.187         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.12          |
|    n_updates            | 8040          |
|    policy_gradient_loss | -0.001        |
|    value_loss           | 1.42          |
-------------------------------------------
Progress update from timestep 321900: Updated shield, loss is 0.04456137493252754
Progress update from timestep 322000: Updated shield, loss is 0.034443359822034836
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 322000: Saved shield_loss.csv. 
Eval num_timesteps=322000, episode_reward=11.51 +/- 12.10
Episode length: 16.80 +/- 16.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | 11.5     |
| time/              |          |
|    total_timesteps | 322000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.58     |
|    ep_rew_mean     | 2.92     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 805      |
|    time_elapsed    | 72141    |
|    total_timesteps | 322000   |
---------------------------------
Progress update from timestep 322100: Updated shield, loss is 0.034832488745450974
Progress update from timestep 322200: Updated shield, loss is 0.0351107083261013
Eval num_timesteps=322200, episode_reward=15.91 +/- 15.46
Episode length: 23.60 +/- 22.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.6          |
|    mean_reward          | 15.9          |
| time/                   |               |
|    total_timesteps      | 322200        |
| train/                  |               |
|    approx_kl            | 0.00026116686 |
|    clip_fraction        | 0.00848       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.021        |
|    explained_variance   | 0.104         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.48          |
|    n_updates            | 8050          |
|    policy_gradient_loss | -0.00125      |
|    value_loss           | 2.27          |
-------------------------------------------
Progress update from timestep 322300: Updated shield, loss is 0.05186405032873154
Progress update from timestep 322400: Updated shield, loss is 0.04157869517803192
Eval num_timesteps=322400, episode_reward=22.75 +/- 14.27
Episode length: 33.60 +/- 20.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 22.7     |
| time/              |          |
|    total_timesteps | 322400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.56     |
|    ep_rew_mean     | 2.2      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 806      |
|    time_elapsed    | 72260    |
|    total_timesteps | 322400   |
---------------------------------
Progress update from timestep 322500: Updated shield, loss is 0.038398150354623795
Progress update from timestep 322600: Updated shield, loss is 0.03388608619570732
Eval num_timesteps=322600, episode_reward=24.64 +/- 12.52
Episode length: 35.80 +/- 17.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.8        |
|    mean_reward          | 24.6        |
| time/                   |             |
|    total_timesteps      | 322600      |
| train/                  |             |
|    approx_kl            | 0.000462273 |
|    clip_fraction        | 0.0143      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0278     |
|    explained_variance   | 0.0907      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.811       |
|    n_updates            | 8060        |
|    policy_gradient_loss | -0.000762   |
|    value_loss           | 1.72        |
-----------------------------------------
Progress update from timestep 322700: Updated shield, loss is 0.04266718029975891
Progress update from timestep 322800: Updated shield, loss is 0.04301982745528221
Eval num_timesteps=322800, episode_reward=10.74 +/- 12.09
Episode length: 16.20 +/- 17.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | 10.7     |
| time/              |          |
|    total_timesteps | 322800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.34     |
|    ep_rew_mean     | 2.74     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 807      |
|    time_elapsed    | 72356    |
|    total_timesteps | 322800   |
---------------------------------
Progress update from timestep 322900: Updated shield, loss is 0.047630228102207184
Progress update from timestep 323000: Updated shield, loss is 0.04139901325106621
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 323000: Saved shield_loss.csv. 
Eval num_timesteps=323000, episode_reward=12.21 +/- 4.02
Episode length: 18.20 +/- 6.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.2         |
|    mean_reward          | 12.2         |
| time/                   |              |
|    total_timesteps      | 323000       |
| train/                  |              |
|    approx_kl            | 0.0003296765 |
|    clip_fraction        | 0.0136       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0233      |
|    explained_variance   | 0.135        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.734        |
|    n_updates            | 8070         |
|    policy_gradient_loss | -0.0018      |
|    value_loss           | 1.9          |
------------------------------------------
Progress update from timestep 323100: Updated shield, loss is 0.04370942339301109
Progress update from timestep 323200: Updated shield, loss is 0.040430180728435516
Eval num_timesteps=323200, episode_reward=20.65 +/- 12.30
Episode length: 29.60 +/- 16.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.6     |
|    mean_reward     | 20.6     |
| time/              |          |
|    total_timesteps | 323200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.11     |
|    ep_rew_mean     | 2.56     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 808      |
|    time_elapsed    | 72442    |
|    total_timesteps | 323200   |
---------------------------------
Progress update from timestep 323300: Updated shield, loss is 0.04670165106654167
Progress update from timestep 323400: Updated shield, loss is 0.03895078971982002
Eval num_timesteps=323400, episode_reward=3.15 +/- 2.20
Episode length: 5.40 +/- 3.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 5.4           |
|    mean_reward          | 3.15          |
| time/                   |               |
|    total_timesteps      | 323400        |
| train/                  |               |
|    approx_kl            | 0.00014148987 |
|    clip_fraction        | 0.00402       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00947      |
|    explained_variance   | 0.204         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.2           |
|    n_updates            | 8080          |
|    policy_gradient_loss | -0.000631     |
|    value_loss           | 1.89          |
-------------------------------------------
Progress update from timestep 323500: Updated shield, loss is 0.04256400465965271
Progress update from timestep 323600: Updated shield, loss is 0.035300519317388535
Eval num_timesteps=323600, episode_reward=11.11 +/- 11.50
Episode length: 16.60 +/- 16.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | 11.1     |
| time/              |          |
|    total_timesteps | 323600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.38     |
|    ep_rew_mean     | 2.77     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 809      |
|    time_elapsed    | 72513    |
|    total_timesteps | 323600   |
---------------------------------
Progress update from timestep 323700: Updated shield, loss is 0.041895490139722824
Progress update from timestep 323800: Updated shield, loss is 0.03794324770569801
Eval num_timesteps=323800, episode_reward=20.55 +/- 16.15
Episode length: 30.80 +/- 23.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30.8         |
|    mean_reward          | 20.6         |
| time/                   |              |
|    total_timesteps      | 323800       |
| train/                  |              |
|    approx_kl            | 6.023658e-05 |
|    clip_fraction        | 0.00335      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00885     |
|    explained_variance   | 0.151        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.491        |
|    n_updates            | 8090         |
|    policy_gradient_loss | -0.000353    |
|    value_loss           | 1.74         |
------------------------------------------
Progress update from timestep 323900: Updated shield, loss is 0.03637170046567917
Progress update from timestep 324000: Updated shield, loss is 0.03948090970516205
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 324000: Saved shield_loss.csv. 
Eval num_timesteps=324000, episode_reward=10.92 +/- 11.53
Episode length: 17.00 +/- 17.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | 10.9     |
| time/              |          |
|    total_timesteps | 324000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.38     |
|    ep_rew_mean     | 2.75     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 810      |
|    time_elapsed    | 72605    |
|    total_timesteps | 324000   |
---------------------------------
Progress update from timestep 324100: Updated shield, loss is 0.045445799827575684
Progress update from timestep 324200: Updated shield, loss is 0.04172554612159729
Eval num_timesteps=324200, episode_reward=10.71 +/- 12.62
Episode length: 16.20 +/- 18.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.2         |
|    mean_reward          | 10.7         |
| time/                   |              |
|    total_timesteps      | 324200       |
| train/                  |              |
|    approx_kl            | 3.896535e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.000496    |
|    explained_variance   | 0.164        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.534        |
|    n_updates            | 8100         |
|    policy_gradient_loss | -8.65e-09    |
|    value_loss           | 1.6          |
------------------------------------------
Progress update from timestep 324300: Updated shield, loss is 0.04391429200768471
Progress update from timestep 324400: Updated shield, loss is 0.03625546395778656
Eval num_timesteps=324400, episode_reward=24.52 +/- 13.08
Episode length: 35.40 +/- 18.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 24.5     |
| time/              |          |
|    total_timesteps | 324400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.68     |
|    ep_rew_mean     | 2.26     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 811      |
|    time_elapsed    | 72718    |
|    total_timesteps | 324400   |
---------------------------------
Progress update from timestep 324500: Updated shield, loss is 0.05586986243724823
Progress update from timestep 324600: Updated shield, loss is 0.0520840659737587
Eval num_timesteps=324600, episode_reward=25.73 +/- 11.29
Episode length: 37.20 +/- 15.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 37.2          |
|    mean_reward          | 25.7          |
| time/                   |               |
|    total_timesteps      | 324600        |
| train/                  |               |
|    approx_kl            | 0.00017699707 |
|    clip_fraction        | 0.00871       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.026        |
|    explained_variance   | 0.111         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.642         |
|    n_updates            | 8110          |
|    policy_gradient_loss | -0.00177      |
|    value_loss           | 1.39          |
-------------------------------------------
Progress update from timestep 324700: Updated shield, loss is 0.03706071525812149
Progress update from timestep 324800: Updated shield, loss is 0.0358036607503891
Eval num_timesteps=324800, episode_reward=10.33 +/- 12.29
Episode length: 15.80 +/- 17.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | 10.3     |
| time/              |          |
|    total_timesteps | 324800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.41     |
|    ep_rew_mean     | 2.81     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 812      |
|    time_elapsed    | 72831    |
|    total_timesteps | 324800   |
---------------------------------
Progress update from timestep 324900: Updated shield, loss is 0.04127369821071625
Progress update from timestep 325000: Updated shield, loss is 0.04899424687027931
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 325000: Saved shield_loss.csv. 
Eval num_timesteps=325000, episode_reward=9.68 +/- 8.51
Episode length: 14.40 +/- 11.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.4         |
|    mean_reward          | 9.68         |
| time/                   |              |
|    total_timesteps      | 325000       |
| train/                  |              |
|    approx_kl            | 0.0004076354 |
|    clip_fraction        | 0.00737      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0256      |
|    explained_variance   | 0.113        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.782        |
|    n_updates            | 8120         |
|    policy_gradient_loss | -0.000722    |
|    value_loss           | 1.78         |
------------------------------------------
Progress update from timestep 325100: Updated shield, loss is 0.04298800230026245
Progress update from timestep 325200: Updated shield, loss is 0.048364195972681046
Eval num_timesteps=325200, episode_reward=10.30 +/- 12.75
Episode length: 15.40 +/- 17.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | 10.3     |
| time/              |          |
|    total_timesteps | 325200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.1      |
|    ep_rew_mean     | 2.62     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 813      |
|    time_elapsed    | 72924    |
|    total_timesteps | 325200   |
---------------------------------
Progress update from timestep 325300: Updated shield, loss is 0.042576782405376434
Progress update from timestep 325400: Updated shield, loss is 0.04197089374065399
Eval num_timesteps=325400, episode_reward=15.91 +/- 16.18
Episode length: 23.00 +/- 22.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23            |
|    mean_reward          | 15.9          |
| time/                   |               |
|    total_timesteps      | 325400        |
| train/                  |               |
|    approx_kl            | 0.00013084592 |
|    clip_fraction        | 0.00201       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0049       |
|    explained_variance   | 0.177         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.542         |
|    n_updates            | 8130          |
|    policy_gradient_loss | -0.000193     |
|    value_loss           | 2.15          |
-------------------------------------------
Progress update from timestep 325500: Updated shield, loss is 0.051307644695043564
Progress update from timestep 325600: Updated shield, loss is 0.049818288534879684
Eval num_timesteps=325600, episode_reward=15.80 +/- 16.20
Episode length: 22.80 +/- 22.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 15.8     |
| time/              |          |
|    total_timesteps | 325600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.6      |
|    ep_rew_mean     | 2.98     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 814      |
|    time_elapsed    | 73017    |
|    total_timesteps | 325600   |
---------------------------------
Progress update from timestep 325700: Updated shield, loss is 0.041905727237463
Progress update from timestep 325800: Updated shield, loss is 0.04387446120381355
Eval num_timesteps=325800, episode_reward=17.64 +/- 14.99
Episode length: 25.80 +/- 20.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.8          |
|    mean_reward          | 17.6          |
| time/                   |               |
|    total_timesteps      | 325800        |
| train/                  |               |
|    approx_kl            | 4.7898004e-05 |
|    clip_fraction        | 0.00223       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.014        |
|    explained_variance   | 0.325         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.956         |
|    n_updates            | 8140          |
|    policy_gradient_loss | -0.000342     |
|    value_loss           | 2.21          |
-------------------------------------------
Progress update from timestep 325900: Updated shield, loss is 0.051521603018045425
Progress update from timestep 326000: Updated shield, loss is 0.044565070420503616
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 326000: Saved shield_loss.csv. 
Eval num_timesteps=326000, episode_reward=16.49 +/- 14.24
Episode length: 24.80 +/- 20.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 326000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.51     |
|    ep_rew_mean     | 2.16     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 815      |
|    time_elapsed    | 73118    |
|    total_timesteps | 326000   |
---------------------------------
Progress update from timestep 326100: Updated shield, loss is 0.03790824115276337
Progress update from timestep 326200: Updated shield, loss is 0.04488138109445572
Eval num_timesteps=326200, episode_reward=19.13 +/- 13.52
Episode length: 27.20 +/- 18.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 27.2         |
|    mean_reward          | 19.1         |
| time/                   |              |
|    total_timesteps      | 326200       |
| train/                  |              |
|    approx_kl            | 9.776095e-05 |
|    clip_fraction        | 0.00491      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0126      |
|    explained_variance   | 0.0111       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.958        |
|    n_updates            | 8150         |
|    policy_gradient_loss | -0.000746    |
|    value_loss           | 1.87         |
------------------------------------------
Progress update from timestep 326300: Updated shield, loss is 0.047021035104990005
Progress update from timestep 326400: Updated shield, loss is 0.04364427179098129
Eval num_timesteps=326400, episode_reward=17.89 +/- 12.69
Episode length: 27.00 +/- 18.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27       |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 326400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.18     |
|    ep_rew_mean     | 2.62     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 816      |
|    time_elapsed    | 73218    |
|    total_timesteps | 326400   |
---------------------------------
Progress update from timestep 326500: Updated shield, loss is 0.04373633861541748
Progress update from timestep 326600: Updated shield, loss is 0.044206198304891586
Eval num_timesteps=326600, episode_reward=11.29 +/- 13.07
Episode length: 16.60 +/- 17.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.6          |
|    mean_reward          | 11.3          |
| time/                   |               |
|    total_timesteps      | 326600        |
| train/                  |               |
|    approx_kl            | 3.3054286e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00396      |
|    explained_variance   | 0.198         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.475         |
|    n_updates            | 8160          |
|    policy_gradient_loss | -3.84e-05     |
|    value_loss           | 1.85          |
-------------------------------------------
Progress update from timestep 326700: Updated shield, loss is 0.035369887948036194
Progress update from timestep 326800: Updated shield, loss is 0.039680786430835724
Eval num_timesteps=326800, episode_reward=12.30 +/- 12.87
Episode length: 17.60 +/- 17.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | 12.3     |
| time/              |          |
|    total_timesteps | 326800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.26     |
|    ep_rew_mean     | 1.93     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 817      |
|    time_elapsed    | 73317    |
|    total_timesteps | 326800   |
---------------------------------
Progress update from timestep 326900: Updated shield, loss is 0.04255779832601547
Progress update from timestep 327000: Updated shield, loss is 0.03472244367003441
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 327000: Saved shield_loss.csv. 
Eval num_timesteps=327000, episode_reward=11.88 +/- 12.65
Episode length: 17.00 +/- 16.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17            |
|    mean_reward          | 11.9          |
| time/                   |               |
|    total_timesteps      | 327000        |
| train/                  |               |
|    approx_kl            | 0.00020326559 |
|    clip_fraction        | 0.00134       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00864      |
|    explained_variance   | 0.0763        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.443         |
|    n_updates            | 8170          |
|    policy_gradient_loss | -0.000231     |
|    value_loss           | 1.45          |
-------------------------------------------
Progress update from timestep 327100: Updated shield, loss is 0.0345364511013031
Progress update from timestep 327200: Updated shield, loss is 0.03519362583756447
Eval num_timesteps=327200, episode_reward=12.32 +/- 12.49
Episode length: 18.00 +/- 17.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | 12.3     |
| time/              |          |
|    total_timesteps | 327200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.19     |
|    ep_rew_mean     | 2.64     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 818      |
|    time_elapsed    | 73407    |
|    total_timesteps | 327200   |
---------------------------------
Progress update from timestep 327300: Updated shield, loss is 0.033323127776384354
Progress update from timestep 327400: Updated shield, loss is 0.039555467665195465
Eval num_timesteps=327400, episode_reward=25.45 +/- 12.42
Episode length: 36.80 +/- 16.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.8          |
|    mean_reward          | 25.4          |
| time/                   |               |
|    total_timesteps      | 327400        |
| train/                  |               |
|    approx_kl            | 0.00011642479 |
|    clip_fraction        | 0.0067        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00659      |
|    explained_variance   | 0.126         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.539         |
|    n_updates            | 8180          |
|    policy_gradient_loss | -0.000543     |
|    value_loss           | 1.34          |
-------------------------------------------
Progress update from timestep 327500: Updated shield, loss is 0.037526167929172516
Progress update from timestep 327600: Updated shield, loss is 0.033199992030858994
Eval num_timesteps=327600, episode_reward=17.88 +/- 13.63
Episode length: 26.40 +/- 19.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.4     |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 327600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.1      |
|    ep_rew_mean     | 2.54     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 819      |
|    time_elapsed    | 73523    |
|    total_timesteps | 327600   |
---------------------------------
Progress update from timestep 327700: Updated shield, loss is 0.040478646755218506
Progress update from timestep 327800: Updated shield, loss is 0.034904055297374725
Eval num_timesteps=327800, episode_reward=22.14 +/- 16.06
Episode length: 31.80 +/- 22.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 31.8          |
|    mean_reward          | 22.1          |
| time/                   |               |
|    total_timesteps      | 327800        |
| train/                  |               |
|    approx_kl            | 0.00013324937 |
|    clip_fraction        | 0.00692       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0183       |
|    explained_variance   | 0.142         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.646         |
|    n_updates            | 8190          |
|    policy_gradient_loss | -0.000877     |
|    value_loss           | 1.93          |
-------------------------------------------
Progress update from timestep 327900: Updated shield, loss is 0.04311346635222435
Progress update from timestep 328000: Updated shield, loss is 0.028727421537041664
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 328000: Saved shield_loss.csv. 
Eval num_timesteps=328000, episode_reward=23.31 +/- 14.32
Episode length: 33.80 +/- 20.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 23.3     |
| time/              |          |
|    total_timesteps | 328000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.81     |
|    ep_rew_mean     | 2.34     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 820      |
|    time_elapsed    | 73615    |
|    total_timesteps | 328000   |
---------------------------------
Progress update from timestep 328100: Updated shield, loss is 0.04094446077942848
Progress update from timestep 328200: Updated shield, loss is 0.04974872246384621
Eval num_timesteps=328200, episode_reward=10.65 +/- 13.40
Episode length: 15.40 +/- 17.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.4         |
|    mean_reward          | 10.6         |
| time/                   |              |
|    total_timesteps      | 328200       |
| train/                  |              |
|    approx_kl            | 3.328875e-05 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0218      |
|    explained_variance   | 0.213        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.739        |
|    n_updates            | 8200         |
|    policy_gradient_loss | -0.000473    |
|    value_loss           | 1.35         |
------------------------------------------
Progress update from timestep 328300: Updated shield, loss is 0.0475509949028492
Progress update from timestep 328400: Updated shield, loss is 0.04677814990282059
Eval num_timesteps=328400, episode_reward=16.67 +/- 14.26
Episode length: 25.00 +/- 20.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 328400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.67     |
|    ep_rew_mean     | 2.27     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 821      |
|    time_elapsed    | 73708    |
|    total_timesteps | 328400   |
---------------------------------
Progress update from timestep 328500: Updated shield, loss is 0.03592126443982124
Progress update from timestep 328600: Updated shield, loss is 0.038052137941122055
Eval num_timesteps=328600, episode_reward=24.83 +/- 12.81
Episode length: 35.40 +/- 17.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.4         |
|    mean_reward          | 24.8         |
| time/                   |              |
|    total_timesteps      | 328600       |
| train/                  |              |
|    approx_kl            | 8.554816e-05 |
|    clip_fraction        | 0.00446      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00905     |
|    explained_variance   | 0.0883       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.76         |
|    n_updates            | 8210         |
|    policy_gradient_loss | -0.000499    |
|    value_loss           | 1.44         |
------------------------------------------
Progress update from timestep 328700: Updated shield, loss is 0.040820639580488205
Progress update from timestep 328800: Updated shield, loss is 0.03654026985168457
Eval num_timesteps=328800, episode_reward=10.65 +/- 12.54
Episode length: 15.80 +/- 17.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | 10.7     |
| time/              |          |
|    total_timesteps | 328800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.09     |
|    ep_rew_mean     | 2.55     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 822      |
|    time_elapsed    | 73807    |
|    total_timesteps | 328800   |
---------------------------------
Progress update from timestep 328900: Updated shield, loss is 0.04127132520079613
Progress update from timestep 329000: Updated shield, loss is 0.05331699177622795
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 329000: Saved shield_loss.csv. 
Eval num_timesteps=329000, episode_reward=24.42 +/- 12.15
Episode length: 35.80 +/- 17.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.8          |
|    mean_reward          | 24.4          |
| time/                   |               |
|    total_timesteps      | 329000        |
| train/                  |               |
|    approx_kl            | 0.00038295976 |
|    clip_fraction        | 0.00848       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0263       |
|    explained_variance   | 0.179         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.733         |
|    n_updates            | 8220          |
|    policy_gradient_loss | -0.00114      |
|    value_loss           | 1.67          |
-------------------------------------------
Progress update from timestep 329100: Updated shield, loss is 0.05078692361712456
Progress update from timestep 329200: Updated shield, loss is 0.04059910401701927
Eval num_timesteps=329200, episode_reward=4.93 +/- 1.61
Episode length: 8.00 +/- 2.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8        |
|    mean_reward     | 4.93     |
| time/              |          |
|    total_timesteps | 329200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.3      |
|    ep_rew_mean     | 2.73     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 823      |
|    time_elapsed    | 73892    |
|    total_timesteps | 329200   |
---------------------------------
Progress update from timestep 329300: Updated shield, loss is 0.03880836442112923
Progress update from timestep 329400: Updated shield, loss is 0.05737842991948128
Eval num_timesteps=329400, episode_reward=22.78 +/- 15.71
Episode length: 32.40 +/- 21.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32.4          |
|    mean_reward          | 22.8          |
| time/                   |               |
|    total_timesteps      | 329400        |
| train/                  |               |
|    approx_kl            | 0.00028290576 |
|    clip_fraction        | 0.00603       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0124       |
|    explained_variance   | 0.113         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.861         |
|    n_updates            | 8230          |
|    policy_gradient_loss | -0.000652     |
|    value_loss           | 1.64          |
-------------------------------------------
Progress update from timestep 329500: Updated shield, loss is 0.054131828248500824
Progress update from timestep 329600: Updated shield, loss is 0.06043586507439613
Eval num_timesteps=329600, episode_reward=16.46 +/- 14.78
Episode length: 24.40 +/- 20.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 329600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.89     |
|    ep_rew_mean     | 3.13     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 824      |
|    time_elapsed    | 73990    |
|    total_timesteps | 329600   |
---------------------------------
Progress update from timestep 329700: Updated shield, loss is 0.04259378835558891
Progress update from timestep 329800: Updated shield, loss is 0.03418739512562752
Eval num_timesteps=329800, episode_reward=11.22 +/- 11.36
Episode length: 17.20 +/- 16.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17.2          |
|    mean_reward          | 11.2          |
| time/                   |               |
|    total_timesteps      | 329800        |
| train/                  |               |
|    approx_kl            | 8.8145745e-05 |
|    clip_fraction        | 0.00246       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.01         |
|    explained_variance   | 0.317         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.41          |
|    n_updates            | 8240          |
|    policy_gradient_loss | -0.000268     |
|    value_loss           | 2.01          |
-------------------------------------------
Progress update from timestep 329900: Updated shield, loss is 0.046582795679569244
Progress update from timestep 330000: Updated shield, loss is 0.04745985567569733
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 330000: Saved shield_loss.csv. 
Eval num_timesteps=330000, episode_reward=12.48 +/- 11.77
Episode length: 18.40 +/- 16.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.4     |
|    mean_reward     | 12.5     |
| time/              |          |
|    total_timesteps | 330000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.34     |
|    ep_rew_mean     | 2.72     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 825      |
|    time_elapsed    | 74080    |
|    total_timesteps | 330000   |
---------------------------------
Progress update from timestep 330100: Updated shield, loss is 0.03628642112016678
Progress update from timestep 330200: Updated shield, loss is 0.04721507430076599
Eval num_timesteps=330200, episode_reward=10.57 +/- 11.45
Episode length: 16.20 +/- 16.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.2          |
|    mean_reward          | 10.6          |
| time/                   |               |
|    total_timesteps      | 330200        |
| train/                  |               |
|    approx_kl            | 0.00021641223 |
|    clip_fraction        | 0.00469       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.019        |
|    explained_variance   | 0.132         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.06          |
|    n_updates            | 8250          |
|    policy_gradient_loss | -0.000657     |
|    value_loss           | 2.21          |
-------------------------------------------
Progress update from timestep 330300: Updated shield, loss is 0.04186038300395012
Progress update from timestep 330400: Updated shield, loss is 0.06646155565977097
Eval num_timesteps=330400, episode_reward=4.74 +/- 4.49
Episode length: 7.60 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7.6      |
|    mean_reward     | 4.74     |
| time/              |          |
|    total_timesteps | 330400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.49     |
|    ep_rew_mean     | 2.86     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 826      |
|    time_elapsed    | 74162    |
|    total_timesteps | 330400   |
---------------------------------
Progress update from timestep 330500: Updated shield, loss is 0.048590224236249924
Progress update from timestep 330600: Updated shield, loss is 0.05716175585985184
Eval num_timesteps=330600, episode_reward=18.07 +/- 13.88
Episode length: 26.60 +/- 19.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.6          |
|    mean_reward          | 18.1          |
| time/                   |               |
|    total_timesteps      | 330600        |
| train/                  |               |
|    approx_kl            | 5.6733592e-05 |
|    clip_fraction        | 0.00268       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0121       |
|    explained_variance   | 0.155         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.09          |
|    n_updates            | 8260          |
|    policy_gradient_loss | -0.000677     |
|    value_loss           | 2.15          |
-------------------------------------------
Progress update from timestep 330700: Updated shield, loss is 0.050520118325948715
Progress update from timestep 330800: Updated shield, loss is 0.04896366596221924
Eval num_timesteps=330800, episode_reward=18.20 +/- 15.14
Episode length: 26.20 +/- 20.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | 18.2     |
| time/              |          |
|    total_timesteps | 330800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.23     |
|    ep_rew_mean     | 2.64     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 827      |
|    time_elapsed    | 74262    |
|    total_timesteps | 330800   |
---------------------------------
Progress update from timestep 330900: Updated shield, loss is 0.04289823770523071
Progress update from timestep 331000: Updated shield, loss is 0.039038319140672684
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 331000: Saved shield_loss.csv. 
Eval num_timesteps=331000, episode_reward=18.60 +/- 13.62
Episode length: 27.00 +/- 19.02
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 27            |
|    mean_reward          | 18.6          |
| time/                   |               |
|    total_timesteps      | 331000        |
| train/                  |               |
|    approx_kl            | 0.00018690492 |
|    clip_fraction        | 0.00737       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0321       |
|    explained_variance   | 0.482         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.908         |
|    n_updates            | 8270          |
|    policy_gradient_loss | -0.00101      |
|    value_loss           | 1.43          |
-------------------------------------------
Progress update from timestep 331100: Updated shield, loss is 0.039542797952890396
Progress update from timestep 331200: Updated shield, loss is 0.04290043190121651
Eval num_timesteps=331200, episode_reward=23.96 +/- 14.46
Episode length: 34.00 +/- 19.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 24       |
| time/              |          |
|    total_timesteps | 331200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.82     |
|    ep_rew_mean     | 2.31     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 828      |
|    time_elapsed    | 74379    |
|    total_timesteps | 331200   |
---------------------------------
Progress update from timestep 331300: Updated shield, loss is 0.039549414068460464
Progress update from timestep 331400: Updated shield, loss is 0.047542113810777664
Eval num_timesteps=331400, episode_reward=19.05 +/- 14.08
Episode length: 27.00 +/- 18.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 27           |
|    mean_reward          | 19           |
| time/                   |              |
|    total_timesteps      | 331400       |
| train/                  |              |
|    approx_kl            | 7.948263e-05 |
|    clip_fraction        | 0.00313      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0102      |
|    explained_variance   | 0.123        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.432        |
|    n_updates            | 8280         |
|    policy_gradient_loss | -0.000203    |
|    value_loss           | 1.22         |
------------------------------------------
Progress update from timestep 331500: Updated shield, loss is 0.042872074991464615
Progress update from timestep 331600: Updated shield, loss is 0.03727131709456444
Eval num_timesteps=331600, episode_reward=22.78 +/- 14.83
Episode length: 33.00 +/- 20.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 22.8     |
| time/              |          |
|    total_timesteps | 331600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.4      |
|    ep_rew_mean     | 2.75     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 829      |
|    time_elapsed    | 74478    |
|    total_timesteps | 331600   |
---------------------------------
Progress update from timestep 331700: Updated shield, loss is 0.04110345616936684
Progress update from timestep 331800: Updated shield, loss is 0.04333106428384781
Eval num_timesteps=331800, episode_reward=17.20 +/- 13.64
Episode length: 26.20 +/- 20.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.2          |
|    mean_reward          | 17.2          |
| time/                   |               |
|    total_timesteps      | 331800        |
| train/                  |               |
|    approx_kl            | 0.00019291369 |
|    clip_fraction        | 0.0058        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0117       |
|    explained_variance   | 0.218         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.839         |
|    n_updates            | 8290          |
|    policy_gradient_loss | -0.00119      |
|    value_loss           | 1.51          |
-------------------------------------------
Progress update from timestep 331900: Updated shield, loss is 0.032850231975317
Progress update from timestep 332000: Updated shield, loss is 0.03253644332289696
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 332000: Saved shield_loss.csv. 
Eval num_timesteps=332000, episode_reward=18.71 +/- 14.05
Episode length: 27.40 +/- 19.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.4     |
|    mean_reward     | 18.7     |
| time/              |          |
|    total_timesteps | 332000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.14     |
|    ep_rew_mean     | 2.57     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 830      |
|    time_elapsed    | 74593    |
|    total_timesteps | 332000   |
---------------------------------
Progress update from timestep 332100: Updated shield, loss is 0.04009145498275757
Progress update from timestep 332200: Updated shield, loss is 0.039894960820674896
Eval num_timesteps=332200, episode_reward=15.60 +/- 15.03
Episode length: 23.40 +/- 21.81
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 23.4           |
|    mean_reward          | 15.6           |
| time/                   |                |
|    total_timesteps      | 332200         |
| train/                  |                |
|    approx_kl            | 0.000102378915 |
|    clip_fraction        | 0.00446        |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.0118        |
|    explained_variance   | 0.254          |
|    learning_rate        | 0.0001         |
|    loss                 | 0.575          |
|    n_updates            | 8300           |
|    policy_gradient_loss | -0.0006        |
|    value_loss           | 1.51           |
--------------------------------------------
Progress update from timestep 332300: Updated shield, loss is 0.047334935516119
Progress update from timestep 332400: Updated shield, loss is 0.036412645131349564
Eval num_timesteps=332400, episode_reward=35.36 +/- 1.09
Episode length: 50.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 35.4     |
| time/              |          |
|    total_timesteps | 332400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.1      |
|    ep_rew_mean     | 2.59     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 831      |
|    time_elapsed    | 74729    |
|    total_timesteps | 332400   |
---------------------------------
Progress update from timestep 332500: Updated shield, loss is 0.03700195252895355
Progress update from timestep 332600: Updated shield, loss is 0.03464148938655853
Eval num_timesteps=332600, episode_reward=12.04 +/- 12.02
Episode length: 18.00 +/- 17.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 18            |
|    mean_reward          | 12            |
| time/                   |               |
|    total_timesteps      | 332600        |
| train/                  |               |
|    approx_kl            | 4.1675736e-05 |
|    clip_fraction        | 0.0029        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0105       |
|    explained_variance   | 0.115         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.02          |
|    n_updates            | 8310          |
|    policy_gradient_loss | -0.000709     |
|    value_loss           | 1.38          |
-------------------------------------------
Progress update from timestep 332700: Updated shield, loss is 0.034105997532606125
Progress update from timestep 332800: Updated shield, loss is 0.04268714040517807
Eval num_timesteps=332800, episode_reward=20.01 +/- 13.55
Episode length: 29.00 +/- 18.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29       |
|    mean_reward     | 20       |
| time/              |          |
|    total_timesteps | 332800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.1      |
|    ep_rew_mean     | 2.5      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 832      |
|    time_elapsed    | 74830    |
|    total_timesteps | 332800   |
---------------------------------
Progress update from timestep 332900: Updated shield, loss is 0.0433526337146759
Progress update from timestep 333000: Updated shield, loss is 0.04964367672801018
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 333000: Saved shield_loss.csv. 
Eval num_timesteps=333000, episode_reward=17.55 +/- 14.01
Episode length: 25.80 +/- 20.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.8         |
|    mean_reward          | 17.5         |
| time/                   |              |
|    total_timesteps      | 333000       |
| train/                  |              |
|    approx_kl            | 8.395506e-05 |
|    clip_fraction        | 0.00469      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00855     |
|    explained_variance   | 0.121        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.21         |
|    n_updates            | 8320         |
|    policy_gradient_loss | -0.000313    |
|    value_loss           | 2.52         |
------------------------------------------
Progress update from timestep 333100: Updated shield, loss is 0.03397974744439125
Progress update from timestep 333200: Updated shield, loss is 0.04787507653236389
Eval num_timesteps=333200, episode_reward=18.75 +/- 14.31
Episode length: 26.60 +/- 19.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.6     |
|    mean_reward     | 18.8     |
| time/              |          |
|    total_timesteps | 333200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.58     |
|    ep_rew_mean     | 2.94     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 833      |
|    time_elapsed    | 74924    |
|    total_timesteps | 333200   |
---------------------------------
Progress update from timestep 333300: Updated shield, loss is 0.04202474281191826
Progress update from timestep 333400: Updated shield, loss is 0.03726353868842125
Eval num_timesteps=333400, episode_reward=17.44 +/- 15.62
Episode length: 24.80 +/- 21.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.8          |
|    mean_reward          | 17.4          |
| time/                   |               |
|    total_timesteps      | 333400        |
| train/                  |               |
|    approx_kl            | 3.9516948e-05 |
|    clip_fraction        | 0.00179       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00819      |
|    explained_variance   | 0.0684        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.98          |
|    n_updates            | 8330          |
|    policy_gradient_loss | -0.000193     |
|    value_loss           | 2.1           |
-------------------------------------------
Progress update from timestep 333500: Updated shield, loss is 0.04344924911856651
Progress update from timestep 333600: Updated shield, loss is 0.040812425315380096
Eval num_timesteps=333600, episode_reward=17.03 +/- 14.74
Episode length: 25.20 +/- 20.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 333600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.15     |
|    ep_rew_mean     | 2.63     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 834      |
|    time_elapsed    | 75024    |
|    total_timesteps | 333600   |
---------------------------------
Progress update from timestep 333700: Updated shield, loss is 0.03916093707084656
Progress update from timestep 333800: Updated shield, loss is 0.04056521877646446
Eval num_timesteps=333800, episode_reward=35.58 +/- 1.22
Episode length: 50.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 50            |
|    mean_reward          | 35.6          |
| time/                   |               |
|    total_timesteps      | 333800        |
| train/                  |               |
|    approx_kl            | 1.1764635e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0104       |
|    explained_variance   | 0.327         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.04          |
|    n_updates            | 8340          |
|    policy_gradient_loss | -0.00012      |
|    value_loss           | 2.47          |
-------------------------------------------
Progress update from timestep 333900: Updated shield, loss is 0.036851607263088226
Progress update from timestep 334000: Updated shield, loss is 0.03732074424624443
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 334000: Saved shield_loss.csv. 
Eval num_timesteps=334000, episode_reward=24.12 +/- 11.91
Episode length: 36.00 +/- 17.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 24.1     |
| time/              |          |
|    total_timesteps | 334000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.89     |
|    ep_rew_mean     | 3.15     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 835      |
|    time_elapsed    | 75142    |
|    total_timesteps | 334000   |
---------------------------------
Progress update from timestep 334100: Updated shield, loss is 0.02755780890583992
Progress update from timestep 334200: Updated shield, loss is 0.0383354127407074
Eval num_timesteps=334200, episode_reward=27.51 +/- 12.92
Episode length: 39.00 +/- 17.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 39           |
|    mean_reward          | 27.5         |
| time/                   |              |
|    total_timesteps      | 334200       |
| train/                  |              |
|    approx_kl            | 0.0018820827 |
|    clip_fraction        | 0.00558      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0136      |
|    explained_variance   | 0.437        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.836        |
|    n_updates            | 8350         |
|    policy_gradient_loss | -0.00112     |
|    value_loss           | 1.76         |
------------------------------------------
Progress update from timestep 334300: Updated shield, loss is 0.03923961520195007
Progress update from timestep 334400: Updated shield, loss is 0.04945914074778557
Eval num_timesteps=334400, episode_reward=12.29 +/- 11.40
Episode length: 18.60 +/- 16.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 12.3     |
| time/              |          |
|    total_timesteps | 334400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.62     |
|    ep_rew_mean     | 2.94     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 836      |
|    time_elapsed    | 75252    |
|    total_timesteps | 334400   |
---------------------------------
Progress update from timestep 334500: Updated shield, loss is 0.03295286372303963
Progress update from timestep 334600: Updated shield, loss is 0.03820636123418808
Eval num_timesteps=334600, episode_reward=14.76 +/- 16.10
Episode length: 22.00 +/- 22.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22            |
|    mean_reward          | 14.8          |
| time/                   |               |
|    total_timesteps      | 334600        |
| train/                  |               |
|    approx_kl            | 1.3878623e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0185       |
|    explained_variance   | 0.411         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.152         |
|    n_updates            | 8360          |
|    policy_gradient_loss | -0.000105     |
|    value_loss           | 2.02          |
-------------------------------------------
Progress update from timestep 334700: Updated shield, loss is 0.0447741374373436
Progress update from timestep 334800: Updated shield, loss is 0.04242691025137901
Eval num_timesteps=334800, episode_reward=14.47 +/- 9.03
Episode length: 21.40 +/- 11.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 14.5     |
| time/              |          |
|    total_timesteps | 334800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.26     |
|    ep_rew_mean     | 2.69     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 837      |
|    time_elapsed    | 75348    |
|    total_timesteps | 334800   |
---------------------------------
Progress update from timestep 334900: Updated shield, loss is 0.037421297281980515
Progress update from timestep 335000: Updated shield, loss is 0.03337526321411133
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 335000: Saved shield_loss.csv. 
Eval num_timesteps=335000, episode_reward=8.83 +/- 12.35
Episode length: 13.80 +/- 18.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 13.8         |
|    mean_reward          | 8.83         |
| time/                   |              |
|    total_timesteps      | 335000       |
| train/                  |              |
|    approx_kl            | 0.0007612924 |
|    clip_fraction        | 0.0161       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0282      |
|    explained_variance   | 0.0355       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.685        |
|    n_updates            | 8370         |
|    policy_gradient_loss | -0.0017      |
|    value_loss           | 1.47         |
------------------------------------------
Progress update from timestep 335100: Updated shield, loss is 0.035615094006061554
Progress update from timestep 335200: Updated shield, loss is 0.03233129158616066
Eval num_timesteps=335200, episode_reward=11.80 +/- 11.97
Episode length: 18.00 +/- 17.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | 11.8     |
| time/              |          |
|    total_timesteps | 335200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.92     |
|    ep_rew_mean     | 2.42     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 838      |
|    time_elapsed    | 75440    |
|    total_timesteps | 335200   |
---------------------------------
Progress update from timestep 335300: Updated shield, loss is 0.03932871297001839
Progress update from timestep 335400: Updated shield, loss is 0.037824634462594986
Eval num_timesteps=335400, episode_reward=23.53 +/- 14.81
Episode length: 33.60 +/- 20.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.6          |
|    mean_reward          | 23.5          |
| time/                   |               |
|    total_timesteps      | 335400        |
| train/                  |               |
|    approx_kl            | 0.00022649705 |
|    clip_fraction        | 0.0103        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.026        |
|    explained_variance   | 0.274         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.935         |
|    n_updates            | 8380          |
|    policy_gradient_loss | -0.00122      |
|    value_loss           | 1.33          |
-------------------------------------------
Progress update from timestep 335500: Updated shield, loss is 0.03140275552868843
Progress update from timestep 335600: Updated shield, loss is 0.03964615985751152
Eval num_timesteps=335600, episode_reward=12.61 +/- 12.15
Episode length: 18.40 +/- 15.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.4     |
|    mean_reward     | 12.6     |
| time/              |          |
|    total_timesteps | 335600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.82     |
|    ep_rew_mean     | 3.06     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 839      |
|    time_elapsed    | 75558    |
|    total_timesteps | 335600   |
---------------------------------
Progress update from timestep 335700: Updated shield, loss is 0.03583792597055435
Progress update from timestep 335800: Updated shield, loss is 0.047478724271059036
Eval num_timesteps=335800, episode_reward=29.30 +/- 10.42
Episode length: 42.60 +/- 14.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.6          |
|    mean_reward          | 29.3          |
| time/                   |               |
|    total_timesteps      | 335800        |
| train/                  |               |
|    approx_kl            | 8.4444546e-05 |
|    clip_fraction        | 0.00268       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00675      |
|    explained_variance   | 0.303         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.28          |
|    n_updates            | 8390          |
|    policy_gradient_loss | -0.000265     |
|    value_loss           | 1.97          |
-------------------------------------------
Progress update from timestep 335900: Updated shield, loss is 0.043038032948970795
Progress update from timestep 336000: Updated shield, loss is 0.03684472665190697
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 336000: Saved shield_loss.csv. 
Eval num_timesteps=336000, episode_reward=11.60 +/- 12.26
Episode length: 17.40 +/- 17.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 11.6     |
| time/              |          |
|    total_timesteps | 336000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.95     |
|    ep_rew_mean     | 2.45     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 840      |
|    time_elapsed    | 75673    |
|    total_timesteps | 336000   |
---------------------------------
Progress update from timestep 336100: Updated shield, loss is 0.033815350383520126
Progress update from timestep 336200: Updated shield, loss is 0.04122237116098404
Eval num_timesteps=336200, episode_reward=17.24 +/- 15.13
Episode length: 24.80 +/- 20.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.8          |
|    mean_reward          | 17.2          |
| time/                   |               |
|    total_timesteps      | 336200        |
| train/                  |               |
|    approx_kl            | 0.00026574053 |
|    clip_fraction        | 0.0143        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0269       |
|    explained_variance   | 0.184         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.668         |
|    n_updates            | 8400          |
|    policy_gradient_loss | -0.00144      |
|    value_loss           | 1.68          |
-------------------------------------------
Progress update from timestep 336300: Updated shield, loss is 0.03527834266424179
Progress update from timestep 336400: Updated shield, loss is 0.04480081424117088
Eval num_timesteps=336400, episode_reward=32.29 +/- 4.48
Episode length: 46.80 +/- 6.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | 32.3     |
| time/              |          |
|    total_timesteps | 336400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.02     |
|    ep_rew_mean     | 2.51     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 841      |
|    time_elapsed    | 75783    |
|    total_timesteps | 336400   |
---------------------------------
Progress update from timestep 336500: Updated shield, loss is 0.04157960042357445
Progress update from timestep 336600: Updated shield, loss is 0.03423037379980087
Eval num_timesteps=336600, episode_reward=29.02 +/- 10.36
Episode length: 42.60 +/- 14.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.6          |
|    mean_reward          | 29            |
| time/                   |               |
|    total_timesteps      | 336600        |
| train/                  |               |
|    approx_kl            | 1.4042598e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00101      |
|    explained_variance   | 0.121         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.03          |
|    n_updates            | 8410          |
|    policy_gradient_loss | 2.93e-08      |
|    value_loss           | 2             |
-------------------------------------------
Progress update from timestep 336700: Updated shield, loss is 0.04066700115799904
Progress update from timestep 336800: Updated shield, loss is 0.043649379163980484
Eval num_timesteps=336800, episode_reward=15.95 +/- 15.23
Episode length: 23.60 +/- 21.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 15.9     |
| time/              |          |
|    total_timesteps | 336800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.22     |
|    ep_rew_mean     | 2.64     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 842      |
|    time_elapsed    | 75897    |
|    total_timesteps | 336800   |
---------------------------------
Progress update from timestep 336900: Updated shield, loss is 0.040190886706113815
Progress update from timestep 337000: Updated shield, loss is 0.04580012708902359
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 337000: Saved shield_loss.csv. 
Eval num_timesteps=337000, episode_reward=17.90 +/- 14.55
Episode length: 25.80 +/- 19.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.8         |
|    mean_reward          | 17.9         |
| time/                   |              |
|    total_timesteps      | 337000       |
| train/                  |              |
|    approx_kl            | 0.0003432595 |
|    clip_fraction        | 0.00804      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0189      |
|    explained_variance   | 0.169        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.587        |
|    n_updates            | 8420         |
|    policy_gradient_loss | -0.00059     |
|    value_loss           | 1.55         |
------------------------------------------
Progress update from timestep 337100: Updated shield, loss is 0.04266325384378433
Progress update from timestep 337200: Updated shield, loss is 0.046101901680231094
Eval num_timesteps=337200, episode_reward=22.96 +/- 15.53
Episode length: 32.80 +/- 21.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 23       |
| time/              |          |
|    total_timesteps | 337200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.25     |
|    ep_rew_mean     | 2.78     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 843      |
|    time_elapsed    | 75991    |
|    total_timesteps | 337200   |
---------------------------------
Progress update from timestep 337300: Updated shield, loss is 0.043409109115600586
Progress update from timestep 337400: Updated shield, loss is 0.04038064181804657
Eval num_timesteps=337400, episode_reward=6.70 +/- 2.94
Episode length: 10.80 +/- 4.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 10.8          |
|    mean_reward          | 6.7           |
| time/                   |               |
|    total_timesteps      | 337400        |
| train/                  |               |
|    approx_kl            | 0.00057298737 |
|    clip_fraction        | 0.0112        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0351       |
|    explained_variance   | 0.101         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.816         |
|    n_updates            | 8430          |
|    policy_gradient_loss | -0.00134      |
|    value_loss           | 2.13          |
-------------------------------------------
Progress update from timestep 337500: Updated shield, loss is 0.04145418480038643
Progress update from timestep 337600: Updated shield, loss is 0.0417887307703495
Eval num_timesteps=337600, episode_reward=5.79 +/- 2.30
Episode length: 9.20 +/- 3.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.2      |
|    mean_reward     | 5.79     |
| time/              |          |
|    total_timesteps | 337600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.53     |
|    ep_rew_mean     | 2.92     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 844      |
|    time_elapsed    | 76059    |
|    total_timesteps | 337600   |
---------------------------------
Progress update from timestep 337700: Updated shield, loss is 0.038916315883398056
Progress update from timestep 337800: Updated shield, loss is 0.04275176301598549
Eval num_timesteps=337800, episode_reward=10.45 +/- 12.69
Episode length: 15.40 +/- 17.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.4          |
|    mean_reward          | 10.4          |
| time/                   |               |
|    total_timesteps      | 337800        |
| train/                  |               |
|    approx_kl            | 0.00024991366 |
|    clip_fraction        | 0.00982       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0142       |
|    explained_variance   | 0.157         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.753         |
|    n_updates            | 8440          |
|    policy_gradient_loss | -0.00117      |
|    value_loss           | 2.35          |
-------------------------------------------
Progress update from timestep 337900: Updated shield, loss is 0.04438349977135658
Progress update from timestep 338000: Updated shield, loss is 0.04397495836019516
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 338000: Saved shield_loss.csv. 
Eval num_timesteps=338000, episode_reward=17.09 +/- 15.17
Episode length: 24.60 +/- 20.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 17.1     |
| time/              |          |
|    total_timesteps | 338000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.96     |
|    ep_rew_mean     | 2.45     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 845      |
|    time_elapsed    | 76154    |
|    total_timesteps | 338000   |
---------------------------------
Progress update from timestep 338100: Updated shield, loss is 0.038855310529470444
Progress update from timestep 338200: Updated shield, loss is 0.04397129267454147
Eval num_timesteps=338200, episode_reward=22.49 +/- 14.69
Episode length: 33.00 +/- 20.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33            |
|    mean_reward          | 22.5          |
| time/                   |               |
|    total_timesteps      | 338200        |
| train/                  |               |
|    approx_kl            | 0.00040142026 |
|    clip_fraction        | 0.0116        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0127       |
|    explained_variance   | 0.0403        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.682         |
|    n_updates            | 8450          |
|    policy_gradient_loss | -0.000901     |
|    value_loss           | 1.57          |
-------------------------------------------
Progress update from timestep 338300: Updated shield, loss is 0.03516567125916481
Progress update from timestep 338400: Updated shield, loss is 0.04197973385453224
Eval num_timesteps=338400, episode_reward=19.05 +/- 13.97
Episode length: 27.00 +/- 18.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27       |
|    mean_reward     | 19       |
| time/              |          |
|    total_timesteps | 338400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.99     |
|    ep_rew_mean     | 2.48     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 846      |
|    time_elapsed    | 76270    |
|    total_timesteps | 338400   |
---------------------------------
Progress update from timestep 338500: Updated shield, loss is 0.04898027330636978
Progress update from timestep 338600: Updated shield, loss is 0.04157255217432976
Eval num_timesteps=338600, episode_reward=16.25 +/- 15.81
Episode length: 23.40 +/- 21.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.4          |
|    mean_reward          | 16.3          |
| time/                   |               |
|    total_timesteps      | 338600        |
| train/                  |               |
|    approx_kl            | 0.00025475217 |
|    clip_fraction        | 0.00603       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00679      |
|    explained_variance   | 0.184         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.887         |
|    n_updates            | 8460          |
|    policy_gradient_loss | -0.000603     |
|    value_loss           | 1.6           |
-------------------------------------------
Progress update from timestep 338700: Updated shield, loss is 0.039242640137672424
Progress update from timestep 338800: Updated shield, loss is 0.04279613122344017
Eval num_timesteps=338800, episode_reward=15.77 +/- 11.89
Episode length: 23.20 +/- 16.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 15.8     |
| time/              |          |
|    total_timesteps | 338800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.17     |
|    ep_rew_mean     | 2.65     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 847      |
|    time_elapsed    | 76365    |
|    total_timesteps | 338800   |
---------------------------------
Progress update from timestep 338900: Updated shield, loss is 0.038755279034376144
Progress update from timestep 339000: Updated shield, loss is 0.03278971463441849
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 339000: Saved shield_loss.csv. 
Eval num_timesteps=339000, episode_reward=30.65 +/- 8.80
Episode length: 44.00 +/- 12.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 44           |
|    mean_reward          | 30.6         |
| time/                   |              |
|    total_timesteps      | 339000       |
| train/                  |              |
|    approx_kl            | 0.0001488421 |
|    clip_fraction        | 0.00513      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0121      |
|    explained_variance   | 0.255        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.861        |
|    n_updates            | 8470         |
|    policy_gradient_loss | -0.000467    |
|    value_loss           | 1.3          |
------------------------------------------
Progress update from timestep 339100: Updated shield, loss is 0.03761908411979675
Progress update from timestep 339200: Updated shield, loss is 0.05178157985210419
Eval num_timesteps=339200, episode_reward=17.30 +/- 15.39
Episode length: 24.60 +/- 20.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 17.3     |
| time/              |          |
|    total_timesteps | 339200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.69     |
|    ep_rew_mean     | 3.05     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 848      |
|    time_elapsed    | 76476    |
|    total_timesteps | 339200   |
---------------------------------
Progress update from timestep 339300: Updated shield, loss is 0.04186763986945152
Progress update from timestep 339400: Updated shield, loss is 0.03752008080482483
Eval num_timesteps=339400, episode_reward=19.14 +/- 13.53
Episode length: 27.60 +/- 18.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 27.6         |
|    mean_reward          | 19.1         |
| time/                   |              |
|    total_timesteps      | 339400       |
| train/                  |              |
|    approx_kl            | 9.919454e-05 |
|    clip_fraction        | 0.00558      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.029       |
|    explained_variance   | 0.135        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.47         |
|    n_updates            | 8480         |
|    policy_gradient_loss | -0.000823    |
|    value_loss           | 2.55         |
------------------------------------------
Progress update from timestep 339500: Updated shield, loss is 0.045176196843385696
Progress update from timestep 339600: Updated shield, loss is 0.03202183172106743
Eval num_timesteps=339600, episode_reward=21.49 +/- 13.57
Episode length: 30.20 +/- 18.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 21.5     |
| time/              |          |
|    total_timesteps | 339600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.25     |
|    ep_rew_mean     | 2.69     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 849      |
|    time_elapsed    | 76589    |
|    total_timesteps | 339600   |
---------------------------------
Progress update from timestep 339700: Updated shield, loss is 0.03714428469538689
Progress update from timestep 339800: Updated shield, loss is 0.04626807942986488
Eval num_timesteps=339800, episode_reward=17.49 +/- 15.29
Episode length: 25.00 +/- 20.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25           |
|    mean_reward          | 17.5         |
| time/                   |              |
|    total_timesteps      | 339800       |
| train/                  |              |
|    approx_kl            | 0.0005699225 |
|    clip_fraction        | 0.0112       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.021       |
|    explained_variance   | 0.121        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.859        |
|    n_updates            | 8490         |
|    policy_gradient_loss | -0.00147     |
|    value_loss           | 1.56         |
------------------------------------------
Progress update from timestep 339900: Updated shield, loss is 0.04054666683077812
Progress update from timestep 340000: Updated shield, loss is 0.037943508476018906
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 340000: Saved shield_loss.csv. 
Eval num_timesteps=340000, episode_reward=34.69 +/- 1.30
Episode length: 50.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 34.7     |
| time/              |          |
|    total_timesteps | 340000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.35     |
|    ep_rew_mean     | 2.79     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 850      |
|    time_elapsed    | 76704    |
|    total_timesteps | 340000   |
---------------------------------
Progress update from timestep 340100: Updated shield, loss is 0.03822367265820503
Progress update from timestep 340200: Updated shield, loss is 0.04561635106801987
Eval num_timesteps=340200, episode_reward=19.93 +/- 13.27
Episode length: 28.40 +/- 18.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 28.4          |
|    mean_reward          | 19.9          |
| time/                   |               |
|    total_timesteps      | 340200        |
| train/                  |               |
|    approx_kl            | -3.162443e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.000842     |
|    explained_variance   | 0.156         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.639         |
|    n_updates            | 8500          |
|    policy_gradient_loss | -7.89e-08     |
|    value_loss           | 1.64          |
-------------------------------------------
Progress update from timestep 340300: Updated shield, loss is 0.04750562459230423
Progress update from timestep 340400: Updated shield, loss is 0.036459486931562424
Eval num_timesteps=340400, episode_reward=21.86 +/- 15.48
Episode length: 32.00 +/- 22.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32       |
|    mean_reward     | 21.9     |
| time/              |          |
|    total_timesteps | 340400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.66     |
|    ep_rew_mean     | 2.96     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 851      |
|    time_elapsed    | 76798    |
|    total_timesteps | 340400   |
---------------------------------
Progress update from timestep 340500: Updated shield, loss is 0.04358770698308945
Progress update from timestep 340600: Updated shield, loss is 0.038009367883205414
Eval num_timesteps=340600, episode_reward=15.82 +/- 15.73
Episode length: 23.20 +/- 21.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.2         |
|    mean_reward          | 15.8         |
| time/                   |              |
|    total_timesteps      | 340600       |
| train/                  |              |
|    approx_kl            | 0.0020855456 |
|    clip_fraction        | 0.00781      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0187      |
|    explained_variance   | 0.0192       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.79         |
|    n_updates            | 8510         |
|    policy_gradient_loss | -0.00101     |
|    value_loss           | 3.02         |
------------------------------------------
Progress update from timestep 340700: Updated shield, loss is 0.03323394060134888
Progress update from timestep 340800: Updated shield, loss is 0.039059776812791824
Eval num_timesteps=340800, episode_reward=29.09 +/- 10.23
Episode length: 42.80 +/- 14.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.8     |
|    mean_reward     | 29.1     |
| time/              |          |
|    total_timesteps | 340800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.73     |
|    ep_rew_mean     | 2.27     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 852      |
|    time_elapsed    | 76914    |
|    total_timesteps | 340800   |
---------------------------------
Progress update from timestep 340900: Updated shield, loss is 0.03691988065838814
Progress update from timestep 341000: Updated shield, loss is 0.03486190736293793
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 341000: Saved shield_loss.csv. 
Eval num_timesteps=341000, episode_reward=16.64 +/- 15.50
Episode length: 23.80 +/- 21.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.8          |
|    mean_reward          | 16.6          |
| time/                   |               |
|    total_timesteps      | 341000        |
| train/                  |               |
|    approx_kl            | 0.00033870246 |
|    clip_fraction        | 0.0123        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0139       |
|    explained_variance   | 0.142         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.392         |
|    n_updates            | 8520          |
|    policy_gradient_loss | -0.0014       |
|    value_loss           | 1.2           |
-------------------------------------------
Progress update from timestep 341100: Updated shield, loss is 0.042246125638484955
Progress update from timestep 341200: Updated shield, loss is 0.029646582901477814
Eval num_timesteps=341200, episode_reward=17.73 +/- 14.31
Episode length: 26.00 +/- 19.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | 17.7     |
| time/              |          |
|    total_timesteps | 341200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.77     |
|    ep_rew_mean     | 2.29     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 853      |
|    time_elapsed    | 77009    |
|    total_timesteps | 341200   |
---------------------------------
Progress update from timestep 341300: Updated shield, loss is 0.039268799126148224
Progress update from timestep 341400: Updated shield, loss is 0.03794398903846741
Eval num_timesteps=341400, episode_reward=23.32 +/- 12.85
Episode length: 35.00 +/- 18.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35           |
|    mean_reward          | 23.3         |
| time/                   |              |
|    total_timesteps      | 341400       |
| train/                  |              |
|    approx_kl            | 0.0003757302 |
|    clip_fraction        | 0.00848      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0176      |
|    explained_variance   | 0.211        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.837        |
|    n_updates            | 8530         |
|    policy_gradient_loss | -0.000616    |
|    value_loss           | 1.28         |
------------------------------------------
Progress update from timestep 341500: Updated shield, loss is 0.04341113939881325
Progress update from timestep 341600: Updated shield, loss is 0.03754991292953491
Eval num_timesteps=341600, episode_reward=11.14 +/- 12.20
Episode length: 16.80 +/- 17.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | 11.1     |
| time/              |          |
|    total_timesteps | 341600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.7      |
|    ep_rew_mean     | 3.1      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 854      |
|    time_elapsed    | 77107    |
|    total_timesteps | 341600   |
---------------------------------
Progress update from timestep 341700: Updated shield, loss is 0.045978255569934845
Progress update from timestep 341800: Updated shield, loss is 0.043224260210990906
Eval num_timesteps=341800, episode_reward=9.48 +/- 12.58
Episode length: 14.40 +/- 17.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.4          |
|    mean_reward          | 9.48          |
| time/                   |               |
|    total_timesteps      | 341800        |
| train/                  |               |
|    approx_kl            | 0.00059201784 |
|    clip_fraction        | 0.00848       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0175       |
|    explained_variance   | 0.127         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.11          |
|    n_updates            | 8540          |
|    policy_gradient_loss | -0.00102      |
|    value_loss           | 1.66          |
-------------------------------------------
Progress update from timestep 341900: Updated shield, loss is 0.03777218610048294
Progress update from timestep 342000: Updated shield, loss is 0.034225817769765854
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 342000: Saved shield_loss.csv. 
Eval num_timesteps=342000, episode_reward=18.05 +/- 14.93
Episode length: 25.60 +/- 20.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 18       |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.87     |
|    ep_rew_mean     | 2.45     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 855      |
|    time_elapsed    | 77197    |
|    total_timesteps | 342000   |
---------------------------------
Progress update from timestep 342100: Updated shield, loss is 0.03978881239891052
Progress update from timestep 342200: Updated shield, loss is 0.03542639687657356
Eval num_timesteps=342200, episode_reward=15.09 +/- 15.39
Episode length: 22.60 +/- 22.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22.6         |
|    mean_reward          | 15.1         |
| time/                   |              |
|    total_timesteps      | 342200       |
| train/                  |              |
|    approx_kl            | 0.0006705081 |
|    clip_fraction        | 0.025        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0411      |
|    explained_variance   | 0.324        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.62         |
|    n_updates            | 8550         |
|    policy_gradient_loss | -0.00299     |
|    value_loss           | 1.95         |
------------------------------------------
Progress update from timestep 342300: Updated shield, loss is 0.03860503435134888
Progress update from timestep 342400: Updated shield, loss is 0.03426589444279671
Eval num_timesteps=342400, episode_reward=23.13 +/- 14.84
Episode length: 33.20 +/- 20.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | 23.1     |
| time/              |          |
|    total_timesteps | 342400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.3      |
|    ep_rew_mean     | 2.7      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 856      |
|    time_elapsed    | 77290    |
|    total_timesteps | 342400   |
---------------------------------
Progress update from timestep 342500: Updated shield, loss is 0.045971836894750595
Progress update from timestep 342600: Updated shield, loss is 0.0456840917468071
Eval num_timesteps=342600, episode_reward=4.28 +/- 3.07
Episode length: 7.00 +/- 4.24
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 7              |
|    mean_reward          | 4.28           |
| time/                   |                |
|    total_timesteps      | 342600         |
| train/                  |                |
|    approx_kl            | 0.000101803584 |
|    clip_fraction        | 0.00558        |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.00696       |
|    explained_variance   | 0.323          |
|    learning_rate        | 0.0001         |
|    loss                 | 0.728          |
|    n_updates            | 8560           |
|    policy_gradient_loss | -0.000661      |
|    value_loss           | 1.22           |
--------------------------------------------
Progress update from timestep 342700: Updated shield, loss is 0.035205211490392685
Progress update from timestep 342800: Updated shield, loss is 0.03875521570444107
Eval num_timesteps=342800, episode_reward=10.11 +/- 12.98
Episode length: 15.00 +/- 17.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | 10.1     |
| time/              |          |
|    total_timesteps | 342800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.66     |
|    ep_rew_mean     | 2.23     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 857      |
|    time_elapsed    | 77368    |
|    total_timesteps | 342800   |
---------------------------------
Progress update from timestep 342900: Updated shield, loss is 0.03678858280181885
Progress update from timestep 343000: Updated shield, loss is 0.026117432862520218
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 343000: Saved shield_loss.csv. 
Eval num_timesteps=343000, episode_reward=22.09 +/- 17.44
Episode length: 30.80 +/- 23.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 30.8          |
|    mean_reward          | 22.1          |
| time/                   |               |
|    total_timesteps      | 343000        |
| train/                  |               |
|    approx_kl            | 0.00055417523 |
|    clip_fraction        | 0.00714       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0173       |
|    explained_variance   | 0.225         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.434         |
|    n_updates            | 8570          |
|    policy_gradient_loss | -0.000733     |
|    value_loss           | 1.34          |
-------------------------------------------
Progress update from timestep 343100: Updated shield, loss is 0.03785460814833641
Progress update from timestep 343200: Updated shield, loss is 0.045848291367292404
Eval num_timesteps=343200, episode_reward=22.34 +/- 14.44
Episode length: 33.20 +/- 20.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | 22.3     |
| time/              |          |
|    total_timesteps | 343200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.56     |
|    ep_rew_mean     | 2.97     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 858      |
|    time_elapsed    | 77482    |
|    total_timesteps | 343200   |
---------------------------------
Progress update from timestep 343300: Updated shield, loss is 0.02756127342581749
Progress update from timestep 343400: Updated shield, loss is 0.03744049370288849
Eval num_timesteps=343400, episode_reward=23.76 +/- 14.72
Episode length: 34.00 +/- 19.96
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34            |
|    mean_reward          | 23.8          |
| time/                   |               |
|    total_timesteps      | 343400        |
| train/                  |               |
|    approx_kl            | 0.00020315354 |
|    clip_fraction        | 0.00625       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.023        |
|    explained_variance   | 0.374         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.977         |
|    n_updates            | 8580          |
|    policy_gradient_loss | -0.000715     |
|    value_loss           | 1.84          |
-------------------------------------------
Progress update from timestep 343500: Updated shield, loss is 0.038314733654260635
Progress update from timestep 343600: Updated shield, loss is 0.04305768758058548
Eval num_timesteps=343600, episode_reward=27.69 +/- 12.06
Episode length: 39.00 +/- 16.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 27.7     |
| time/              |          |
|    total_timesteps | 343600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.99     |
|    ep_rew_mean     | 2.41     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 859      |
|    time_elapsed    | 77597    |
|    total_timesteps | 343600   |
---------------------------------
Progress update from timestep 343700: Updated shield, loss is 0.03555323928594589
Progress update from timestep 343800: Updated shield, loss is 0.04556519165635109
Eval num_timesteps=343800, episode_reward=4.23 +/- 2.73
Episode length: 7.00 +/- 4.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 7             |
|    mean_reward          | 4.23          |
| time/                   |               |
|    total_timesteps      | 343800        |
| train/                  |               |
|    approx_kl            | 0.00015474774 |
|    clip_fraction        | 0.00357       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0055       |
|    explained_variance   | 0.118         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.555         |
|    n_updates            | 8590          |
|    policy_gradient_loss | -0.000241     |
|    value_loss           | 1.94          |
-------------------------------------------
Progress update from timestep 343900: Updated shield, loss is 0.05263679102063179
Progress update from timestep 344000: Updated shield, loss is 0.04549725353717804
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 344000: Saved shield_loss.csv. 
Eval num_timesteps=344000, episode_reward=9.26 +/- 13.25
Episode length: 13.80 +/- 18.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.8     |
|    mean_reward     | 9.26     |
| time/              |          |
|    total_timesteps | 344000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4        |
|    ep_rew_mean     | 2.53     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 860      |
|    time_elapsed    | 77672    |
|    total_timesteps | 344000   |
---------------------------------
Progress update from timestep 344100: Updated shield, loss is 0.04805733263492584
Progress update from timestep 344200: Updated shield, loss is 0.04072166606783867
Eval num_timesteps=344200, episode_reward=28.34 +/- 12.89
Episode length: 41.00 +/- 18.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 41            |
|    mean_reward          | 28.3          |
| time/                   |               |
|    total_timesteps      | 344200        |
| train/                  |               |
|    approx_kl            | 4.3888045e-05 |
|    clip_fraction        | 0.00357       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0172       |
|    explained_variance   | 0.0842        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.54          |
|    n_updates            | 8600          |
|    policy_gradient_loss | -0.000523     |
|    value_loss           | 1.71          |
-------------------------------------------
Progress update from timestep 344300: Updated shield, loss is 0.039814166724681854
Progress update from timestep 344400: Updated shield, loss is 0.04072427377104759
Eval num_timesteps=344400, episode_reward=9.79 +/- 13.15
Episode length: 14.60 +/- 18.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 9.79     |
| time/              |          |
|    total_timesteps | 344400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.34     |
|    ep_rew_mean     | 2.74     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 861      |
|    time_elapsed    | 77786    |
|    total_timesteps | 344400   |
---------------------------------
Progress update from timestep 344500: Updated shield, loss is 0.03895087540149689
Progress update from timestep 344600: Updated shield, loss is 0.0357666090130806
Eval num_timesteps=344600, episode_reward=11.94 +/- 12.09
Episode length: 17.40 +/- 16.63
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17.4          |
|    mean_reward          | 11.9          |
| time/                   |               |
|    total_timesteps      | 344600        |
| train/                  |               |
|    approx_kl            | 0.00020090296 |
|    clip_fraction        | 0.0114        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0204       |
|    explained_variance   | 0.184         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.941         |
|    n_updates            | 8610          |
|    policy_gradient_loss | -0.00158      |
|    value_loss           | 2.15          |
-------------------------------------------
Progress update from timestep 344700: Updated shield, loss is 0.04102155566215515
Progress update from timestep 344800: Updated shield, loss is 0.033331651240587234
Eval num_timesteps=344800, episode_reward=16.41 +/- 15.86
Episode length: 23.80 +/- 21.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | 16.4     |
| time/              |          |
|    total_timesteps | 344800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.69     |
|    ep_rew_mean     | 3.06     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 862      |
|    time_elapsed    | 77877    |
|    total_timesteps | 344800   |
---------------------------------
Progress update from timestep 344900: Updated shield, loss is 0.04194748401641846
Progress update from timestep 345000: Updated shield, loss is 0.04039886221289635
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 345000: Saved shield_loss.csv. 
Eval num_timesteps=345000, episode_reward=15.85 +/- 16.14
Episode length: 22.80 +/- 22.25
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.8          |
|    mean_reward          | 15.9          |
| time/                   |               |
|    total_timesteps      | 345000        |
| train/                  |               |
|    approx_kl            | 0.00033602185 |
|    clip_fraction        | 0.00848       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0135       |
|    explained_variance   | 0.216         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.625         |
|    n_updates            | 8620          |
|    policy_gradient_loss | -0.00108      |
|    value_loss           | 1.7           |
-------------------------------------------
Progress update from timestep 345100: Updated shield, loss is 0.043863583356142044
Progress update from timestep 345200: Updated shield, loss is 0.039632707834243774
Eval num_timesteps=345200, episode_reward=28.91 +/- 11.73
Episode length: 41.80 +/- 16.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 28.9     |
| time/              |          |
|    total_timesteps | 345200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4        |
|    ep_rew_mean     | 2.48     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 863      |
|    time_elapsed    | 77991    |
|    total_timesteps | 345200   |
---------------------------------
Progress update from timestep 345300: Updated shield, loss is 0.042497940361499786
Progress update from timestep 345400: Updated shield, loss is 0.042659781873226166
Eval num_timesteps=345400, episode_reward=12.50 +/- 12.90
Episode length: 18.00 +/- 17.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18           |
|    mean_reward          | 12.5         |
| time/                   |              |
|    total_timesteps      | 345400       |
| train/                  |              |
|    approx_kl            | 0.0010835014 |
|    clip_fraction        | 0.00915      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0225      |
|    explained_variance   | 0.22         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.51         |
|    n_updates            | 8630         |
|    policy_gradient_loss | -0.001       |
|    value_loss           | 1.65         |
------------------------------------------
Progress update from timestep 345500: Updated shield, loss is 0.036544039845466614
Progress update from timestep 345600: Updated shield, loss is 0.040750037878751755
Eval num_timesteps=345600, episode_reward=27.89 +/- 13.22
Episode length: 40.60 +/- 18.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.6     |
|    mean_reward     | 27.9     |
| time/              |          |
|    total_timesteps | 345600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.96     |
|    ep_rew_mean     | 2.48     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 864      |
|    time_elapsed    | 78082    |
|    total_timesteps | 345600   |
---------------------------------
Progress update from timestep 345700: Updated shield, loss is 0.04674718901515007
Progress update from timestep 345800: Updated shield, loss is 0.04598554968833923
Eval num_timesteps=345800, episode_reward=22.57 +/- 15.63
Episode length: 32.40 +/- 21.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32.4          |
|    mean_reward          | 22.6          |
| time/                   |               |
|    total_timesteps      | 345800        |
| train/                  |               |
|    approx_kl            | 0.00012311389 |
|    clip_fraction        | 0.00201       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.017        |
|    explained_variance   | 0.182         |
|    learning_rate        | 0.0001        |
|    loss                 | 1             |
|    n_updates            | 8640          |
|    policy_gradient_loss | -0.000729     |
|    value_loss           | 1.63          |
-------------------------------------------
Progress update from timestep 345900: Updated shield, loss is 0.04707612842321396
Progress update from timestep 346000: Updated shield, loss is 0.04055190086364746
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 346000: Saved shield_loss.csv. 
Eval num_timesteps=346000, episode_reward=16.20 +/- 9.99
Episode length: 24.00 +/- 14.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16.2     |
| time/              |          |
|    total_timesteps | 346000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.76     |
|    ep_rew_mean     | 2.3      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 865      |
|    time_elapsed    | 78194    |
|    total_timesteps | 346000   |
---------------------------------
Progress update from timestep 346100: Updated shield, loss is 0.04027591273188591
Progress update from timestep 346200: Updated shield, loss is 0.03639570623636246
Eval num_timesteps=346200, episode_reward=22.65 +/- 16.76
Episode length: 31.60 +/- 22.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 31.6          |
|    mean_reward          | 22.6          |
| time/                   |               |
|    total_timesteps      | 346200        |
| train/                  |               |
|    approx_kl            | 0.00025339247 |
|    clip_fraction        | 0.0241        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0287       |
|    explained_variance   | -0.127        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.479         |
|    n_updates            | 8650          |
|    policy_gradient_loss | -0.00152      |
|    value_loss           | 2             |
-------------------------------------------
Progress update from timestep 346300: Updated shield, loss is 0.036577124148607254
Progress update from timestep 346400: Updated shield, loss is 0.03534018620848656
Eval num_timesteps=346400, episode_reward=28.07 +/- 13.38
Episode length: 40.60 +/- 18.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.6     |
|    mean_reward     | 28.1     |
| time/              |          |
|    total_timesteps | 346400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.87     |
|    ep_rew_mean     | 2.42     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 866      |
|    time_elapsed    | 78310    |
|    total_timesteps | 346400   |
---------------------------------
Progress update from timestep 346500: Updated shield, loss is 0.04304158687591553
Progress update from timestep 346600: Updated shield, loss is 0.03824739158153534
Eval num_timesteps=346600, episode_reward=17.85 +/- 14.55
Episode length: 26.00 +/- 19.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26            |
|    mean_reward          | 17.8          |
| time/                   |               |
|    total_timesteps      | 346600        |
| train/                  |               |
|    approx_kl            | 0.00030701613 |
|    clip_fraction        | 0.0129        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0196       |
|    explained_variance   | 0.103         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.43          |
|    n_updates            | 8660          |
|    policy_gradient_loss | -0.0012       |
|    value_loss           | 1.45          |
-------------------------------------------
Progress update from timestep 346700: Updated shield, loss is 0.03818007558584213
Progress update from timestep 346800: Updated shield, loss is 0.024961315095424652
Eval num_timesteps=346800, episode_reward=16.54 +/- 14.37
Episode length: 24.80 +/- 20.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 346800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.41     |
|    ep_rew_mean     | 2.77     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 867      |
|    time_elapsed    | 78408    |
|    total_timesteps | 346800   |
---------------------------------
Progress update from timestep 346900: Updated shield, loss is 0.035648565739393234
Progress update from timestep 347000: Updated shield, loss is 0.03732123598456383
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 347000: Saved shield_loss.csv. 
Eval num_timesteps=347000, episode_reward=19.09 +/- 13.13
Episode length: 27.60 +/- 18.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 27.6          |
|    mean_reward          | 19.1          |
| time/                   |               |
|    total_timesteps      | 347000        |
| train/                  |               |
|    approx_kl            | 0.00042593773 |
|    clip_fraction        | 0.01          |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0186       |
|    explained_variance   | 0.2           |
|    learning_rate        | 0.0001        |
|    loss                 | 0.862         |
|    n_updates            | 8670          |
|    policy_gradient_loss | -0.00123      |
|    value_loss           | 2.23          |
-------------------------------------------
Progress update from timestep 347100: Updated shield, loss is 0.040768541395664215
Progress update from timestep 347200: Updated shield, loss is 0.0393308661878109
Eval num_timesteps=347200, episode_reward=25.46 +/- 14.04
Episode length: 35.40 +/- 18.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 25.5     |
| time/              |          |
|    total_timesteps | 347200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.02     |
|    ep_rew_mean     | 2.47     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 868      |
|    time_elapsed    | 78503    |
|    total_timesteps | 347200   |
---------------------------------
Progress update from timestep 347300: Updated shield, loss is 0.030546559020876884
Progress update from timestep 347400: Updated shield, loss is 0.040774162858724594
Eval num_timesteps=347400, episode_reward=11.15 +/- 13.21
Episode length: 16.00 +/- 17.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16            |
|    mean_reward          | 11.1          |
| time/                   |               |
|    total_timesteps      | 347400        |
| train/                  |               |
|    approx_kl            | 0.00021602656 |
|    clip_fraction        | 0.00647       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0143       |
|    explained_variance   | 0.107         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.964         |
|    n_updates            | 8680          |
|    policy_gradient_loss | -0.00101      |
|    value_loss           | 1.35          |
-------------------------------------------
Progress update from timestep 347500: Updated shield, loss is 0.038679610937833786
Progress update from timestep 347600: Updated shield, loss is 0.04259505867958069
Eval num_timesteps=347600, episode_reward=3.16 +/- 1.63
Episode length: 5.40 +/- 2.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5.4      |
|    mean_reward     | 3.16     |
| time/              |          |
|    total_timesteps | 347600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.22     |
|    ep_rew_mean     | 2.67     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 869      |
|    time_elapsed    | 78580    |
|    total_timesteps | 347600   |
---------------------------------
Progress update from timestep 347700: Updated shield, loss is 0.036626022309064865
Progress update from timestep 347800: Updated shield, loss is 0.04157061502337456
Eval num_timesteps=347800, episode_reward=15.22 +/- 15.31
Episode length: 23.00 +/- 22.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23           |
|    mean_reward          | 15.2         |
| time/                   |              |
|    total_timesteps      | 347800       |
| train/                  |              |
|    approx_kl            | 0.0018971099 |
|    clip_fraction        | 0.0167       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0315      |
|    explained_variance   | 0.136        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.37         |
|    n_updates            | 8690         |
|    policy_gradient_loss | -0.00196     |
|    value_loss           | 2.09         |
------------------------------------------
Progress update from timestep 347900: Updated shield, loss is 0.03269776701927185
Progress update from timestep 348000: Updated shield, loss is 0.04197918251156807
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 348000: Saved shield_loss.csv. 
Eval num_timesteps=348000, episode_reward=17.93 +/- 13.55
Episode length: 26.60 +/- 19.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.6     |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 348000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.92     |
|    ep_rew_mean     | 2.45     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 870      |
|    time_elapsed    | 78679    |
|    total_timesteps | 348000   |
---------------------------------
Progress update from timestep 348100: Updated shield, loss is 0.03392363339662552
Progress update from timestep 348200: Updated shield, loss is 0.03364092856645584
Eval num_timesteps=348200, episode_reward=11.14 +/- 11.82
Episode length: 16.60 +/- 16.91
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.6          |
|    mean_reward          | 11.1          |
| time/                   |               |
|    total_timesteps      | 348200        |
| train/                  |               |
|    approx_kl            | 0.00028638027 |
|    clip_fraction        | 0.0145        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0196       |
|    explained_variance   | 0.0215        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.598         |
|    n_updates            | 8700          |
|    policy_gradient_loss | -0.00189      |
|    value_loss           | 1.41          |
-------------------------------------------
Progress update from timestep 348300: Updated shield, loss is 0.036639563739299774
Progress update from timestep 348400: Updated shield, loss is 0.029332466423511505
Eval num_timesteps=348400, episode_reward=19.65 +/- 13.78
Episode length: 28.80 +/- 19.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.8     |
|    mean_reward     | 19.6     |
| time/              |          |
|    total_timesteps | 348400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.16     |
|    ep_rew_mean     | 2.66     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 871      |
|    time_elapsed    | 78777    |
|    total_timesteps | 348400   |
---------------------------------
Progress update from timestep 348500: Updated shield, loss is 0.039589665830135345
Progress update from timestep 348600: Updated shield, loss is 0.03990551084280014
Eval num_timesteps=348600, episode_reward=23.11 +/- 14.83
Episode length: 33.20 +/- 20.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.2         |
|    mean_reward          | 23.1         |
| time/                   |              |
|    total_timesteps      | 348600       |
| train/                  |              |
|    approx_kl            | 5.667841e-05 |
|    clip_fraction        | 0.00692      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00706     |
|    explained_variance   | 0.301        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.443        |
|    n_updates            | 8710         |
|    policy_gradient_loss | -0.000408    |
|    value_loss           | 1.74         |
------------------------------------------
Progress update from timestep 348700: Updated shield, loss is 0.046240679919719696
Progress update from timestep 348800: Updated shield, loss is 0.04492524266242981
Eval num_timesteps=348800, episode_reward=16.46 +/- 14.88
Episode length: 24.40 +/- 21.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 348800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.66     |
|    ep_rew_mean     | 3.02     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 872      |
|    time_elapsed    | 78891    |
|    total_timesteps | 348800   |
---------------------------------
Progress update from timestep 348900: Updated shield, loss is 0.04760533198714256
Progress update from timestep 349000: Updated shield, loss is 0.038704946637153625
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 349000: Saved shield_loss.csv. 
Eval num_timesteps=349000, episode_reward=23.12 +/- 15.29
Episode length: 32.80 +/- 21.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32.8          |
|    mean_reward          | 23.1          |
| time/                   |               |
|    total_timesteps      | 349000        |
| train/                  |               |
|    approx_kl            | 0.00031753271 |
|    clip_fraction        | 0.0121        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0218       |
|    explained_variance   | 0.244         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.522         |
|    n_updates            | 8720          |
|    policy_gradient_loss | -0.000937     |
|    value_loss           | 2.07          |
-------------------------------------------
Progress update from timestep 349100: Updated shield, loss is 0.028284011408686638
Progress update from timestep 349200: Updated shield, loss is 0.03469006344676018
Eval num_timesteps=349200, episode_reward=13.99 +/- 10.89
Episode length: 20.40 +/- 15.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 14       |
| time/              |          |
|    total_timesteps | 349200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.91     |
|    ep_rew_mean     | 2.43     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 873      |
|    time_elapsed    | 78989    |
|    total_timesteps | 349200   |
---------------------------------
Progress update from timestep 349300: Updated shield, loss is 0.03196680545806885
Progress update from timestep 349400: Updated shield, loss is 0.04943059757351875
Eval num_timesteps=349400, episode_reward=24.85 +/- 13.84
Episode length: 35.20 +/- 18.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.2         |
|    mean_reward          | 24.8         |
| time/                   |              |
|    total_timesteps      | 349400       |
| train/                  |              |
|    approx_kl            | 2.794244e-05 |
|    clip_fraction        | 0.00134      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00746     |
|    explained_variance   | 0.0602       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.983        |
|    n_updates            | 8730         |
|    policy_gradient_loss | -0.000237    |
|    value_loss           | 2.04         |
------------------------------------------
Progress update from timestep 349500: Updated shield, loss is 0.03930822014808655
Progress update from timestep 349600: Updated shield, loss is 0.046152349561452866
Eval num_timesteps=349600, episode_reward=17.73 +/- 14.09
Episode length: 26.20 +/- 20.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | 17.7     |
| time/              |          |
|    total_timesteps | 349600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.81     |
|    ep_rew_mean     | 2.35     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 874      |
|    time_elapsed    | 79084    |
|    total_timesteps | 349600   |
---------------------------------
Progress update from timestep 349700: Updated shield, loss is 0.036048419773578644
Progress update from timestep 349800: Updated shield, loss is 0.03720628470182419
Eval num_timesteps=349800, episode_reward=14.30 +/- 10.90
Episode length: 21.20 +/- 14.91
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 21.2          |
|    mean_reward          | 14.3          |
| time/                   |               |
|    total_timesteps      | 349800        |
| train/                  |               |
|    approx_kl            | 0.00030310583 |
|    clip_fraction        | 0.0125        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0204       |
|    explained_variance   | 0.229         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.632         |
|    n_updates            | 8740          |
|    policy_gradient_loss | -0.00129      |
|    value_loss           | 1.39          |
-------------------------------------------
Progress update from timestep 349900: Updated shield, loss is 0.0313011109828949
Progress update from timestep 350000: Updated shield, loss is 0.04071207344532013
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 350000: Saved shield_loss.csv. 
Eval num_timesteps=350000, episode_reward=12.90 +/- 12.15
Episode length: 18.60 +/- 16.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 12.9     |
| time/              |          |
|    total_timesteps | 350000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.32     |
|    ep_rew_mean     | 2.77     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 875      |
|    time_elapsed    | 79186    |
|    total_timesteps | 350000   |
---------------------------------
Progress update from timestep 350100: Updated shield, loss is 0.03931967914104462
Progress update from timestep 350200: Updated shield, loss is 0.048751335591077805
Eval num_timesteps=350200, episode_reward=16.73 +/- 14.52
Episode length: 24.60 +/- 20.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.6          |
|    mean_reward          | 16.7          |
| time/                   |               |
|    total_timesteps      | 350200        |
| train/                  |               |
|    approx_kl            | 0.00016795064 |
|    clip_fraction        | 0.00335       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0115       |
|    explained_variance   | 0.271         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.693         |
|    n_updates            | 8750          |
|    policy_gradient_loss | -0.000421     |
|    value_loss           | 1.38          |
-------------------------------------------
Progress update from timestep 350300: Updated shield, loss is 0.039096899330616
Progress update from timestep 350400: Updated shield, loss is 0.03296155110001564
Eval num_timesteps=350400, episode_reward=13.21 +/- 11.99
Episode length: 19.80 +/- 17.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.8     |
|    mean_reward     | 13.2     |
| time/              |          |
|    total_timesteps | 350400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.18     |
|    ep_rew_mean     | 2.68     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 876      |
|    time_elapsed    | 79282    |
|    total_timesteps | 350400   |
---------------------------------
Progress update from timestep 350500: Updated shield, loss is 0.03995448350906372
Progress update from timestep 350600: Updated shield, loss is 0.035643238574266434
Eval num_timesteps=350600, episode_reward=16.30 +/- 14.52
Episode length: 24.40 +/- 21.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.4          |
|    mean_reward          | 16.3          |
| time/                   |               |
|    total_timesteps      | 350600        |
| train/                  |               |
|    approx_kl            | 0.00018408203 |
|    clip_fraction        | 0.00737       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0196       |
|    explained_variance   | 0.155         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.89          |
|    n_updates            | 8760          |
|    policy_gradient_loss | -0.000894     |
|    value_loss           | 2.07          |
-------------------------------------------
Progress update from timestep 350700: Updated shield, loss is 0.042600367218256
Progress update from timestep 350800: Updated shield, loss is 0.036816809326410294
Eval num_timesteps=350800, episode_reward=12.55 +/- 11.61
Episode length: 18.40 +/- 15.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.4     |
|    mean_reward     | 12.6     |
| time/              |          |
|    total_timesteps | 350800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.57     |
|    ep_rew_mean     | 2.97     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 877      |
|    time_elapsed    | 79373    |
|    total_timesteps | 350800   |
---------------------------------
Progress update from timestep 350900: Updated shield, loss is 0.038832686841487885
Progress update from timestep 351000: Updated shield, loss is 0.031234169378876686
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 351000: Saved shield_loss.csv. 
Eval num_timesteps=351000, episode_reward=8.51 +/- 14.12
Episode length: 12.40 +/- 18.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 12.4         |
|    mean_reward          | 8.51         |
| time/                   |              |
|    total_timesteps      | 351000       |
| train/                  |              |
|    approx_kl            | 0.0004070049 |
|    clip_fraction        | 0.0105       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0178      |
|    explained_variance   | 0.216        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.9          |
|    n_updates            | 8770         |
|    policy_gradient_loss | -0.00165     |
|    value_loss           | 2.64         |
------------------------------------------
Progress update from timestep 351100: Updated shield, loss is 0.03467091917991638
Progress update from timestep 351200: Updated shield, loss is 0.03124356083571911
Eval num_timesteps=351200, episode_reward=10.23 +/- 12.42
Episode length: 15.40 +/- 17.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | 10.2     |
| time/              |          |
|    total_timesteps | 351200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.21     |
|    ep_rew_mean     | 3.47     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 878      |
|    time_elapsed    | 79470    |
|    total_timesteps | 351200   |
---------------------------------
Progress update from timestep 351300: Updated shield, loss is 0.04224960878491402
Progress update from timestep 351400: Updated shield, loss is 0.028367551043629646
Eval num_timesteps=351400, episode_reward=19.05 +/- 13.02
Episode length: 28.00 +/- 18.62
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 28             |
|    mean_reward          | 19             |
| time/                   |                |
|    total_timesteps      | 351400         |
| train/                  |                |
|    approx_kl            | -3.1312602e-11 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.000161      |
|    explained_variance   | 0.155          |
|    learning_rate        | 0.0001         |
|    loss                 | 0.512          |
|    n_updates            | 8780           |
|    policy_gradient_loss | -1.37e-08      |
|    value_loss           | 2.42           |
--------------------------------------------
Progress update from timestep 351500: Updated shield, loss is 0.025629520416259766
Progress update from timestep 351600: Updated shield, loss is 0.04480525851249695
Eval num_timesteps=351600, episode_reward=22.63 +/- 14.27
Episode length: 33.60 +/- 20.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 22.6     |
| time/              |          |
|    total_timesteps | 351600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.21     |
|    ep_rew_mean     | 2.62     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 879      |
|    time_elapsed    | 79567    |
|    total_timesteps | 351600   |
---------------------------------
Progress update from timestep 351700: Updated shield, loss is 0.04035995528101921
Progress update from timestep 351800: Updated shield, loss is 0.040328316390514374
Eval num_timesteps=351800, episode_reward=23.02 +/- 14.98
Episode length: 33.00 +/- 20.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33            |
|    mean_reward          | 23            |
| time/                   |               |
|    total_timesteps      | 351800        |
| train/                  |               |
|    approx_kl            | 7.2867064e-05 |
|    clip_fraction        | 0.00268       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0062       |
|    explained_variance   | 0.136         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.07          |
|    n_updates            | 8790          |
|    policy_gradient_loss | -0.000182     |
|    value_loss           | 2.06          |
-------------------------------------------
Progress update from timestep 351900: Updated shield, loss is 0.03951696306467056
Progress update from timestep 352000: Updated shield, loss is 0.043162014335393906
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 352000: Saved shield_loss.csv. 
Eval num_timesteps=352000, episode_reward=7.21 +/- 3.50
Episode length: 11.40 +/- 5.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 11.4     |
|    mean_reward     | 7.21     |
| time/              |          |
|    total_timesteps | 352000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.23     |
|    ep_rew_mean     | 2.62     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 880      |
|    time_elapsed    | 79666    |
|    total_timesteps | 352000   |
---------------------------------
Progress update from timestep 352100: Updated shield, loss is 0.03790442273020744
Progress update from timestep 352200: Updated shield, loss is 0.0387377105653286
Eval num_timesteps=352200, episode_reward=13.05 +/- 11.91
Episode length: 18.80 +/- 15.71
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 18.8          |
|    mean_reward          | 13            |
| time/                   |               |
|    total_timesteps      | 352200        |
| train/                  |               |
|    approx_kl            | 0.00023453138 |
|    clip_fraction        | 0.00513       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.014        |
|    explained_variance   | 0.108         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.979         |
|    n_updates            | 8800          |
|    policy_gradient_loss | -0.00074      |
|    value_loss           | 1.73          |
-------------------------------------------
Progress update from timestep 352300: Updated shield, loss is 0.04008939117193222
Progress update from timestep 352400: Updated shield, loss is 0.03967679291963577
Eval num_timesteps=352400, episode_reward=29.07 +/- 11.93
Episode length: 41.60 +/- 16.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 29.1     |
| time/              |          |
|    total_timesteps | 352400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.15     |
|    ep_rew_mean     | 2.58     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 881      |
|    time_elapsed    | 79760    |
|    total_timesteps | 352400   |
---------------------------------
Progress update from timestep 352500: Updated shield, loss is 0.036268360912799835
Progress update from timestep 352600: Updated shield, loss is 0.048459552228450775
Eval num_timesteps=352600, episode_reward=12.29 +/- 12.59
Episode length: 18.00 +/- 16.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18           |
|    mean_reward          | 12.3         |
| time/                   |              |
|    total_timesteps      | 352600       |
| train/                  |              |
|    approx_kl            | 0.0012939209 |
|    clip_fraction        | 0.00915      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.017       |
|    explained_variance   | 0.118        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.944        |
|    n_updates            | 8810         |
|    policy_gradient_loss | -0.00147     |
|    value_loss           | 1.65         |
------------------------------------------
Progress update from timestep 352700: Updated shield, loss is 0.04700573906302452
Progress update from timestep 352800: Updated shield, loss is 0.0393921360373497
Eval num_timesteps=352800, episode_reward=17.31 +/- 14.10
Episode length: 25.60 +/- 20.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 17.3     |
| time/              |          |
|    total_timesteps | 352800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.97     |
|    ep_rew_mean     | 2.49     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 882      |
|    time_elapsed    | 79858    |
|    total_timesteps | 352800   |
---------------------------------
Progress update from timestep 352900: Updated shield, loss is 0.037359464913606644
Progress update from timestep 353000: Updated shield, loss is 0.03938216716051102
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 353000: Saved shield_loss.csv. 
Eval num_timesteps=353000, episode_reward=17.20 +/- 15.47
Episode length: 24.40 +/- 20.91
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.4          |
|    mean_reward          | 17.2          |
| time/                   |               |
|    total_timesteps      | 353000        |
| train/                  |               |
|    approx_kl            | 0.00051828055 |
|    clip_fraction        | 0.0145        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0321       |
|    explained_variance   | 0.289         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.657         |
|    n_updates            | 8820          |
|    policy_gradient_loss | -0.00168      |
|    value_loss           | 2.19          |
-------------------------------------------
Progress update from timestep 353100: Updated shield, loss is 0.044998180121183395
Progress update from timestep 353200: Updated shield, loss is 0.038459695875644684
Eval num_timesteps=353200, episode_reward=17.83 +/- 15.03
Episode length: 25.20 +/- 20.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 17.8     |
| time/              |          |
|    total_timesteps | 353200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.3      |
|    ep_rew_mean     | 2.68     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 883      |
|    time_elapsed    | 79956    |
|    total_timesteps | 353200   |
---------------------------------
Progress update from timestep 353300: Updated shield, loss is 0.03827448561787605
Progress update from timestep 353400: Updated shield, loss is 0.036718063056468964
Eval num_timesteps=353400, episode_reward=9.94 +/- 12.15
Episode length: 15.60 +/- 17.87
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15.6       |
|    mean_reward          | 9.94       |
| time/                   |            |
|    total_timesteps      | 353400     |
| train/                  |            |
|    approx_kl            | 3.1004e-05 |
|    clip_fraction        | 0.00134    |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.0113    |
|    explained_variance   | 0.315      |
|    learning_rate        | 0.0001     |
|    loss                 | 1.03       |
|    n_updates            | 8830       |
|    policy_gradient_loss | -0.00012   |
|    value_loss           | 2          |
----------------------------------------
Progress update from timestep 353500: Updated shield, loss is 0.03524864464998245
Progress update from timestep 353600: Updated shield, loss is 0.03978965803980827
Eval num_timesteps=353600, episode_reward=12.34 +/- 12.05
Episode length: 18.40 +/- 17.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.4     |
|    mean_reward     | 12.3     |
| time/              |          |
|    total_timesteps | 353600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.95     |
|    ep_rew_mean     | 2.43     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 884      |
|    time_elapsed    | 80051    |
|    total_timesteps | 353600   |
---------------------------------
Progress update from timestep 353700: Updated shield, loss is 0.04132493957877159
Progress update from timestep 353800: Updated shield, loss is 0.03066515550017357
Eval num_timesteps=353800, episode_reward=23.16 +/- 15.23
Episode length: 33.00 +/- 20.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33            |
|    mean_reward          | 23.2          |
| time/                   |               |
|    total_timesteps      | 353800        |
| train/                  |               |
|    approx_kl            | 0.00044905022 |
|    clip_fraction        | 0.0121        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0187       |
|    explained_variance   | 0.0898        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.925         |
|    n_updates            | 8840          |
|    policy_gradient_loss | -0.000691     |
|    value_loss           | 1.6           |
-------------------------------------------
Progress update from timestep 353900: Updated shield, loss is 0.043296609073877335
Progress update from timestep 354000: Updated shield, loss is 0.03846399113535881
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 354000: Saved shield_loss.csv. 
Eval num_timesteps=354000, episode_reward=27.72 +/- 12.41
Episode length: 41.00 +/- 18.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41       |
|    mean_reward     | 27.7     |
| time/              |          |
|    total_timesteps | 354000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.65     |
|    ep_rew_mean     | 2.24     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 885      |
|    time_elapsed    | 80148    |
|    total_timesteps | 354000   |
---------------------------------
Progress update from timestep 354100: Updated shield, loss is 0.040274616330862045
Progress update from timestep 354200: Updated shield, loss is 0.04207771271467209
Eval num_timesteps=354200, episode_reward=16.04 +/- 15.66
Episode length: 23.60 +/- 21.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.6          |
|    mean_reward          | 16            |
| time/                   |               |
|    total_timesteps      | 354200        |
| train/                  |               |
|    approx_kl            | 0.00023537899 |
|    clip_fraction        | 0.00335       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00802      |
|    explained_variance   | 0.233         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.486         |
|    n_updates            | 8850          |
|    policy_gradient_loss | -0.00059      |
|    value_loss           | 1.12          |
-------------------------------------------
Progress update from timestep 354300: Updated shield, loss is 0.044024985283613205
Progress update from timestep 354400: Updated shield, loss is 0.04301461949944496
Eval num_timesteps=354400, episode_reward=23.29 +/- 15.53
Episode length: 32.80 +/- 21.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 23.3     |
| time/              |          |
|    total_timesteps | 354400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.73     |
|    ep_rew_mean     | 2.25     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 886      |
|    time_elapsed    | 80246    |
|    total_timesteps | 354400   |
---------------------------------
Progress update from timestep 354500: Updated shield, loss is 0.029570696875452995
Progress update from timestep 354600: Updated shield, loss is 0.04056800529360771
Eval num_timesteps=354600, episode_reward=12.11 +/- 12.15
Episode length: 17.60 +/- 16.79
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17.6          |
|    mean_reward          | 12.1          |
| time/                   |               |
|    total_timesteps      | 354600        |
| train/                  |               |
|    approx_kl            | 0.00052358664 |
|    clip_fraction        | 0.0129        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0205       |
|    explained_variance   | 0.22          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.778         |
|    n_updates            | 8860          |
|    policy_gradient_loss | -0.00197      |
|    value_loss           | 0.944         |
-------------------------------------------
Progress update from timestep 354700: Updated shield, loss is 0.03715898469090462
Progress update from timestep 354800: Updated shield, loss is 0.04018939658999443
Eval num_timesteps=354800, episode_reward=15.79 +/- 14.97
Episode length: 23.80 +/- 21.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | 15.8     |
| time/              |          |
|    total_timesteps | 354800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.33     |
|    ep_rew_mean     | 2.73     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 887      |
|    time_elapsed    | 80340    |
|    total_timesteps | 354800   |
---------------------------------
Progress update from timestep 354900: Updated shield, loss is 0.03710068017244339
Progress update from timestep 355000: Updated shield, loss is 0.04163835570216179
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 355000: Saved shield_loss.csv. 
Eval num_timesteps=355000, episode_reward=10.53 +/- 12.07
Episode length: 15.80 +/- 17.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.8          |
|    mean_reward          | 10.5          |
| time/                   |               |
|    total_timesteps      | 355000        |
| train/                  |               |
|    approx_kl            | 0.00014596482 |
|    clip_fraction        | 0.00313       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0198       |
|    explained_variance   | 0.125         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.24          |
|    n_updates            | 8870          |
|    policy_gradient_loss | -0.000616     |
|    value_loss           | 2.14          |
-------------------------------------------
Progress update from timestep 355100: Updated shield, loss is 0.043441373854875565
Progress update from timestep 355200: Updated shield, loss is 0.04096440598368645
Eval num_timesteps=355200, episode_reward=26.00 +/- 12.35
Episode length: 37.60 +/- 17.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.6     |
|    mean_reward     | 26       |
| time/              |          |
|    total_timesteps | 355200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.4      |
|    ep_rew_mean     | 2.82     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 888      |
|    time_elapsed    | 80445    |
|    total_timesteps | 355200   |
---------------------------------
Progress update from timestep 355300: Updated shield, loss is 0.03746740520000458
Progress update from timestep 355400: Updated shield, loss is 0.034460343420505524
Eval num_timesteps=355400, episode_reward=15.74 +/- 15.30
Episode length: 23.40 +/- 21.73
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.4          |
|    mean_reward          | 15.7          |
| time/                   |               |
|    total_timesteps      | 355400        |
| train/                  |               |
|    approx_kl            | 0.00038669645 |
|    clip_fraction        | 0.0138        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0236       |
|    explained_variance   | 0.0599        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.945         |
|    n_updates            | 8880          |
|    policy_gradient_loss | -0.00178      |
|    value_loss           | 2.23          |
-------------------------------------------
Progress update from timestep 355500: Updated shield, loss is 0.045598093420267105
Progress update from timestep 355600: Updated shield, loss is 0.03803606331348419
Eval num_timesteps=355600, episode_reward=18.09 +/- 14.35
Episode length: 26.40 +/- 20.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.4     |
|    mean_reward     | 18.1     |
| time/              |          |
|    total_timesteps | 355600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.02     |
|    ep_rew_mean     | 2.51     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 889      |
|    time_elapsed    | 80538    |
|    total_timesteps | 355600   |
---------------------------------
Progress update from timestep 355700: Updated shield, loss is 0.04289838299155235
Progress update from timestep 355800: Updated shield, loss is 0.048648491501808167
Eval num_timesteps=355800, episode_reward=8.27 +/- 5.79
Episode length: 12.40 +/- 7.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 12.4          |
|    mean_reward          | 8.27          |
| time/                   |               |
|    total_timesteps      | 355800        |
| train/                  |               |
|    approx_kl            | 0.00047518956 |
|    clip_fraction        | 0.00871       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00867      |
|    explained_variance   | -0.0116       |
|    learning_rate        | 0.0001        |
|    loss                 | 1.03          |
|    n_updates            | 8890          |
|    policy_gradient_loss | -0.000876     |
|    value_loss           | 1.51          |
-------------------------------------------
Progress update from timestep 355900: Updated shield, loss is 0.03973749652504921
Progress update from timestep 356000: Updated shield, loss is 0.03450458496809006
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 356000: Saved shield_loss.csv. 
Eval num_timesteps=356000, episode_reward=17.58 +/- 15.20
Episode length: 25.00 +/- 20.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 356000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.19     |
|    ep_rew_mean     | 2.66     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 890      |
|    time_elapsed    | 80622    |
|    total_timesteps | 356000   |
---------------------------------
Progress update from timestep 356100: Updated shield, loss is 0.04571465030312538
Progress update from timestep 356200: Updated shield, loss is 0.04140455275774002
Eval num_timesteps=356200, episode_reward=21.55 +/- 15.87
Episode length: 31.60 +/- 22.57
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 31.6          |
|    mean_reward          | 21.5          |
| time/                   |               |
|    total_timesteps      | 356200        |
| train/                  |               |
|    approx_kl            | 0.00044372285 |
|    clip_fraction        | 0.0107        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0142       |
|    explained_variance   | 0.36          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.469         |
|    n_updates            | 8900          |
|    policy_gradient_loss | -0.0016       |
|    value_loss           | 1.63          |
-------------------------------------------
Progress update from timestep 356300: Updated shield, loss is 0.04295258969068527
Progress update from timestep 356400: Updated shield, loss is 0.038955576717853546
Eval num_timesteps=356400, episode_reward=12.81 +/- 12.59
Episode length: 18.60 +/- 16.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 12.8     |
| time/              |          |
|    total_timesteps | 356400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.49     |
|    ep_rew_mean     | 2.88     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 891      |
|    time_elapsed    | 80740    |
|    total_timesteps | 356400   |
---------------------------------
Progress update from timestep 356500: Updated shield, loss is 0.03902968764305115
Progress update from timestep 356600: Updated shield, loss is 0.04009934142231941
Eval num_timesteps=356600, episode_reward=23.26 +/- 14.70
Episode length: 33.40 +/- 20.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.4         |
|    mean_reward          | 23.3         |
| time/                   |              |
|    total_timesteps      | 356600       |
| train/                  |              |
|    approx_kl            | 0.0004973578 |
|    clip_fraction        | 0.0134       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0226      |
|    explained_variance   | 0.207        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.462        |
|    n_updates            | 8910         |
|    policy_gradient_loss | -0.00104     |
|    value_loss           | 1.91         |
------------------------------------------
Progress update from timestep 356700: Updated shield, loss is 0.04078975319862366
Progress update from timestep 356800: Updated shield, loss is 0.04008406400680542
Eval num_timesteps=356800, episode_reward=12.05 +/- 12.55
Episode length: 17.40 +/- 16.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 12       |
| time/              |          |
|    total_timesteps | 356800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.18     |
|    ep_rew_mean     | 2.65     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 892      |
|    time_elapsed    | 80835    |
|    total_timesteps | 356800   |
---------------------------------
Progress update from timestep 356900: Updated shield, loss is 0.03815268352627754
Progress update from timestep 357000: Updated shield, loss is 0.049311794340610504
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 357000: Saved shield_loss.csv. 
Eval num_timesteps=357000, episode_reward=12.77 +/- 12.62
Episode length: 18.60 +/- 16.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 18.6          |
|    mean_reward          | 12.8          |
| time/                   |               |
|    total_timesteps      | 357000        |
| train/                  |               |
|    approx_kl            | 0.00016328131 |
|    clip_fraction        | 0.00692       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.013        |
|    explained_variance   | 0.203         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.434         |
|    n_updates            | 8920          |
|    policy_gradient_loss | -0.000699     |
|    value_loss           | 2.18          |
-------------------------------------------
Progress update from timestep 357100: Updated shield, loss is 0.04193329066038132
Progress update from timestep 357200: Updated shield, loss is 0.041231539100408554
Eval num_timesteps=357200, episode_reward=22.61 +/- 14.61
Episode length: 33.20 +/- 20.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | 22.6     |
| time/              |          |
|    total_timesteps | 357200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.18     |
|    ep_rew_mean     | 2.59     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 893      |
|    time_elapsed    | 80927    |
|    total_timesteps | 357200   |
---------------------------------
Progress update from timestep 357300: Updated shield, loss is 0.042988672852516174
Progress update from timestep 357400: Updated shield, loss is 0.04137715324759483
Eval num_timesteps=357400, episode_reward=17.36 +/- 14.53
Episode length: 25.20 +/- 20.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.2          |
|    mean_reward          | 17.4          |
| time/                   |               |
|    total_timesteps      | 357400        |
| train/                  |               |
|    approx_kl            | 1.8380573e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0078       |
|    explained_variance   | 0.197         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.987         |
|    n_updates            | 8930          |
|    policy_gradient_loss | -9.67e-05     |
|    value_loss           | 2.02          |
-------------------------------------------
Progress update from timestep 357500: Updated shield, loss is 0.03744138032197952
Progress update from timestep 357600: Updated shield, loss is 0.035721778869628906
Eval num_timesteps=357600, episode_reward=23.27 +/- 14.67
Episode length: 33.60 +/- 20.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 23.3     |
| time/              |          |
|    total_timesteps | 357600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.43     |
|    ep_rew_mean     | 2.77     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 894      |
|    time_elapsed    | 81028    |
|    total_timesteps | 357600   |
---------------------------------
Progress update from timestep 357700: Updated shield, loss is 0.036556340754032135
Progress update from timestep 357800: Updated shield, loss is 0.040730614215135574
Eval num_timesteps=357800, episode_reward=16.19 +/- 16.29
Episode length: 23.00 +/- 22.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23            |
|    mean_reward          | 16.2          |
| time/                   |               |
|    total_timesteps      | 357800        |
| train/                  |               |
|    approx_kl            | 3.2339034e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.000318     |
|    explained_variance   | 0.214         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.405         |
|    n_updates            | 8940          |
|    policy_gradient_loss | -7.16e-08     |
|    value_loss           | 1.68          |
-------------------------------------------
Progress update from timestep 357900: Updated shield, loss is 0.04136849567294121
Progress update from timestep 358000: Updated shield, loss is 0.033521149307489395
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 358000: Saved shield_loss.csv. 
Eval num_timesteps=358000, episode_reward=13.98 +/- 11.32
Episode length: 20.60 +/- 15.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 14       |
| time/              |          |
|    total_timesteps | 358000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.12     |
|    ep_rew_mean     | 2.6      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 895      |
|    time_elapsed    | 81118    |
|    total_timesteps | 358000   |
---------------------------------
Progress update from timestep 358100: Updated shield, loss is 0.04526659473776817
Progress update from timestep 358200: Updated shield, loss is 0.03732043504714966
Eval num_timesteps=358200, episode_reward=18.30 +/- 15.83
Episode length: 25.80 +/- 21.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.8          |
|    mean_reward          | 18.3          |
| time/                   |               |
|    total_timesteps      | 358200        |
| train/                  |               |
|    approx_kl            | 6.5549546e-05 |
|    clip_fraction        | 0.00179       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0144       |
|    explained_variance   | 0.246         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.768         |
|    n_updates            | 8950          |
|    policy_gradient_loss | -0.000414     |
|    value_loss           | 2.15          |
-------------------------------------------
Progress update from timestep 358300: Updated shield, loss is 0.04527377337217331
Progress update from timestep 358400: Updated shield, loss is 0.028987040743231773
Eval num_timesteps=358400, episode_reward=11.37 +/- 11.55
Episode length: 17.60 +/- 17.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | 11.4     |
| time/              |          |
|    total_timesteps | 358400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.67     |
|    ep_rew_mean     | 2.28     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 896      |
|    time_elapsed    | 81209    |
|    total_timesteps | 358400   |
---------------------------------
Progress update from timestep 358500: Updated shield, loss is 0.03878523036837578
Progress update from timestep 358600: Updated shield, loss is 0.04076527804136276
Eval num_timesteps=358600, episode_reward=20.86 +/- 12.98
Episode length: 31.00 +/- 18.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 31           |
|    mean_reward          | 20.9         |
| time/                   |              |
|    total_timesteps      | 358600       |
| train/                  |              |
|    approx_kl            | 0.0005241554 |
|    clip_fraction        | 0.00937      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0289      |
|    explained_variance   | 0.157        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.463        |
|    n_updates            | 8960         |
|    policy_gradient_loss | -0.00144     |
|    value_loss           | 1.26         |
------------------------------------------
Progress update from timestep 358700: Updated shield, loss is 0.034435905516147614
Progress update from timestep 358800: Updated shield, loss is 0.03156368061900139
Eval num_timesteps=358800, episode_reward=9.67 +/- 12.57
Episode length: 14.60 +/- 17.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 9.67     |
| time/              |          |
|    total_timesteps | 358800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.55     |
|    ep_rew_mean     | 2.88     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 897      |
|    time_elapsed    | 81305    |
|    total_timesteps | 358800   |
---------------------------------
Progress update from timestep 358900: Updated shield, loss is 0.035857561975717545
Progress update from timestep 359000: Updated shield, loss is 0.03561535105109215
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 359000: Saved shield_loss.csv. 
Eval num_timesteps=359000, episode_reward=17.02 +/- 15.29
Episode length: 24.60 +/- 20.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.6          |
|    mean_reward          | 17            |
| time/                   |               |
|    total_timesteps      | 359000        |
| train/                  |               |
|    approx_kl            | -4.573459e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.000736     |
|    explained_variance   | 0.252         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.63          |
|    n_updates            | 8970          |
|    policy_gradient_loss | -3.99e-07     |
|    value_loss           | 2.1           |
-------------------------------------------
Progress update from timestep 359100: Updated shield, loss is 0.03492492064833641
Progress update from timestep 359200: Updated shield, loss is 0.037074487656354904
Eval num_timesteps=359200, episode_reward=16.32 +/- 14.43
Episode length: 24.40 +/- 20.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 16.3     |
| time/              |          |
|    total_timesteps | 359200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.45     |
|    ep_rew_mean     | 2.78     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 898      |
|    time_elapsed    | 81399    |
|    total_timesteps | 359200   |
---------------------------------
Progress update from timestep 359300: Updated shield, loss is 0.03952329605817795
Progress update from timestep 359400: Updated shield, loss is 0.03791627287864685
Eval num_timesteps=359400, episode_reward=8.74 +/- 6.72
Episode length: 13.40 +/- 9.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 13.4         |
|    mean_reward          | 8.74         |
| time/                   |              |
|    total_timesteps      | 359400       |
| train/                  |              |
|    approx_kl            | 6.635189e-05 |
|    clip_fraction        | 0.00223      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0104      |
|    explained_variance   | 0.184        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.881        |
|    n_updates            | 8980         |
|    policy_gradient_loss | -0.000157    |
|    value_loss           | 2.01         |
------------------------------------------
Progress update from timestep 359500: Updated shield, loss is 0.029230032116174698
Progress update from timestep 359600: Updated shield, loss is 0.03249767795205116
Eval num_timesteps=359600, episode_reward=4.71 +/- 2.90
Episode length: 7.80 +/- 4.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7.8      |
|    mean_reward     | 4.71     |
| time/              |          |
|    total_timesteps | 359600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.55     |
|    ep_rew_mean     | 2.87     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 899      |
|    time_elapsed    | 81468    |
|    total_timesteps | 359600   |
---------------------------------
Progress update from timestep 359700: Updated shield, loss is 0.03543495759367943
Progress update from timestep 359800: Updated shield, loss is 0.04523415118455887
Eval num_timesteps=359800, episode_reward=9.91 +/- 11.81
Episode length: 15.40 +/- 17.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.4         |
|    mean_reward          | 9.91         |
| time/                   |              |
|    total_timesteps      | 359800       |
| train/                  |              |
|    approx_kl            | 0.0001921717 |
|    clip_fraction        | 0.00781      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0278      |
|    explained_variance   | 0.0807       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.28         |
|    n_updates            | 8990         |
|    policy_gradient_loss | -0.00169     |
|    value_loss           | 2.68         |
------------------------------------------
Progress update from timestep 359900: Updated shield, loss is 0.034812476485967636
Progress update from timestep 360000: Updated shield, loss is 0.03746488317847252
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 360000: Saved shield_loss.csv. 
Eval num_timesteps=360000, episode_reward=20.29 +/- 13.87
Episode length: 28.80 +/- 18.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.8     |
|    mean_reward     | 20.3     |
| time/              |          |
|    total_timesteps | 360000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.06     |
|    ep_rew_mean     | 2.51     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 900      |
|    time_elapsed    | 81571    |
|    total_timesteps | 360000   |
---------------------------------
Progress update from timestep 360100: Updated shield, loss is 0.0341065376996994
Progress update from timestep 360200: Updated shield, loss is 0.03498997539281845
Eval num_timesteps=360200, episode_reward=20.11 +/- 13.77
Episode length: 29.20 +/- 19.57
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 29.2          |
|    mean_reward          | 20.1          |
| time/                   |               |
|    total_timesteps      | 360200        |
| train/                  |               |
|    approx_kl            | 0.00027916263 |
|    clip_fraction        | 0.00491       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0134       |
|    explained_variance   | 0.149         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.912         |
|    n_updates            | 9000          |
|    policy_gradient_loss | -0.00098      |
|    value_loss           | 1.79          |
-------------------------------------------
Progress update from timestep 360300: Updated shield, loss is 0.039977774024009705
Progress update from timestep 360400: Updated shield, loss is 0.044365379959344864
Eval num_timesteps=360400, episode_reward=18.06 +/- 13.13
Episode length: 27.20 +/- 19.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.2     |
|    mean_reward     | 18.1     |
| time/              |          |
|    total_timesteps | 360400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.27     |
|    ep_rew_mean     | 2.7      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 901      |
|    time_elapsed    | 81672    |
|    total_timesteps | 360400   |
---------------------------------
Progress update from timestep 360500: Updated shield, loss is 0.04295562207698822
Progress update from timestep 360600: Updated shield, loss is 0.033086031675338745
Eval num_timesteps=360600, episode_reward=16.68 +/- 15.57
Episode length: 24.20 +/- 21.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.2          |
|    mean_reward          | 16.7          |
| time/                   |               |
|    total_timesteps      | 360600        |
| train/                  |               |
|    approx_kl            | 2.1940761e-05 |
|    clip_fraction        | 0.00179       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00683      |
|    explained_variance   | 0.192         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.617         |
|    n_updates            | 9010          |
|    policy_gradient_loss | -0.000458     |
|    value_loss           | 1.62          |
-------------------------------------------
Progress update from timestep 360700: Updated shield, loss is 0.032835256308317184
Progress update from timestep 360800: Updated shield, loss is 0.034343499690294266
Eval num_timesteps=360800, episode_reward=29.24 +/- 10.51
Episode length: 42.60 +/- 14.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.6     |
|    mean_reward     | 29.2     |
| time/              |          |
|    total_timesteps | 360800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.04     |
|    ep_rew_mean     | 2.51     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 902      |
|    time_elapsed    | 81788    |
|    total_timesteps | 360800   |
---------------------------------
Progress update from timestep 360900: Updated shield, loss is 0.043296150863170624
Progress update from timestep 361000: Updated shield, loss is 0.0442853718996048
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 361000: Saved shield_loss.csv. 
Eval num_timesteps=361000, episode_reward=15.73 +/- 15.82
Episode length: 23.00 +/- 22.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23           |
|    mean_reward          | 15.7         |
| time/                   |              |
|    total_timesteps      | 361000       |
| train/                  |              |
|    approx_kl            | 0.0011427484 |
|    clip_fraction        | 0.0158       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0288      |
|    explained_variance   | 0.0876       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.882        |
|    n_updates            | 9020         |
|    policy_gradient_loss | -0.00137     |
|    value_loss           | 1.56         |
------------------------------------------
Progress update from timestep 361100: Updated shield, loss is 0.03877391666173935
Progress update from timestep 361200: Updated shield, loss is 0.04879448562860489
Eval num_timesteps=361200, episode_reward=17.12 +/- 13.81
Episode length: 25.80 +/- 19.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | 17.1     |
| time/              |          |
|    total_timesteps | 361200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.04     |
|    ep_rew_mean     | 2.48     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 903      |
|    time_elapsed    | 81884    |
|    total_timesteps | 361200   |
---------------------------------
Progress update from timestep 361300: Updated shield, loss is 0.035570841282606125
Progress update from timestep 361400: Updated shield, loss is 0.0349213182926178
Eval num_timesteps=361400, episode_reward=17.72 +/- 14.15
Episode length: 25.80 +/- 19.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.8          |
|    mean_reward          | 17.7          |
| time/                   |               |
|    total_timesteps      | 361400        |
| train/                  |               |
|    approx_kl            | 0.00011113994 |
|    clip_fraction        | 0.00313       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00702      |
|    explained_variance   | 0.135         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.629         |
|    n_updates            | 9030          |
|    policy_gradient_loss | -0.000187     |
|    value_loss           | 1.61          |
-------------------------------------------
Progress update from timestep 361500: Updated shield, loss is 0.042579688131809235
Progress update from timestep 361600: Updated shield, loss is 0.031386297196149826
Eval num_timesteps=361600, episode_reward=18.09 +/- 15.08
Episode length: 25.60 +/- 20.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 18.1     |
| time/              |          |
|    total_timesteps | 361600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.28     |
|    ep_rew_mean     | 2.72     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 904      |
|    time_elapsed    | 81978    |
|    total_timesteps | 361600   |
---------------------------------
Progress update from timestep 361700: Updated shield, loss is 0.04812101274728775
Progress update from timestep 361800: Updated shield, loss is 0.045488473027944565
Eval num_timesteps=361800, episode_reward=11.29 +/- 12.35
Episode length: 16.80 +/- 16.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.8         |
|    mean_reward          | 11.3         |
| time/                   |              |
|    total_timesteps      | 361800       |
| train/                  |              |
|    approx_kl            | 0.0001235153 |
|    clip_fraction        | 0.00268      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00141     |
|    explained_variance   | 0.23         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.786        |
|    n_updates            | 9040         |
|    policy_gradient_loss | -0.000286    |
|    value_loss           | 1.8          |
------------------------------------------
Progress update from timestep 361900: Updated shield, loss is 0.04053483158349991
Progress update from timestep 362000: Updated shield, loss is 0.037215765565633774
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 362000: Saved shield_loss.csv. 
Eval num_timesteps=362000, episode_reward=16.98 +/- 14.40
Episode length: 25.00 +/- 20.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 362000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.42     |
|    ep_rew_mean     | 2.77     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 905      |
|    time_elapsed    | 82091    |
|    total_timesteps | 362000   |
---------------------------------
Progress update from timestep 362100: Updated shield, loss is 0.03937278687953949
Progress update from timestep 362200: Updated shield, loss is 0.04001197963953018
Eval num_timesteps=362200, episode_reward=18.34 +/- 13.83
Episode length: 27.00 +/- 19.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 27            |
|    mean_reward          | 18.3          |
| time/                   |               |
|    total_timesteps      | 362200        |
| train/                  |               |
|    approx_kl            | 0.00039793996 |
|    clip_fraction        | 0.00647       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00696      |
|    explained_variance   | 0.112         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.819         |
|    n_updates            | 9050          |
|    policy_gradient_loss | -0.000865     |
|    value_loss           | 1.38          |
-------------------------------------------
Progress update from timestep 362300: Updated shield, loss is 0.04122979938983917
Progress update from timestep 362400: Updated shield, loss is 0.040545251220464706
Eval num_timesteps=362400, episode_reward=19.06 +/- 15.16
Episode length: 27.20 +/- 20.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.2     |
|    mean_reward     | 19.1     |
| time/              |          |
|    total_timesteps | 362400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.74     |
|    ep_rew_mean     | 3.01     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 906      |
|    time_elapsed    | 82184    |
|    total_timesteps | 362400   |
---------------------------------
Progress update from timestep 362500: Updated shield, loss is 0.04170146957039833
Progress update from timestep 362600: Updated shield, loss is 0.03642354533076286
Eval num_timesteps=362600, episode_reward=12.46 +/- 10.54
Episode length: 19.00 +/- 15.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 19            |
|    mean_reward          | 12.5          |
| time/                   |               |
|    total_timesteps      | 362600        |
| train/                  |               |
|    approx_kl            | 0.00014879144 |
|    clip_fraction        | 0.00513       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00856      |
|    explained_variance   | 0.114         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.22          |
|    n_updates            | 9060          |
|    policy_gradient_loss | -0.000412     |
|    value_loss           | 1.76          |
-------------------------------------------
Progress update from timestep 362700: Updated shield, loss is 0.03977790102362633
Progress update from timestep 362800: Updated shield, loss is 0.04685724899172783
Eval num_timesteps=362800, episode_reward=22.71 +/- 15.83
Episode length: 32.40 +/- 21.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.4     |
|    mean_reward     | 22.7     |
| time/              |          |
|    total_timesteps | 362800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.81     |
|    ep_rew_mean     | 2.39     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 907      |
|    time_elapsed    | 82282    |
|    total_timesteps | 362800   |
---------------------------------
Progress update from timestep 362900: Updated shield, loss is 0.040539637207984924
Progress update from timestep 363000: Updated shield, loss is 0.035830214619636536
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 363000: Saved shield_loss.csv. 
Eval num_timesteps=363000, episode_reward=23.70 +/- 14.13
Episode length: 34.00 +/- 19.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34           |
|    mean_reward          | 23.7         |
| time/                   |              |
|    total_timesteps      | 363000       |
| train/                  |              |
|    approx_kl            | 5.478093e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00536     |
|    explained_variance   | 0.112        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.73         |
|    n_updates            | 9070         |
|    policy_gradient_loss | -5.22e-05    |
|    value_loss           | 1.56         |
------------------------------------------
Progress update from timestep 363100: Updated shield, loss is 0.04253259673714638
Progress update from timestep 363200: Updated shield, loss is 0.036373771727085114
Eval num_timesteps=363200, episode_reward=21.73 +/- 15.19
Episode length: 32.20 +/- 21.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 21.7     |
| time/              |          |
|    total_timesteps | 363200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.41     |
|    ep_rew_mean     | 2.78     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 908      |
|    time_elapsed    | 82376    |
|    total_timesteps | 363200   |
---------------------------------
Progress update from timestep 363300: Updated shield, loss is 0.043430034071207047
Progress update from timestep 363400: Updated shield, loss is 0.038201868534088135
Eval num_timesteps=363400, episode_reward=30.94 +/- 7.72
Episode length: 44.60 +/- 10.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 44.6         |
|    mean_reward          | 30.9         |
| time/                   |              |
|    total_timesteps      | 363400       |
| train/                  |              |
|    approx_kl            | 4.513133e-05 |
|    clip_fraction        | 0.00067      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00914     |
|    explained_variance   | 0.293        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.554        |
|    n_updates            | 9080         |
|    policy_gradient_loss | -0.000189    |
|    value_loss           | 1.73         |
------------------------------------------
Progress update from timestep 363500: Updated shield, loss is 0.037354569882154465
Progress update from timestep 363600: Updated shield, loss is 0.03804665431380272
Eval num_timesteps=363600, episode_reward=15.95 +/- 16.07
Episode length: 23.00 +/- 22.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 363600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.05     |
|    ep_rew_mean     | 2.51     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 909      |
|    time_elapsed    | 82492    |
|    total_timesteps | 363600   |
---------------------------------
Progress update from timestep 363700: Updated shield, loss is 0.03882628306746483
Progress update from timestep 363800: Updated shield, loss is 0.03380408510565758
Eval num_timesteps=363800, episode_reward=3.47 +/- 2.18
Episode length: 5.80 +/- 3.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 5.8           |
|    mean_reward          | 3.47          |
| time/                   |               |
|    total_timesteps      | 363800        |
| train/                  |               |
|    approx_kl            | 4.9913135e-05 |
|    clip_fraction        | 0.00179       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00785      |
|    explained_variance   | 0.114         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.421         |
|    n_updates            | 9090          |
|    policy_gradient_loss | -0.000191     |
|    value_loss           | 1.59          |
-------------------------------------------
Progress update from timestep 363900: Updated shield, loss is 0.04154040664434433
Progress update from timestep 364000: Updated shield, loss is 0.040247462689876556
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 364000: Saved shield_loss.csv. 
Eval num_timesteps=364000, episode_reward=3.52 +/- 2.53
Episode length: 5.80 +/- 3.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5.8      |
|    mean_reward     | 3.52     |
| time/              |          |
|    total_timesteps | 364000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.05     |
|    ep_rew_mean     | 2.48     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 910      |
|    time_elapsed    | 82549    |
|    total_timesteps | 364000   |
---------------------------------
Progress update from timestep 364100: Updated shield, loss is 0.04016255587339401
Progress update from timestep 364200: Updated shield, loss is 0.04203644022345543
Eval num_timesteps=364200, episode_reward=16.80 +/- 15.34
Episode length: 24.20 +/- 21.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.2         |
|    mean_reward          | 16.8         |
| time/                   |              |
|    total_timesteps      | 364200       |
| train/                  |              |
|    approx_kl            | 6.418434e-11 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00052     |
|    explained_variance   | 0.0569       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.956        |
|    n_updates            | 9100         |
|    policy_gradient_loss | -2.69e-08    |
|    value_loss           | 1.51         |
------------------------------------------
Progress update from timestep 364300: Updated shield, loss is 0.032971106469631195
Progress update from timestep 364400: Updated shield, loss is 0.042493898421525955
Eval num_timesteps=364400, episode_reward=27.62 +/- 13.36
Episode length: 38.60 +/- 18.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 38.6     |
|    mean_reward     | 27.6     |
| time/              |          |
|    total_timesteps | 364400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.54     |
|    ep_rew_mean     | 2.92     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 911      |
|    time_elapsed    | 82641    |
|    total_timesteps | 364400   |
---------------------------------
Progress update from timestep 364500: Updated shield, loss is 0.03555925562977791
Progress update from timestep 364600: Updated shield, loss is 0.030714131891727448
Eval num_timesteps=364600, episode_reward=17.16 +/- 14.59
Episode length: 25.60 +/- 20.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.6          |
|    mean_reward          | 17.2          |
| time/                   |               |
|    total_timesteps      | 364600        |
| train/                  |               |
|    approx_kl            | 0.00024543572 |
|    clip_fraction        | 0.00848       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0114       |
|    explained_variance   | 0.183         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.94          |
|    n_updates            | 9110          |
|    policy_gradient_loss | -0.000615     |
|    value_loss           | 1.9           |
-------------------------------------------
Progress update from timestep 364700: Updated shield, loss is 0.03831130266189575
Progress update from timestep 364800: Updated shield, loss is 0.037383079528808594
Eval num_timesteps=364800, episode_reward=9.44 +/- 12.45
Episode length: 14.60 +/- 18.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 9.44     |
| time/              |          |
|    total_timesteps | 364800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.18     |
|    ep_rew_mean     | 2.63     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 912      |
|    time_elapsed    | 82732    |
|    total_timesteps | 364800   |
---------------------------------
Progress update from timestep 364900: Updated shield, loss is 0.03896046057343483
Progress update from timestep 365000: Updated shield, loss is 0.035471703857183456
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 365000: Saved shield_loss.csv. 
Eval num_timesteps=365000, episode_reward=10.33 +/- 12.68
Episode length: 15.40 +/- 17.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.4          |
|    mean_reward          | 10.3          |
| time/                   |               |
|    total_timesteps      | 365000        |
| train/                  |               |
|    approx_kl            | 0.00014236443 |
|    clip_fraction        | 0.00759       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0237       |
|    explained_variance   | 0.194         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.854         |
|    n_updates            | 9120          |
|    policy_gradient_loss | -0.00108      |
|    value_loss           | 1.5           |
-------------------------------------------
Progress update from timestep 365100: Updated shield, loss is 0.04289800301194191
Progress update from timestep 365200: Updated shield, loss is 0.028135621920228004
Eval num_timesteps=365200, episode_reward=28.36 +/- 12.82
Episode length: 41.00 +/- 18.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41       |
|    mean_reward     | 28.4     |
| time/              |          |
|    total_timesteps | 365200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.76     |
|    ep_rew_mean     | 2.3      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 913      |
|    time_elapsed    | 82845    |
|    total_timesteps | 365200   |
---------------------------------
Progress update from timestep 365300: Updated shield, loss is 0.03752265125513077
Progress update from timestep 365400: Updated shield, loss is 0.04485488682985306
Eval num_timesteps=365400, episode_reward=14.50 +/- 16.32
Episode length: 21.60 +/- 23.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 21.6         |
|    mean_reward          | 14.5         |
| time/                   |              |
|    total_timesteps      | 365400       |
| train/                  |              |
|    approx_kl            | 0.0042138356 |
|    clip_fraction        | 0.0225       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0287      |
|    explained_variance   | 0.195        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.835        |
|    n_updates            | 9130         |
|    policy_gradient_loss | -0.00204     |
|    value_loss           | 1.28         |
------------------------------------------
Progress update from timestep 365500: Updated shield, loss is 0.0399947315454483
Progress update from timestep 365600: Updated shield, loss is 0.038580961525440216
Eval num_timesteps=365600, episode_reward=22.77 +/- 14.06
Episode length: 33.80 +/- 20.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 22.8     |
| time/              |          |
|    total_timesteps | 365600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.82     |
|    ep_rew_mean     | 2.31     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 914      |
|    time_elapsed    | 82938    |
|    total_timesteps | 365600   |
---------------------------------
Progress update from timestep 365700: Updated shield, loss is 0.03689669072628021
Progress update from timestep 365800: Updated shield, loss is 0.03697195649147034
Eval num_timesteps=365800, episode_reward=10.57 +/- 12.04
Episode length: 16.00 +/- 17.12
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16         |
|    mean_reward          | 10.6       |
| time/                   |            |
|    total_timesteps      | 365800     |
| train/                  |            |
|    approx_kl            | 0.00275432 |
|    clip_fraction        | 0.0132     |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.02      |
|    explained_variance   | 0.149      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.689      |
|    n_updates            | 9140       |
|    policy_gradient_loss | -0.00175   |
|    value_loss           | 1.23       |
----------------------------------------
Progress update from timestep 365900: Updated shield, loss is 0.04223437234759331
Progress update from timestep 366000: Updated shield, loss is 0.041095901280641556
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 366000: Saved shield_loss.csv. 
Eval num_timesteps=366000, episode_reward=16.26 +/- 14.44
Episode length: 24.40 +/- 20.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 16.3     |
| time/              |          |
|    total_timesteps | 366000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.66     |
|    ep_rew_mean     | 2.21     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 915      |
|    time_elapsed    | 83032    |
|    total_timesteps | 366000   |
---------------------------------
Progress update from timestep 366100: Updated shield, loss is 0.03630515933036804
Progress update from timestep 366200: Updated shield, loss is 0.03626369312405586
Eval num_timesteps=366200, episode_reward=10.57 +/- 12.54
Episode length: 15.80 +/- 17.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.8          |
|    mean_reward          | 10.6          |
| time/                   |               |
|    total_timesteps      | 366200        |
| train/                  |               |
|    approx_kl            | 0.00019737097 |
|    clip_fraction        | 0.00893       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0169       |
|    explained_variance   | 0.214         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.779         |
|    n_updates            | 9150          |
|    policy_gradient_loss | -0.0011       |
|    value_loss           | 1.04          |
-------------------------------------------
Progress update from timestep 366300: Updated shield, loss is 0.04075866565108299
Progress update from timestep 366400: Updated shield, loss is 0.04711950197815895
Eval num_timesteps=366400, episode_reward=12.12 +/- 11.45
Episode length: 18.60 +/- 17.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 12.1     |
| time/              |          |
|    total_timesteps | 366400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.04     |
|    ep_rew_mean     | 2.58     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 916      |
|    time_elapsed    | 83128    |
|    total_timesteps | 366400   |
---------------------------------
Progress update from timestep 366500: Updated shield, loss is 0.04435291886329651
Progress update from timestep 366600: Updated shield, loss is 0.04554908350110054
Eval num_timesteps=366600, episode_reward=16.89 +/- 14.91
Episode length: 24.60 +/- 20.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.6          |
|    mean_reward          | 16.9          |
| time/                   |               |
|    total_timesteps      | 366600        |
| train/                  |               |
|    approx_kl            | 0.00012794572 |
|    clip_fraction        | 0.00781       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0134       |
|    explained_variance   | 0.118         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.455         |
|    n_updates            | 9160          |
|    policy_gradient_loss | -0.000662     |
|    value_loss           | 2.27          |
-------------------------------------------
Progress update from timestep 366700: Updated shield, loss is 0.04605639725923538
Progress update from timestep 366800: Updated shield, loss is 0.041235893964767456
Eval num_timesteps=366800, episode_reward=9.05 +/- 12.27
Episode length: 14.20 +/- 18.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.2     |
|    mean_reward     | 9.05     |
| time/              |          |
|    total_timesteps | 366800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.02     |
|    ep_rew_mean     | 2.48     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 917      |
|    time_elapsed    | 83225    |
|    total_timesteps | 366800   |
---------------------------------
Progress update from timestep 366900: Updated shield, loss is 0.03605172038078308
Progress update from timestep 367000: Updated shield, loss is 0.05361795052886009
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 367000: Saved shield_loss.csv. 
Eval num_timesteps=367000, episode_reward=21.80 +/- 16.01
Episode length: 31.60 +/- 22.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 31.6          |
|    mean_reward          | 21.8          |
| time/                   |               |
|    total_timesteps      | 367000        |
| train/                  |               |
|    approx_kl            | 0.00041007323 |
|    clip_fraction        | 0.00647       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0186       |
|    explained_variance   | 0.0232        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.917         |
|    n_updates            | 9170          |
|    policy_gradient_loss | -0.000526     |
|    value_loss           | 1.59          |
-------------------------------------------
Progress update from timestep 367100: Updated shield, loss is 0.044129349291324615
Progress update from timestep 367200: Updated shield, loss is 0.04333500564098358
Eval num_timesteps=367200, episode_reward=10.89 +/- 13.01
Episode length: 15.80 +/- 17.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | 10.9     |
| time/              |          |
|    total_timesteps | 367200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.18     |
|    ep_rew_mean     | 2.59     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 918      |
|    time_elapsed    | 83337    |
|    total_timesteps | 367200   |
---------------------------------
Progress update from timestep 367300: Updated shield, loss is 0.037217624485492706
Progress update from timestep 367400: Updated shield, loss is 0.047178566455841064
Eval num_timesteps=367400, episode_reward=10.10 +/- 11.93
Episode length: 15.40 +/- 17.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.4         |
|    mean_reward          | 10.1         |
| time/                   |              |
|    total_timesteps      | 367400       |
| train/                  |              |
|    approx_kl            | 5.570715e-05 |
|    clip_fraction        | 0.00223      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00616     |
|    explained_variance   | 0.145        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.518        |
|    n_updates            | 9180         |
|    policy_gradient_loss | -0.000334    |
|    value_loss           | 1.69         |
------------------------------------------
Progress update from timestep 367500: Updated shield, loss is 0.05065212398767471
Progress update from timestep 367600: Updated shield, loss is 0.03167593479156494
Eval num_timesteps=367600, episode_reward=22.36 +/- 15.37
Episode length: 32.40 +/- 21.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.4     |
|    mean_reward     | 22.4     |
| time/              |          |
|    total_timesteps | 367600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.57     |
|    ep_rew_mean     | 2.95     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 919      |
|    time_elapsed    | 83452    |
|    total_timesteps | 367600   |
---------------------------------
Progress update from timestep 367700: Updated shield, loss is 0.04304889962077141
Progress update from timestep 367800: Updated shield, loss is 0.04360488802194595
Eval num_timesteps=367800, episode_reward=28.41 +/- 13.82
Episode length: 40.40 +/- 19.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 40.4          |
|    mean_reward          | 28.4          |
| time/                   |               |
|    total_timesteps      | 367800        |
| train/                  |               |
|    approx_kl            | 3.8731482e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00066      |
|    explained_variance   | 0.159         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.79          |
|    n_updates            | 9190          |
|    policy_gradient_loss | 1.66e-08      |
|    value_loss           | 1.84          |
-------------------------------------------
Progress update from timestep 367900: Updated shield, loss is 0.04875867813825607
Progress update from timestep 368000: Updated shield, loss is 0.04989632964134216
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 368000: Saved shield_loss.csv. 
Eval num_timesteps=368000, episode_reward=21.19 +/- 15.82
Episode length: 31.40 +/- 22.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.4     |
|    mean_reward     | 21.2     |
| time/              |          |
|    total_timesteps | 368000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.07     |
|    ep_rew_mean     | 2.48     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 920      |
|    time_elapsed    | 83566    |
|    total_timesteps | 368000   |
---------------------------------
Progress update from timestep 368100: Updated shield, loss is 0.03811432421207428
Progress update from timestep 368200: Updated shield, loss is 0.03562389686703682
Eval num_timesteps=368200, episode_reward=22.17 +/- 15.56
Episode length: 32.20 +/- 21.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32.2          |
|    mean_reward          | 22.2          |
| time/                   |               |
|    total_timesteps      | 368200        |
| train/                  |               |
|    approx_kl            | 2.2429087e-05 |
|    clip_fraction        | 0.000893      |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00431      |
|    explained_variance   | 0.0648        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.75          |
|    n_updates            | 9200          |
|    policy_gradient_loss | -0.000271     |
|    value_loss           | 1.85          |
-------------------------------------------
Progress update from timestep 368300: Updated shield, loss is 0.029175177216529846
Progress update from timestep 368400: Updated shield, loss is 0.053590077906847
Eval num_timesteps=368400, episode_reward=14.12 +/- 10.81
Episode length: 21.60 +/- 15.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 14.1     |
| time/              |          |
|    total_timesteps | 368400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.18     |
|    ep_rew_mean     | 2.58     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 921      |
|    time_elapsed    | 83658    |
|    total_timesteps | 368400   |
---------------------------------
Progress update from timestep 368500: Updated shield, loss is 0.03649286925792694
Progress update from timestep 368600: Updated shield, loss is 0.034360360354185104
Eval num_timesteps=368600, episode_reward=11.05 +/- 12.66
Episode length: 16.60 +/- 17.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.6         |
|    mean_reward          | 11           |
| time/                   |              |
|    total_timesteps      | 368600       |
| train/                  |              |
|    approx_kl            | 6.224213e-05 |
|    clip_fraction        | 0.00313      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0107      |
|    explained_variance   | 0.156        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.349        |
|    n_updates            | 9210         |
|    policy_gradient_loss | -0.000562    |
|    value_loss           | 1.37         |
------------------------------------------
Progress update from timestep 368700: Updated shield, loss is 0.03586525470018387
Progress update from timestep 368800: Updated shield, loss is 0.03852220997214317
Eval num_timesteps=368800, episode_reward=16.28 +/- 15.00
Episode length: 24.20 +/- 21.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 16.3     |
| time/              |          |
|    total_timesteps | 368800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.57     |
|    ep_rew_mean     | 2.93     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 922      |
|    time_elapsed    | 83756    |
|    total_timesteps | 368800   |
---------------------------------
Progress update from timestep 368900: Updated shield, loss is 0.03854236379265785
Progress update from timestep 369000: Updated shield, loss is 0.04217411205172539
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 369000: Saved shield_loss.csv. 
Eval num_timesteps=369000, episode_reward=17.31 +/- 14.44
Episode length: 26.00 +/- 20.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26            |
|    mean_reward          | 17.3          |
| time/                   |               |
|    total_timesteps      | 369000        |
| train/                  |               |
|    approx_kl            | 0.00041193073 |
|    clip_fraction        | 0.0107        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0117       |
|    explained_variance   | 0.218         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.537         |
|    n_updates            | 9220          |
|    policy_gradient_loss | -0.00121      |
|    value_loss           | 1.85          |
-------------------------------------------
Progress update from timestep 369100: Updated shield, loss is 0.03364270552992821
Progress update from timestep 369200: Updated shield, loss is 0.04720849171280861
Eval num_timesteps=369200, episode_reward=27.97 +/- 11.92
Episode length: 41.40 +/- 17.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | 28       |
| time/              |          |
|    total_timesteps | 369200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.84     |
|    ep_rew_mean     | 2.39     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 923      |
|    time_elapsed    | 83850    |
|    total_timesteps | 369200   |
---------------------------------
Progress update from timestep 369300: Updated shield, loss is 0.038146499544382095
Progress update from timestep 369400: Updated shield, loss is 0.03358207270503044
Eval num_timesteps=369400, episode_reward=16.98 +/- 11.30
Episode length: 24.60 +/- 15.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.6          |
|    mean_reward          | 17            |
| time/                   |               |
|    total_timesteps      | 369400        |
| train/                  |               |
|    approx_kl            | -9.660133e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.000412     |
|    explained_variance   | 0.188         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.02          |
|    n_updates            | 9230          |
|    policy_gradient_loss | -7.23e-07     |
|    value_loss           | 1.56          |
-------------------------------------------
Progress update from timestep 369500: Updated shield, loss is 0.03237897902727127
Progress update from timestep 369600: Updated shield, loss is 0.03881927952170372
Eval num_timesteps=369600, episode_reward=17.48 +/- 13.69
Episode length: 26.00 +/- 19.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | 17.5     |
| time/              |          |
|    total_timesteps | 369600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.14     |
|    ep_rew_mean     | 2.6      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 924      |
|    time_elapsed    | 83972    |
|    total_timesteps | 369600   |
---------------------------------
Progress update from timestep 369700: Updated shield, loss is 0.03610607609152794
Progress update from timestep 369800: Updated shield, loss is 0.04161905124783516
Eval num_timesteps=369800, episode_reward=16.24 +/- 15.83
Episode length: 23.40 +/- 21.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.4          |
|    mean_reward          | 16.2          |
| time/                   |               |
|    total_timesteps      | 369800        |
| train/                  |               |
|    approx_kl            | 0.00037558397 |
|    clip_fraction        | 0.00893       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0149       |
|    explained_variance   | 0.141         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.761         |
|    n_updates            | 9240          |
|    policy_gradient_loss | -0.000986     |
|    value_loss           | 2.37          |
-------------------------------------------
Progress update from timestep 369900: Updated shield, loss is 0.03598644584417343
Progress update from timestep 370000: Updated shield, loss is 0.03471190109848976
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 370000: Saved shield_loss.csv. 
Eval num_timesteps=370000, episode_reward=11.46 +/- 12.79
Episode length: 16.80 +/- 16.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | 11.5     |
| time/              |          |
|    total_timesteps | 370000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.03     |
|    ep_rew_mean     | 2.53     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 925      |
|    time_elapsed    | 84063    |
|    total_timesteps | 370000   |
---------------------------------
Progress update from timestep 370100: Updated shield, loss is 0.03438453748822212
Progress update from timestep 370200: Updated shield, loss is 0.03430059924721718
Eval num_timesteps=370200, episode_reward=17.49 +/- 14.72
Episode length: 25.40 +/- 20.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.4         |
|    mean_reward          | 17.5         |
| time/                   |              |
|    total_timesteps      | 370200       |
| train/                  |              |
|    approx_kl            | 5.750421e-05 |
|    clip_fraction        | 0.00558      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00677     |
|    explained_variance   | 0.0972       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.575        |
|    n_updates            | 9250         |
|    policy_gradient_loss | -0.000974    |
|    value_loss           | 1.82         |
------------------------------------------
Progress update from timestep 370300: Updated shield, loss is 0.03662163019180298
Progress update from timestep 370400: Updated shield, loss is 0.0356108583509922
Eval num_timesteps=370400, episode_reward=9.84 +/- 12.96
Episode length: 14.60 +/- 17.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 9.84     |
| time/              |          |
|    total_timesteps | 370400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.23     |
|    ep_rew_mean     | 2.66     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 926      |
|    time_elapsed    | 84163    |
|    total_timesteps | 370400   |
---------------------------------
Progress update from timestep 370500: Updated shield, loss is 0.034476522356271744
Progress update from timestep 370600: Updated shield, loss is 0.03604859113693237
Eval num_timesteps=370600, episode_reward=8.45 +/- 12.55
Episode length: 13.20 +/- 18.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 13.2          |
|    mean_reward          | 8.45          |
| time/                   |               |
|    total_timesteps      | 370600        |
| train/                  |               |
|    approx_kl            | 0.00048451452 |
|    clip_fraction        | 0.0225        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0478       |
|    explained_variance   | 0.251         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.6           |
|    n_updates            | 9260          |
|    policy_gradient_loss | -0.00242      |
|    value_loss           | 1.98          |
-------------------------------------------
Progress update from timestep 370700: Updated shield, loss is 0.043947506695985794
Progress update from timestep 370800: Updated shield, loss is 0.027400333434343338
Eval num_timesteps=370800, episode_reward=12.49 +/- 12.54
Episode length: 18.00 +/- 16.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | 12.5     |
| time/              |          |
|    total_timesteps | 370800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.27     |
|    ep_rew_mean     | 2.73     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 927      |
|    time_elapsed    | 84257    |
|    total_timesteps | 370800   |
---------------------------------
Progress update from timestep 370900: Updated shield, loss is 0.03071259707212448
Progress update from timestep 371000: Updated shield, loss is 0.03196130692958832
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 371000: Saved shield_loss.csv. 
Eval num_timesteps=371000, episode_reward=17.46 +/- 15.03
Episode length: 25.20 +/- 20.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.2          |
|    mean_reward          | 17.5          |
| time/                   |               |
|    total_timesteps      | 371000        |
| train/                  |               |
|    approx_kl            | 0.00016101178 |
|    clip_fraction        | 0.00692       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0116       |
|    explained_variance   | 0.113         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.692         |
|    n_updates            | 9270          |
|    policy_gradient_loss | -0.00065      |
|    value_loss           | 2.37          |
-------------------------------------------
Progress update from timestep 371100: Updated shield, loss is 0.03047601506114006
Progress update from timestep 371200: Updated shield, loss is 0.04639844596385956
Eval num_timesteps=371200, episode_reward=30.69 +/- 10.37
Episode length: 43.00 +/- 14.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43       |
|    mean_reward     | 30.7     |
| time/              |          |
|    total_timesteps | 371200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.98     |
|    ep_rew_mean     | 2.45     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 928      |
|    time_elapsed    | 84377    |
|    total_timesteps | 371200   |
---------------------------------
Progress update from timestep 371300: Updated shield, loss is 0.04487822204828262
Progress update from timestep 371400: Updated shield, loss is 0.03235182538628578
Eval num_timesteps=371400, episode_reward=6.05 +/- 2.40
Episode length: 9.60 +/- 3.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 9.6           |
|    mean_reward          | 6.05          |
| time/                   |               |
|    total_timesteps      | 371400        |
| train/                  |               |
|    approx_kl            | 4.2866563e-05 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00665      |
|    explained_variance   | 0.159         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.74          |
|    n_updates            | 9280          |
|    policy_gradient_loss | -0.000198     |
|    value_loss           | 1.64          |
-------------------------------------------
Progress update from timestep 371500: Updated shield, loss is 0.0430675745010376
Progress update from timestep 371600: Updated shield, loss is 0.03444325923919678
Eval num_timesteps=371600, episode_reward=22.79 +/- 13.92
Episode length: 33.80 +/- 19.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 22.8     |
| time/              |          |
|    total_timesteps | 371600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4        |
|    ep_rew_mean     | 2.49     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 929      |
|    time_elapsed    | 84477    |
|    total_timesteps | 371600   |
---------------------------------
Progress update from timestep 371700: Updated shield, loss is 0.060888659209012985
Progress update from timestep 371800: Updated shield, loss is 0.035137541592121124
Eval num_timesteps=371800, episode_reward=18.70 +/- 14.71
Episode length: 26.40 +/- 19.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.4          |
|    mean_reward          | 18.7          |
| time/                   |               |
|    total_timesteps      | 371800        |
| train/                  |               |
|    approx_kl            | 0.00010810806 |
|    clip_fraction        | 0.00424       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0112       |
|    explained_variance   | 0.297         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.764         |
|    n_updates            | 9290          |
|    policy_gradient_loss | -0.000431     |
|    value_loss           | 1.52          |
-------------------------------------------
Progress update from timestep 371900: Updated shield, loss is 0.040553756058216095
Progress update from timestep 372000: Updated shield, loss is 0.04062929376959801
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 372000: Saved shield_loss.csv. 
Eval num_timesteps=372000, episode_reward=10.09 +/- 11.66
Episode length: 15.60 +/- 17.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 10.1     |
| time/              |          |
|    total_timesteps | 372000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.49     |
|    ep_rew_mean     | 2.89     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 930      |
|    time_elapsed    | 84576    |
|    total_timesteps | 372000   |
---------------------------------
Progress update from timestep 372100: Updated shield, loss is 0.0365457646548748
Progress update from timestep 372200: Updated shield, loss is 0.04397564381361008
Eval num_timesteps=372200, episode_reward=28.58 +/- 12.94
Episode length: 41.00 +/- 18.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 41            |
|    mean_reward          | 28.6          |
| time/                   |               |
|    total_timesteps      | 372200        |
| train/                  |               |
|    approx_kl            | 0.00021256138 |
|    clip_fraction        | 0.0058        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0143       |
|    explained_variance   | 0.286         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.639         |
|    n_updates            | 9300          |
|    policy_gradient_loss | -0.000339     |
|    value_loss           | 1.72          |
-------------------------------------------
Progress update from timestep 372300: Updated shield, loss is 0.04562763124704361
Progress update from timestep 372400: Updated shield, loss is 0.0443168468773365
Eval num_timesteps=372400, episode_reward=25.28 +/- 12.47
Episode length: 36.40 +/- 17.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 25.3     |
| time/              |          |
|    total_timesteps | 372400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.63     |
|    ep_rew_mean     | 3        |
| time/              |          |
|    fps             | 4        |
|    iterations      | 931      |
|    time_elapsed    | 84669    |
|    total_timesteps | 372400   |
---------------------------------
Progress update from timestep 372500: Updated shield, loss is 0.03903422877192497
Progress update from timestep 372600: Updated shield, loss is 0.035235144197940826
Eval num_timesteps=372600, episode_reward=13.73 +/- 11.54
Episode length: 20.80 +/- 16.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 20.8         |
|    mean_reward          | 13.7         |
| time/                   |              |
|    total_timesteps      | 372600       |
| train/                  |              |
|    approx_kl            | 7.057679e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00071     |
|    explained_variance   | 0.137        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.61         |
|    n_updates            | 9310         |
|    policy_gradient_loss | -2.6e-07     |
|    value_loss           | 2.05         |
------------------------------------------
Progress update from timestep 372700: Updated shield, loss is 0.04410361126065254
Progress update from timestep 372800: Updated shield, loss is 0.04476378858089447
Eval num_timesteps=372800, episode_reward=25.45 +/- 11.27
Episode length: 37.20 +/- 15.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.2     |
|    mean_reward     | 25.4     |
| time/              |          |
|    total_timesteps | 372800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.36     |
|    ep_rew_mean     | 2.67     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 932      |
|    time_elapsed    | 84782    |
|    total_timesteps | 372800   |
---------------------------------
Progress update from timestep 372900: Updated shield, loss is 0.04512876644730568
Progress update from timestep 373000: Updated shield, loss is 0.046865977346897125
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 373000: Saved shield_loss.csv. 
Eval num_timesteps=373000, episode_reward=23.12 +/- 15.32
Episode length: 32.80 +/- 21.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32.8          |
|    mean_reward          | 23.1          |
| time/                   |               |
|    total_timesteps      | 373000        |
| train/                  |               |
|    approx_kl            | 0.00057917787 |
|    clip_fraction        | 0.0105        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0225       |
|    explained_variance   | 0.191         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.744         |
|    n_updates            | 9320          |
|    policy_gradient_loss | -0.0014       |
|    value_loss           | 2.11          |
-------------------------------------------
Progress update from timestep 373100: Updated shield, loss is 0.040245357900857925
Progress update from timestep 373200: Updated shield, loss is 0.044137902557849884
Eval num_timesteps=373200, episode_reward=22.27 +/- 15.89
Episode length: 32.00 +/- 22.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32       |
|    mean_reward     | 22.3     |
| time/              |          |
|    total_timesteps | 373200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.34     |
|    ep_rew_mean     | 2.72     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 933      |
|    time_elapsed    | 84879    |
|    total_timesteps | 373200   |
---------------------------------
Progress update from timestep 373300: Updated shield, loss is 0.041130200028419495
Progress update from timestep 373400: Updated shield, loss is 0.04756348952651024
Eval num_timesteps=373400, episode_reward=13.16 +/- 11.49
Episode length: 19.40 +/- 15.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 19.4          |
|    mean_reward          | 13.2          |
| time/                   |               |
|    total_timesteps      | 373400        |
| train/                  |               |
|    approx_kl            | 0.00018281872 |
|    clip_fraction        | 0.00335       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00712      |
|    explained_variance   | 0.156         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.713         |
|    n_updates            | 9330          |
|    policy_gradient_loss | -0.000309     |
|    value_loss           | 2.09          |
-------------------------------------------
Progress update from timestep 373500: Updated shield, loss is 0.04203244671225548
Progress update from timestep 373600: Updated shield, loss is 0.04669933766126633
Eval num_timesteps=373600, episode_reward=23.33 +/- 13.33
Episode length: 34.40 +/- 19.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 23.3     |
| time/              |          |
|    total_timesteps | 373600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.41     |
|    ep_rew_mean     | 2.83     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 934      |
|    time_elapsed    | 84981    |
|    total_timesteps | 373600   |
---------------------------------
Progress update from timestep 373700: Updated shield, loss is 0.045310743153095245
Progress update from timestep 373800: Updated shield, loss is 0.04713660851120949
Eval num_timesteps=373800, episode_reward=22.27 +/- 16.32
Episode length: 31.60 +/- 22.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 31.6          |
|    mean_reward          | 22.3          |
| time/                   |               |
|    total_timesteps      | 373800        |
| train/                  |               |
|    approx_kl            | 0.00024066323 |
|    clip_fraction        | 0.00781       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0237       |
|    explained_variance   | 0.241         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.824         |
|    n_updates            | 9340          |
|    policy_gradient_loss | -0.00106      |
|    value_loss           | 1.7           |
-------------------------------------------
Progress update from timestep 373900: Updated shield, loss is 0.04122321680188179
Progress update from timestep 374000: Updated shield, loss is 0.03866074979305267
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 374000: Saved shield_loss.csv. 
Eval num_timesteps=374000, episode_reward=27.30 +/- 13.25
Episode length: 40.40 +/- 19.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.4     |
|    mean_reward     | 27.3     |
| time/              |          |
|    total_timesteps | 374000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.58     |
|    ep_rew_mean     | 2.9      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 935      |
|    time_elapsed    | 85095    |
|    total_timesteps | 374000   |
---------------------------------
Progress update from timestep 374100: Updated shield, loss is 0.03996181860566139
Progress update from timestep 374200: Updated shield, loss is 0.03643368184566498
Eval num_timesteps=374200, episode_reward=11.88 +/- 12.45
Episode length: 17.40 +/- 17.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17.4          |
|    mean_reward          | 11.9          |
| time/                   |               |
|    total_timesteps      | 374200        |
| train/                  |               |
|    approx_kl            | 0.00014059953 |
|    clip_fraction        | 0.00647       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0198       |
|    explained_variance   | 0.0658        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.07          |
|    n_updates            | 9350          |
|    policy_gradient_loss | -0.00097      |
|    value_loss           | 2.12          |
-------------------------------------------
Progress update from timestep 374300: Updated shield, loss is 0.036445703357458115
Progress update from timestep 374400: Updated shield, loss is 0.036587346345186234
Eval num_timesteps=374400, episode_reward=23.56 +/- 14.32
Episode length: 33.80 +/- 19.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 23.6     |
| time/              |          |
|    total_timesteps | 374400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.44     |
|    ep_rew_mean     | 2.84     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 936      |
|    time_elapsed    | 85191    |
|    total_timesteps | 374400   |
---------------------------------
Progress update from timestep 374500: Updated shield, loss is 0.0477561429142952
Progress update from timestep 374600: Updated shield, loss is 0.037668365985155106
Eval num_timesteps=374600, episode_reward=9.57 +/- 11.91
Episode length: 15.00 +/- 17.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15           |
|    mean_reward          | 9.57         |
| time/                   |              |
|    total_timesteps      | 374600       |
| train/                  |              |
|    approx_kl            | 0.0003618118 |
|    clip_fraction        | 0.00937      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0226      |
|    explained_variance   | 0.163        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.975        |
|    n_updates            | 9360         |
|    policy_gradient_loss | -0.00133     |
|    value_loss           | 2.31         |
------------------------------------------
Progress update from timestep 374700: Updated shield, loss is 0.03918875381350517
Progress update from timestep 374800: Updated shield, loss is 0.03473055362701416
Eval num_timesteps=374800, episode_reward=16.45 +/- 15.27
Episode length: 24.00 +/- 21.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 374800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.22     |
|    ep_rew_mean     | 2.64     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 937      |
|    time_elapsed    | 85287    |
|    total_timesteps | 374800   |
---------------------------------
Progress update from timestep 374900: Updated shield, loss is 0.037701934576034546
Progress update from timestep 375000: Updated shield, loss is 0.03829194977879524
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 375000: Saved shield_loss.csv. 
Eval num_timesteps=375000, episode_reward=17.30 +/- 15.51
Episode length: 24.60 +/- 20.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.6          |
|    mean_reward          | 17.3          |
| time/                   |               |
|    total_timesteps      | 375000        |
| train/                  |               |
|    approx_kl            | 0.00017228475 |
|    clip_fraction        | 0.00804       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0207       |
|    explained_variance   | 0.188         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.633         |
|    n_updates            | 9370          |
|    policy_gradient_loss | -0.00137      |
|    value_loss           | 1.98          |
-------------------------------------------
Progress update from timestep 375100: Updated shield, loss is 0.03453655168414116
Progress update from timestep 375200: Updated shield, loss is 0.04098356515169144
Eval num_timesteps=375200, episode_reward=25.01 +/- 14.24
Episode length: 35.00 +/- 18.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 25       |
| time/              |          |
|    total_timesteps | 375200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.26     |
|    ep_rew_mean     | 2.62     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 938      |
|    time_elapsed    | 85383    |
|    total_timesteps | 375200   |
---------------------------------
Progress update from timestep 375300: Updated shield, loss is 0.03125688061118126
Progress update from timestep 375400: Updated shield, loss is 0.03524766489863396
Eval num_timesteps=375400, episode_reward=10.79 +/- 12.97
Episode length: 15.80 +/- 17.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.8        |
|    mean_reward          | 10.8        |
| time/                   |             |
|    total_timesteps      | 375400      |
| train/                  |             |
|    approx_kl            | 8.09906e-05 |
|    clip_fraction        | 0.00156     |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0038     |
|    explained_variance   | 0.169       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.532       |
|    n_updates            | 9380        |
|    policy_gradient_loss | -0.000161   |
|    value_loss           | 1.86        |
-----------------------------------------
Progress update from timestep 375500: Updated shield, loss is 0.03356155753135681
Progress update from timestep 375600: Updated shield, loss is 0.033252399414777756
Eval num_timesteps=375600, episode_reward=2.84 +/- 1.50
Episode length: 5.00 +/- 2.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 2.84     |
| time/              |          |
|    total_timesteps | 375600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.97     |
|    ep_rew_mean     | 3.18     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 939      |
|    time_elapsed    | 85456    |
|    total_timesteps | 375600   |
---------------------------------
Progress update from timestep 375700: Updated shield, loss is 0.03959790989756584
Progress update from timestep 375800: Updated shield, loss is 0.044265881180763245
Eval num_timesteps=375800, episode_reward=12.01 +/- 13.37
Episode length: 17.60 +/- 17.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17.6          |
|    mean_reward          | 12            |
| time/                   |               |
|    total_timesteps      | 375800        |
| train/                  |               |
|    approx_kl            | 6.6626785e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00537      |
|    explained_variance   | 0.313         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.753         |
|    n_updates            | 9390          |
|    policy_gradient_loss | -7.76e-05     |
|    value_loss           | 1.87          |
-------------------------------------------
Progress update from timestep 375900: Updated shield, loss is 0.031534381210803986
Progress update from timestep 376000: Updated shield, loss is 0.04110781475901604
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 376000: Saved shield_loss.csv. 
Eval num_timesteps=376000, episode_reward=13.60 +/- 12.67
Episode length: 19.60 +/- 16.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 13.6     |
| time/              |          |
|    total_timesteps | 376000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.42     |
|    ep_rew_mean     | 2.75     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 940      |
|    time_elapsed    | 85554    |
|    total_timesteps | 376000   |
---------------------------------
Progress update from timestep 376100: Updated shield, loss is 0.03935050591826439
Progress update from timestep 376200: Updated shield, loss is 0.04534926265478134
Eval num_timesteps=376200, episode_reward=8.96 +/- 13.36
Episode length: 13.40 +/- 18.38
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 13.4          |
|    mean_reward          | 8.96          |
| time/                   |               |
|    total_timesteps      | 376200        |
| train/                  |               |
|    approx_kl            | 0.00014010258 |
|    clip_fraction        | 0.0029        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00577      |
|    explained_variance   | 0.203         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.19          |
|    n_updates            | 9400          |
|    policy_gradient_loss | -0.000268     |
|    value_loss           | 2.17          |
-------------------------------------------
Progress update from timestep 376300: Updated shield, loss is 0.047336969524621964
Progress update from timestep 376400: Updated shield, loss is 0.040010012686252594
Eval num_timesteps=376400, episode_reward=6.39 +/- 2.63
Episode length: 10.20 +/- 3.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10.2     |
|    mean_reward     | 6.39     |
| time/              |          |
|    total_timesteps | 376400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.24     |
|    ep_rew_mean     | 2.63     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 941      |
|    time_elapsed    | 85633    |
|    total_timesteps | 376400   |
---------------------------------
Progress update from timestep 376500: Updated shield, loss is 0.039807483553886414
Progress update from timestep 376600: Updated shield, loss is 0.04270869493484497
Eval num_timesteps=376600, episode_reward=23.45 +/- 14.42
Episode length: 33.80 +/- 19.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.8         |
|    mean_reward          | 23.5         |
| time/                   |              |
|    total_timesteps      | 376600       |
| train/                  |              |
|    approx_kl            | 5.399949e-05 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00512     |
|    explained_variance   | 0.349        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.34         |
|    n_updates            | 9410         |
|    policy_gradient_loss | -8.68e-05    |
|    value_loss           | 2.35         |
------------------------------------------
Progress update from timestep 376700: Updated shield, loss is 0.038903336971998215
Progress update from timestep 376800: Updated shield, loss is 0.04504469409584999
Eval num_timesteps=376800, episode_reward=16.97 +/- 15.27
Episode length: 24.60 +/- 20.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 376800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.62     |
|    ep_rew_mean     | 2.95     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 942      |
|    time_elapsed    | 85727    |
|    total_timesteps | 376800   |
---------------------------------
Progress update from timestep 376900: Updated shield, loss is 0.03993768244981766
Progress update from timestep 377000: Updated shield, loss is 0.04094039276242256
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 377000: Saved shield_loss.csv. 
Eval num_timesteps=377000, episode_reward=23.06 +/- 15.01
Episode length: 33.20 +/- 20.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.2         |
|    mean_reward          | 23.1         |
| time/                   |              |
|    total_timesteps      | 377000       |
| train/                  |              |
|    approx_kl            | 0.0001354297 |
|    clip_fraction        | 0.0029       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00532     |
|    explained_variance   | 0.363        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.66         |
|    n_updates            | 9420         |
|    policy_gradient_loss | -0.000278    |
|    value_loss           | 2.15         |
------------------------------------------
Progress update from timestep 377100: Updated shield, loss is 0.05016326531767845
Progress update from timestep 377200: Updated shield, loss is 0.0372123122215271
Eval num_timesteps=377200, episode_reward=12.97 +/- 12.38
Episode length: 18.80 +/- 16.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.8     |
|    mean_reward     | 13       |
| time/              |          |
|    total_timesteps | 377200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.57     |
|    ep_rew_mean     | 2.95     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 943      |
|    time_elapsed    | 85819    |
|    total_timesteps | 377200   |
---------------------------------
Progress update from timestep 377300: Updated shield, loss is 0.039738211780786514
Progress update from timestep 377400: Updated shield, loss is 0.043441690504550934
Eval num_timesteps=377400, episode_reward=12.78 +/- 12.12
Episode length: 18.40 +/- 16.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 18.4          |
|    mean_reward          | 12.8          |
| time/                   |               |
|    total_timesteps      | 377400        |
| train/                  |               |
|    approx_kl            | 0.00023852916 |
|    clip_fraction        | 0.0103        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0269       |
|    explained_variance   | 0.134         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.14          |
|    n_updates            | 9430          |
|    policy_gradient_loss | -0.00118      |
|    value_loss           | 2.5           |
-------------------------------------------
Progress update from timestep 377500: Updated shield, loss is 0.046037979423999786
Progress update from timestep 377600: Updated shield, loss is 0.05791005864739418
Eval num_timesteps=377600, episode_reward=5.04 +/- 2.44
Episode length: 8.00 +/- 3.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8        |
|    mean_reward     | 5.04     |
| time/              |          |
|    total_timesteps | 377600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.17     |
|    ep_rew_mean     | 2.57     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 944      |
|    time_elapsed    | 85900    |
|    total_timesteps | 377600   |
---------------------------------
Progress update from timestep 377700: Updated shield, loss is 0.04244929924607277
Progress update from timestep 377800: Updated shield, loss is 0.038903869688510895
Eval num_timesteps=377800, episode_reward=18.73 +/- 14.49
Episode length: 26.60 +/- 19.49
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.6          |
|    mean_reward          | 18.7          |
| time/                   |               |
|    total_timesteps      | 377800        |
| train/                  |               |
|    approx_kl            | 0.00015345981 |
|    clip_fraction        | 0.0058        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.013        |
|    explained_variance   | 0.262         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.06          |
|    n_updates            | 9440          |
|    policy_gradient_loss | -0.0011       |
|    value_loss           | 1.67          |
-------------------------------------------
Progress update from timestep 377900: Updated shield, loss is 0.039398498833179474
Progress update from timestep 378000: Updated shield, loss is 0.052084822207689285
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 378000: Saved shield_loss.csv. 
Eval num_timesteps=378000, episode_reward=25.20 +/- 11.43
Episode length: 36.80 +/- 16.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 25.2     |
| time/              |          |
|    total_timesteps | 378000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.79     |
|    ep_rew_mean     | 2.36     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 945      |
|    time_elapsed    | 86007    |
|    total_timesteps | 378000   |
---------------------------------
Progress update from timestep 378100: Updated shield, loss is 0.050642721354961395
Progress update from timestep 378200: Updated shield, loss is 0.03822825103998184
Eval num_timesteps=378200, episode_reward=16.89 +/- 15.30
Episode length: 24.40 +/- 20.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.4         |
|    mean_reward          | 16.9         |
| time/                   |              |
|    total_timesteps      | 378200       |
| train/                  |              |
|    approx_kl            | 9.362406e-05 |
|    clip_fraction        | 0.00491      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0116      |
|    explained_variance   | 0.147        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.506        |
|    n_updates            | 9450         |
|    policy_gradient_loss | -0.000849    |
|    value_loss           | 1.24         |
------------------------------------------
Progress update from timestep 378300: Updated shield, loss is 0.03917401656508446
Progress update from timestep 378400: Updated shield, loss is 0.03622591868042946
Eval num_timesteps=378400, episode_reward=23.11 +/- 14.49
Episode length: 33.60 +/- 20.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 23.1     |
| time/              |          |
|    total_timesteps | 378400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.12     |
|    ep_rew_mean     | 2.59     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 946      |
|    time_elapsed    | 86123    |
|    total_timesteps | 378400   |
---------------------------------
Progress update from timestep 378500: Updated shield, loss is 0.043558355420827866
Progress update from timestep 378600: Updated shield, loss is 0.03907068446278572
Eval num_timesteps=378600, episode_reward=9.96 +/- 13.07
Episode length: 14.60 +/- 18.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.6         |
|    mean_reward          | 9.96         |
| time/                   |              |
|    total_timesteps      | 378600       |
| train/                  |              |
|    approx_kl            | 8.794584e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0049      |
|    explained_variance   | 0.0994       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.598        |
|    n_updates            | 9460         |
|    policy_gradient_loss | -0.000285    |
|    value_loss           | 1.52         |
------------------------------------------
Progress update from timestep 378700: Updated shield, loss is 0.04821009188890457
Progress update from timestep 378800: Updated shield, loss is 0.03999033570289612
Eval num_timesteps=378800, episode_reward=30.24 +/- 9.60
Episode length: 43.40 +/- 13.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.4     |
|    mean_reward     | 30.2     |
| time/              |          |
|    total_timesteps | 378800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.63     |
|    ep_rew_mean     | 2.98     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 947      |
|    time_elapsed    | 86237    |
|    total_timesteps | 378800   |
---------------------------------
Progress update from timestep 378900: Updated shield, loss is 0.032864365726709366
Progress update from timestep 379000: Updated shield, loss is 0.04027823731303215
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 379000: Saved shield_loss.csv. 
Eval num_timesteps=379000, episode_reward=15.43 +/- 16.07
Episode length: 22.60 +/- 22.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.6          |
|    mean_reward          | 15.4          |
| time/                   |               |
|    total_timesteps      | 379000        |
| train/                  |               |
|    approx_kl            | -8.474542e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.000245     |
|    explained_variance   | 0.164         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.14          |
|    n_updates            | 9470          |
|    policy_gradient_loss | 2.16e-07      |
|    value_loss           | 2.03          |
-------------------------------------------
Progress update from timestep 379100: Updated shield, loss is 0.04375646263360977
Progress update from timestep 379200: Updated shield, loss is 0.041696567088365555
Eval num_timesteps=379200, episode_reward=15.40 +/- 15.63
Episode length: 22.80 +/- 22.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 15.4     |
| time/              |          |
|    total_timesteps | 379200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.15     |
|    ep_rew_mean     | 2.63     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 948      |
|    time_elapsed    | 86330    |
|    total_timesteps | 379200   |
---------------------------------
Progress update from timestep 379300: Updated shield, loss is 0.04254171997308731
Progress update from timestep 379400: Updated shield, loss is 0.030776044353842735
Eval num_timesteps=379400, episode_reward=8.61 +/- 12.95
Episode length: 13.20 +/- 18.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 13.2         |
|    mean_reward          | 8.61         |
| time/                   |              |
|    total_timesteps      | 379400       |
| train/                  |              |
|    approx_kl            | 6.457568e-05 |
|    clip_fraction        | 0.000446     |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00905     |
|    explained_variance   | 0.158        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.32         |
|    n_updates            | 9480         |
|    policy_gradient_loss | -0.000114    |
|    value_loss           | 1.91         |
------------------------------------------
Progress update from timestep 379500: Updated shield, loss is 0.03402459993958473
Progress update from timestep 379600: Updated shield, loss is 0.04006507247686386
Eval num_timesteps=379600, episode_reward=18.18 +/- 14.24
Episode length: 26.20 +/- 19.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | 18.2     |
| time/              |          |
|    total_timesteps | 379600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.13     |
|    ep_rew_mean     | 2.57     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 949      |
|    time_elapsed    | 86426    |
|    total_timesteps | 379600   |
---------------------------------
Progress update from timestep 379700: Updated shield, loss is 0.047498252242803574
Progress update from timestep 379800: Updated shield, loss is 0.0407852865755558
Eval num_timesteps=379800, episode_reward=29.29 +/- 10.96
Episode length: 42.20 +/- 15.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.2          |
|    mean_reward          | 29.3          |
| time/                   |               |
|    total_timesteps      | 379800        |
| train/                  |               |
|    approx_kl            | 0.00022760763 |
|    clip_fraction        | 0.00402       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00524      |
|    explained_variance   | 0.276         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.943         |
|    n_updates            | 9490          |
|    policy_gradient_loss | -0.000298     |
|    value_loss           | 1.48          |
-------------------------------------------
Progress update from timestep 379900: Updated shield, loss is 0.03609844669699669
Progress update from timestep 380000: Updated shield, loss is 0.03363006189465523
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 380000: Saved shield_loss.csv. 
Eval num_timesteps=380000, episode_reward=22.54 +/- 16.44
Episode length: 31.80 +/- 22.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.8     |
|    mean_reward     | 22.5     |
| time/              |          |
|    total_timesteps | 380000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.16     |
|    ep_rew_mean     | 2.56     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 950      |
|    time_elapsed    | 86544    |
|    total_timesteps | 380000   |
---------------------------------
Progress update from timestep 380100: Updated shield, loss is 0.035555239766836166
Progress update from timestep 380200: Updated shield, loss is 0.03443372622132301
Eval num_timesteps=380200, episode_reward=11.76 +/- 11.39
Episode length: 17.80 +/- 16.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17.8          |
|    mean_reward          | 11.8          |
| time/                   |               |
|    total_timesteps      | 380200        |
| train/                  |               |
|    approx_kl            | 0.00021341337 |
|    clip_fraction        | 0.00201       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00571      |
|    explained_variance   | 0.105         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.438         |
|    n_updates            | 9500          |
|    policy_gradient_loss | -0.00031      |
|    value_loss           | 1.98          |
-------------------------------------------
Progress update from timestep 380300: Updated shield, loss is 0.04853234812617302
Progress update from timestep 380400: Updated shield, loss is 0.0404624342918396
Eval num_timesteps=380400, episode_reward=21.80 +/- 15.18
Episode length: 32.40 +/- 21.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.4     |
|    mean_reward     | 21.8     |
| time/              |          |
|    total_timesteps | 380400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.04     |
|    ep_rew_mean     | 2.47     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 951      |
|    time_elapsed    | 86657    |
|    total_timesteps | 380400   |
---------------------------------
Progress update from timestep 380500: Updated shield, loss is 0.04719366505742073
Progress update from timestep 380600: Updated shield, loss is 0.043221961706876755
Eval num_timesteps=380600, episode_reward=12.81 +/- 13.58
Episode length: 18.40 +/- 18.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 18.4          |
|    mean_reward          | 12.8          |
| time/                   |               |
|    total_timesteps      | 380600        |
| train/                  |               |
|    approx_kl            | 0.00038108384 |
|    clip_fraction        | 0.00937       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0133       |
|    explained_variance   | 0.283         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.568         |
|    n_updates            | 9510          |
|    policy_gradient_loss | -0.00105      |
|    value_loss           | 1.89          |
-------------------------------------------
Progress update from timestep 380700: Updated shield, loss is 0.04707784205675125
Progress update from timestep 380800: Updated shield, loss is 0.04864728823304176
Eval num_timesteps=380800, episode_reward=19.91 +/- 13.55
Episode length: 28.60 +/- 19.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.6     |
|    mean_reward     | 19.9     |
| time/              |          |
|    total_timesteps | 380800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5        |
|    ep_rew_mean     | 3.21     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 952      |
|    time_elapsed    | 86770    |
|    total_timesteps | 380800   |
---------------------------------
Progress update from timestep 380900: Updated shield, loss is 0.04572417214512825
Progress update from timestep 381000: Updated shield, loss is 0.04150867462158203
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 381000: Saved shield_loss.csv. 
Eval num_timesteps=381000, episode_reward=10.25 +/- 12.71
Episode length: 15.40 +/- 18.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.4          |
|    mean_reward          | 10.3          |
| time/                   |               |
|    total_timesteps      | 381000        |
| train/                  |               |
|    approx_kl            | 0.00011611134 |
|    clip_fraction        | 0.000893      |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00769      |
|    explained_variance   | 0.224         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.07          |
|    n_updates            | 9520          |
|    policy_gradient_loss | -0.000353     |
|    value_loss           | 1.96          |
-------------------------------------------
Progress update from timestep 381100: Updated shield, loss is 0.043779224157333374
Progress update from timestep 381200: Updated shield, loss is 0.04058771952986717
Eval num_timesteps=381200, episode_reward=9.08 +/- 12.73
Episode length: 13.80 +/- 18.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.8     |
|    mean_reward     | 9.08     |
| time/              |          |
|    total_timesteps | 381200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.09     |
|    ep_rew_mean     | 2.54     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 953      |
|    time_elapsed    | 86863    |
|    total_timesteps | 381200   |
---------------------------------
Progress update from timestep 381300: Updated shield, loss is 0.03580474480986595
Progress update from timestep 381400: Updated shield, loss is 0.032857511192560196
Eval num_timesteps=381400, episode_reward=3.68 +/- 0.94
Episode length: 6.20 +/- 1.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 6.2          |
|    mean_reward          | 3.68         |
| time/                   |              |
|    total_timesteps      | 381400       |
| train/                  |              |
|    approx_kl            | 3.465695e-05 |
|    clip_fraction        | 0.00134      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00622     |
|    explained_variance   | 0.265        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.409        |
|    n_updates            | 9530         |
|    policy_gradient_loss | -0.000365    |
|    value_loss           | 1.43         |
------------------------------------------
Progress update from timestep 381500: Updated shield, loss is 0.03396736457943916
Progress update from timestep 381600: Updated shield, loss is 0.03456459939479828
Eval num_timesteps=381600, episode_reward=23.95 +/- 14.12
Episode length: 34.20 +/- 19.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 24       |
| time/              |          |
|    total_timesteps | 381600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.57     |
|    ep_rew_mean     | 2.98     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 954      |
|    time_elapsed    | 86961    |
|    total_timesteps | 381600   |
---------------------------------
Progress update from timestep 381700: Updated shield, loss is 0.03959857299923897
Progress update from timestep 381800: Updated shield, loss is 0.042880427092313766
Eval num_timesteps=381800, episode_reward=16.29 +/- 9.94
Episode length: 23.60 +/- 13.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.6          |
|    mean_reward          | 16.3          |
| time/                   |               |
|    total_timesteps      | 381800        |
| train/                  |               |
|    approx_kl            | 2.1813064e-05 |
|    clip_fraction        | 0.00179       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00421      |
|    explained_variance   | 0.126         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.805         |
|    n_updates            | 9540          |
|    policy_gradient_loss | -0.000128     |
|    value_loss           | 1.85          |
-------------------------------------------
Progress update from timestep 381900: Updated shield, loss is 0.03832552209496498
Progress update from timestep 382000: Updated shield, loss is 0.035215724259614944
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 382000: Saved shield_loss.csv. 
Eval num_timesteps=382000, episode_reward=25.65 +/- 12.63
Episode length: 36.40 +/- 17.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 25.6     |
| time/              |          |
|    total_timesteps | 382000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.08     |
|    ep_rew_mean     | 2.57     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 955      |
|    time_elapsed    | 87073    |
|    total_timesteps | 382000   |
---------------------------------
Progress update from timestep 382100: Updated shield, loss is 0.03250284120440483
Progress update from timestep 382200: Updated shield, loss is 0.039091356098651886
Eval num_timesteps=382200, episode_reward=28.48 +/- 13.14
Episode length: 40.80 +/- 18.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 40.8          |
|    mean_reward          | 28.5          |
| time/                   |               |
|    total_timesteps      | 382200        |
| train/                  |               |
|    approx_kl            | 0.00013987826 |
|    clip_fraction        | 0.00491       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00684      |
|    explained_variance   | 0.301         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.03          |
|    n_updates            | 9550          |
|    policy_gradient_loss | -0.000573     |
|    value_loss           | 1.94          |
-------------------------------------------
Progress update from timestep 382300: Updated shield, loss is 0.041197437793016434
Progress update from timestep 382400: Updated shield, loss is 0.0430467464029789
Eval num_timesteps=382400, episode_reward=10.73 +/- 11.49
Episode length: 16.60 +/- 16.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | 10.7     |
| time/              |          |
|    total_timesteps | 382400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.63     |
|    ep_rew_mean     | 2.95     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 956      |
|    time_elapsed    | 87187    |
|    total_timesteps | 382400   |
---------------------------------
Progress update from timestep 382500: Updated shield, loss is 0.031213320791721344
Progress update from timestep 382600: Updated shield, loss is 0.036886051297187805
Eval num_timesteps=382600, episode_reward=22.04 +/- 15.33
Episode length: 32.40 +/- 21.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32.4          |
|    mean_reward          | 22            |
| time/                   |               |
|    total_timesteps      | 382600        |
| train/                  |               |
|    approx_kl            | 0.00015700891 |
|    clip_fraction        | 0.00759       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00747      |
|    explained_variance   | 0.192         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.658         |
|    n_updates            | 9560          |
|    policy_gradient_loss | -0.000456     |
|    value_loss           | 1.59          |
-------------------------------------------
Progress update from timestep 382700: Updated shield, loss is 0.038469281047582626
Progress update from timestep 382800: Updated shield, loss is 0.03965827450156212
Eval num_timesteps=382800, episode_reward=17.40 +/- 14.41
Episode length: 25.20 +/- 20.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 382800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.76     |
|    ep_rew_mean     | 3.07     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 957      |
|    time_elapsed    | 87283    |
|    total_timesteps | 382800   |
---------------------------------
Progress update from timestep 382900: Updated shield, loss is 0.036283183842897415
Progress update from timestep 383000: Updated shield, loss is 0.03002135269343853
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 383000: Saved shield_loss.csv. 
Eval num_timesteps=383000, episode_reward=18.05 +/- 15.34
Episode length: 25.40 +/- 20.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.4         |
|    mean_reward          | 18           |
| time/                   |              |
|    total_timesteps      | 383000       |
| train/                  |              |
|    approx_kl            | 0.0007281877 |
|    clip_fraction        | 0.00848      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0121      |
|    explained_variance   | 0.249        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.19         |
|    n_updates            | 9570         |
|    policy_gradient_loss | -0.000992    |
|    value_loss           | 1.95         |
------------------------------------------
Progress update from timestep 383100: Updated shield, loss is 0.039391227066516876
Progress update from timestep 383200: Updated shield, loss is 0.03863978013396263
Eval num_timesteps=383200, episode_reward=9.16 +/- 5.86
Episode length: 14.20 +/- 8.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.2     |
|    mean_reward     | 9.16     |
| time/              |          |
|    total_timesteps | 383200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.03     |
|    ep_rew_mean     | 2.49     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 958      |
|    time_elapsed    | 87370    |
|    total_timesteps | 383200   |
---------------------------------
Progress update from timestep 383300: Updated shield, loss is 0.03396332263946533
Progress update from timestep 383400: Updated shield, loss is 0.04020906612277031
Eval num_timesteps=383400, episode_reward=23.67 +/- 14.21
Episode length: 34.20 +/- 19.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.2          |
|    mean_reward          | 23.7          |
| time/                   |               |
|    total_timesteps      | 383400        |
| train/                  |               |
|    approx_kl            | 0.00062083744 |
|    clip_fraction        | 0.00893       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0131       |
|    explained_variance   | 0.144         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.489         |
|    n_updates            | 9580          |
|    policy_gradient_loss | -0.000948     |
|    value_loss           | 1.62          |
-------------------------------------------
Progress update from timestep 383500: Updated shield, loss is 0.04258629307150841
Progress update from timestep 383600: Updated shield, loss is 0.03494662791490555
Eval num_timesteps=383600, episode_reward=16.32 +/- 14.87
Episode length: 24.20 +/- 21.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 16.3     |
| time/              |          |
|    total_timesteps | 383600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.51     |
|    ep_rew_mean     | 2.85     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 959      |
|    time_elapsed    | 87467    |
|    total_timesteps | 383600   |
---------------------------------
Progress update from timestep 383700: Updated shield, loss is 0.03265834227204323
Progress update from timestep 383800: Updated shield, loss is 0.040786221623420715
Eval num_timesteps=383800, episode_reward=10.82 +/- 12.49
Episode length: 16.00 +/- 17.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | 10.8         |
| time/                   |              |
|    total_timesteps      | 383800       |
| train/                  |              |
|    approx_kl            | 0.0001412905 |
|    clip_fraction        | 0.00402      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00671     |
|    explained_variance   | 0.189        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.13         |
|    n_updates            | 9590         |
|    policy_gradient_loss | -0.000441    |
|    value_loss           | 2.11         |
------------------------------------------
Progress update from timestep 383900: Updated shield, loss is 0.04118482023477554
Progress update from timestep 384000: Updated shield, loss is 0.04311012849211693
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 384000: Saved shield_loss.csv. 
Eval num_timesteps=384000, episode_reward=21.78 +/- 15.13
Episode length: 32.20 +/- 21.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 21.8     |
| time/              |          |
|    total_timesteps | 384000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.67     |
|    ep_rew_mean     | 3        |
| time/              |          |
|    fps             | 4        |
|    iterations      | 960      |
|    time_elapsed    | 87579    |
|    total_timesteps | 384000   |
---------------------------------
Progress update from timestep 384100: Updated shield, loss is 0.04265540465712547
Progress update from timestep 384200: Updated shield, loss is 0.042406074702739716
Eval num_timesteps=384200, episode_reward=11.83 +/- 11.49
Episode length: 18.00 +/- 16.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 18            |
|    mean_reward          | 11.8          |
| time/                   |               |
|    total_timesteps      | 384200        |
| train/                  |               |
|    approx_kl            | 1.1063354e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.000297     |
|    explained_variance   | 0.199         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.387         |
|    n_updates            | 9600          |
|    policy_gradient_loss | 1.46e-08      |
|    value_loss           | 1.76          |
-------------------------------------------
Progress update from timestep 384300: Updated shield, loss is 0.04480646178126335
Progress update from timestep 384400: Updated shield, loss is 0.041443467140197754
Eval num_timesteps=384400, episode_reward=15.94 +/- 14.25
Episode length: 24.20 +/- 21.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 15.9     |
| time/              |          |
|    total_timesteps | 384400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.37     |
|    ep_rew_mean     | 2.81     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 961      |
|    time_elapsed    | 87668    |
|    total_timesteps | 384400   |
---------------------------------
Progress update from timestep 384500: Updated shield, loss is 0.039302483201026917
Progress update from timestep 384600: Updated shield, loss is 0.03270060941576958
Eval num_timesteps=384600, episode_reward=18.66 +/- 14.31
Episode length: 26.80 +/- 18.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.8         |
|    mean_reward          | 18.7         |
| time/                   |              |
|    total_timesteps      | 384600       |
| train/                  |              |
|    approx_kl            | 8.718214e-05 |
|    clip_fraction        | 0.00379      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00471     |
|    explained_variance   | 0.158        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.21         |
|    n_updates            | 9610         |
|    policy_gradient_loss | -0.000374    |
|    value_loss           | 1.95         |
------------------------------------------
Progress update from timestep 384700: Updated shield, loss is 0.03979082033038139
Progress update from timestep 384800: Updated shield, loss is 0.054385844618082047
Eval num_timesteps=384800, episode_reward=11.94 +/- 13.07
Episode length: 17.80 +/- 18.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | 11.9     |
| time/              |          |
|    total_timesteps | 384800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.32     |
|    ep_rew_mean     | 2.77     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 962      |
|    time_elapsed    | 87765    |
|    total_timesteps | 384800   |
---------------------------------
Progress update from timestep 384900: Updated shield, loss is 0.042548391968011856
Progress update from timestep 385000: Updated shield, loss is 0.035693198442459106
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 385000: Saved shield_loss.csv. 
Eval num_timesteps=385000, episode_reward=12.33 +/- 11.77
Episode length: 18.60 +/- 16.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 18.6          |
|    mean_reward          | 12.3          |
| time/                   |               |
|    total_timesteps      | 385000        |
| train/                  |               |
|    approx_kl            | 0.00020691635 |
|    clip_fraction        | 0.00893       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0252       |
|    explained_variance   | 0.102         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.859         |
|    n_updates            | 9620          |
|    policy_gradient_loss | -0.00146      |
|    value_loss           | 1.43          |
-------------------------------------------
Progress update from timestep 385100: Updated shield, loss is 0.03928837552666664
Progress update from timestep 385200: Updated shield, loss is 0.03689521923661232
Eval num_timesteps=385200, episode_reward=16.68 +/- 16.03
Episode length: 23.80 +/- 21.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 385200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.3      |
|    ep_rew_mean     | 2.71     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 963      |
|    time_elapsed    | 87859    |
|    total_timesteps | 385200   |
---------------------------------
Progress update from timestep 385300: Updated shield, loss is 0.0384383499622345
Progress update from timestep 385400: Updated shield, loss is 0.05245494842529297
Eval num_timesteps=385400, episode_reward=18.05 +/- 13.13
Episode length: 27.00 +/- 19.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 27            |
|    mean_reward          | 18            |
| time/                   |               |
|    total_timesteps      | 385400        |
| train/                  |               |
|    approx_kl            | 0.00083196413 |
|    clip_fraction        | 0.0219        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0345       |
|    explained_variance   | 0.226         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.684         |
|    n_updates            | 9630          |
|    policy_gradient_loss | -0.0021       |
|    value_loss           | 1.67          |
-------------------------------------------
Progress update from timestep 385500: Updated shield, loss is 0.02575342357158661
Progress update from timestep 385600: Updated shield, loss is 0.04081824794411659
Eval num_timesteps=385600, episode_reward=27.78 +/- 9.40
Episode length: 39.60 +/- 13.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39.6     |
|    mean_reward     | 27.8     |
| time/              |          |
|    total_timesteps | 385600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.97     |
|    ep_rew_mean     | 2.46     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 964      |
|    time_elapsed    | 87947    |
|    total_timesteps | 385600   |
---------------------------------
Progress update from timestep 385700: Updated shield, loss is 0.044062402099370956
Progress update from timestep 385800: Updated shield, loss is 0.042271099984645844
Eval num_timesteps=385800, episode_reward=4.30 +/- 2.83
Episode length: 7.20 +/- 4.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 7.2           |
|    mean_reward          | 4.3           |
| time/                   |               |
|    total_timesteps      | 385800        |
| train/                  |               |
|    approx_kl            | 0.00016019653 |
|    clip_fraction        | 0.00179       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.006        |
|    explained_variance   | 0.0436        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.958         |
|    n_updates            | 9640          |
|    policy_gradient_loss | -0.000186     |
|    value_loss           | 1.88          |
-------------------------------------------
Progress update from timestep 385900: Updated shield, loss is 0.04451756924390793
Progress update from timestep 386000: Updated shield, loss is 0.03974458575248718
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 386000: Saved shield_loss.csv. 
Eval num_timesteps=386000, episode_reward=18.64 +/- 13.98
Episode length: 27.00 +/- 19.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27       |
|    mean_reward     | 18.6     |
| time/              |          |
|    total_timesteps | 386000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.54     |
|    ep_rew_mean     | 2.1      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 965      |
|    time_elapsed    | 88027    |
|    total_timesteps | 386000   |
---------------------------------
Progress update from timestep 386100: Updated shield, loss is 0.03632095083594322
Progress update from timestep 386200: Updated shield, loss is 0.0408640094101429
Eval num_timesteps=386200, episode_reward=23.52 +/- 14.05
Episode length: 34.20 +/- 19.66
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 34.2           |
|    mean_reward          | 23.5           |
| time/                   |                |
|    total_timesteps      | 386200         |
| train/                  |                |
|    approx_kl            | -1.2375624e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.000329      |
|    explained_variance   | 0.178          |
|    learning_rate        | 0.0001         |
|    loss                 | 0.327          |
|    n_updates            | 9650           |
|    policy_gradient_loss | -2.27e-07      |
|    value_loss           | 1.1            |
--------------------------------------------
Progress update from timestep 386300: Updated shield, loss is 0.042474620044231415
Progress update from timestep 386400: Updated shield, loss is 0.04862202703952789
Eval num_timesteps=386400, episode_reward=17.70 +/- 14.69
Episode length: 25.60 +/- 20.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 17.7     |
| time/              |          |
|    total_timesteps | 386400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.04     |
|    ep_rew_mean     | 2.51     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 966      |
|    time_elapsed    | 88137    |
|    total_timesteps | 386400   |
---------------------------------
Progress update from timestep 386500: Updated shield, loss is 0.031124647706747055
Progress update from timestep 386600: Updated shield, loss is 0.030520005151629448
Eval num_timesteps=386600, episode_reward=18.12 +/- 13.57
Episode length: 26.80 +/- 19.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.8         |
|    mean_reward          | 18.1         |
| time/                   |              |
|    total_timesteps      | 386600       |
| train/                  |              |
|    approx_kl            | 5.769623e-05 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0062      |
|    explained_variance   | 0.218        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.24         |
|    n_updates            | 9660         |
|    policy_gradient_loss | -0.000136    |
|    value_loss           | 1.96         |
------------------------------------------
Progress update from timestep 386700: Updated shield, loss is 0.030860787257552147
Progress update from timestep 386800: Updated shield, loss is 0.05433918535709381
Eval num_timesteps=386800, episode_reward=13.81 +/- 11.14
Episode length: 20.40 +/- 15.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 13.8     |
| time/              |          |
|    total_timesteps | 386800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.55     |
|    ep_rew_mean     | 2.89     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 967      |
|    time_elapsed    | 88229    |
|    total_timesteps | 386800   |
---------------------------------
Progress update from timestep 386900: Updated shield, loss is 0.039220258593559265
Progress update from timestep 387000: Updated shield, loss is 0.04809186980128288
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 387000: Saved shield_loss.csv. 
Eval num_timesteps=387000, episode_reward=28.91 +/- 13.41
Episode length: 40.80 +/- 18.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 40.8         |
|    mean_reward          | 28.9         |
| time/                   |              |
|    total_timesteps      | 387000       |
| train/                  |              |
|    approx_kl            | 0.0010189314 |
|    clip_fraction        | 0.0109       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0159      |
|    explained_variance   | 0.212        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.707        |
|    n_updates            | 9670         |
|    policy_gradient_loss | -0.00133     |
|    value_loss           | 1.54         |
------------------------------------------
Progress update from timestep 387100: Updated shield, loss is 0.03878775238990784
Progress update from timestep 387200: Updated shield, loss is 0.03687215968966484
Eval num_timesteps=387200, episode_reward=28.41 +/- 13.82
Episode length: 40.40 +/- 19.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.4     |
|    mean_reward     | 28.4     |
| time/              |          |
|    total_timesteps | 387200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.04     |
|    ep_rew_mean     | 2.5      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 968      |
|    time_elapsed    | 88342    |
|    total_timesteps | 387200   |
---------------------------------
Progress update from timestep 387300: Updated shield, loss is 0.04811409115791321
Progress update from timestep 387400: Updated shield, loss is 0.04802282527089119
Eval num_timesteps=387400, episode_reward=9.86 +/- 12.42
Episode length: 15.00 +/- 17.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15           |
|    mean_reward          | 9.86         |
| time/                   |              |
|    total_timesteps      | 387400       |
| train/                  |              |
|    approx_kl            | 0.0020430335 |
|    clip_fraction        | 0.0127       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0127      |
|    explained_variance   | 0.142        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.591        |
|    n_updates            | 9680         |
|    policy_gradient_loss | -0.00142     |
|    value_loss           | 1.76         |
------------------------------------------
Progress update from timestep 387500: Updated shield, loss is 0.051311105489730835
Progress update from timestep 387600: Updated shield, loss is 0.043514858931303024
Eval num_timesteps=387600, episode_reward=22.07 +/- 14.85
Episode length: 32.60 +/- 21.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.6     |
|    mean_reward     | 22.1     |
| time/              |          |
|    total_timesteps | 387600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.05     |
|    ep_rew_mean     | 2.54     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 969      |
|    time_elapsed    | 88436    |
|    total_timesteps | 387600   |
---------------------------------
Progress update from timestep 387700: Updated shield, loss is 0.03201330825686455
Progress update from timestep 387800: Updated shield, loss is 0.02678682468831539
Eval num_timesteps=387800, episode_reward=11.95 +/- 11.68
Episode length: 18.00 +/- 16.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 18            |
|    mean_reward          | 11.9          |
| time/                   |               |
|    total_timesteps      | 387800        |
| train/                  |               |
|    approx_kl            | 0.00021709627 |
|    clip_fraction        | 0.00513       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0165       |
|    explained_variance   | 0.143         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.755         |
|    n_updates            | 9690          |
|    policy_gradient_loss | -0.000701     |
|    value_loss           | 1.95          |
-------------------------------------------
Progress update from timestep 387900: Updated shield, loss is 0.03849568963050842
Progress update from timestep 388000: Updated shield, loss is 0.041080497205257416
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 388000: Saved shield_loss.csv. 
Eval num_timesteps=388000, episode_reward=4.26 +/- 1.84
Episode length: 6.80 +/- 2.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6.8      |
|    mean_reward     | 4.26     |
| time/              |          |
|    total_timesteps | 388000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.35     |
|    ep_rew_mean     | 2.78     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 970      |
|    time_elapsed    | 88512    |
|    total_timesteps | 388000   |
---------------------------------
Progress update from timestep 388100: Updated shield, loss is 0.04384081810712814
Progress update from timestep 388200: Updated shield, loss is 0.03807247057557106
Eval num_timesteps=388200, episode_reward=17.00 +/- 14.27
Episode length: 25.00 +/- 20.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25            |
|    mean_reward          | 17            |
| time/                   |               |
|    total_timesteps      | 388200        |
| train/                  |               |
|    approx_kl            | 0.00010953097 |
|    clip_fraction        | 0.00558       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0274       |
|    explained_variance   | 0.123         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.725         |
|    n_updates            | 9700          |
|    policy_gradient_loss | -0.000685     |
|    value_loss           | 1.88          |
-------------------------------------------
Progress update from timestep 388300: Updated shield, loss is 0.034128155559301376
Progress update from timestep 388400: Updated shield, loss is 0.03806353360414505
Eval num_timesteps=388400, episode_reward=2.09 +/- 0.83
Episode length: 4.00 +/- 1.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 2.09     |
| time/              |          |
|    total_timesteps | 388400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.36     |
|    ep_rew_mean     | 2.78     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 971      |
|    time_elapsed    | 88605    |
|    total_timesteps | 388400   |
---------------------------------
Progress update from timestep 388500: Updated shield, loss is 0.04098663479089737
Progress update from timestep 388600: Updated shield, loss is 0.037848882377147675
Eval num_timesteps=388600, episode_reward=18.47 +/- 13.53
Episode length: 26.60 +/- 19.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.6         |
|    mean_reward          | 18.5         |
| time/                   |              |
|    total_timesteps      | 388600       |
| train/                  |              |
|    approx_kl            | 7.545231e-05 |
|    clip_fraction        | 0.00179      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00429     |
|    explained_variance   | 0.346        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.56         |
|    n_updates            | 9710         |
|    policy_gradient_loss | -0.000327    |
|    value_loss           | 1.52         |
------------------------------------------
Progress update from timestep 388700: Updated shield, loss is 0.038261521607637405
Progress update from timestep 388800: Updated shield, loss is 0.04580959305167198
Eval num_timesteps=388800, episode_reward=15.63 +/- 15.45
Episode length: 23.20 +/- 21.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 15.6     |
| time/              |          |
|    total_timesteps | 388800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.26     |
|    ep_rew_mean     | 2.72     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 972      |
|    time_elapsed    | 88701    |
|    total_timesteps | 388800   |
---------------------------------
Progress update from timestep 388900: Updated shield, loss is 0.04151681438088417
Progress update from timestep 389000: Updated shield, loss is 0.03838682547211647
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 389000: Saved shield_loss.csv. 
Eval num_timesteps=389000, episode_reward=14.92 +/- 11.40
Episode length: 21.80 +/- 14.91
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 21.8          |
|    mean_reward          | 14.9          |
| time/                   |               |
|    total_timesteps      | 389000        |
| train/                  |               |
|    approx_kl            | 0.00013595555 |
|    clip_fraction        | 0.00313       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.012        |
|    explained_variance   | 0.132         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.53          |
|    n_updates            | 9720          |
|    policy_gradient_loss | -0.000408     |
|    value_loss           | 2.25          |
-------------------------------------------
Progress update from timestep 389100: Updated shield, loss is 0.042089905589818954
Progress update from timestep 389200: Updated shield, loss is 0.03108314424753189
Eval num_timesteps=389200, episode_reward=24.49 +/- 13.56
Episode length: 35.60 +/- 18.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 24.5     |
| time/              |          |
|    total_timesteps | 389200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.68     |
|    ep_rew_mean     | 3        |
| time/              |          |
|    fps             | 4        |
|    iterations      | 973      |
|    time_elapsed    | 88790    |
|    total_timesteps | 389200   |
---------------------------------
Progress update from timestep 389300: Updated shield, loss is 0.037064939737319946
Progress update from timestep 389400: Updated shield, loss is 0.03971204534173012
Eval num_timesteps=389400, episode_reward=20.06 +/- 12.99
Episode length: 29.20 +/- 18.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 29.2          |
|    mean_reward          | 20.1          |
| time/                   |               |
|    total_timesteps      | 389400        |
| train/                  |               |
|    approx_kl            | 1.7767476e-05 |
|    clip_fraction        | 0.00179       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0059       |
|    explained_variance   | 0.0202        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.91          |
|    n_updates            | 9730          |
|    policy_gradient_loss | -0.000197     |
|    value_loss           | 1.98          |
-------------------------------------------
Progress update from timestep 389500: Updated shield, loss is 0.04321425035595894
Progress update from timestep 389600: Updated shield, loss is 0.05042183771729469
Eval num_timesteps=389600, episode_reward=7.89 +/- 4.51
Episode length: 12.20 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 12.2     |
|    mean_reward     | 7.89     |
| time/              |          |
|    total_timesteps | 389600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.98     |
|    ep_rew_mean     | 2.44     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 974      |
|    time_elapsed    | 88876    |
|    total_timesteps | 389600   |
---------------------------------
Progress update from timestep 389700: Updated shield, loss is 0.03180622309446335
Progress update from timestep 389800: Updated shield, loss is 0.039025284349918365
Eval num_timesteps=389800, episode_reward=12.42 +/- 12.43
Episode length: 18.20 +/- 17.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.2         |
|    mean_reward          | 12.4         |
| time/                   |              |
|    total_timesteps      | 389800       |
| train/                  |              |
|    approx_kl            | 6.964478e-05 |
|    clip_fraction        | 0.00268      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0112      |
|    explained_variance   | 0.0925       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.32         |
|    n_updates            | 9740         |
|    policy_gradient_loss | -0.000961    |
|    value_loss           | 1.98         |
------------------------------------------
Progress update from timestep 389900: Updated shield, loss is 0.04697379469871521
Progress update from timestep 390000: Updated shield, loss is 0.04222440347075462
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 390000: Saved shield_loss.csv. 
Eval num_timesteps=390000, episode_reward=10.33 +/- 11.54
Episode length: 16.00 +/- 17.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | 10.3     |
| time/              |          |
|    total_timesteps | 390000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.43     |
|    ep_rew_mean     | 2.77     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 975      |
|    time_elapsed    | 88967    |
|    total_timesteps | 390000   |
---------------------------------
Progress update from timestep 390100: Updated shield, loss is 0.035019613802433014
Progress update from timestep 390200: Updated shield, loss is 0.04099404811859131
Eval num_timesteps=390200, episode_reward=14.28 +/- 12.38
Episode length: 21.20 +/- 17.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 21.2         |
|    mean_reward          | 14.3         |
| time/                   |              |
|    total_timesteps      | 390200       |
| train/                  |              |
|    approx_kl            | 5.950694e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.000597    |
|    explained_variance   | 0.214        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.852        |
|    n_updates            | 9750         |
|    policy_gradient_loss | -5.08e-07    |
|    value_loss           | 1.98         |
------------------------------------------
Progress update from timestep 390300: Updated shield, loss is 0.04253169521689415
Progress update from timestep 390400: Updated shield, loss is 0.043375082314014435
Eval num_timesteps=390400, episode_reward=25.72 +/- 12.73
Episode length: 36.40 +/- 17.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 25.7     |
| time/              |          |
|    total_timesteps | 390400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.88     |
|    ep_rew_mean     | 3.16     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 976      |
|    time_elapsed    | 89059    |
|    total_timesteps | 390400   |
---------------------------------
Progress update from timestep 390500: Updated shield, loss is 0.050351474434137344
Progress update from timestep 390600: Updated shield, loss is 0.039892565459012985
Eval num_timesteps=390600, episode_reward=11.75 +/- 11.88
Episode length: 17.40 +/- 16.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17.4          |
|    mean_reward          | 11.7          |
| time/                   |               |
|    total_timesteps      | 390600        |
| train/                  |               |
|    approx_kl            | 2.3210174e-05 |
|    clip_fraction        | 0.00112       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00373      |
|    explained_variance   | 0.181         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.717         |
|    n_updates            | 9760          |
|    policy_gradient_loss | -0.000317     |
|    value_loss           | 2.01          |
-------------------------------------------
Progress update from timestep 390700: Updated shield, loss is 0.04515500366687775
Progress update from timestep 390800: Updated shield, loss is 0.042731113731861115
Eval num_timesteps=390800, episode_reward=17.53 +/- 14.79
Episode length: 25.20 +/- 20.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 17.5     |
| time/              |          |
|    total_timesteps | 390800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.28     |
|    ep_rew_mean     | 2.72     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 977      |
|    time_elapsed    | 89154    |
|    total_timesteps | 390800   |
---------------------------------
Progress update from timestep 390900: Updated shield, loss is 0.04089890792965889
Progress update from timestep 391000: Updated shield, loss is 0.033135056495666504
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 391000: Saved shield_loss.csv. 
Eval num_timesteps=391000, episode_reward=11.09 +/- 11.22
Episode length: 17.00 +/- 16.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17            |
|    mean_reward          | 11.1          |
| time/                   |               |
|    total_timesteps      | 391000        |
| train/                  |               |
|    approx_kl            | 0.00024135433 |
|    clip_fraction        | 0.00446       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0107       |
|    explained_variance   | 0.157         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.628         |
|    n_updates            | 9770          |
|    policy_gradient_loss | -0.000359     |
|    value_loss           | 1.74          |
-------------------------------------------
Progress update from timestep 391100: Updated shield, loss is 0.048933420330286026
Progress update from timestep 391200: Updated shield, loss is 0.0433625802397728
Eval num_timesteps=391200, episode_reward=6.72 +/- 3.94
Episode length: 10.40 +/- 5.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10.4     |
|    mean_reward     | 6.72     |
| time/              |          |
|    total_timesteps | 391200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.33     |
|    ep_rew_mean     | 2.74     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 978      |
|    time_elapsed    | 89227    |
|    total_timesteps | 391200   |
---------------------------------
Progress update from timestep 391300: Updated shield, loss is 0.03417125344276428
Progress update from timestep 391400: Updated shield, loss is 0.03271070495247841
Eval num_timesteps=391400, episode_reward=11.84 +/- 12.45
Episode length: 17.00 +/- 16.54
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 17             |
|    mean_reward          | 11.8           |
| time/                   |                |
|    total_timesteps      | 391400         |
| train/                  |                |
|    approx_kl            | -5.2425873e-11 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.000228      |
|    explained_variance   | 0.085          |
|    learning_rate        | 0.0001         |
|    loss                 | 0.599          |
|    n_updates            | 9780           |
|    policy_gradient_loss | 4.26e-10       |
|    value_loss           | 1.48           |
--------------------------------------------
Progress update from timestep 391500: Updated shield, loss is 0.03504998981952667
Progress update from timestep 391600: Updated shield, loss is 0.04571721702814102
Eval num_timesteps=391600, episode_reward=16.22 +/- 15.89
Episode length: 23.40 +/- 21.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 16.2     |
| time/              |          |
|    total_timesteps | 391600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.25     |
|    ep_rew_mean     | 2.7      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 979      |
|    time_elapsed    | 89312    |
|    total_timesteps | 391600   |
---------------------------------
Progress update from timestep 391700: Updated shield, loss is 0.042921654880046844
Progress update from timestep 391800: Updated shield, loss is 0.04011257365345955
Eval num_timesteps=391800, episode_reward=19.68 +/- 13.15
Episode length: 28.40 +/- 17.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 28.4          |
|    mean_reward          | 19.7          |
| time/                   |               |
|    total_timesteps      | 391800        |
| train/                  |               |
|    approx_kl            | 0.00015671292 |
|    clip_fraction        | 0.00357       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0101       |
|    explained_variance   | 0.162         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.38          |
|    n_updates            | 9790          |
|    policy_gradient_loss | -0.000629     |
|    value_loss           | 2.17          |
-------------------------------------------
Progress update from timestep 391900: Updated shield, loss is 0.03509995713829994
Progress update from timestep 392000: Updated shield, loss is 0.03861425444483757
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 392000: Saved shield_loss.csv. 
Eval num_timesteps=392000, episode_reward=21.45 +/- 16.44
Episode length: 31.20 +/- 23.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.2     |
|    mean_reward     | 21.4     |
| time/              |          |
|    total_timesteps | 392000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.63     |
|    ep_rew_mean     | 2.97     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 980      |
|    time_elapsed    | 89407    |
|    total_timesteps | 392000   |
---------------------------------
Progress update from timestep 392100: Updated shield, loss is 0.04909162223339081
Progress update from timestep 392200: Updated shield, loss is 0.044955283403396606
Eval num_timesteps=392200, episode_reward=22.41 +/- 16.22
Episode length: 31.80 +/- 22.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 31.8          |
|    mean_reward          | 22.4          |
| time/                   |               |
|    total_timesteps      | 392200        |
| train/                  |               |
|    approx_kl            | 0.00031022524 |
|    clip_fraction        | 0.00536       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0158       |
|    explained_variance   | 0.271         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.8           |
|    n_updates            | 9800          |
|    policy_gradient_loss | -0.000738     |
|    value_loss           | 2.47          |
-------------------------------------------
Progress update from timestep 392300: Updated shield, loss is 0.03765460476279259
Progress update from timestep 392400: Updated shield, loss is 0.03799610584974289
Eval num_timesteps=392400, episode_reward=16.73 +/- 16.36
Episode length: 23.60 +/- 21.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 392400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.11     |
|    ep_rew_mean     | 2.57     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 981      |
|    time_elapsed    | 89496    |
|    total_timesteps | 392400   |
---------------------------------
Progress update from timestep 392500: Updated shield, loss is 0.03454097732901573
Progress update from timestep 392600: Updated shield, loss is 0.035598915070295334
Eval num_timesteps=392600, episode_reward=10.62 +/- 11.73
Episode length: 16.20 +/- 17.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.2         |
|    mean_reward          | 10.6         |
| time/                   |              |
|    total_timesteps      | 392600       |
| train/                  |              |
|    approx_kl            | 0.0005219888 |
|    clip_fraction        | 0.00379      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00979     |
|    explained_variance   | 0.175        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.618        |
|    n_updates            | 9810         |
|    policy_gradient_loss | -0.000813    |
|    value_loss           | 1.99         |
------------------------------------------
Progress update from timestep 392700: Updated shield, loss is 0.03546719625592232
Progress update from timestep 392800: Updated shield, loss is 0.03455701097846031
Eval num_timesteps=392800, episode_reward=10.79 +/- 12.71
Episode length: 16.00 +/- 17.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | 10.8     |
| time/              |          |
|    total_timesteps | 392800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.94     |
|    ep_rew_mean     | 2.43     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 982      |
|    time_elapsed    | 89589    |
|    total_timesteps | 392800   |
---------------------------------
Progress update from timestep 392900: Updated shield, loss is 0.03058466501533985
Progress update from timestep 393000: Updated shield, loss is 0.038722943514585495
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 393000: Saved shield_loss.csv. 
Eval num_timesteps=393000, episode_reward=5.83 +/- 3.09
Episode length: 9.20 +/- 4.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 9.2           |
|    mean_reward          | 5.83          |
| time/                   |               |
|    total_timesteps      | 393000        |
| train/                  |               |
|    approx_kl            | 9.3365044e-05 |
|    clip_fraction        | 0.00246       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00574      |
|    explained_variance   | 0.191         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.373         |
|    n_updates            | 9820          |
|    policy_gradient_loss | -0.000189     |
|    value_loss           | 1.34          |
-------------------------------------------
Progress update from timestep 393100: Updated shield, loss is 0.029119208455085754
Progress update from timestep 393200: Updated shield, loss is 0.03798835724592209
Eval num_timesteps=393200, episode_reward=17.07 +/- 13.77
Episode length: 25.60 +/- 19.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 17.1     |
| time/              |          |
|    total_timesteps | 393200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.57     |
|    ep_rew_mean     | 2.88     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 983      |
|    time_elapsed    | 89663    |
|    total_timesteps | 393200   |
---------------------------------
Progress update from timestep 393300: Updated shield, loss is 0.0404476672410965
Progress update from timestep 393400: Updated shield, loss is 0.02959500625729561
Eval num_timesteps=393400, episode_reward=17.35 +/- 13.30
Episode length: 26.20 +/- 19.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.2          |
|    mean_reward          | 17.3          |
| time/                   |               |
|    total_timesteps      | 393400        |
| train/                  |               |
|    approx_kl            | 7.3888026e-05 |
|    clip_fraction        | 0.00313       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0137       |
|    explained_variance   | 0.217         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.861         |
|    n_updates            | 9830          |
|    policy_gradient_loss | -0.000518     |
|    value_loss           | 2.22          |
-------------------------------------------
Progress update from timestep 393500: Updated shield, loss is 0.03810001537203789
Progress update from timestep 393600: Updated shield, loss is 0.04523434117436409
Eval num_timesteps=393600, episode_reward=15.43 +/- 10.20
Episode length: 23.20 +/- 14.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 15.4     |
| time/              |          |
|    total_timesteps | 393600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.02     |
|    ep_rew_mean     | 3.24     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 984      |
|    time_elapsed    | 89751    |
|    total_timesteps | 393600   |
---------------------------------
Progress update from timestep 393700: Updated shield, loss is 0.035469941794872284
Progress update from timestep 393800: Updated shield, loss is 0.030738193541765213
Eval num_timesteps=393800, episode_reward=30.25 +/- 10.17
Episode length: 43.20 +/- 13.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 43.2          |
|    mean_reward          | 30.2          |
| time/                   |               |
|    total_timesteps      | 393800        |
| train/                  |               |
|    approx_kl            | 1.2664417e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00522      |
|    explained_variance   | 0.252         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.08          |
|    n_updates            | 9840          |
|    policy_gradient_loss | -3.14e-05     |
|    value_loss           | 2.75          |
-------------------------------------------
Progress update from timestep 393900: Updated shield, loss is 0.04344351217150688
Progress update from timestep 394000: Updated shield, loss is 0.04016268625855446
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 394000: Saved shield_loss.csv. 
Eval num_timesteps=394000, episode_reward=10.23 +/- 11.69
Episode length: 15.80 +/- 17.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | 10.2     |
| time/              |          |
|    total_timesteps | 394000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.07     |
|    ep_rew_mean     | 2.51     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 985      |
|    time_elapsed    | 89850    |
|    total_timesteps | 394000   |
---------------------------------
Progress update from timestep 394100: Updated shield, loss is 0.02863997407257557
Progress update from timestep 394200: Updated shield, loss is 0.037249427288770676
Eval num_timesteps=394200, episode_reward=10.97 +/- 12.31
Episode length: 16.20 +/- 16.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.2          |
|    mean_reward          | 11            |
| time/                   |               |
|    total_timesteps      | 394200        |
| train/                  |               |
|    approx_kl            | 1.3757152e-05 |
|    clip_fraction        | 0.000446      |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00638      |
|    explained_variance   | 0.221         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.94          |
|    n_updates            | 9850          |
|    policy_gradient_loss | -0.000116     |
|    value_loss           | 2.18          |
-------------------------------------------
Progress update from timestep 394300: Updated shield, loss is 0.03313896432518959
Progress update from timestep 394400: Updated shield, loss is 0.029276246204972267
Eval num_timesteps=394400, episode_reward=7.04 +/- 3.65
Episode length: 10.60 +/- 4.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10.6     |
|    mean_reward     | 7.04     |
| time/              |          |
|    total_timesteps | 394400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.32     |
|    ep_rew_mean     | 2.73     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 986      |
|    time_elapsed    | 89926    |
|    total_timesteps | 394400   |
---------------------------------
Progress update from timestep 394500: Updated shield, loss is 0.04070301726460457
Progress update from timestep 394600: Updated shield, loss is 0.04106990247964859
Eval num_timesteps=394600, episode_reward=8.84 +/- 12.43
Episode length: 13.80 +/- 18.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 13.8         |
|    mean_reward          | 8.84         |
| time/                   |              |
|    total_timesteps      | 394600       |
| train/                  |              |
|    approx_kl            | 0.0013409294 |
|    clip_fraction        | 0.0114       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0255      |
|    explained_variance   | 0.0694       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.458        |
|    n_updates            | 9860         |
|    policy_gradient_loss | -0.00138     |
|    value_loss           | 1.84         |
------------------------------------------
Progress update from timestep 394700: Updated shield, loss is 0.027533413842320442
Progress update from timestep 394800: Updated shield, loss is 0.03950502350926399
Eval num_timesteps=394800, episode_reward=23.90 +/- 12.65
Episode length: 35.20 +/- 18.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 23.9     |
| time/              |          |
|    total_timesteps | 394800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.29     |
|    ep_rew_mean     | 2.7      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 987      |
|    time_elapsed    | 90014    |
|    total_timesteps | 394800   |
---------------------------------
Progress update from timestep 394900: Updated shield, loss is 0.045100051909685135
Progress update from timestep 395000: Updated shield, loss is 0.04404638335108757
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 395000: Saved shield_loss.csv. 
Eval num_timesteps=395000, episode_reward=25.81 +/- 12.79
Episode length: 36.40 +/- 17.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.4         |
|    mean_reward          | 25.8         |
| time/                   |              |
|    total_timesteps      | 395000       |
| train/                  |              |
|    approx_kl            | 0.0005456918 |
|    clip_fraction        | 0.0121       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0315      |
|    explained_variance   | 0.202        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.26         |
|    n_updates            | 9870         |
|    policy_gradient_loss | -0.00143     |
|    value_loss           | 1.72         |
------------------------------------------
Progress update from timestep 395100: Updated shield, loss is 0.0359518900513649
Progress update from timestep 395200: Updated shield, loss is 0.034396495670080185
Eval num_timesteps=395200, episode_reward=27.72 +/- 13.53
Episode length: 40.40 +/- 19.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.4     |
|    mean_reward     | 27.7     |
| time/              |          |
|    total_timesteps | 395200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.63     |
|    ep_rew_mean     | 2.95     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 988      |
|    time_elapsed    | 90134    |
|    total_timesteps | 395200   |
---------------------------------
Progress update from timestep 395300: Updated shield, loss is 0.03969430923461914
Progress update from timestep 395400: Updated shield, loss is 0.04561319574713707
Eval num_timesteps=395400, episode_reward=21.89 +/- 16.79
Episode length: 31.20 +/- 23.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 31.2          |
|    mean_reward          | 21.9          |
| time/                   |               |
|    total_timesteps      | 395400        |
| train/                  |               |
|    approx_kl            | 0.00019193464 |
|    clip_fraction        | 0.00446       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00699      |
|    explained_variance   | 0.207         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.35          |
|    n_updates            | 9880          |
|    policy_gradient_loss | -0.000183     |
|    value_loss           | 2.74          |
-------------------------------------------
Progress update from timestep 395500: Updated shield, loss is 0.03996100276708603
Progress update from timestep 395600: Updated shield, loss is 0.042622897773981094
Eval num_timesteps=395600, episode_reward=17.85 +/- 9.98
Episode length: 26.00 +/- 12.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | 17.8     |
| time/              |          |
|    total_timesteps | 395600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.59     |
|    ep_rew_mean     | 2.97     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 989      |
|    time_elapsed    | 90223    |
|    total_timesteps | 395600   |
---------------------------------
Progress update from timestep 395700: Updated shield, loss is 0.042665548622608185
Progress update from timestep 395800: Updated shield, loss is 0.04273359104990959
Eval num_timesteps=395800, episode_reward=13.96 +/- 11.83
Episode length: 20.20 +/- 15.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 20.2          |
|    mean_reward          | 14            |
| time/                   |               |
|    total_timesteps      | 395800        |
| train/                  |               |
|    approx_kl            | 0.00023599571 |
|    clip_fraction        | 0.00647       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0211       |
|    explained_variance   | 0.0934        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.788         |
|    n_updates            | 9890          |
|    policy_gradient_loss | -0.00088      |
|    value_loss           | 1.6           |
-------------------------------------------
Progress update from timestep 395900: Updated shield, loss is 0.028897996991872787
Progress update from timestep 396000: Updated shield, loss is 0.0354032926261425
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 396000: Saved shield_loss.csv. 
Eval num_timesteps=396000, episode_reward=29.73 +/- 10.08
Episode length: 42.80 +/- 14.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.8     |
|    mean_reward     | 29.7     |
| time/              |          |
|    total_timesteps | 396000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.24     |
|    ep_rew_mean     | 2.6      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 990      |
|    time_elapsed    | 90328    |
|    total_timesteps | 396000   |
---------------------------------
Progress update from timestep 396100: Updated shield, loss is 0.03572063148021698
Progress update from timestep 396200: Updated shield, loss is 0.04031742736697197
Eval num_timesteps=396200, episode_reward=22.74 +/- 15.31
Episode length: 32.60 +/- 21.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32.6          |
|    mean_reward          | 22.7          |
| time/                   |               |
|    total_timesteps      | 396200        |
| train/                  |               |
|    approx_kl            | 0.00045548347 |
|    clip_fraction        | 0.00915       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0261       |
|    explained_variance   | 0.278         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.799         |
|    n_updates            | 9900          |
|    policy_gradient_loss | -0.00108      |
|    value_loss           | 2.35          |
-------------------------------------------
Progress update from timestep 396300: Updated shield, loss is 0.039866819977760315
Progress update from timestep 396400: Updated shield, loss is 0.042007770389318466
Eval num_timesteps=396400, episode_reward=15.84 +/- 14.97
Episode length: 23.80 +/- 21.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | 15.8     |
| time/              |          |
|    total_timesteps | 396400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.81     |
|    ep_rew_mean     | 2.39     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 991      |
|    time_elapsed    | 90421    |
|    total_timesteps | 396400   |
---------------------------------
Progress update from timestep 396500: Updated shield, loss is 0.03952774778008461
Progress update from timestep 396600: Updated shield, loss is 0.0329844132065773
Eval num_timesteps=396600, episode_reward=29.06 +/- 10.89
Episode length: 42.20 +/- 15.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.2          |
|    mean_reward          | 29.1          |
| time/                   |               |
|    total_timesteps      | 396600        |
| train/                  |               |
|    approx_kl            | 0.00043617605 |
|    clip_fraction        | 0.0105        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0223       |
|    explained_variance   | 0.182         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.478         |
|    n_updates            | 9910          |
|    policy_gradient_loss | -0.00155      |
|    value_loss           | 1.57          |
-------------------------------------------
Progress update from timestep 396700: Updated shield, loss is 0.03550468385219574
Progress update from timestep 396800: Updated shield, loss is 0.03267545625567436
Eval num_timesteps=396800, episode_reward=10.13 +/- 13.33
Episode length: 14.60 +/- 17.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 10.1     |
| time/              |          |
|    total_timesteps | 396800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.37     |
|    ep_rew_mean     | 2.83     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 992      |
|    time_elapsed    | 90534    |
|    total_timesteps | 396800   |
---------------------------------
Progress update from timestep 396900: Updated shield, loss is 0.03248025104403496
Progress update from timestep 397000: Updated shield, loss is 0.042441219091415405
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 397000: Saved shield_loss.csv. 
Eval num_timesteps=397000, episode_reward=23.85 +/- 12.99
Episode length: 35.20 +/- 18.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.2          |
|    mean_reward          | 23.8          |
| time/                   |               |
|    total_timesteps      | 397000        |
| train/                  |               |
|    approx_kl            | 7.6812925e-05 |
|    clip_fraction        | 0.00134       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00585      |
|    explained_variance   | 0.242         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.84          |
|    n_updates            | 9920          |
|    policy_gradient_loss | -0.000308     |
|    value_loss           | 2.02          |
-------------------------------------------
Progress update from timestep 397100: Updated shield, loss is 0.039772797375917435
Progress update from timestep 397200: Updated shield, loss is 0.035777755081653595
Eval num_timesteps=397200, episode_reward=9.45 +/- 12.17
Episode length: 14.60 +/- 17.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 9.45     |
| time/              |          |
|    total_timesteps | 397200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.06     |
|    ep_rew_mean     | 2.52     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 993      |
|    time_elapsed    | 90635    |
|    total_timesteps | 397200   |
---------------------------------
Progress update from timestep 397300: Updated shield, loss is 0.03347712382674217
Progress update from timestep 397400: Updated shield, loss is 0.03695334494113922
Eval num_timesteps=397400, episode_reward=22.44 +/- 14.10
Episode length: 31.60 +/- 18.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 31.6          |
|    mean_reward          | 22.4          |
| time/                   |               |
|    total_timesteps      | 397400        |
| train/                  |               |
|    approx_kl            | 0.00029221404 |
|    clip_fraction        | 0.0125        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0217       |
|    explained_variance   | 0.202         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.941         |
|    n_updates            | 9930          |
|    policy_gradient_loss | -0.00108      |
|    value_loss           | 1.73          |
-------------------------------------------
Progress update from timestep 397500: Updated shield, loss is 0.033454686403274536
Progress update from timestep 397600: Updated shield, loss is 0.036706190556287766
Eval num_timesteps=397600, episode_reward=11.54 +/- 12.55
Episode length: 16.80 +/- 17.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | 11.5     |
| time/              |          |
|    total_timesteps | 397600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.17     |
|    ep_rew_mean     | 2.64     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 994      |
|    time_elapsed    | 90749    |
|    total_timesteps | 397600   |
---------------------------------
Progress update from timestep 397700: Updated shield, loss is 0.04399143531918526
Progress update from timestep 397800: Updated shield, loss is 0.03168829157948494
Eval num_timesteps=397800, episode_reward=4.62 +/- 3.00
Episode length: 7.60 +/- 4.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 7.6          |
|    mean_reward          | 4.62         |
| time/                   |              |
|    total_timesteps      | 397800       |
| train/                  |              |
|    approx_kl            | 1.994502e-05 |
|    clip_fraction        | 0.000446     |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00597     |
|    explained_variance   | 0.086        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.867        |
|    n_updates            | 9940         |
|    policy_gradient_loss | -0.0003      |
|    value_loss           | 1.53         |
------------------------------------------
Progress update from timestep 397900: Updated shield, loss is 0.03436516970396042
Progress update from timestep 398000: Updated shield, loss is 0.03957221284508705
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 398000: Saved shield_loss.csv. 
Eval num_timesteps=398000, episode_reward=17.90 +/- 14.01
Episode length: 26.60 +/- 19.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.6     |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 398000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.59     |
|    ep_rew_mean     | 2.87     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 995      |
|    time_elapsed    | 90824    |
|    total_timesteps | 398000   |
---------------------------------
Progress update from timestep 398100: Updated shield, loss is 0.04045138508081436
Progress update from timestep 398200: Updated shield, loss is 0.03016272932291031
Eval num_timesteps=398200, episode_reward=18.75 +/- 13.02
Episode length: 27.60 +/- 18.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 27.6          |
|    mean_reward          | 18.7          |
| time/                   |               |
|    total_timesteps      | 398200        |
| train/                  |               |
|    approx_kl            | 0.00013176995 |
|    clip_fraction        | 0.00335       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00805      |
|    explained_variance   | 0.185         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.884         |
|    n_updates            | 9950          |
|    policy_gradient_loss | -0.000173     |
|    value_loss           | 1.52          |
-------------------------------------------
Progress update from timestep 398300: Updated shield, loss is 0.042224396020174026
Progress update from timestep 398400: Updated shield, loss is 0.03414836525917053
Eval num_timesteps=398400, episode_reward=8.27 +/- 12.62
Episode length: 13.00 +/- 18.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13       |
|    mean_reward     | 8.27     |
| time/              |          |
|    total_timesteps | 398400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.17     |
|    ep_rew_mean     | 2.64     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 996      |
|    time_elapsed    | 90920    |
|    total_timesteps | 398400   |
---------------------------------
Progress update from timestep 398500: Updated shield, loss is 0.03650721162557602
Progress update from timestep 398600: Updated shield, loss is 0.04412909969687462
Eval num_timesteps=398600, episode_reward=17.58 +/- 14.85
Episode length: 25.40 +/- 20.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.4         |
|    mean_reward          | 17.6         |
| time/                   |              |
|    total_timesteps      | 398600       |
| train/                  |              |
|    approx_kl            | 3.985536e-11 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00026     |
|    explained_variance   | 0.181        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.588        |
|    n_updates            | 9960         |
|    policy_gradient_loss | -5.04e-09    |
|    value_loss           | 1.71         |
------------------------------------------
Progress update from timestep 398700: Updated shield, loss is 0.02857229858636856
Progress update from timestep 398800: Updated shield, loss is 0.03775859251618385
Eval num_timesteps=398800, episode_reward=11.08 +/- 13.00
Episode length: 16.00 +/- 17.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | 11.1     |
| time/              |          |
|    total_timesteps | 398800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.85     |
|    ep_rew_mean     | 2.34     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 997      |
|    time_elapsed    | 91025    |
|    total_timesteps | 398800   |
---------------------------------
Progress update from timestep 398900: Updated shield, loss is 0.03549337014555931
Progress update from timestep 399000: Updated shield, loss is 0.029335642233490944
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 399000: Saved shield_loss.csv. 
Eval num_timesteps=399000, episode_reward=17.02 +/- 13.95
Episode length: 25.60 +/- 20.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.6          |
|    mean_reward          | 17            |
| time/                   |               |
|    total_timesteps      | 399000        |
| train/                  |               |
|    approx_kl            | 3.3248267e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00754      |
|    explained_variance   | 0.168         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.376         |
|    n_updates            | 9970          |
|    policy_gradient_loss | -0.000423     |
|    value_loss           | 1.17          |
-------------------------------------------
Progress update from timestep 399100: Updated shield, loss is 0.03260073438286781
Progress update from timestep 399200: Updated shield, loss is 0.034347161650657654
Eval num_timesteps=399200, episode_reward=24.15 +/- 13.43
Episode length: 35.00 +/- 18.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 24.1     |
| time/              |          |
|    total_timesteps | 399200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.31     |
|    ep_rew_mean     | 2.75     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 998      |
|    time_elapsed    | 91132    |
|    total_timesteps | 399200   |
---------------------------------
Progress update from timestep 399300: Updated shield, loss is 0.03040442243218422
Progress update from timestep 399400: Updated shield, loss is 0.0352562814950943
Eval num_timesteps=399400, episode_reward=22.82 +/- 15.67
Episode length: 32.40 +/- 21.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32.4          |
|    mean_reward          | 22.8          |
| time/                   |               |
|    total_timesteps      | 399400        |
| train/                  |               |
|    approx_kl            | 0.00040150576 |
|    clip_fraction        | 0.015         |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0201       |
|    explained_variance   | 0.169         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.797         |
|    n_updates            | 9980          |
|    policy_gradient_loss | -0.00186      |
|    value_loss           | 1.47          |
-------------------------------------------
Progress update from timestep 399500: Updated shield, loss is 0.044088203459978104
Progress update from timestep 399600: Updated shield, loss is 0.03114267811179161
Eval num_timesteps=399600, episode_reward=15.18 +/- 16.26
Episode length: 22.20 +/- 22.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 15.2     |
| time/              |          |
|    total_timesteps | 399600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.68     |
|    ep_rew_mean     | 2.25     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 999      |
|    time_elapsed    | 91216    |
|    total_timesteps | 399600   |
---------------------------------
Progress update from timestep 399700: Updated shield, loss is 0.04277303069829941
Progress update from timestep 399800: Updated shield, loss is 0.046640727669000626
Eval num_timesteps=399800, episode_reward=19.15 +/- 13.24
Episode length: 27.60 +/- 18.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 27.6         |
|    mean_reward          | 19.2         |
| time/                   |              |
|    total_timesteps      | 399800       |
| train/                  |              |
|    approx_kl            | 0.0005558533 |
|    clip_fraction        | 0.0145       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0253      |
|    explained_variance   | 0.238        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.74         |
|    n_updates            | 9990         |
|    policy_gradient_loss | -0.00155     |
|    value_loss           | 1.33         |
------------------------------------------
Progress update from timestep 399900: Updated shield, loss is 0.03572644293308258
Progress update from timestep 400000: Updated shield, loss is 0.03302856162190437
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 400000: Saved shield_loss.csv. 
Eval num_timesteps=400000, episode_reward=16.90 +/- 14.69
Episode length: 25.00 +/- 20.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 400000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.99     |
|    ep_rew_mean     | 2.41     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1000     |
|    time_elapsed    | 91307    |
|    total_timesteps | 400000   |
---------------------------------
Progress update from timestep 400100: Updated shield, loss is 0.040495555847883224
Progress update from timestep 400200: Updated shield, loss is 0.03891884535551071
Eval num_timesteps=400200, episode_reward=23.18 +/- 14.51
Episode length: 33.80 +/- 20.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.8          |
|    mean_reward          | 23.2          |
| time/                   |               |
|    total_timesteps      | 400200        |
| train/                  |               |
|    approx_kl            | 0.00051333517 |
|    clip_fraction        | 0.00759       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0126       |
|    explained_variance   | 0.11          |
|    learning_rate        | 0.0001        |
|    loss                 | 1.21          |
|    n_updates            | 10000         |
|    policy_gradient_loss | -0.00105      |
|    value_loss           | 2.28          |
-------------------------------------------
Progress update from timestep 400300: Updated shield, loss is 0.03267620876431465
Progress update from timestep 400400: Updated shield, loss is 0.04866645857691765
Eval num_timesteps=400400, episode_reward=15.88 +/- 16.10
Episode length: 22.80 +/- 22.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 15.9     |
| time/              |          |
|    total_timesteps | 400400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.1      |
|    ep_rew_mean     | 2.6      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1001     |
|    time_elapsed    | 91397    |
|    total_timesteps | 400400   |
---------------------------------
Progress update from timestep 400500: Updated shield, loss is 0.03743623197078705
Progress update from timestep 400600: Updated shield, loss is 0.04852632060647011
Eval num_timesteps=400600, episode_reward=3.03 +/- 1.97
Episode length: 5.20 +/- 2.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 5.2          |
|    mean_reward          | 3.03         |
| time/                   |              |
|    total_timesteps      | 400600       |
| train/                  |              |
|    approx_kl            | 9.728345e-11 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.000176    |
|    explained_variance   | 0.193        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.799        |
|    n_updates            | 10010        |
|    policy_gradient_loss | -1.81e-08    |
|    value_loss           | 2.52         |
------------------------------------------
Progress update from timestep 400700: Updated shield, loss is 0.037841469049453735
Progress update from timestep 400800: Updated shield, loss is 0.04117869958281517
Eval num_timesteps=400800, episode_reward=17.16 +/- 14.70
Episode length: 25.20 +/- 20.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 17.2     |
| time/              |          |
|    total_timesteps | 400800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.59     |
|    ep_rew_mean     | 2.97     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1002     |
|    time_elapsed    | 91466    |
|    total_timesteps | 400800   |
---------------------------------
Progress update from timestep 400900: Updated shield, loss is 0.03550102189183235
Progress update from timestep 401000: Updated shield, loss is 0.04207164794206619
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 401000: Saved shield_loss.csv. 
Eval num_timesteps=401000, episode_reward=16.17 +/- 14.06
Episode length: 24.60 +/- 20.79
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.6          |
|    mean_reward          | 16.2          |
| time/                   |               |
|    total_timesteps      | 401000        |
| train/                  |               |
|    approx_kl            | 0.00012786638 |
|    clip_fraction        | 0.00647       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0174       |
|    explained_variance   | 0.141         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.885         |
|    n_updates            | 10020         |
|    policy_gradient_loss | -0.00102      |
|    value_loss           | 1.93          |
-------------------------------------------
Progress update from timestep 401100: Updated shield, loss is 0.03460125997662544
Progress update from timestep 401200: Updated shield, loss is 0.04140053689479828
Eval num_timesteps=401200, episode_reward=15.20 +/- 10.71
Episode length: 22.40 +/- 14.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 15.2     |
| time/              |          |
|    total_timesteps | 401200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.3      |
|    ep_rew_mean     | 2.73     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1003     |
|    time_elapsed    | 91554    |
|    total_timesteps | 401200   |
---------------------------------
Progress update from timestep 401300: Updated shield, loss is 0.02901582419872284
Progress update from timestep 401400: Updated shield, loss is 0.037310902029275894
Eval num_timesteps=401400, episode_reward=23.18 +/- 14.29
Episode length: 33.80 +/- 19.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.8         |
|    mean_reward          | 23.2         |
| time/                   |              |
|    total_timesteps      | 401400       |
| train/                  |              |
|    approx_kl            | 7.524824e-05 |
|    clip_fraction        | 0.00379      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0108      |
|    explained_variance   | 0.372        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.494        |
|    n_updates            | 10030        |
|    policy_gradient_loss | -0.000393    |
|    value_loss           | 1.66         |
------------------------------------------
Progress update from timestep 401500: Updated shield, loss is 0.03779074549674988
Progress update from timestep 401600: Updated shield, loss is 0.03984880819916725
Eval num_timesteps=401600, episode_reward=10.89 +/- 11.91
Episode length: 16.60 +/- 16.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | 10.9     |
| time/              |          |
|    total_timesteps | 401600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.09     |
|    ep_rew_mean     | 2.54     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1004     |
|    time_elapsed    | 91660    |
|    total_timesteps | 401600   |
---------------------------------
Progress update from timestep 401700: Updated shield, loss is 0.04633985459804535
Progress update from timestep 401800: Updated shield, loss is 0.032021112740039825
Eval num_timesteps=401800, episode_reward=29.44 +/- 12.32
Episode length: 41.60 +/- 16.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.6         |
|    mean_reward          | 29.4         |
| time/                   |              |
|    total_timesteps      | 401800       |
| train/                  |              |
|    approx_kl            | 0.0006119487 |
|    clip_fraction        | 0.00335      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0152      |
|    explained_variance   | 0.149        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.514        |
|    n_updates            | 10040        |
|    policy_gradient_loss | -0.000612    |
|    value_loss           | 1.14         |
------------------------------------------
Progress update from timestep 401900: Updated shield, loss is 0.04919425770640373
Progress update from timestep 402000: Updated shield, loss is 0.0366496779024601
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 402000: Saved shield_loss.csv. 
Eval num_timesteps=402000, episode_reward=13.31 +/- 10.93
Episode length: 20.00 +/- 15.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20       |
|    mean_reward     | 13.3     |
| time/              |          |
|    total_timesteps | 402000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.43     |
|    ep_rew_mean     | 2.84     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1005     |
|    time_elapsed    | 91748    |
|    total_timesteps | 402000   |
---------------------------------
Progress update from timestep 402100: Updated shield, loss is 0.0322556309401989
Progress update from timestep 402200: Updated shield, loss is 0.034865062683820724
Eval num_timesteps=402200, episode_reward=18.47 +/- 14.08
Episode length: 27.00 +/- 19.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 27            |
|    mean_reward          | 18.5          |
| time/                   |               |
|    total_timesteps      | 402200        |
| train/                  |               |
|    approx_kl            | 0.00029949358 |
|    clip_fraction        | 0.0067        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0204       |
|    explained_variance   | 0.182         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.93          |
|    n_updates            | 10050         |
|    policy_gradient_loss | -0.000985     |
|    value_loss           | 1.67          |
-------------------------------------------
Progress update from timestep 402300: Updated shield, loss is 0.03960016742348671
Progress update from timestep 402400: Updated shield, loss is 0.03944234922528267
Eval num_timesteps=402400, episode_reward=14.57 +/- 10.29
Episode length: 21.60 +/- 14.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 14.6     |
| time/              |          |
|    total_timesteps | 402400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.45     |
|    ep_rew_mean     | 2.86     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1006     |
|    time_elapsed    | 91833    |
|    total_timesteps | 402400   |
---------------------------------
Progress update from timestep 402500: Updated shield, loss is 0.04007994383573532
Progress update from timestep 402600: Updated shield, loss is 0.041265521198511124
Eval num_timesteps=402600, episode_reward=17.89 +/- 13.83
Episode length: 26.60 +/- 20.10
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.6          |
|    mean_reward          | 17.9          |
| time/                   |               |
|    total_timesteps      | 402600        |
| train/                  |               |
|    approx_kl            | 0.00014466215 |
|    clip_fraction        | 0.0058        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0125       |
|    explained_variance   | 0.0957        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.52          |
|    n_updates            | 10060         |
|    policy_gradient_loss | -0.000757     |
|    value_loss           | 2.4           |
-------------------------------------------
Progress update from timestep 402700: Updated shield, loss is 0.036702364683151245
Progress update from timestep 402800: Updated shield, loss is 0.037892796099185944
Eval num_timesteps=402800, episode_reward=17.23 +/- 13.90
Episode length: 25.80 +/- 20.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | 17.2     |
| time/              |          |
|    total_timesteps | 402800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.73     |
|    ep_rew_mean     | 3.04     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1007     |
|    time_elapsed    | 91923    |
|    total_timesteps | 402800   |
---------------------------------
Progress update from timestep 402900: Updated shield, loss is 0.024834873154759407
Progress update from timestep 403000: Updated shield, loss is 0.035143833607435226
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 403000: Saved shield_loss.csv. 
Eval num_timesteps=403000, episode_reward=15.62 +/- 15.06
Episode length: 23.60 +/- 21.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.6          |
|    mean_reward          | 15.6          |
| time/                   |               |
|    total_timesteps      | 403000        |
| train/                  |               |
|    approx_kl            | 0.00020970554 |
|    clip_fraction        | 0.00804       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0311       |
|    explained_variance   | 0.105         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.44          |
|    n_updates            | 10070         |
|    policy_gradient_loss | -0.00126      |
|    value_loss           | 1.85          |
-------------------------------------------
Progress update from timestep 403100: Updated shield, loss is 0.03572314977645874
Progress update from timestep 403200: Updated shield, loss is 0.038378238677978516
Eval num_timesteps=403200, episode_reward=16.03 +/- 15.10
Episode length: 23.80 +/- 21.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 403200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.25     |
|    ep_rew_mean     | 2.76     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1008     |
|    time_elapsed    | 92009    |
|    total_timesteps | 403200   |
---------------------------------
Progress update from timestep 403300: Updated shield, loss is 0.03541761264204979
Progress update from timestep 403400: Updated shield, loss is 0.041015397757291794
Eval num_timesteps=403400, episode_reward=16.23 +/- 14.91
Episode length: 24.00 +/- 21.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 24          |
|    mean_reward          | 16.2        |
| time/                   |             |
|    total_timesteps      | 403400      |
| train/                  |             |
|    approx_kl            | 0.002652893 |
|    clip_fraction        | 0.0132      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.018      |
|    explained_variance   | 0.139       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.861       |
|    n_updates            | 10080       |
|    policy_gradient_loss | -0.00159    |
|    value_loss           | 1.63        |
-----------------------------------------
Progress update from timestep 403500: Updated shield, loss is 0.04299936071038246
Progress update from timestep 403600: Updated shield, loss is 0.03581281751394272
Eval num_timesteps=403600, episode_reward=10.16 +/- 12.82
Episode length: 15.00 +/- 17.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | 10.2     |
| time/              |          |
|    total_timesteps | 403600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.54     |
|    ep_rew_mean     | 2.14     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1009     |
|    time_elapsed    | 92098    |
|    total_timesteps | 403600   |
---------------------------------
Progress update from timestep 403700: Updated shield, loss is 0.039627909660339355
Progress update from timestep 403800: Updated shield, loss is 0.039378613233566284
Eval num_timesteps=403800, episode_reward=22.72 +/- 14.44
Episode length: 33.40 +/- 20.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.4          |
|    mean_reward          | 22.7          |
| time/                   |               |
|    total_timesteps      | 403800        |
| train/                  |               |
|    approx_kl            | 0.00019210977 |
|    clip_fraction        | 0.0067        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0165       |
|    explained_variance   | 0.0324        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.14          |
|    n_updates            | 10090         |
|    policy_gradient_loss | -0.0012       |
|    value_loss           | 1.83          |
-------------------------------------------
Progress update from timestep 403900: Updated shield, loss is 0.027728261426091194
Progress update from timestep 404000: Updated shield, loss is 0.03205500543117523
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 404000: Saved shield_loss.csv. 
Eval num_timesteps=404000, episode_reward=16.75 +/- 14.64
Episode length: 24.80 +/- 20.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 404000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.96     |
|    ep_rew_mean     | 2.45     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1010     |
|    time_elapsed    | 92192    |
|    total_timesteps | 404000   |
---------------------------------
Progress update from timestep 404100: Updated shield, loss is 0.03874334692955017
Progress update from timestep 404200: Updated shield, loss is 0.04359590262174606
Eval num_timesteps=404200, episode_reward=16.69 +/- 15.48
Episode length: 24.00 +/- 21.28
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24            |
|    mean_reward          | 16.7          |
| time/                   |               |
|    total_timesteps      | 404200        |
| train/                  |               |
|    approx_kl            | 0.00051214907 |
|    clip_fraction        | 0.01          |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0186       |
|    explained_variance   | 0.213         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.734         |
|    n_updates            | 10100         |
|    policy_gradient_loss | -0.000968     |
|    value_loss           | 1.85          |
-------------------------------------------
Progress update from timestep 404300: Updated shield, loss is 0.0365106575191021
Progress update from timestep 404400: Updated shield, loss is 0.03476627171039581
Eval num_timesteps=404400, episode_reward=13.65 +/- 11.75
Episode length: 21.00 +/- 17.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | 13.7     |
| time/              |          |
|    total_timesteps | 404400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.48     |
|    ep_rew_mean     | 2.86     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1011     |
|    time_elapsed    | 92277    |
|    total_timesteps | 404400   |
---------------------------------
Progress update from timestep 404500: Updated shield, loss is 0.04507366195321083
Progress update from timestep 404600: Updated shield, loss is 0.02931063063442707
Eval num_timesteps=404600, episode_reward=19.86 +/- 12.62
Episode length: 29.00 +/- 17.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 29            |
|    mean_reward          | 19.9          |
| time/                   |               |
|    total_timesteps      | 404600        |
| train/                  |               |
|    approx_kl            | 1.5017483e-05 |
|    clip_fraction        | 0.00201       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00474      |
|    explained_variance   | 0.272         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.697         |
|    n_updates            | 10110         |
|    policy_gradient_loss | -0.000269     |
|    value_loss           | 1.56          |
-------------------------------------------
Progress update from timestep 404700: Updated shield, loss is 0.04818933084607124
Progress update from timestep 404800: Updated shield, loss is 0.03431068733334541
Eval num_timesteps=404800, episode_reward=2.93 +/- 1.89
Episode length: 5.00 +/- 2.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 2.93     |
| time/              |          |
|    total_timesteps | 404800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.47     |
|    ep_rew_mean     | 2.82     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1012     |
|    time_elapsed    | 92349    |
|    total_timesteps | 404800   |
---------------------------------
Progress update from timestep 404900: Updated shield, loss is 0.033124927431344986
Progress update from timestep 405000: Updated shield, loss is 0.05269412323832512
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 405000: Saved shield_loss.csv. 
Eval num_timesteps=405000, episode_reward=16.61 +/- 14.23
Episode length: 25.00 +/- 20.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 25          |
|    mean_reward          | 16.6        |
| time/                   |             |
|    total_timesteps      | 405000      |
| train/                  |             |
|    approx_kl            | 0.000269044 |
|    clip_fraction        | 0.00737     |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0135     |
|    explained_variance   | 0.363       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.438       |
|    n_updates            | 10120       |
|    policy_gradient_loss | -0.000622   |
|    value_loss           | 1.72        |
-----------------------------------------
Progress update from timestep 405100: Updated shield, loss is 0.03613095358014107
Progress update from timestep 405200: Updated shield, loss is 0.0467730388045311
Eval num_timesteps=405200, episode_reward=11.28 +/- 12.45
Episode length: 16.40 +/- 17.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | 11.3     |
| time/              |          |
|    total_timesteps | 405200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.41     |
|    ep_rew_mean     | 2.78     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1013     |
|    time_elapsed    | 92442    |
|    total_timesteps | 405200   |
---------------------------------
Progress update from timestep 405300: Updated shield, loss is 0.041366059333086014
Progress update from timestep 405400: Updated shield, loss is 0.037839192897081375
Eval num_timesteps=405400, episode_reward=22.53 +/- 15.10
Episode length: 32.60 +/- 21.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.6         |
|    mean_reward          | 22.5         |
| time/                   |              |
|    total_timesteps      | 405400       |
| train/                  |              |
|    approx_kl            | 9.306635e-05 |
|    clip_fraction        | 0.00335      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00901     |
|    explained_variance   | 0.27         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.847        |
|    n_updates            | 10130        |
|    policy_gradient_loss | -0.000567    |
|    value_loss           | 1.73         |
------------------------------------------
Progress update from timestep 405500: Updated shield, loss is 0.03947087377309799
Progress update from timestep 405600: Updated shield, loss is 0.04038875922560692
Eval num_timesteps=405600, episode_reward=6.07 +/- 3.45
Episode length: 9.40 +/- 5.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.4      |
|    mean_reward     | 6.07     |
| time/              |          |
|    total_timesteps | 405600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.76     |
|    ep_rew_mean     | 3.08     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1014     |
|    time_elapsed    | 92517    |
|    total_timesteps | 405600   |
---------------------------------
Progress update from timestep 405700: Updated shield, loss is 0.04092889651656151
Progress update from timestep 405800: Updated shield, loss is 0.03862974792718887
Eval num_timesteps=405800, episode_reward=12.76 +/- 12.12
Episode length: 18.40 +/- 16.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.4         |
|    mean_reward          | 12.8         |
| time/                   |              |
|    total_timesteps      | 405800       |
| train/                  |              |
|    approx_kl            | 7.828228e-05 |
|    clip_fraction        | 0.00446      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00577     |
|    explained_variance   | 0.424        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.909        |
|    n_updates            | 10140        |
|    policy_gradient_loss | -0.00106     |
|    value_loss           | 1.46         |
------------------------------------------
Progress update from timestep 405900: Updated shield, loss is 0.048470545560121536
Progress update from timestep 406000: Updated shield, loss is 0.04538952186703682
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 406000: Saved shield_loss.csv. 
Eval num_timesteps=406000, episode_reward=10.17 +/- 11.68
Episode length: 15.80 +/- 17.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | 10.2     |
| time/              |          |
|    total_timesteps | 406000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.02     |
|    ep_rew_mean     | 2.52     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1015     |
|    time_elapsed    | 92603    |
|    total_timesteps | 406000   |
---------------------------------
Progress update from timestep 406100: Updated shield, loss is 0.03755848854780197
Progress update from timestep 406200: Updated shield, loss is 0.04044507071375847
Eval num_timesteps=406200, episode_reward=16.63 +/- 15.54
Episode length: 24.00 +/- 21.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24           |
|    mean_reward          | 16.6         |
| time/                   |              |
|    total_timesteps      | 406200       |
| train/                  |              |
|    approx_kl            | 8.897725e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00502     |
|    explained_variance   | 0.185        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.647        |
|    n_updates            | 10150        |
|    policy_gradient_loss | -0.000109    |
|    value_loss           | 1.67         |
------------------------------------------
Progress update from timestep 406300: Updated shield, loss is 0.034539978951215744
Progress update from timestep 406400: Updated shield, loss is 0.035940319299697876
Eval num_timesteps=406400, episode_reward=14.81 +/- 16.07
Episode length: 22.00 +/- 22.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22       |
|    mean_reward     | 14.8     |
| time/              |          |
|    total_timesteps | 406400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.37     |
|    ep_rew_mean     | 2.78     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1016     |
|    time_elapsed    | 92688    |
|    total_timesteps | 406400   |
---------------------------------
Progress update from timestep 406500: Updated shield, loss is 0.03828750550746918
Progress update from timestep 406600: Updated shield, loss is 0.033288490027189255
Eval num_timesteps=406600, episode_reward=6.75 +/- 3.63
Episode length: 10.80 +/- 5.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 10.8          |
|    mean_reward          | 6.75          |
| time/                   |               |
|    total_timesteps      | 406600        |
| train/                  |               |
|    approx_kl            | 1.5203286e-05 |
|    clip_fraction        | 0.00268       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00539      |
|    explained_variance   | 0.194         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.394         |
|    n_updates            | 10160         |
|    policy_gradient_loss | -0.000128     |
|    value_loss           | 1.68          |
-------------------------------------------
Progress update from timestep 406700: Updated shield, loss is 0.04407186806201935
Progress update from timestep 406800: Updated shield, loss is 0.037116967141628265
Eval num_timesteps=406800, episode_reward=16.58 +/- 14.66
Episode length: 24.60 +/- 20.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 406800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.82     |
|    ep_rew_mean     | 2.39     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1017     |
|    time_elapsed    | 92760    |
|    total_timesteps | 406800   |
---------------------------------
Progress update from timestep 406900: Updated shield, loss is 0.04598585516214371
Progress update from timestep 407000: Updated shield, loss is 0.049635227769613266
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 407000: Saved shield_loss.csv. 
Eval num_timesteps=407000, episode_reward=16.87 +/- 15.06
Episode length: 24.60 +/- 21.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.6          |
|    mean_reward          | 16.9          |
| time/                   |               |
|    total_timesteps      | 407000        |
| train/                  |               |
|    approx_kl            | 0.00036177182 |
|    clip_fraction        | 0.0058        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.013        |
|    explained_variance   | 0.162         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.15          |
|    n_updates            | 10170         |
|    policy_gradient_loss | -0.000727     |
|    value_loss           | 1.5           |
-------------------------------------------
Progress update from timestep 407100: Updated shield, loss is 0.02677249349653721
Progress update from timestep 407200: Updated shield, loss is 0.03825562074780464
Eval num_timesteps=407200, episode_reward=23.13 +/- 14.47
Episode length: 33.60 +/- 20.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 23.1     |
| time/              |          |
|    total_timesteps | 407200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.1      |
|    ep_rew_mean     | 2.52     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1018     |
|    time_elapsed    | 92867    |
|    total_timesteps | 407200   |
---------------------------------
Progress update from timestep 407300: Updated shield, loss is 0.051664259284734726
Progress update from timestep 407400: Updated shield, loss is 0.037512559443712234
Eval num_timesteps=407400, episode_reward=18.16 +/- 13.43
Episode length: 26.80 +/- 19.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.8          |
|    mean_reward          | 18.2          |
| time/                   |               |
|    total_timesteps      | 407400        |
| train/                  |               |
|    approx_kl            | 0.00011070824 |
|    clip_fraction        | 0.00179       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0046       |
|    explained_variance   | 0.197         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.693         |
|    n_updates            | 10180         |
|    policy_gradient_loss | -0.000253     |
|    value_loss           | 1.27          |
-------------------------------------------
Progress update from timestep 407500: Updated shield, loss is 0.03935834765434265
Progress update from timestep 407600: Updated shield, loss is 0.03539643436670303
Eval num_timesteps=407600, episode_reward=22.08 +/- 14.41
Episode length: 33.00 +/- 21.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 22.1     |
| time/              |          |
|    total_timesteps | 407600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.76     |
|    ep_rew_mean     | 3.08     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1019     |
|    time_elapsed    | 92980    |
|    total_timesteps | 407600   |
---------------------------------
Progress update from timestep 407700: Updated shield, loss is 0.04213687404990196
Progress update from timestep 407800: Updated shield, loss is 0.037603918462991714
Eval num_timesteps=407800, episode_reward=15.65 +/- 15.97
Episode length: 22.80 +/- 22.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 22.8       |
|    mean_reward          | 15.7       |
| time/                   |            |
|    total_timesteps      | 407800     |
| train/                  |            |
|    approx_kl            | 0.00038015 |
|    clip_fraction        | 0.00915    |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.019     |
|    explained_variance   | 0.188      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.953      |
|    n_updates            | 10190      |
|    policy_gradient_loss | -0.000935  |
|    value_loss           | 1.93       |
----------------------------------------
Progress update from timestep 407900: Updated shield, loss is 0.0346321240067482
Progress update from timestep 408000: Updated shield, loss is 0.04812806099653244
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 408000: Saved shield_loss.csv. 
Eval num_timesteps=408000, episode_reward=26.82 +/- 11.73
Episode length: 37.20 +/- 15.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.2     |
|    mean_reward     | 26.8     |
| time/              |          |
|    total_timesteps | 408000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.86     |
|    ep_rew_mean     | 3.13     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1020     |
|    time_elapsed    | 93090    |
|    total_timesteps | 408000   |
---------------------------------
Progress update from timestep 408100: Updated shield, loss is 0.04091142117977142
Progress update from timestep 408200: Updated shield, loss is 0.03639109060168266
Eval num_timesteps=408200, episode_reward=17.30 +/- 14.98
Episode length: 25.00 +/- 20.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25            |
|    mean_reward          | 17.3          |
| time/                   |               |
|    total_timesteps      | 408200        |
| train/                  |               |
|    approx_kl            | 0.00010060181 |
|    clip_fraction        | 0.00357       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00773      |
|    explained_variance   | 0.136         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.12          |
|    n_updates            | 10200         |
|    policy_gradient_loss | -0.000221     |
|    value_loss           | 2.3           |
-------------------------------------------
Progress update from timestep 408300: Updated shield, loss is 0.04320264980196953
Progress update from timestep 408400: Updated shield, loss is 0.04582790285348892
Eval num_timesteps=408400, episode_reward=11.08 +/- 11.78
Episode length: 16.60 +/- 16.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | 11.1     |
| time/              |          |
|    total_timesteps | 408400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.9      |
|    ep_rew_mean     | 2.43     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1021     |
|    time_elapsed    | 93195    |
|    total_timesteps | 408400   |
---------------------------------
Progress update from timestep 408500: Updated shield, loss is 0.03700108081102371
Progress update from timestep 408600: Updated shield, loss is 0.030014462769031525
Eval num_timesteps=408600, episode_reward=23.22 +/- 15.68
Episode length: 32.80 +/- 21.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32.8          |
|    mean_reward          | 23.2          |
| time/                   |               |
|    total_timesteps      | 408600        |
| train/                  |               |
|    approx_kl            | 0.00069943914 |
|    clip_fraction        | 0.00893       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0149       |
|    explained_variance   | 0.18          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.791         |
|    n_updates            | 10210         |
|    policy_gradient_loss | -0.000668     |
|    value_loss           | 1.81          |
-------------------------------------------
Progress update from timestep 408700: Updated shield, loss is 0.040900930762290955
Progress update from timestep 408800: Updated shield, loss is 0.03943303972482681
Eval num_timesteps=408800, episode_reward=11.08 +/- 12.96
Episode length: 15.80 +/- 17.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | 11.1     |
| time/              |          |
|    total_timesteps | 408800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.32     |
|    ep_rew_mean     | 2.71     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1022     |
|    time_elapsed    | 93282    |
|    total_timesteps | 408800   |
---------------------------------
Progress update from timestep 408900: Updated shield, loss is 0.04055258631706238
Progress update from timestep 409000: Updated shield, loss is 0.03561164811253548
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 409000: Saved shield_loss.csv. 
Eval num_timesteps=409000, episode_reward=11.73 +/- 12.19
Episode length: 17.40 +/- 16.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.4        |
|    mean_reward          | 11.7        |
| time/                   |             |
|    total_timesteps      | 409000      |
| train/                  |             |
|    approx_kl            | 0.000651491 |
|    clip_fraction        | 0.00759     |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0296     |
|    explained_variance   | 0.313       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.759       |
|    n_updates            | 10220       |
|    policy_gradient_loss | -0.00123    |
|    value_loss           | 2.35        |
-----------------------------------------
Progress update from timestep 409100: Updated shield, loss is 0.05244046449661255
Progress update from timestep 409200: Updated shield, loss is 0.04145429655909538
Eval num_timesteps=409200, episode_reward=12.05 +/- 12.09
Episode length: 17.80 +/- 16.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | 12       |
| time/              |          |
|    total_timesteps | 409200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.36     |
|    ep_rew_mean     | 2.74     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1023     |
|    time_elapsed    | 93377    |
|    total_timesteps | 409200   |
---------------------------------
Progress update from timestep 409300: Updated shield, loss is 0.032512273639440536
Progress update from timestep 409400: Updated shield, loss is 0.0322970449924469
Eval num_timesteps=409400, episode_reward=16.69 +/- 15.29
Episode length: 24.40 +/- 21.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.4          |
|    mean_reward          | 16.7          |
| time/                   |               |
|    total_timesteps      | 409400        |
| train/                  |               |
|    approx_kl            | 2.6173957e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.000209     |
|    explained_variance   | 0.0583        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.798         |
|    n_updates            | 10230         |
|    policy_gradient_loss | 1.01e-09      |
|    value_loss           | 1.85          |
-------------------------------------------
Progress update from timestep 409500: Updated shield, loss is 0.05203185975551605
Progress update from timestep 409600: Updated shield, loss is 0.039733607321977615
Eval num_timesteps=409600, episode_reward=16.76 +/- 15.54
Episode length: 24.20 +/- 21.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 409600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.37     |
|    ep_rew_mean     | 2.68     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1024     |
|    time_elapsed    | 93468    |
|    total_timesteps | 409600   |
---------------------------------
Progress update from timestep 409700: Updated shield, loss is 0.04196242615580559
Progress update from timestep 409800: Updated shield, loss is 0.031220344826579094
Eval num_timesteps=409800, episode_reward=16.66 +/- 15.04
Episode length: 24.40 +/- 20.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.4          |
|    mean_reward          | 16.7          |
| time/                   |               |
|    total_timesteps      | 409800        |
| train/                  |               |
|    approx_kl            | 0.00014606667 |
|    clip_fraction        | 0.00714       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.01         |
|    explained_variance   | 0.165         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.03          |
|    n_updates            | 10240         |
|    policy_gradient_loss | -0.00096      |
|    value_loss           | 2.32          |
-------------------------------------------
Progress update from timestep 409900: Updated shield, loss is 0.03672323748469353
Progress update from timestep 410000: Updated shield, loss is 0.042421139776706696
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 410000: Saved shield_loss.csv. 
Eval num_timesteps=410000, episode_reward=16.86 +/- 15.83
Episode length: 24.00 +/- 21.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 410000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.35     |
|    ep_rew_mean     | 2.76     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1025     |
|    time_elapsed    | 93554    |
|    total_timesteps | 410000   |
---------------------------------
Progress update from timestep 410100: Updated shield, loss is 0.0421772338449955
Progress update from timestep 410200: Updated shield, loss is 0.0503828339278698
Eval num_timesteps=410200, episode_reward=9.65 +/- 11.97
Episode length: 15.00 +/- 17.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15            |
|    mean_reward          | 9.65          |
| time/                   |               |
|    total_timesteps      | 410200        |
| train/                  |               |
|    approx_kl            | 0.00018254844 |
|    clip_fraction        | 0.00357       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0111       |
|    explained_variance   | 0.129         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.35          |
|    n_updates            | 10250         |
|    policy_gradient_loss | -0.000491     |
|    value_loss           | 2.97          |
-------------------------------------------
Progress update from timestep 410300: Updated shield, loss is 0.0357808992266655
Progress update from timestep 410400: Updated shield, loss is 0.04455569013953209
Eval num_timesteps=410400, episode_reward=22.32 +/- 16.30
Episode length: 31.80 +/- 22.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.8     |
|    mean_reward     | 22.3     |
| time/              |          |
|    total_timesteps | 410400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.7      |
|    ep_rew_mean     | 2.24     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1026     |
|    time_elapsed    | 93657    |
|    total_timesteps | 410400   |
---------------------------------
Progress update from timestep 410500: Updated shield, loss is 0.03563014045357704
Progress update from timestep 410600: Updated shield, loss is 0.04520883783698082
Eval num_timesteps=410600, episode_reward=3.03 +/- 1.64
Episode length: 5.20 +/- 2.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 5.2          |
|    mean_reward          | 3.03         |
| time/                   |              |
|    total_timesteps      | 410600       |
| train/                  |              |
|    approx_kl            | 0.0001469497 |
|    clip_fraction        | 0.00804      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0198      |
|    explained_variance   | 0.133        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.46         |
|    n_updates            | 10260        |
|    policy_gradient_loss | -0.000834    |
|    value_loss           | 1.48         |
------------------------------------------
Progress update from timestep 410700: Updated shield, loss is 0.03854164108633995
Progress update from timestep 410800: Updated shield, loss is 0.035117026418447495
Eval num_timesteps=410800, episode_reward=15.36 +/- 15.23
Episode length: 23.20 +/- 21.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 15.4     |
| time/              |          |
|    total_timesteps | 410800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.68     |
|    ep_rew_mean     | 2.27     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1027     |
|    time_elapsed    | 93725    |
|    total_timesteps | 410800   |
---------------------------------
Progress update from timestep 410900: Updated shield, loss is 0.027262382209300995
Progress update from timestep 411000: Updated shield, loss is 0.03982514515519142
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 411000: Saved shield_loss.csv. 
Eval num_timesteps=411000, episode_reward=28.45 +/- 12.07
Episode length: 41.40 +/- 17.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.4         |
|    mean_reward          | 28.4         |
| time/                   |              |
|    total_timesteps      | 411000       |
| train/                  |              |
|    approx_kl            | 0.0010708428 |
|    clip_fraction        | 0.00446      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00747     |
|    explained_variance   | 0.101        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.762        |
|    n_updates            | 10270        |
|    policy_gradient_loss | -0.000397    |
|    value_loss           | 1.69         |
------------------------------------------
Progress update from timestep 411100: Updated shield, loss is 0.03897731378674507
Progress update from timestep 411200: Updated shield, loss is 0.038096725940704346
Eval num_timesteps=411200, episode_reward=30.61 +/- 8.30
Episode length: 44.40 +/- 11.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.4     |
|    mean_reward     | 30.6     |
| time/              |          |
|    total_timesteps | 411200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.52     |
|    ep_rew_mean     | 2.1      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1028     |
|    time_elapsed    | 93832    |
|    total_timesteps | 411200   |
---------------------------------
Progress update from timestep 411300: Updated shield, loss is 0.028115542605519295
Progress update from timestep 411400: Updated shield, loss is 0.04065385460853577
Eval num_timesteps=411400, episode_reward=21.81 +/- 15.56
Episode length: 32.00 +/- 22.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32            |
|    mean_reward          | 21.8          |
| time/                   |               |
|    total_timesteps      | 411400        |
| train/                  |               |
|    approx_kl            | 0.00079913053 |
|    clip_fraction        | 0.017         |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0352       |
|    explained_variance   | 0.17          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.82          |
|    n_updates            | 10280         |
|    policy_gradient_loss | -0.00182      |
|    value_loss           | 1.26          |
-------------------------------------------
Progress update from timestep 411500: Updated shield, loss is 0.03939908742904663
Progress update from timestep 411600: Updated shield, loss is 0.04576820507645607
Eval num_timesteps=411600, episode_reward=12.35 +/- 11.97
Episode length: 18.40 +/- 16.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.4     |
|    mean_reward     | 12.4     |
| time/              |          |
|    total_timesteps | 411600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.8      |
|    ep_rew_mean     | 2.31     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1029     |
|    time_elapsed    | 93913    |
|    total_timesteps | 411600   |
---------------------------------
Progress update from timestep 411700: Updated shield, loss is 0.04445957392454147
Progress update from timestep 411800: Updated shield, loss is 0.027513867244124413
Eval num_timesteps=411800, episode_reward=22.52 +/- 13.74
Episode length: 33.80 +/- 20.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.8          |
|    mean_reward          | 22.5          |
| time/                   |               |
|    total_timesteps      | 411800        |
| train/                  |               |
|    approx_kl            | 5.4435623e-05 |
|    clip_fraction        | 0.00246       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00536      |
|    explained_variance   | 0.135         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.497         |
|    n_updates            | 10290         |
|    policy_gradient_loss | -0.000264     |
|    value_loss           | 1.41          |
-------------------------------------------
Progress update from timestep 411900: Updated shield, loss is 0.03645430877804756
Progress update from timestep 412000: Updated shield, loss is 0.03777460753917694
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 412000: Saved shield_loss.csv. 
Eval num_timesteps=412000, episode_reward=12.69 +/- 12.14
Episode length: 18.40 +/- 16.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.4     |
|    mean_reward     | 12.7     |
| time/              |          |
|    total_timesteps | 412000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.02     |
|    ep_rew_mean     | 2.47     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1030     |
|    time_elapsed    | 93995    |
|    total_timesteps | 412000   |
---------------------------------
Progress update from timestep 412100: Updated shield, loss is 0.03306691721081734
Progress update from timestep 412200: Updated shield, loss is 0.04862375929951668
Eval num_timesteps=412200, episode_reward=9.65 +/- 12.52
Episode length: 14.80 +/- 17.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.8         |
|    mean_reward          | 9.65         |
| time/                   |              |
|    total_timesteps      | 412200       |
| train/                  |              |
|    approx_kl            | 6.671793e-11 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.000396    |
|    explained_variance   | 0.289        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.385        |
|    n_updates            | 10300        |
|    policy_gradient_loss | -5.89e-08    |
|    value_loss           | 1.23         |
------------------------------------------
Progress update from timestep 412300: Updated shield, loss is 0.03323744237422943
Progress update from timestep 412400: Updated shield, loss is 0.042805254459381104
Eval num_timesteps=412400, episode_reward=17.55 +/- 15.71
Episode length: 24.80 +/- 21.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 412400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.04     |
|    ep_rew_mean     | 2.49     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1031     |
|    time_elapsed    | 94078    |
|    total_timesteps | 412400   |
---------------------------------
Progress update from timestep 412500: Updated shield, loss is 0.034396443516016006
Progress update from timestep 412600: Updated shield, loss is 0.03978649899363518
Eval num_timesteps=412600, episode_reward=28.34 +/- 12.83
Episode length: 41.00 +/- 18.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 41            |
|    mean_reward          | 28.3          |
| time/                   |               |
|    total_timesteps      | 412600        |
| train/                  |               |
|    approx_kl            | 0.00011561863 |
|    clip_fraction        | 0.00246       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00796      |
|    explained_variance   | 0.226         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.222         |
|    n_updates            | 10310         |
|    policy_gradient_loss | -0.000572     |
|    value_loss           | 1.16          |
-------------------------------------------
Progress update from timestep 412700: Updated shield, loss is 0.040288638323545456
Progress update from timestep 412800: Updated shield, loss is 0.035154905170202255
Eval num_timesteps=412800, episode_reward=30.03 +/- 10.06
Episode length: 43.00 +/- 14.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43       |
|    mean_reward     | 30       |
| time/              |          |
|    total_timesteps | 412800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.97     |
|    ep_rew_mean     | 2.45     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1032     |
|    time_elapsed    | 94198    |
|    total_timesteps | 412800   |
---------------------------------
Progress update from timestep 412900: Updated shield, loss is 0.04656660556793213
Progress update from timestep 413000: Updated shield, loss is 0.04263601452112198
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 413000: Saved shield_loss.csv. 
Eval num_timesteps=413000, episode_reward=16.28 +/- 15.93
Episode length: 23.60 +/- 21.81
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.6          |
|    mean_reward          | 16.3          |
| time/                   |               |
|    total_timesteps      | 413000        |
| train/                  |               |
|    approx_kl            | 8.4907824e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.000395     |
|    explained_variance   | 0.115         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.712         |
|    n_updates            | 10320         |
|    policy_gradient_loss | -4.74e-08     |
|    value_loss           | 1.58          |
-------------------------------------------
Progress update from timestep 413100: Updated shield, loss is 0.028262609615921974
Progress update from timestep 413200: Updated shield, loss is 0.04164101928472519
Eval num_timesteps=413200, episode_reward=4.50 +/- 2.91
Episode length: 7.20 +/- 4.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7.2      |
|    mean_reward     | 4.5      |
| time/              |          |
|    total_timesteps | 413200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.38     |
|    ep_rew_mean     | 2.81     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1033     |
|    time_elapsed    | 94270    |
|    total_timesteps | 413200   |
---------------------------------
Progress update from timestep 413300: Updated shield, loss is 0.04229125380516052
Progress update from timestep 413400: Updated shield, loss is 0.040432076901197433
Eval num_timesteps=413400, episode_reward=24.25 +/- 12.82
Episode length: 35.60 +/- 18.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.6          |
|    mean_reward          | 24.2          |
| time/                   |               |
|    total_timesteps      | 413400        |
| train/                  |               |
|    approx_kl            | 4.2468502e-05 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00628      |
|    explained_variance   | 0.29          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.652         |
|    n_updates            | 10330         |
|    policy_gradient_loss | -0.000302     |
|    value_loss           | 1.66          |
-------------------------------------------
Progress update from timestep 413500: Updated shield, loss is 0.029725247994065285
Progress update from timestep 413600: Updated shield, loss is 0.033780694007873535
Eval num_timesteps=413600, episode_reward=19.31 +/- 11.98
Episode length: 29.00 +/- 17.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29       |
|    mean_reward     | 19.3     |
| time/              |          |
|    total_timesteps | 413600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.39     |
|    ep_rew_mean     | 2.82     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1034     |
|    time_elapsed    | 94376    |
|    total_timesteps | 413600   |
---------------------------------
Progress update from timestep 413700: Updated shield, loss is 0.0407991036772728
Progress update from timestep 413800: Updated shield, loss is 0.03619351238012314
Eval num_timesteps=413800, episode_reward=23.80 +/- 14.45
Episode length: 33.80 +/- 19.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.8          |
|    mean_reward          | 23.8          |
| time/                   |               |
|    total_timesteps      | 413800        |
| train/                  |               |
|    approx_kl            | 4.5707424e-05 |
|    clip_fraction        | 0.00067       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0167       |
|    explained_variance   | 0.0828        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.997         |
|    n_updates            | 10340         |
|    policy_gradient_loss | -0.000461     |
|    value_loss           | 2.11          |
-------------------------------------------
Progress update from timestep 413900: Updated shield, loss is 0.037103600800037384
Progress update from timestep 414000: Updated shield, loss is 0.03305331617593765
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 414000: Saved shield_loss.csv. 
Eval num_timesteps=414000, episode_reward=15.90 +/- 15.81
Episode length: 23.20 +/- 22.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 15.9     |
| time/              |          |
|    total_timesteps | 414000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.78     |
|    ep_rew_mean     | 3.14     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1035     |
|    time_elapsed    | 94462    |
|    total_timesteps | 414000   |
---------------------------------
Progress update from timestep 414100: Updated shield, loss is 0.03371512517333031
Progress update from timestep 414200: Updated shield, loss is 0.0365774966776371
Eval num_timesteps=414200, episode_reward=10.22 +/- 12.76
Episode length: 15.20 +/- 17.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.2         |
|    mean_reward          | 10.2         |
| time/                   |              |
|    total_timesteps      | 414200       |
| train/                  |              |
|    approx_kl            | 5.073523e-05 |
|    clip_fraction        | 0.00223      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0134      |
|    explained_variance   | 0.192        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.76         |
|    n_updates            | 10350        |
|    policy_gradient_loss | -0.000623    |
|    value_loss           | 3.43         |
------------------------------------------
Progress update from timestep 414300: Updated shield, loss is 0.04167047142982483
Progress update from timestep 414400: Updated shield, loss is 0.03342956304550171
Eval num_timesteps=414400, episode_reward=28.56 +/- 12.94
Episode length: 41.00 +/- 18.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41       |
|    mean_reward     | 28.6     |
| time/              |          |
|    total_timesteps | 414400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.77     |
|    ep_rew_mean     | 3.11     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1036     |
|    time_elapsed    | 94547    |
|    total_timesteps | 414400   |
---------------------------------
Progress update from timestep 414500: Updated shield, loss is 0.04277157783508301
Progress update from timestep 414600: Updated shield, loss is 0.04459275305271149
Eval num_timesteps=414600, episode_reward=11.09 +/- 12.93
Episode length: 16.00 +/- 17.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16            |
|    mean_reward          | 11.1          |
| time/                   |               |
|    total_timesteps      | 414600        |
| train/                  |               |
|    approx_kl            | 0.00069862604 |
|    clip_fraction        | 0.0123        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0163       |
|    explained_variance   | 0.411         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.96          |
|    n_updates            | 10360         |
|    policy_gradient_loss | -0.00126      |
|    value_loss           | 2.12          |
-------------------------------------------
Progress update from timestep 414700: Updated shield, loss is 0.042804084718227386
Progress update from timestep 414800: Updated shield, loss is 0.03733864054083824
Eval num_timesteps=414800, episode_reward=10.48 +/- 13.34
Episode length: 15.20 +/- 17.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | 10.5     |
| time/              |          |
|    total_timesteps | 414800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.79     |
|    ep_rew_mean     | 3.1      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1037     |
|    time_elapsed    | 94631    |
|    total_timesteps | 414800   |
---------------------------------
Progress update from timestep 414900: Updated shield, loss is 0.03218671679496765
Progress update from timestep 415000: Updated shield, loss is 0.03977326676249504
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 415000: Saved shield_loss.csv. 
Eval num_timesteps=415000, episode_reward=3.97 +/- 2.78
Episode length: 6.40 +/- 3.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 6.4          |
|    mean_reward          | 3.97         |
| time/                   |              |
|    total_timesteps      | 415000       |
| train/                  |              |
|    approx_kl            | 5.832653e-05 |
|    clip_fraction        | 0.00357      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0114      |
|    explained_variance   | 0.322        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.565        |
|    n_updates            | 10370        |
|    policy_gradient_loss | -0.000251    |
|    value_loss           | 2.16         |
------------------------------------------
Progress update from timestep 415100: Updated shield, loss is 0.02881571464240551
Progress update from timestep 415200: Updated shield, loss is 0.0518929697573185
Eval num_timesteps=415200, episode_reward=21.75 +/- 16.06
Episode length: 31.60 +/- 22.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.6     |
|    mean_reward     | 21.8     |
| time/              |          |
|    total_timesteps | 415200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.07     |
|    ep_rew_mean     | 2.5      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1038     |
|    time_elapsed    | 94717    |
|    total_timesteps | 415200   |
---------------------------------
Progress update from timestep 415300: Updated shield, loss is 0.042595624923706055
Progress update from timestep 415400: Updated shield, loss is 0.03573087602853775
Eval num_timesteps=415400, episode_reward=10.47 +/- 11.82
Episode length: 16.20 +/- 17.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.2          |
|    mean_reward          | 10.5          |
| time/                   |               |
|    total_timesteps      | 415400        |
| train/                  |               |
|    approx_kl            | 0.00011241216 |
|    clip_fraction        | 0.00402       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00517      |
|    explained_variance   | 0.146         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.966         |
|    n_updates            | 10380         |
|    policy_gradient_loss | -0.00054      |
|    value_loss           | 2             |
-------------------------------------------
Progress update from timestep 415500: Updated shield, loss is 0.03815273940563202
Progress update from timestep 415600: Updated shield, loss is 0.03577268496155739
Eval num_timesteps=415600, episode_reward=24.41 +/- 14.28
Episode length: 34.60 +/- 19.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 24.4     |
| time/              |          |
|    total_timesteps | 415600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.39     |
|    ep_rew_mean     | 2.81     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1039     |
|    time_elapsed    | 94805    |
|    total_timesteps | 415600   |
---------------------------------
Progress update from timestep 415700: Updated shield, loss is 0.03773321583867073
Progress update from timestep 415800: Updated shield, loss is 0.03996552526950836
Eval num_timesteps=415800, episode_reward=17.29 +/- 14.57
Episode length: 25.20 +/- 20.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.2          |
|    mean_reward          | 17.3          |
| time/                   |               |
|    total_timesteps      | 415800        |
| train/                  |               |
|    approx_kl            | 0.00011649136 |
|    clip_fraction        | 0.00379       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0129       |
|    explained_variance   | 0.193         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.738         |
|    n_updates            | 10390         |
|    policy_gradient_loss | -0.000454     |
|    value_loss           | 1.56          |
-------------------------------------------
Progress update from timestep 415900: Updated shield, loss is 0.03266137093305588
Progress update from timestep 416000: Updated shield, loss is 0.03563649207353592
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 416000: Saved shield_loss.csv. 
Eval num_timesteps=416000, episode_reward=12.64 +/- 11.42
Episode length: 18.80 +/- 16.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.8     |
|    mean_reward     | 12.6     |
| time/              |          |
|    total_timesteps | 416000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.62     |
|    ep_rew_mean     | 2.19     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1040     |
|    time_elapsed    | 94890    |
|    total_timesteps | 416000   |
---------------------------------
Progress update from timestep 416100: Updated shield, loss is 0.03541368246078491
Progress update from timestep 416200: Updated shield, loss is 0.040895119309425354
Eval num_timesteps=416200, episode_reward=3.57 +/- 1.22
Episode length: 6.00 +/- 1.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 6            |
|    mean_reward          | 3.57         |
| time/                   |              |
|    total_timesteps      | 416200       |
| train/                  |              |
|    approx_kl            | 7.630487e-05 |
|    clip_fraction        | 0.00536      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0137      |
|    explained_variance   | 0.359        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.461        |
|    n_updates            | 10400        |
|    policy_gradient_loss | -0.000725    |
|    value_loss           | 1.8          |
------------------------------------------
Progress update from timestep 416300: Updated shield, loss is 0.04022986814379692
Progress update from timestep 416400: Updated shield, loss is 0.034476958215236664
Eval num_timesteps=416400, episode_reward=3.38 +/- 1.96
Episode length: 5.80 +/- 2.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5.8      |
|    mean_reward     | 3.38     |
| time/              |          |
|    total_timesteps | 416400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.84     |
|    ep_rew_mean     | 3.1      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1041     |
|    time_elapsed    | 94940    |
|    total_timesteps | 416400   |
---------------------------------
Progress update from timestep 416500: Updated shield, loss is 0.035905010998249054
Progress update from timestep 416600: Updated shield, loss is 0.03694871813058853
Eval num_timesteps=416600, episode_reward=16.37 +/- 14.92
Episode length: 24.20 +/- 21.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.2          |
|    mean_reward          | 16.4          |
| time/                   |               |
|    total_timesteps      | 416600        |
| train/                  |               |
|    approx_kl            | 0.00038496446 |
|    clip_fraction        | 0.00469       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.027        |
|    explained_variance   | 0.103         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.2           |
|    n_updates            | 10410         |
|    policy_gradient_loss | -0.000674     |
|    value_loss           | 1.91          |
-------------------------------------------
Progress update from timestep 416700: Updated shield, loss is 0.035266682505607605
Progress update from timestep 416800: Updated shield, loss is 0.0336591862142086
Eval num_timesteps=416800, episode_reward=22.55 +/- 15.59
Episode length: 32.40 +/- 21.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.4     |
|    mean_reward     | 22.6     |
| time/              |          |
|    total_timesteps | 416800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.18     |
|    ep_rew_mean     | 2.55     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1042     |
|    time_elapsed    | 95041    |
|    total_timesteps | 416800   |
---------------------------------
Progress update from timestep 416900: Updated shield, loss is 0.0389140248298645
Progress update from timestep 417000: Updated shield, loss is 0.032058171927928925
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 417000: Saved shield_loss.csv. 
Eval num_timesteps=417000, episode_reward=23.55 +/- 14.13
Episode length: 34.00 +/- 19.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34           |
|    mean_reward          | 23.5         |
| time/                   |              |
|    total_timesteps      | 417000       |
| train/                  |              |
|    approx_kl            | 0.0006370838 |
|    clip_fraction        | 0.01         |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0211      |
|    explained_variance   | 0.139        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.486        |
|    n_updates            | 10420        |
|    policy_gradient_loss | -0.000573    |
|    value_loss           | 1.65         |
------------------------------------------
Progress update from timestep 417100: Updated shield, loss is 0.0380922332406044
Progress update from timestep 417200: Updated shield, loss is 0.046475328505039215
Eval num_timesteps=417200, episode_reward=18.50 +/- 12.65
Episode length: 27.60 +/- 18.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.6     |
|    mean_reward     | 18.5     |
| time/              |          |
|    total_timesteps | 417200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.88     |
|    ep_rew_mean     | 2.39     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1043     |
|    time_elapsed    | 95123    |
|    total_timesteps | 417200   |
---------------------------------
Progress update from timestep 417300: Updated shield, loss is 0.04751398414373398
Progress update from timestep 417400: Updated shield, loss is 0.0406746082007885
Eval num_timesteps=417400, episode_reward=16.72 +/- 14.70
Episode length: 24.60 +/- 21.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 24.6        |
|    mean_reward          | 16.7        |
| time/                   |             |
|    total_timesteps      | 417400      |
| train/                  |             |
|    approx_kl            | 0.000730468 |
|    clip_fraction        | 0.00603     |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0128     |
|    explained_variance   | 0.361       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.899       |
|    n_updates            | 10430       |
|    policy_gradient_loss | -0.00104    |
|    value_loss           | 1.68        |
-----------------------------------------
Progress update from timestep 417500: Updated shield, loss is 0.038480352610349655
Progress update from timestep 417600: Updated shield, loss is 0.03057527355849743
Eval num_timesteps=417600, episode_reward=17.83 +/- 14.97
Episode length: 25.80 +/- 20.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | 17.8     |
| time/              |          |
|    total_timesteps | 417600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.13     |
|    ep_rew_mean     | 3.3      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1044     |
|    time_elapsed    | 95226    |
|    total_timesteps | 417600   |
---------------------------------
Progress update from timestep 417700: Updated shield, loss is 0.04065447673201561
Progress update from timestep 417800: Updated shield, loss is 0.036958158016204834
Eval num_timesteps=417800, episode_reward=12.27 +/- 11.32
Episode length: 18.60 +/- 16.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.6         |
|    mean_reward          | 12.3         |
| time/                   |              |
|    total_timesteps      | 417800       |
| train/                  |              |
|    approx_kl            | 0.0040274276 |
|    clip_fraction        | 0.00781      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0225      |
|    explained_variance   | 0.269        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.78         |
|    n_updates            | 10440        |
|    policy_gradient_loss | -0.00102     |
|    value_loss           | 2.82         |
------------------------------------------
Progress update from timestep 417900: Updated shield, loss is 0.04202663153409958
Progress update from timestep 418000: Updated shield, loss is 0.028803588822484016
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 418000: Saved shield_loss.csv. 
Eval num_timesteps=418000, episode_reward=34.25 +/- 0.83
Episode length: 50.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 34.2     |
| time/              |          |
|    total_timesteps | 418000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.87     |
|    ep_rew_mean     | 2.39     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1045     |
|    time_elapsed    | 95327    |
|    total_timesteps | 418000   |
---------------------------------
Progress update from timestep 418100: Updated shield, loss is 0.0413544587790966
Progress update from timestep 418200: Updated shield, loss is 0.042300574481487274
Eval num_timesteps=418200, episode_reward=23.38 +/- 15.03
Episode length: 33.40 +/- 20.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.4          |
|    mean_reward          | 23.4          |
| time/                   |               |
|    total_timesteps      | 418200        |
| train/                  |               |
|    approx_kl            | 0.00013452556 |
|    clip_fraction        | 0.00424       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.011        |
|    explained_variance   | 0.142         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.374         |
|    n_updates            | 10450         |
|    policy_gradient_loss | -0.000239     |
|    value_loss           | 1.44          |
-------------------------------------------
Progress update from timestep 418300: Updated shield, loss is 0.042245060205459595
Progress update from timestep 418400: Updated shield, loss is 0.03529926761984825
Eval num_timesteps=418400, episode_reward=28.41 +/- 13.85
Episode length: 40.40 +/- 19.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.4     |
|    mean_reward     | 28.4     |
| time/              |          |
|    total_timesteps | 418400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.35     |
|    ep_rew_mean     | 2.74     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1046     |
|    time_elapsed    | 95413    |
|    total_timesteps | 418400   |
---------------------------------
Progress update from timestep 418500: Updated shield, loss is 0.040655747056007385
Progress update from timestep 418600: Updated shield, loss is 0.03092554584145546
Eval num_timesteps=418600, episode_reward=18.04 +/- 15.58
Episode length: 25.60 +/- 20.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.6          |
|    mean_reward          | 18            |
| time/                   |               |
|    total_timesteps      | 418600        |
| train/                  |               |
|    approx_kl            | 1.1813686e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.000506     |
|    explained_variance   | 0.261         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.488         |
|    n_updates            | 10460         |
|    policy_gradient_loss | -4.82e-08     |
|    value_loss           | 1.63          |
-------------------------------------------
Progress update from timestep 418700: Updated shield, loss is 0.027042880654335022
Progress update from timestep 418800: Updated shield, loss is 0.038958679884672165
Eval num_timesteps=418800, episode_reward=29.93 +/- 9.17
Episode length: 43.60 +/- 12.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.6     |
|    mean_reward     | 29.9     |
| time/              |          |
|    total_timesteps | 418800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.01     |
|    ep_rew_mean     | 2.5      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1047     |
|    time_elapsed    | 95503    |
|    total_timesteps | 418800   |
---------------------------------
Progress update from timestep 418900: Updated shield, loss is 0.051969073712825775
Progress update from timestep 419000: Updated shield, loss is 0.032289374619722366
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 419000: Saved shield_loss.csv. 
Eval num_timesteps=419000, episode_reward=17.90 +/- 14.79
Episode length: 26.00 +/- 20.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26           |
|    mean_reward          | 17.9         |
| time/                   |              |
|    total_timesteps      | 419000       |
| train/                  |              |
|    approx_kl            | 0.0005189954 |
|    clip_fraction        | 0.0112       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.015       |
|    explained_variance   | 0.248        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.199        |
|    n_updates            | 10470        |
|    policy_gradient_loss | -0.000779    |
|    value_loss           | 1.43         |
------------------------------------------
Progress update from timestep 419100: Updated shield, loss is 0.04339079558849335
Progress update from timestep 419200: Updated shield, loss is 0.041647423058748245
Eval num_timesteps=419200, episode_reward=18.50 +/- 14.79
Episode length: 26.80 +/- 20.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.8     |
|    mean_reward     | 18.5     |
| time/              |          |
|    total_timesteps | 419200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.16     |
|    ep_rew_mean     | 2.6      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1048     |
|    time_elapsed    | 95593    |
|    total_timesteps | 419200   |
---------------------------------
Progress update from timestep 419300: Updated shield, loss is 0.04476071521639824
Progress update from timestep 419400: Updated shield, loss is 0.03080708347260952
Eval num_timesteps=419400, episode_reward=10.57 +/- 11.86
Episode length: 16.20 +/- 17.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.2        |
|    mean_reward          | 10.6        |
| time/                   |             |
|    total_timesteps      | 419400      |
| train/                  |             |
|    approx_kl            | 0.000142067 |
|    clip_fraction        | 0.00446     |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.00924    |
|    explained_variance   | 0.197       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.668       |
|    n_updates            | 10480       |
|    policy_gradient_loss | -0.000947   |
|    value_loss           | 1.75        |
-----------------------------------------
Progress update from timestep 419500: Updated shield, loss is 0.0364069789648056
Progress update from timestep 419600: Updated shield, loss is 0.0439329519867897
Eval num_timesteps=419600, episode_reward=14.23 +/- 11.97
Episode length: 20.60 +/- 15.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 14.2     |
| time/              |          |
|    total_timesteps | 419600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.51     |
|    ep_rew_mean     | 2.89     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1049     |
|    time_elapsed    | 95679    |
|    total_timesteps | 419600   |
---------------------------------
Progress update from timestep 419700: Updated shield, loss is 0.03867041692137718
Progress update from timestep 419800: Updated shield, loss is 0.038373734802007675
Eval num_timesteps=419800, episode_reward=12.51 +/- 13.14
Episode length: 18.00 +/- 17.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18           |
|    mean_reward          | 12.5         |
| time/                   |              |
|    total_timesteps      | 419800       |
| train/                  |              |
|    approx_kl            | 0.0005927815 |
|    clip_fraction        | 0.00558      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0137      |
|    explained_variance   | 0.0983       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.75         |
|    n_updates            | 10490        |
|    policy_gradient_loss | -0.000909    |
|    value_loss           | 1.55         |
------------------------------------------
Progress update from timestep 419900: Updated shield, loss is 0.04336019605398178
Progress update from timestep 420000: Updated shield, loss is 0.031179822981357574
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 420000: Saved shield_loss.csv. 
Eval num_timesteps=420000, episode_reward=24.01 +/- 13.79
Episode length: 34.60 +/- 18.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 24       |
| time/              |          |
|    total_timesteps | 420000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.33     |
|    ep_rew_mean     | 2.79     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1050     |
|    time_elapsed    | 95760    |
|    total_timesteps | 420000   |
---------------------------------
Progress update from timestep 420100: Updated shield, loss is 0.034385696053504944
Progress update from timestep 420200: Updated shield, loss is 0.03358585014939308
Eval num_timesteps=420200, episode_reward=9.42 +/- 13.19
Episode length: 14.20 +/- 18.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.2          |
|    mean_reward          | 9.42          |
| time/                   |               |
|    total_timesteps      | 420200        |
| train/                  |               |
|    approx_kl            | 0.00023420235 |
|    clip_fraction        | 0.00982       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0252       |
|    explained_variance   | 0.166         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.416         |
|    n_updates            | 10500         |
|    policy_gradient_loss | -0.000871     |
|    value_loss           | 1.47          |
-------------------------------------------
Progress update from timestep 420300: Updated shield, loss is 0.0408317893743515
Progress update from timestep 420400: Updated shield, loss is 0.03907300531864166
Eval num_timesteps=420400, episode_reward=18.52 +/- 14.40
Episode length: 26.80 +/- 19.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.8     |
|    mean_reward     | 18.5     |
| time/              |          |
|    total_timesteps | 420400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.26     |
|    ep_rew_mean     | 2.7      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1051     |
|    time_elapsed    | 95844    |
|    total_timesteps | 420400   |
---------------------------------
Progress update from timestep 420500: Updated shield, loss is 0.038909394294023514
Progress update from timestep 420600: Updated shield, loss is 0.036341432482004166
Eval num_timesteps=420600, episode_reward=13.02 +/- 13.06
Episode length: 19.00 +/- 18.07
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 19             |
|    mean_reward          | 13             |
| time/                   |                |
|    total_timesteps      | 420600         |
| train/                  |                |
|    approx_kl            | -2.4653803e-11 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -9.42e-05      |
|    explained_variance   | 0.175          |
|    learning_rate        | 0.0001         |
|    loss                 | 0.774          |
|    n_updates            | 10510          |
|    policy_gradient_loss | 8.12e-10       |
|    value_loss           | 2.03           |
--------------------------------------------
Progress update from timestep 420700: Updated shield, loss is 0.03754924237728119
Progress update from timestep 420800: Updated shield, loss is 0.03474388271570206
Eval num_timesteps=420800, episode_reward=16.57 +/- 11.04
Episode length: 24.20 +/- 15.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 420800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.19     |
|    ep_rew_mean     | 2.6      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1052     |
|    time_elapsed    | 95939    |
|    total_timesteps | 420800   |
---------------------------------
Progress update from timestep 420900: Updated shield, loss is 0.0336463525891304
Progress update from timestep 421000: Updated shield, loss is 0.040036968886852264
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 421000: Saved shield_loss.csv. 
Eval num_timesteps=421000, episode_reward=22.99 +/- 14.56
Episode length: 33.40 +/- 20.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.4          |
|    mean_reward          | 23            |
| time/                   |               |
|    total_timesteps      | 421000        |
| train/                  |               |
|    approx_kl            | 8.6856744e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.000422     |
|    explained_variance   | 0.203         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.51          |
|    n_updates            | 10520         |
|    policy_gradient_loss | -2.77e-08     |
|    value_loss           | 2.42          |
-------------------------------------------
Progress update from timestep 421100: Updated shield, loss is 0.032392289489507675
Progress update from timestep 421200: Updated shield, loss is 0.03833572193980217
Eval num_timesteps=421200, episode_reward=21.99 +/- 12.70
Episode length: 31.80 +/- 17.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.8     |
|    mean_reward     | 22       |
| time/              |          |
|    total_timesteps | 421200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.83     |
|    ep_rew_mean     | 2.35     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1053     |
|    time_elapsed    | 96053    |
|    total_timesteps | 421200   |
---------------------------------
Progress update from timestep 421300: Updated shield, loss is 0.03613008186221123
Progress update from timestep 421400: Updated shield, loss is 0.03566917032003403
Eval num_timesteps=421400, episode_reward=11.59 +/- 12.37
Episode length: 17.40 +/- 17.68
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 17.4           |
|    mean_reward          | 11.6           |
| time/                   |                |
|    total_timesteps      | 421400         |
| train/                  |                |
|    approx_kl            | 0.000111857196 |
|    clip_fraction        | 0.00201        |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.0263        |
|    explained_variance   | 0.177          |
|    learning_rate        | 0.0001         |
|    loss                 | 0.57           |
|    n_updates            | 10530          |
|    policy_gradient_loss | -0.000699      |
|    value_loss           | 1.73           |
--------------------------------------------
Progress update from timestep 421500: Updated shield, loss is 0.03120485506951809
Progress update from timestep 421600: Updated shield, loss is 0.04345431551337242
Eval num_timesteps=421600, episode_reward=11.57 +/- 13.50
Episode length: 17.00 +/- 18.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | 11.6     |
| time/              |          |
|    total_timesteps | 421600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.22     |
|    ep_rew_mean     | 2.63     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1054     |
|    time_elapsed    | 96135    |
|    total_timesteps | 421600   |
---------------------------------
Progress update from timestep 421700: Updated shield, loss is 0.03201838955283165
Progress update from timestep 421800: Updated shield, loss is 0.038956739008426666
Eval num_timesteps=421800, episode_reward=17.73 +/- 13.46
Episode length: 26.40 +/- 19.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.4          |
|    mean_reward          | 17.7          |
| time/                   |               |
|    total_timesteps      | 421800        |
| train/                  |               |
|    approx_kl            | -1.189814e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.000345     |
|    explained_variance   | 0.0756        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.925         |
|    n_updates            | 10540         |
|    policy_gradient_loss | -4.46e-08     |
|    value_loss           | 1.69          |
-------------------------------------------
Progress update from timestep 421900: Updated shield, loss is 0.036309752613306046
Progress update from timestep 422000: Updated shield, loss is 0.04622158780694008
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 422000: Saved shield_loss.csv. 
Eval num_timesteps=422000, episode_reward=9.50 +/- 12.04
Episode length: 14.80 +/- 17.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 9.5      |
| time/              |          |
|    total_timesteps | 422000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.1      |
|    ep_rew_mean     | 2.61     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1055     |
|    time_elapsed    | 96220    |
|    total_timesteps | 422000   |
---------------------------------
Progress update from timestep 422100: Updated shield, loss is 0.04593650996685028
Progress update from timestep 422200: Updated shield, loss is 0.03747173398733139
Eval num_timesteps=422200, episode_reward=17.17 +/- 14.66
Episode length: 25.60 +/- 20.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.6         |
|    mean_reward          | 17.2         |
| time/                   |              |
|    total_timesteps      | 422200       |
| train/                  |              |
|    approx_kl            | 0.0027919617 |
|    clip_fraction        | 0.0163       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0348      |
|    explained_variance   | 0.122        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.744        |
|    n_updates            | 10550        |
|    policy_gradient_loss | -0.00175     |
|    value_loss           | 1.82         |
------------------------------------------
Progress update from timestep 422300: Updated shield, loss is 0.0358722023665905
Progress update from timestep 422400: Updated shield, loss is 0.04773038625717163
Eval num_timesteps=422400, episode_reward=29.14 +/- 12.37
Episode length: 41.40 +/- 17.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | 29.1     |
| time/              |          |
|    total_timesteps | 422400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.24     |
|    ep_rew_mean     | 2.63     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1056     |
|    time_elapsed    | 96311    |
|    total_timesteps | 422400   |
---------------------------------
Progress update from timestep 422500: Updated shield, loss is 0.03902922943234444
Progress update from timestep 422600: Updated shield, loss is 0.0431390181183815
Eval num_timesteps=422600, episode_reward=12.18 +/- 11.65
Episode length: 18.40 +/- 16.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 18.4          |
|    mean_reward          | 12.2          |
| time/                   |               |
|    total_timesteps      | 422600        |
| train/                  |               |
|    approx_kl            | 0.00021428669 |
|    clip_fraction        | 0.0107        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0344       |
|    explained_variance   | 0.227         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.02          |
|    n_updates            | 10560         |
|    policy_gradient_loss | -0.00158      |
|    value_loss           | 2.17          |
-------------------------------------------
Progress update from timestep 422700: Updated shield, loss is 0.03841555863618851
Progress update from timestep 422800: Updated shield, loss is 0.035997211933135986
Eval num_timesteps=422800, episode_reward=22.49 +/- 16.08
Episode length: 32.00 +/- 22.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32       |
|    mean_reward     | 22.5     |
| time/              |          |
|    total_timesteps | 422800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.2      |
|    ep_rew_mean     | 2.61     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1057     |
|    time_elapsed    | 96411    |
|    total_timesteps | 422800   |
---------------------------------
Progress update from timestep 422900: Updated shield, loss is 0.05859009921550751
Progress update from timestep 423000: Updated shield, loss is 0.036328453570604324
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 423000: Saved shield_loss.csv. 
Eval num_timesteps=423000, episode_reward=10.29 +/- 11.64
Episode length: 15.80 +/- 17.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.8          |
|    mean_reward          | 10.3          |
| time/                   |               |
|    total_timesteps      | 423000        |
| train/                  |               |
|    approx_kl            | 0.00045528024 |
|    clip_fraction        | 0.0127        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0183       |
|    explained_variance   | 0.304         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.509         |
|    n_updates            | 10570         |
|    policy_gradient_loss | -0.0014       |
|    value_loss           | 1.39          |
-------------------------------------------
Progress update from timestep 423100: Updated shield, loss is 0.08767884224653244
Progress update from timestep 423200: Updated shield, loss is 0.046972643584012985
Eval num_timesteps=423200, episode_reward=18.33 +/- 14.89
Episode length: 26.00 +/- 20.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | 18.3     |
| time/              |          |
|    total_timesteps | 423200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.17     |
|    ep_rew_mean     | 2.56     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1058     |
|    time_elapsed    | 96495    |
|    total_timesteps | 423200   |
---------------------------------
Progress update from timestep 423300: Updated shield, loss is 0.0751076266169548
Progress update from timestep 423400: Updated shield, loss is 0.037414103746414185
Eval num_timesteps=423400, episode_reward=17.47 +/- 13.86
Episode length: 26.20 +/- 20.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.2          |
|    mean_reward          | 17.5          |
| time/                   |               |
|    total_timesteps      | 423400        |
| train/                  |               |
|    approx_kl            | 2.4852607e-05 |
|    clip_fraction        | 0.00134       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00753      |
|    explained_variance   | 0.123         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.608         |
|    n_updates            | 10580         |
|    policy_gradient_loss | -1.29e-05     |
|    value_loss           | 1.5           |
-------------------------------------------
Progress update from timestep 423500: Updated shield, loss is 0.04547314718365669
Progress update from timestep 423600: Updated shield, loss is 0.08204322308301926
Eval num_timesteps=423600, episode_reward=3.76 +/- 1.99
Episode length: 6.20 +/- 2.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6.2      |
|    mean_reward     | 3.76     |
| time/              |          |
|    total_timesteps | 423600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.17     |
|    ep_rew_mean     | 2.56     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1059     |
|    time_elapsed    | 96561    |
|    total_timesteps | 423600   |
---------------------------------
Progress update from timestep 423700: Updated shield, loss is 0.03705916926264763
Progress update from timestep 423800: Updated shield, loss is 0.09387921541929245
Eval num_timesteps=423800, episode_reward=11.06 +/- 12.39
Episode length: 16.40 +/- 16.98
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.4          |
|    mean_reward          | 11.1          |
| time/                   |               |
|    total_timesteps      | 423800        |
| train/                  |               |
|    approx_kl            | 0.00034289501 |
|    clip_fraction        | 0.00692       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0117       |
|    explained_variance   | 0.126         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.489         |
|    n_updates            | 10590         |
|    policy_gradient_loss | -0.000532     |
|    value_loss           | 1.59          |
-------------------------------------------
Progress update from timestep 423900: Updated shield, loss is 0.059084322303533554
Progress update from timestep 424000: Updated shield, loss is 0.052964046597480774
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 424000: Saved shield_loss.csv. 
Eval num_timesteps=424000, episode_reward=24.05 +/- 13.68
Episode length: 34.40 +/- 19.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 24       |
| time/              |          |
|    total_timesteps | 424000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.93     |
|    ep_rew_mean     | 2.48     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1060     |
|    time_elapsed    | 96663    |
|    total_timesteps | 424000   |
---------------------------------
Progress update from timestep 424100: Updated shield, loss is 0.06377843767404556
Progress update from timestep 424200: Updated shield, loss is 0.062433380633592606
Eval num_timesteps=424200, episode_reward=23.05 +/- 15.37
Episode length: 32.80 +/- 21.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.8         |
|    mean_reward          | 23           |
| time/                   |              |
|    total_timesteps      | 424200       |
| train/                  |              |
|    approx_kl            | 0.0004470621 |
|    clip_fraction        | 0.0118       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0253      |
|    explained_variance   | 0.251        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.443        |
|    n_updates            | 10600        |
|    policy_gradient_loss | -0.00196     |
|    value_loss           | 1.36         |
------------------------------------------
Progress update from timestep 424300: Updated shield, loss is 0.04551594331860542
Progress update from timestep 424400: Updated shield, loss is 0.066823810338974
Eval num_timesteps=424400, episode_reward=19.28 +/- 13.12
Episode length: 27.80 +/- 18.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.8     |
|    mean_reward     | 19.3     |
| time/              |          |
|    total_timesteps | 424400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.01     |
|    ep_rew_mean     | 3.33     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1061     |
|    time_elapsed    | 96753    |
|    total_timesteps | 424400   |
---------------------------------
Progress update from timestep 424500: Updated shield, loss is 0.04525044932961464
Progress update from timestep 424600: Updated shield, loss is 0.03645820543169975
Eval num_timesteps=424600, episode_reward=16.12 +/- 16.01
Episode length: 23.20 +/- 22.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.2         |
|    mean_reward          | 16.1         |
| time/                   |              |
|    total_timesteps      | 424600       |
| train/                  |              |
|    approx_kl            | 0.0001093308 |
|    clip_fraction        | 0.00201      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0128      |
|    explained_variance   | 0.25         |
|    learning_rate        | 0.0001       |
|    loss                 | 1.38         |
|    n_updates            | 10610        |
|    policy_gradient_loss | -0.000118    |
|    value_loss           | 2.3          |
------------------------------------------
Progress update from timestep 424700: Updated shield, loss is 0.05395212769508362
Progress update from timestep 424800: Updated shield, loss is 0.04652583971619606
Eval num_timesteps=424800, episode_reward=21.58 +/- 16.26
Episode length: 31.40 +/- 22.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.4     |
|    mean_reward     | 21.6     |
| time/              |          |
|    total_timesteps | 424800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.75     |
|    ep_rew_mean     | 3.06     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1062     |
|    time_elapsed    | 96837    |
|    total_timesteps | 424800   |
---------------------------------
Progress update from timestep 424900: Updated shield, loss is 0.03619439899921417
Progress update from timestep 425000: Updated shield, loss is 0.05630677193403244
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 425000: Saved shield_loss.csv. 
Eval num_timesteps=425000, episode_reward=7.01 +/- 4.17
Episode length: 11.00 +/- 6.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 11           |
|    mean_reward          | 7.01         |
| time/                   |              |
|    total_timesteps      | 425000       |
| train/                  |              |
|    approx_kl            | 9.402876e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.000518    |
|    explained_variance   | 0.293        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.828        |
|    n_updates            | 10620        |
|    policy_gradient_loss | -3.33e-07    |
|    value_loss           | 1.8          |
------------------------------------------
Progress update from timestep 425100: Updated shield, loss is 0.03543790429830551
Progress update from timestep 425200: Updated shield, loss is 0.04501926526427269
Eval num_timesteps=425200, episode_reward=12.23 +/- 10.05
Episode length: 19.00 +/- 15.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 12.2     |
| time/              |          |
|    total_timesteps | 425200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.96     |
|    ep_rew_mean     | 3.21     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1063     |
|    time_elapsed    | 96905    |
|    total_timesteps | 425200   |
---------------------------------
Progress update from timestep 425300: Updated shield, loss is 0.04291277751326561
Progress update from timestep 425400: Updated shield, loss is 0.04136054962873459
Eval num_timesteps=425400, episode_reward=12.20 +/- 11.70
Episode length: 18.60 +/- 16.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 18.6          |
|    mean_reward          | 12.2          |
| time/                   |               |
|    total_timesteps      | 425400        |
| train/                  |               |
|    approx_kl            | 0.00028660876 |
|    clip_fraction        | 0.021         |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0389       |
|    explained_variance   | 0.203         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.26          |
|    n_updates            | 10630         |
|    policy_gradient_loss | -0.00234      |
|    value_loss           | 2.39          |
-------------------------------------------
Progress update from timestep 425500: Updated shield, loss is 0.04613012447953224
Progress update from timestep 425600: Updated shield, loss is 0.0439053438603878
Eval num_timesteps=425600, episode_reward=13.25 +/- 11.15
Episode length: 19.80 +/- 15.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.8     |
|    mean_reward     | 13.3     |
| time/              |          |
|    total_timesteps | 425600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.02     |
|    ep_rew_mean     | 3.25     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1064     |
|    time_elapsed    | 96985    |
|    total_timesteps | 425600   |
---------------------------------
Progress update from timestep 425700: Updated shield, loss is 0.04630446061491966
Progress update from timestep 425800: Updated shield, loss is 0.03862958401441574
Eval num_timesteps=425800, episode_reward=16.35 +/- 15.72
Episode length: 23.60 +/- 21.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.6         |
|    mean_reward          | 16.4         |
| time/                   |              |
|    total_timesteps      | 425800       |
| train/                  |              |
|    approx_kl            | 5.671547e-05 |
|    clip_fraction        | 0.00379      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00537     |
|    explained_variance   | 0.138        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.92         |
|    n_updates            | 10640        |
|    policy_gradient_loss | -0.000792    |
|    value_loss           | 2.74         |
------------------------------------------
Progress update from timestep 425900: Updated shield, loss is 0.04097582772374153
Progress update from timestep 426000: Updated shield, loss is 0.03728831559419632
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 426000: Saved shield_loss.csv. 
Eval num_timesteps=426000, episode_reward=16.04 +/- 15.55
Episode length: 23.40 +/- 21.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 426000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.23     |
|    ep_rew_mean     | 2.72     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1065     |
|    time_elapsed    | 97066    |
|    total_timesteps | 426000   |
---------------------------------
Progress update from timestep 426100: Updated shield, loss is 0.03668346628546715
Progress update from timestep 426200: Updated shield, loss is 0.041575025767087936
Eval num_timesteps=426200, episode_reward=16.47 +/- 14.79
Episode length: 24.40 +/- 20.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.4          |
|    mean_reward          | 16.5          |
| time/                   |               |
|    total_timesteps      | 426200        |
| train/                  |               |
|    approx_kl            | 0.00027765884 |
|    clip_fraction        | 0.0181        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0265       |
|    explained_variance   | 0.15          |
|    learning_rate        | 0.0001        |
|    loss                 | 1.55          |
|    n_updates            | 10650         |
|    policy_gradient_loss | -0.0014       |
|    value_loss           | 2.08          |
-------------------------------------------
Progress update from timestep 426300: Updated shield, loss is 0.040336571633815765
Progress update from timestep 426400: Updated shield, loss is 0.04112064465880394
Eval num_timesteps=426400, episode_reward=6.88 +/- 4.19
Episode length: 11.00 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 11       |
|    mean_reward     | 6.88     |
| time/              |          |
|    total_timesteps | 426400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.58     |
|    ep_rew_mean     | 2.19     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1066     |
|    time_elapsed    | 97143    |
|    total_timesteps | 426400   |
---------------------------------
Progress update from timestep 426500: Updated shield, loss is 0.04529288038611412
Progress update from timestep 426600: Updated shield, loss is 0.042019832879304886
Eval num_timesteps=426600, episode_reward=11.35 +/- 12.83
Episode length: 16.20 +/- 17.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.2          |
|    mean_reward          | 11.3          |
| time/                   |               |
|    total_timesteps      | 426600        |
| train/                  |               |
|    approx_kl            | 1.4113959e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00396      |
|    explained_variance   | 0.207         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.389         |
|    n_updates            | 10660         |
|    policy_gradient_loss | -0.000217     |
|    value_loss           | 1.24          |
-------------------------------------------
Progress update from timestep 426700: Updated shield, loss is 0.02866353467106819
Progress update from timestep 426800: Updated shield, loss is 0.0395747534930706
Eval num_timesteps=426800, episode_reward=11.43 +/- 11.84
Episode length: 17.40 +/- 16.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 11.4     |
| time/              |          |
|    total_timesteps | 426800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.39     |
|    ep_rew_mean     | 2.82     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1067     |
|    time_elapsed    | 97233    |
|    total_timesteps | 426800   |
---------------------------------
Progress update from timestep 426900: Updated shield, loss is 0.03649840131402016
Progress update from timestep 427000: Updated shield, loss is 0.04208330810070038
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 427000: Saved shield_loss.csv. 
Eval num_timesteps=427000, episode_reward=16.81 +/- 14.99
Episode length: 24.40 +/- 21.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.4         |
|    mean_reward          | 16.8         |
| time/                   |              |
|    total_timesteps      | 427000       |
| train/                  |              |
|    approx_kl            | 0.0010636378 |
|    clip_fraction        | 0.0132       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0296      |
|    explained_variance   | 0.266        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.516        |
|    n_updates            | 10670        |
|    policy_gradient_loss | -0.00159     |
|    value_loss           | 1.43         |
------------------------------------------
Progress update from timestep 427100: Updated shield, loss is 0.038871146738529205
Progress update from timestep 427200: Updated shield, loss is 0.043213602155447006
Eval num_timesteps=427200, episode_reward=16.52 +/- 14.72
Episode length: 24.40 +/- 20.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 427200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.25     |
|    ep_rew_mean     | 2.62     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1068     |
|    time_elapsed    | 97333    |
|    total_timesteps | 427200   |
---------------------------------
Progress update from timestep 427300: Updated shield, loss is 0.039443984627723694
Progress update from timestep 427400: Updated shield, loss is 0.03069356083869934
Eval num_timesteps=427400, episode_reward=28.11 +/- 12.24
Episode length: 41.20 +/- 17.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.2         |
|    mean_reward          | 28.1         |
| time/                   |              |
|    total_timesteps      | 427400       |
| train/                  |              |
|    approx_kl            | 0.0004007854 |
|    clip_fraction        | 0.0154       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0227      |
|    explained_variance   | 0.0934       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.699        |
|    n_updates            | 10680        |
|    policy_gradient_loss | -0.00123     |
|    value_loss           | 2.16         |
------------------------------------------
Progress update from timestep 427500: Updated shield, loss is 0.02840251848101616
Progress update from timestep 427600: Updated shield, loss is 0.03202257677912712
Eval num_timesteps=427600, episode_reward=30.33 +/- 10.53
Episode length: 42.80 +/- 14.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.8     |
|    mean_reward     | 30.3     |
| time/              |          |
|    total_timesteps | 427600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.84     |
|    ep_rew_mean     | 2.33     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1069     |
|    time_elapsed    | 97439    |
|    total_timesteps | 427600   |
---------------------------------
Progress update from timestep 427700: Updated shield, loss is 0.047669775784015656
Progress update from timestep 427800: Updated shield, loss is 0.03607633337378502
Eval num_timesteps=427800, episode_reward=23.29 +/- 14.22
Episode length: 33.60 +/- 20.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.6          |
|    mean_reward          | 23.3          |
| time/                   |               |
|    total_timesteps      | 427800        |
| train/                  |               |
|    approx_kl            | 0.00014707547 |
|    clip_fraction        | 0.00379       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0067       |
|    explained_variance   | 0.168         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.892         |
|    n_updates            | 10690         |
|    policy_gradient_loss | -0.000333     |
|    value_loss           | 1.33          |
-------------------------------------------
Progress update from timestep 427900: Updated shield, loss is 0.03244712948799133
Progress update from timestep 428000: Updated shield, loss is 0.047092683613300323
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 428000: Saved shield_loss.csv. 
Eval num_timesteps=428000, episode_reward=3.82 +/- 2.34
Episode length: 6.40 +/- 3.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6.4      |
|    mean_reward     | 3.82     |
| time/              |          |
|    total_timesteps | 428000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.49     |
|    ep_rew_mean     | 2.87     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1070     |
|    time_elapsed    | 97526    |
|    total_timesteps | 428000   |
---------------------------------
Progress update from timestep 428100: Updated shield, loss is 0.03528158739209175
Progress update from timestep 428200: Updated shield, loss is 0.03816307336091995
Eval num_timesteps=428200, episode_reward=16.44 +/- 15.63
Episode length: 23.60 +/- 21.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.6          |
|    mean_reward          | 16.4          |
| time/                   |               |
|    total_timesteps      | 428200        |
| train/                  |               |
|    approx_kl            | 0.00059813773 |
|    clip_fraction        | 0.0154        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0314       |
|    explained_variance   | 0.361         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.08          |
|    n_updates            | 10700         |
|    policy_gradient_loss | -0.00163      |
|    value_loss           | 2.27          |
-------------------------------------------
Progress update from timestep 428300: Updated shield, loss is 0.03920675069093704
Progress update from timestep 428400: Updated shield, loss is 0.03546313941478729
Eval num_timesteps=428400, episode_reward=23.86 +/- 15.78
Episode length: 33.00 +/- 20.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 23.9     |
| time/              |          |
|    total_timesteps | 428400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.75     |
|    ep_rew_mean     | 2.28     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1071     |
|    time_elapsed    | 97610    |
|    total_timesteps | 428400   |
---------------------------------
Progress update from timestep 428500: Updated shield, loss is 0.040095988661050797
Progress update from timestep 428600: Updated shield, loss is 0.04457207769155502
Eval num_timesteps=428600, episode_reward=11.85 +/- 12.04
Episode length: 17.60 +/- 16.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.6         |
|    mean_reward          | 11.8         |
| time/                   |              |
|    total_timesteps      | 428600       |
| train/                  |              |
|    approx_kl            | 0.0002674551 |
|    clip_fraction        | 0.00759      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0165      |
|    explained_variance   | 0.179        |
|    learning_rate        | 0.0001       |
|    loss                 | 1            |
|    n_updates            | 10710        |
|    policy_gradient_loss | -0.000733    |
|    value_loss           | 1.65         |
------------------------------------------
Progress update from timestep 428700: Updated shield, loss is 0.03808078169822693
Progress update from timestep 428800: Updated shield, loss is 0.03541341423988342
Eval num_timesteps=428800, episode_reward=4.39 +/- 2.55
Episode length: 7.20 +/- 3.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7.2      |
|    mean_reward     | 4.39     |
| time/              |          |
|    total_timesteps | 428800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.41     |
|    ep_rew_mean     | 2.79     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1072     |
|    time_elapsed    | 97680    |
|    total_timesteps | 428800   |
---------------------------------
Progress update from timestep 428900: Updated shield, loss is 0.033415816724300385
Progress update from timestep 429000: Updated shield, loss is 0.04024858400225639
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 429000: Saved shield_loss.csv. 
Eval num_timesteps=429000, episode_reward=17.49 +/- 14.22
Episode length: 26.00 +/- 20.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26           |
|    mean_reward          | 17.5         |
| time/                   |              |
|    total_timesteps      | 429000       |
| train/                  |              |
|    approx_kl            | 0.0013096462 |
|    clip_fraction        | 0.0136       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0286      |
|    explained_variance   | 0.0912       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.53         |
|    n_updates            | 10720        |
|    policy_gradient_loss | -0.00191     |
|    value_loss           | 2.16         |
------------------------------------------
Progress update from timestep 429100: Updated shield, loss is 0.03749243915081024
Progress update from timestep 429200: Updated shield, loss is 0.03724464029073715
Eval num_timesteps=429200, episode_reward=22.56 +/- 16.00
Episode length: 32.20 +/- 21.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 22.6     |
| time/              |          |
|    total_timesteps | 429200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.93     |
|    ep_rew_mean     | 2.44     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1073     |
|    time_elapsed    | 97762    |
|    total_timesteps | 429200   |
---------------------------------
Progress update from timestep 429300: Updated shield, loss is 0.047180794179439545
Progress update from timestep 429400: Updated shield, loss is 0.03300362452864647
Eval num_timesteps=429400, episode_reward=11.07 +/- 12.70
Episode length: 17.00 +/- 18.57
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17            |
|    mean_reward          | 11.1          |
| time/                   |               |
|    total_timesteps      | 429400        |
| train/                  |               |
|    approx_kl            | 0.00012777085 |
|    clip_fraction        | 0.00603       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0067       |
|    explained_variance   | 0.0917        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.948         |
|    n_updates            | 10730         |
|    policy_gradient_loss | -0.000486     |
|    value_loss           | 1.63          |
-------------------------------------------
Progress update from timestep 429500: Updated shield, loss is 0.03408049792051315
Progress update from timestep 429600: Updated shield, loss is 0.02863345853984356
Eval num_timesteps=429600, episode_reward=15.36 +/- 16.55
Episode length: 22.20 +/- 22.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 15.4     |
| time/              |          |
|    total_timesteps | 429600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.46     |
|    ep_rew_mean     | 2.84     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1074     |
|    time_elapsed    | 97846    |
|    total_timesteps | 429600   |
---------------------------------
Progress update from timestep 429700: Updated shield, loss is 0.038140036165714264
Progress update from timestep 429800: Updated shield, loss is 0.03014802746474743
Eval num_timesteps=429800, episode_reward=6.65 +/- 6.62
Episode length: 10.40 +/- 9.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 10.4        |
|    mean_reward          | 6.65        |
| time/                   |             |
|    total_timesteps      | 429800      |
| train/                  |             |
|    approx_kl            | 0.000464374 |
|    clip_fraction        | 0.00536     |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0096     |
|    explained_variance   | 0.101       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.611       |
|    n_updates            | 10740       |
|    policy_gradient_loss | -0.000656   |
|    value_loss           | 1.53        |
-----------------------------------------
Progress update from timestep 429900: Updated shield, loss is 0.03289182111620903
Progress update from timestep 430000: Updated shield, loss is 0.029111841693520546
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 430000: Saved shield_loss.csv. 
Eval num_timesteps=430000, episode_reward=8.98 +/- 12.30
Episode length: 14.00 +/- 18.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14       |
|    mean_reward     | 8.98     |
| time/              |          |
|    total_timesteps | 430000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.83     |
|    ep_rew_mean     | 2.41     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1075     |
|    time_elapsed    | 97918    |
|    total_timesteps | 430000   |
---------------------------------
Progress update from timestep 430100: Updated shield, loss is 0.03212818503379822
Progress update from timestep 430200: Updated shield, loss is 0.030602825805544853
Eval num_timesteps=430200, episode_reward=24.89 +/- 13.84
Episode length: 35.00 +/- 18.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35          |
|    mean_reward          | 24.9        |
| time/                   |             |
|    total_timesteps      | 430200      |
| train/                  |             |
|    approx_kl            | 0.000799865 |
|    clip_fraction        | 0.0179      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0275     |
|    explained_variance   | 0.225       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.446       |
|    n_updates            | 10750       |
|    policy_gradient_loss | -0.00149    |
|    value_loss           | 1.11        |
-----------------------------------------
Progress update from timestep 430300: Updated shield, loss is 0.03554527461528778
Progress update from timestep 430400: Updated shield, loss is 0.0342005155980587
Eval num_timesteps=430400, episode_reward=24.05 +/- 14.65
Episode length: 34.00 +/- 19.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 24       |
| time/              |          |
|    total_timesteps | 430400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.83     |
|    ep_rew_mean     | 2.4      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1076     |
|    time_elapsed    | 98011    |
|    total_timesteps | 430400   |
---------------------------------
Progress update from timestep 430500: Updated shield, loss is 0.04189851135015488
Progress update from timestep 430600: Updated shield, loss is 0.034235235303640366
Eval num_timesteps=430600, episode_reward=13.37 +/- 10.71
Episode length: 20.00 +/- 15.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 20           |
|    mean_reward          | 13.4         |
| time/                   |              |
|    total_timesteps      | 430600       |
| train/                  |              |
|    approx_kl            | 4.726774e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.000311    |
|    explained_variance   | 0.114        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.19         |
|    n_updates            | 10760        |
|    policy_gradient_loss | -1.02e-07    |
|    value_loss           | 1.68         |
------------------------------------------
Progress update from timestep 430700: Updated shield, loss is 0.043890755623579025
Progress update from timestep 430800: Updated shield, loss is 0.04420491307973862
Eval num_timesteps=430800, episode_reward=22.38 +/- 14.86
Episode length: 32.80 +/- 21.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 22.4     |
| time/              |          |
|    total_timesteps | 430800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.92     |
|    ep_rew_mean     | 2.44     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1077     |
|    time_elapsed    | 98117    |
|    total_timesteps | 430800   |
---------------------------------
Progress update from timestep 430900: Updated shield, loss is 0.04320371523499489
Progress update from timestep 431000: Updated shield, loss is 0.036340005695819855
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 431000: Saved shield_loss.csv. 
Eval num_timesteps=431000, episode_reward=13.36 +/- 13.58
Episode length: 19.20 +/- 18.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 19.2         |
|    mean_reward          | 13.4         |
| time/                   |              |
|    total_timesteps      | 431000       |
| train/                  |              |
|    approx_kl            | 0.0002062543 |
|    clip_fraction        | 0.0029       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00488     |
|    explained_variance   | 0.199        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.681        |
|    n_updates            | 10770        |
|    policy_gradient_loss | -0.000842    |
|    value_loss           | 1.2          |
------------------------------------------
Progress update from timestep 431100: Updated shield, loss is 0.04475608095526695
Progress update from timestep 431200: Updated shield, loss is 0.04016319289803505
Eval num_timesteps=431200, episode_reward=9.77 +/- 12.08
Episode length: 15.20 +/- 17.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | 9.77     |
| time/              |          |
|    total_timesteps | 431200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.21     |
|    ep_rew_mean     | 2.71     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1078     |
|    time_elapsed    | 98209    |
|    total_timesteps | 431200   |
---------------------------------
Progress update from timestep 431300: Updated shield, loss is 0.0395205058157444
Progress update from timestep 431400: Updated shield, loss is 0.03028976544737816
Eval num_timesteps=431400, episode_reward=17.31 +/- 14.65
Episode length: 25.80 +/- 20.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.8          |
|    mean_reward          | 17.3          |
| time/                   |               |
|    total_timesteps      | 431400        |
| train/                  |               |
|    approx_kl            | 3.1182676e-12 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -7.47e-05     |
|    explained_variance   | 0.217         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.415         |
|    n_updates            | 10780         |
|    policy_gradient_loss | -1.37e-07     |
|    value_loss           | 1.46          |
-------------------------------------------
Progress update from timestep 431500: Updated shield, loss is 0.03947265446186066
Progress update from timestep 431600: Updated shield, loss is 0.03627539426088333
Eval num_timesteps=431600, episode_reward=18.38 +/- 14.36
Episode length: 27.40 +/- 20.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.4     |
|    mean_reward     | 18.4     |
| time/              |          |
|    total_timesteps | 431600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.25     |
|    ep_rew_mean     | 3.43     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1079     |
|    time_elapsed    | 98290    |
|    total_timesteps | 431600   |
---------------------------------
Progress update from timestep 431700: Updated shield, loss is 0.04029085487127304
Progress update from timestep 431800: Updated shield, loss is 0.034859251230955124
Eval num_timesteps=431800, episode_reward=21.26 +/- 16.20
Episode length: 31.20 +/- 23.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 31.2          |
|    mean_reward          | 21.3          |
| time/                   |               |
|    total_timesteps      | 431800        |
| train/                  |               |
|    approx_kl            | 0.00013957304 |
|    clip_fraction        | 0.00424       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00481      |
|    explained_variance   | 0.079         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.32          |
|    n_updates            | 10790         |
|    policy_gradient_loss | -0.000265     |
|    value_loss           | 2.85          |
-------------------------------------------
Progress update from timestep 431900: Updated shield, loss is 0.04305163025856018
Progress update from timestep 432000: Updated shield, loss is 0.04033605754375458
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 432000: Saved shield_loss.csv. 
Eval num_timesteps=432000, episode_reward=16.54 +/- 14.22
Episode length: 24.80 +/- 20.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 432000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.4      |
|    ep_rew_mean     | 2.76     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1080     |
|    time_elapsed    | 98374    |
|    total_timesteps | 432000   |
---------------------------------
Progress update from timestep 432100: Updated shield, loss is 0.04055333137512207
Progress update from timestep 432200: Updated shield, loss is 0.028537461534142494
Eval num_timesteps=432200, episode_reward=23.93 +/- 14.37
Episode length: 34.20 +/- 19.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.2          |
|    mean_reward          | 23.9          |
| time/                   |               |
|    total_timesteps      | 432200        |
| train/                  |               |
|    approx_kl            | 0.00027231622 |
|    clip_fraction        | 0.00848       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0182       |
|    explained_variance   | 0.226         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.822         |
|    n_updates            | 10800         |
|    policy_gradient_loss | -0.00091      |
|    value_loss           | 1.95          |
-------------------------------------------
Progress update from timestep 432300: Updated shield, loss is 0.035167645663022995
Progress update from timestep 432400: Updated shield, loss is 0.030668165534734726
Eval num_timesteps=432400, episode_reward=16.91 +/- 15.07
Episode length: 24.80 +/- 20.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 432400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.19     |
|    ep_rew_mean     | 2.61     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1081     |
|    time_elapsed    | 98458    |
|    total_timesteps | 432400   |
---------------------------------
Progress update from timestep 432500: Updated shield, loss is 0.034132249653339386
Progress update from timestep 432600: Updated shield, loss is 0.048129111528396606
Eval num_timesteps=432600, episode_reward=16.21 +/- 15.85
Episode length: 23.40 +/- 21.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.4         |
|    mean_reward          | 16.2         |
| time/                   |              |
|    total_timesteps      | 432600       |
| train/                  |              |
|    approx_kl            | 0.0002445484 |
|    clip_fraction        | 0.00603      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.013       |
|    explained_variance   | 0.204        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.532        |
|    n_updates            | 10810        |
|    policy_gradient_loss | -0.000898    |
|    value_loss           | 1.5          |
------------------------------------------
Progress update from timestep 432700: Updated shield, loss is 0.038239724934101105
Progress update from timestep 432800: Updated shield, loss is 0.03252527862787247
Eval num_timesteps=432800, episode_reward=17.17 +/- 14.54
Episode length: 25.40 +/- 20.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | 17.2     |
| time/              |          |
|    total_timesteps | 432800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.62     |
|    ep_rew_mean     | 2.87     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1082     |
|    time_elapsed    | 98538    |
|    total_timesteps | 432800   |
---------------------------------
Progress update from timestep 432900: Updated shield, loss is 0.03796497732400894
Progress update from timestep 433000: Updated shield, loss is 0.03250205144286156
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 433000: Saved shield_loss.csv. 
Eval num_timesteps=433000, episode_reward=16.00 +/- 16.45
Episode length: 22.80 +/- 22.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22.8         |
|    mean_reward          | 16           |
| time/                   |              |
|    total_timesteps      | 433000       |
| train/                  |              |
|    approx_kl            | 0.0004835592 |
|    clip_fraction        | 0.0127       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0307      |
|    explained_variance   | 0.221        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.632        |
|    n_updates            | 10820        |
|    policy_gradient_loss | -0.000833    |
|    value_loss           | 1.95         |
------------------------------------------
Progress update from timestep 433100: Updated shield, loss is 0.0362185537815094
Progress update from timestep 433200: Updated shield, loss is 0.0342102088034153
Eval num_timesteps=433200, episode_reward=17.45 +/- 13.90
Episode length: 26.00 +/- 20.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 433200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.45     |
|    ep_rew_mean     | 2.78     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1083     |
|    time_elapsed    | 98641    |
|    total_timesteps | 433200   |
---------------------------------
Progress update from timestep 433300: Updated shield, loss is 0.030927523970603943
Progress update from timestep 433400: Updated shield, loss is 0.03665914759039879
Eval num_timesteps=433400, episode_reward=17.14 +/- 15.11
Episode length: 24.80 +/- 20.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.8          |
|    mean_reward          | 17.1          |
| time/                   |               |
|    total_timesteps      | 433400        |
| train/                  |               |
|    approx_kl            | 0.00018977051 |
|    clip_fraction        | 0.00714       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0204       |
|    explained_variance   | 0.113         |
|    learning_rate        | 0.0001        |
|    loss                 | 1             |
|    n_updates            | 10830         |
|    policy_gradient_loss | -0.00126      |
|    value_loss           | 1.83          |
-------------------------------------------
Progress update from timestep 433500: Updated shield, loss is 0.0287762600928545
Progress update from timestep 433600: Updated shield, loss is 0.04018795117735863
Eval num_timesteps=433600, episode_reward=9.94 +/- 11.75
Episode length: 15.20 +/- 17.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | 9.94     |
| time/              |          |
|    total_timesteps | 433600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.02     |
|    ep_rew_mean     | 2.45     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1084     |
|    time_elapsed    | 98723    |
|    total_timesteps | 433600   |
---------------------------------
Progress update from timestep 433700: Updated shield, loss is 0.03886153921484947
Progress update from timestep 433800: Updated shield, loss is 0.032211340963840485
Eval num_timesteps=433800, episode_reward=23.54 +/- 14.09
Episode length: 34.00 +/- 19.85
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34            |
|    mean_reward          | 23.5          |
| time/                   |               |
|    total_timesteps      | 433800        |
| train/                  |               |
|    approx_kl            | 0.00013796185 |
|    clip_fraction        | 0.00625       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0238       |
|    explained_variance   | 0.179         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.543         |
|    n_updates            | 10840         |
|    policy_gradient_loss | -0.00134      |
|    value_loss           | 1.24          |
-------------------------------------------
Progress update from timestep 433900: Updated shield, loss is 0.029037538915872574
Progress update from timestep 434000: Updated shield, loss is 0.038184165954589844
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 434000: Saved shield_loss.csv. 
Eval num_timesteps=434000, episode_reward=11.20 +/- 11.24
Episode length: 17.00 +/- 16.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | 11.2     |
| time/              |          |
|    total_timesteps | 434000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.19     |
|    ep_rew_mean     | 2.6      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1085     |
|    time_elapsed    | 98809    |
|    total_timesteps | 434000   |
---------------------------------
Progress update from timestep 434100: Updated shield, loss is 0.03955960273742676
Progress update from timestep 434200: Updated shield, loss is 0.029861576855182648
Eval num_timesteps=434200, episode_reward=23.36 +/- 15.24
Episode length: 33.40 +/- 20.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.4         |
|    mean_reward          | 23.4         |
| time/                   |              |
|    total_timesteps      | 434200       |
| train/                  |              |
|    approx_kl            | 0.0018582101 |
|    clip_fraction        | 0.0143       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0141      |
|    explained_variance   | 0.181        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.859        |
|    n_updates            | 10850        |
|    policy_gradient_loss | -0.000728    |
|    value_loss           | 1.66         |
------------------------------------------
Progress update from timestep 434300: Updated shield, loss is 0.02834237925708294
Progress update from timestep 434400: Updated shield, loss is 0.040225688368082047
Eval num_timesteps=434400, episode_reward=22.79 +/- 13.41
Episode length: 34.40 +/- 19.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 22.8     |
| time/              |          |
|    total_timesteps | 434400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.55     |
|    ep_rew_mean     | 2.92     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1086     |
|    time_elapsed    | 98898    |
|    total_timesteps | 434400   |
---------------------------------
Progress update from timestep 434500: Updated shield, loss is 0.03481340408325195
Progress update from timestep 434600: Updated shield, loss is 0.03356953337788582
Eval num_timesteps=434600, episode_reward=10.15 +/- 13.61
Episode length: 14.80 +/- 18.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.8         |
|    mean_reward          | 10.2         |
| time/                   |              |
|    total_timesteps      | 434600       |
| train/                  |              |
|    approx_kl            | 0.0005080981 |
|    clip_fraction        | 0.00201      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.000355    |
|    explained_variance   | 0.202        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.713        |
|    n_updates            | 10860        |
|    policy_gradient_loss | -0.000247    |
|    value_loss           | 1.75         |
------------------------------------------
Progress update from timestep 434700: Updated shield, loss is 0.029119588434696198
Progress update from timestep 434800: Updated shield, loss is 0.03141903132200241
Eval num_timesteps=434800, episode_reward=22.41 +/- 14.82
Episode length: 32.80 +/- 21.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 22.4     |
| time/              |          |
|    total_timesteps | 434800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.87     |
|    ep_rew_mean     | 2.3      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1087     |
|    time_elapsed    | 98999    |
|    total_timesteps | 434800   |
---------------------------------
Progress update from timestep 434900: Updated shield, loss is 0.03603694587945938
Progress update from timestep 435000: Updated shield, loss is 0.027629416435956955
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 435000: Saved shield_loss.csv. 
Eval num_timesteps=435000, episode_reward=16.77 +/- 14.97
Episode length: 24.60 +/- 20.83
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 24.6           |
|    mean_reward          | 16.8           |
| time/                   |                |
|    total_timesteps      | 435000         |
| train/                  |                |
|    approx_kl            | 0.000108244356 |
|    clip_fraction        | 0.00469        |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.0168        |
|    explained_variance   | 0.18           |
|    learning_rate        | 0.0001         |
|    loss                 | 0.244          |
|    n_updates            | 10870          |
|    policy_gradient_loss | -0.000441      |
|    value_loss           | 1.26           |
--------------------------------------------
Progress update from timestep 435100: Updated shield, loss is 0.030652618035674095
Progress update from timestep 435200: Updated shield, loss is 0.03378412500023842
Eval num_timesteps=435200, episode_reward=11.75 +/- 11.53
Episode length: 18.00 +/- 17.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | 11.8     |
| time/              |          |
|    total_timesteps | 435200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.77     |
|    ep_rew_mean     | 2.33     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1088     |
|    time_elapsed    | 99084    |
|    total_timesteps | 435200   |
---------------------------------
Progress update from timestep 435300: Updated shield, loss is 0.04334074258804321
Progress update from timestep 435400: Updated shield, loss is 0.035407714545726776
Eval num_timesteps=435400, episode_reward=8.41 +/- 6.08
Episode length: 12.60 +/- 8.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 12.6          |
|    mean_reward          | 8.41          |
| time/                   |               |
|    total_timesteps      | 435400        |
| train/                  |               |
|    approx_kl            | 0.00035760397 |
|    clip_fraction        | 0.0145        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0169       |
|    explained_variance   | 0.286         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.205         |
|    n_updates            | 10880         |
|    policy_gradient_loss | -0.00116      |
|    value_loss           | 1.21          |
-------------------------------------------
Progress update from timestep 435500: Updated shield, loss is 0.03402252867817879
Progress update from timestep 435600: Updated shield, loss is 0.03249000757932663
Eval num_timesteps=435600, episode_reward=12.65 +/- 10.96
Episode length: 18.80 +/- 15.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.8     |
|    mean_reward     | 12.6     |
| time/              |          |
|    total_timesteps | 435600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.06     |
|    ep_rew_mean     | 2.5      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1089     |
|    time_elapsed    | 99160    |
|    total_timesteps | 435600   |
---------------------------------
Progress update from timestep 435700: Updated shield, loss is 0.03681870922446251
Progress update from timestep 435800: Updated shield, loss is 0.0361410453915596
Eval num_timesteps=435800, episode_reward=4.18 +/- 2.74
Episode length: 6.80 +/- 3.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 6.8          |
|    mean_reward          | 4.18         |
| time/                   |              |
|    total_timesteps      | 435800       |
| train/                  |              |
|    approx_kl            | 7.995758e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.000594    |
|    explained_variance   | 0.109        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.767        |
|    n_updates            | 10890        |
|    policy_gradient_loss | -9.52e-09    |
|    value_loss           | 1.39         |
------------------------------------------
Progress update from timestep 435900: Updated shield, loss is 0.036169860512018204
Progress update from timestep 436000: Updated shield, loss is 0.03811756148934364
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 436000: Saved shield_loss.csv. 
Eval num_timesteps=436000, episode_reward=15.05 +/- 15.88
Episode length: 22.40 +/- 22.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 15       |
| time/              |          |
|    total_timesteps | 436000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.92     |
|    ep_rew_mean     | 2.43     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1090     |
|    time_elapsed    | 99226    |
|    total_timesteps | 436000   |
---------------------------------
Progress update from timestep 436100: Updated shield, loss is 0.029957063496112823
Progress update from timestep 436200: Updated shield, loss is 0.03323690593242645
Eval num_timesteps=436200, episode_reward=17.33 +/- 14.55
Episode length: 25.40 +/- 20.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.4          |
|    mean_reward          | 17.3          |
| time/                   |               |
|    total_timesteps      | 436200        |
| train/                  |               |
|    approx_kl            | 0.00025912622 |
|    clip_fraction        | 0.00781       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.028        |
|    explained_variance   | 0.183         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.436         |
|    n_updates            | 10900         |
|    policy_gradient_loss | -0.00145      |
|    value_loss           | 1.3           |
-------------------------------------------
Progress update from timestep 436300: Updated shield, loss is 0.032521773129701614
Progress update from timestep 436400: Updated shield, loss is 0.03363117203116417
Eval num_timesteps=436400, episode_reward=15.59 +/- 14.30
Episode length: 23.00 +/- 20.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 15.6     |
| time/              |          |
|    total_timesteps | 436400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.37     |
|    ep_rew_mean     | 2.8      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1091     |
|    time_elapsed    | 99312    |
|    total_timesteps | 436400   |
---------------------------------
Progress update from timestep 436500: Updated shield, loss is 0.03762133792042732
Progress update from timestep 436600: Updated shield, loss is 0.037212733179330826
Eval num_timesteps=436600, episode_reward=17.73 +/- 15.26
Episode length: 25.00 +/- 20.71
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25            |
|    mean_reward          | 17.7          |
| time/                   |               |
|    total_timesteps      | 436600        |
| train/                  |               |
|    approx_kl            | 0.00020514878 |
|    clip_fraction        | 0.00692       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0222       |
|    explained_variance   | 0.16          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.475         |
|    n_updates            | 10910         |
|    policy_gradient_loss | -0.00129      |
|    value_loss           | 1.78          |
-------------------------------------------
Progress update from timestep 436700: Updated shield, loss is 0.03641991317272186
Progress update from timestep 436800: Updated shield, loss is 0.04012061282992363
Eval num_timesteps=436800, episode_reward=6.11 +/- 4.13
Episode length: 9.20 +/- 5.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.2      |
|    mean_reward     | 6.11     |
| time/              |          |
|    total_timesteps | 436800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.95     |
|    ep_rew_mean     | 2.43     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1092     |
|    time_elapsed    | 99381    |
|    total_timesteps | 436800   |
---------------------------------
Progress update from timestep 436900: Updated shield, loss is 0.041585296392440796
Progress update from timestep 437000: Updated shield, loss is 0.02980950102210045
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 437000: Saved shield_loss.csv. 
Eval num_timesteps=437000, episode_reward=13.15 +/- 11.39
Episode length: 19.60 +/- 16.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 19.6         |
|    mean_reward          | 13.1         |
| time/                   |              |
|    total_timesteps      | 437000       |
| train/                  |              |
|    approx_kl            | 6.760799e-05 |
|    clip_fraction        | 0.00469      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0162      |
|    explained_variance   | 0.173        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.45         |
|    n_updates            | 10920        |
|    policy_gradient_loss | -0.000635    |
|    value_loss           | 1.55         |
------------------------------------------
Progress update from timestep 437100: Updated shield, loss is 0.03396163135766983
Progress update from timestep 437200: Updated shield, loss is 0.04109589755535126
Eval num_timesteps=437200, episode_reward=22.05 +/- 15.69
Episode length: 32.00 +/- 22.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32       |
|    mean_reward     | 22.1     |
| time/              |          |
|    total_timesteps | 437200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.14     |
|    ep_rew_mean     | 2.6      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1093     |
|    time_elapsed    | 99462    |
|    total_timesteps | 437200   |
---------------------------------
Progress update from timestep 437300: Updated shield, loss is 0.03089408203959465
Progress update from timestep 437400: Updated shield, loss is 0.039614588022232056
Eval num_timesteps=437400, episode_reward=17.12 +/- 15.09
Episode length: 24.60 +/- 20.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.6         |
|    mean_reward          | 17.1         |
| time/                   |              |
|    total_timesteps      | 437400       |
| train/                  |              |
|    approx_kl            | 0.0002136912 |
|    clip_fraction        | 0.00379      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00876     |
|    explained_variance   | 0.167        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.798        |
|    n_updates            | 10930        |
|    policy_gradient_loss | -0.000309    |
|    value_loss           | 2.22         |
------------------------------------------
Progress update from timestep 437500: Updated shield, loss is 0.03884749859571457
Progress update from timestep 437600: Updated shield, loss is 0.04225465655326843
Eval num_timesteps=437600, episode_reward=17.73 +/- 14.31
Episode length: 25.80 +/- 20.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | 17.7     |
| time/              |          |
|    total_timesteps | 437600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.99     |
|    ep_rew_mean     | 2.51     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1094     |
|    time_elapsed    | 99563    |
|    total_timesteps | 437600   |
---------------------------------
Progress update from timestep 437700: Updated shield, loss is 0.037007760256528854
Progress update from timestep 437800: Updated shield, loss is 0.035378724336624146
Eval num_timesteps=437800, episode_reward=9.53 +/- 13.59
Episode length: 13.80 +/- 18.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 13.8         |
|    mean_reward          | 9.53         |
| time/                   |              |
|    total_timesteps      | 437800       |
| train/                  |              |
|    approx_kl            | 0.0001595749 |
|    clip_fraction        | 0.00491      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0136      |
|    explained_variance   | 0.3          |
|    learning_rate        | 0.0001       |
|    loss                 | 0.844        |
|    n_updates            | 10940        |
|    policy_gradient_loss | -0.000424    |
|    value_loss           | 1.47         |
------------------------------------------
Progress update from timestep 437900: Updated shield, loss is 0.038981951773166656
Progress update from timestep 438000: Updated shield, loss is 0.034207042306661606
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 438000: Saved shield_loss.csv. 
Eval num_timesteps=438000, episode_reward=9.14 +/- 12.72
Episode length: 14.00 +/- 18.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14       |
|    mean_reward     | 9.14     |
| time/              |          |
|    total_timesteps | 438000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.45     |
|    ep_rew_mean     | 2.83     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1095     |
|    time_elapsed    | 99646    |
|    total_timesteps | 438000   |
---------------------------------
Progress update from timestep 438100: Updated shield, loss is 0.04146544635295868
Progress update from timestep 438200: Updated shield, loss is 0.041959501802921295
Eval num_timesteps=438200, episode_reward=28.45 +/- 13.19
Episode length: 40.80 +/- 18.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 40.8          |
|    mean_reward          | 28.4          |
| time/                   |               |
|    total_timesteps      | 438200        |
| train/                  |               |
|    approx_kl            | 0.00012404234 |
|    clip_fraction        | 0.00647       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0144       |
|    explained_variance   | 0.216         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.804         |
|    n_updates            | 10950         |
|    policy_gradient_loss | -0.00068      |
|    value_loss           | 1.83          |
-------------------------------------------
Progress update from timestep 438300: Updated shield, loss is 0.03506918624043465
Progress update from timestep 438400: Updated shield, loss is 0.034860365092754364
Eval num_timesteps=438400, episode_reward=29.11 +/- 13.52
Episode length: 40.80 +/- 18.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.8     |
|    mean_reward     | 29.1     |
| time/              |          |
|    total_timesteps | 438400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.07     |
|    ep_rew_mean     | 2.53     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1096     |
|    time_elapsed    | 99747    |
|    total_timesteps | 438400   |
---------------------------------
Progress update from timestep 438500: Updated shield, loss is 0.0302659273147583
Progress update from timestep 438600: Updated shield, loss is 0.035270728170871735
Eval num_timesteps=438600, episode_reward=16.47 +/- 14.36
Episode length: 24.80 +/- 20.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.8         |
|    mean_reward          | 16.5         |
| time/                   |              |
|    total_timesteps      | 438600       |
| train/                  |              |
|    approx_kl            | 0.0004122218 |
|    clip_fraction        | 0.0109       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0205      |
|    explained_variance   | 0.286        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.586        |
|    n_updates            | 10960        |
|    policy_gradient_loss | -0.00083     |
|    value_loss           | 1.45         |
------------------------------------------
Progress update from timestep 438700: Updated shield, loss is 0.040871839970350266
Progress update from timestep 438800: Updated shield, loss is 0.03207644447684288
Eval num_timesteps=438800, episode_reward=22.94 +/- 14.77
Episode length: 33.20 +/- 20.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | 22.9     |
| time/              |          |
|    total_timesteps | 438800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.68     |
|    ep_rew_mean     | 2.31     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1097     |
|    time_elapsed    | 99829    |
|    total_timesteps | 438800   |
---------------------------------
Progress update from timestep 438900: Updated shield, loss is 0.030268507078289986
Progress update from timestep 439000: Updated shield, loss is 0.041972074657678604
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 439000: Saved shield_loss.csv. 
Eval num_timesteps=439000, episode_reward=15.34 +/- 16.13
Episode length: 22.40 +/- 22.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.4          |
|    mean_reward          | 15.3          |
| time/                   |               |
|    total_timesteps      | 439000        |
| train/                  |               |
|    approx_kl            | 0.00015848386 |
|    clip_fraction        | 0.00469       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0156       |
|    explained_variance   | 0.336         |
|    learning_rate        | 0.0001        |
|    loss                 | 1             |
|    n_updates            | 10970         |
|    policy_gradient_loss | -0.000586     |
|    value_loss           | 1.81          |
-------------------------------------------
Progress update from timestep 439100: Updated shield, loss is 0.03528948128223419
Progress update from timestep 439200: Updated shield, loss is 0.03403942286968231
Eval num_timesteps=439200, episode_reward=11.84 +/- 10.83
Episode length: 18.20 +/- 16.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | 11.8     |
| time/              |          |
|    total_timesteps | 439200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.51     |
|    ep_rew_mean     | 2.85     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1098     |
|    time_elapsed    | 99930    |
|    total_timesteps | 439200   |
---------------------------------
Progress update from timestep 439300: Updated shield, loss is 0.032960910350084305
Progress update from timestep 439400: Updated shield, loss is 0.03257523104548454
Eval num_timesteps=439400, episode_reward=7.02 +/- 5.04
Episode length: 11.00 +/- 7.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 11            |
|    mean_reward          | 7.02          |
| time/                   |               |
|    total_timesteps      | 439400        |
| train/                  |               |
|    approx_kl            | 0.00023697216 |
|    clip_fraction        | 0.00826       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0423       |
|    explained_variance   | 0.0199        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.808         |
|    n_updates            | 10980         |
|    policy_gradient_loss | -0.00125      |
|    value_loss           | 1.75          |
-------------------------------------------
Progress update from timestep 439500: Updated shield, loss is 0.03005037270486355
Progress update from timestep 439600: Updated shield, loss is 0.04262842983007431
Eval num_timesteps=439600, episode_reward=10.92 +/- 12.05
Episode length: 16.40 +/- 17.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | 10.9     |
| time/              |          |
|    total_timesteps | 439600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.26     |
|    ep_rew_mean     | 2.69     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1099     |
|    time_elapsed    | 100000   |
|    total_timesteps | 439600   |
---------------------------------
Progress update from timestep 439700: Updated shield, loss is 0.04396563768386841
Progress update from timestep 439800: Updated shield, loss is 0.03885165974497795
Eval num_timesteps=439800, episode_reward=24.00 +/- 14.23
Episode length: 34.20 +/- 19.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.2          |
|    mean_reward          | 24            |
| time/                   |               |
|    total_timesteps      | 439800        |
| train/                  |               |
|    approx_kl            | 0.00028012172 |
|    clip_fraction        | 0.0058        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.017        |
|    explained_variance   | 0.322         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.643         |
|    n_updates            | 10990         |
|    policy_gradient_loss | -0.000475     |
|    value_loss           | 1.6           |
-------------------------------------------
Progress update from timestep 439900: Updated shield, loss is 0.03812948986887932
Progress update from timestep 440000: Updated shield, loss is 0.030491890385746956
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 440000: Saved shield_loss.csv. 
Eval num_timesteps=440000, episode_reward=22.25 +/- 14.07
Episode length: 33.40 +/- 20.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 22.2     |
| time/              |          |
|    total_timesteps | 440000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.01     |
|    ep_rew_mean     | 2.56     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1100     |
|    time_elapsed    | 100087   |
|    total_timesteps | 440000   |
---------------------------------
Progress update from timestep 440100: Updated shield, loss is 0.04621949791908264
Progress update from timestep 440200: Updated shield, loss is 0.03680332750082016
Eval num_timesteps=440200, episode_reward=30.02 +/- 9.47
Episode length: 43.20 +/- 13.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 43.2         |
|    mean_reward          | 30           |
| time/                   |              |
|    total_timesteps      | 440200       |
| train/                  |              |
|    approx_kl            | 0.0009089149 |
|    clip_fraction        | 0.00937      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0291      |
|    explained_variance   | 0.192        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.653        |
|    n_updates            | 11000        |
|    policy_gradient_loss | -0.00126     |
|    value_loss           | 1.38         |
------------------------------------------
Progress update from timestep 440300: Updated shield, loss is 0.0323520302772522
Progress update from timestep 440400: Updated shield, loss is 0.05435682833194733
Eval num_timesteps=440400, episode_reward=9.88 +/- 12.96
Episode length: 14.80 +/- 17.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 9.88     |
| time/              |          |
|    total_timesteps | 440400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.85     |
|    ep_rew_mean     | 2.44     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1101     |
|    time_elapsed    | 100187   |
|    total_timesteps | 440400   |
---------------------------------
Progress update from timestep 440500: Updated shield, loss is 0.034899450838565826
Progress update from timestep 440600: Updated shield, loss is 0.03415222093462944
Eval num_timesteps=440600, episode_reward=25.32 +/- 13.04
Episode length: 36.00 +/- 17.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36           |
|    mean_reward          | 25.3         |
| time/                   |              |
|    total_timesteps      | 440600       |
| train/                  |              |
|    approx_kl            | 0.0004855576 |
|    clip_fraction        | 0.0241       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0345      |
|    explained_variance   | 0.288        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.659        |
|    n_updates            | 11010        |
|    policy_gradient_loss | -0.00248     |
|    value_loss           | 1.86         |
------------------------------------------
Progress update from timestep 440700: Updated shield, loss is 0.02363169938325882
Progress update from timestep 440800: Updated shield, loss is 0.049046363681554794
Eval num_timesteps=440800, episode_reward=18.88 +/- 13.96
Episode length: 27.00 +/- 19.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27       |
|    mean_reward     | 18.9     |
| time/              |          |
|    total_timesteps | 440800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.83     |
|    ep_rew_mean     | 2.3      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1102     |
|    time_elapsed    | 100294   |
|    total_timesteps | 440800   |
---------------------------------
Progress update from timestep 440900: Updated shield, loss is 0.03355177119374275
Progress update from timestep 441000: Updated shield, loss is 0.0328608900308609
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 441000: Saved shield_loss.csv. 
Eval num_timesteps=441000, episode_reward=7.25 +/- 7.72
Episode length: 11.20 +/- 10.83
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 11.2           |
|    mean_reward          | 7.25           |
| time/                   |                |
|    total_timesteps      | 441000         |
| train/                  |                |
|    approx_kl            | 0.000102264516 |
|    clip_fraction        | 0.00446        |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.00548       |
|    explained_variance   | 0.201          |
|    learning_rate        | 0.0001         |
|    loss                 | 0.403          |
|    n_updates            | 11020          |
|    policy_gradient_loss | -0.000227      |
|    value_loss           | 1.17           |
--------------------------------------------
Progress update from timestep 441100: Updated shield, loss is 0.03372031822800636
Progress update from timestep 441200: Updated shield, loss is 0.03154823184013367
Eval num_timesteps=441200, episode_reward=25.24 +/- 13.86
Episode length: 35.40 +/- 18.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 25.2     |
| time/              |          |
|    total_timesteps | 441200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.16     |
|    ep_rew_mean     | 2.57     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1103     |
|    time_elapsed    | 100387   |
|    total_timesteps | 441200   |
---------------------------------
Progress update from timestep 441300: Updated shield, loss is 0.03325160592794418
Progress update from timestep 441400: Updated shield, loss is 0.03407055139541626
Eval num_timesteps=441400, episode_reward=16.07 +/- 16.39
Episode length: 23.00 +/- 22.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23            |
|    mean_reward          | 16.1          |
| time/                   |               |
|    total_timesteps      | 441400        |
| train/                  |               |
|    approx_kl            | 0.00063142873 |
|    clip_fraction        | 0.00558       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0103       |
|    explained_variance   | 0.162         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.592         |
|    n_updates            | 11030         |
|    policy_gradient_loss | -0.000607     |
|    value_loss           | 1.43          |
-------------------------------------------
Progress update from timestep 441500: Updated shield, loss is 0.037198081612586975
Progress update from timestep 441600: Updated shield, loss is 0.037046246230602264
Eval num_timesteps=441600, episode_reward=9.07 +/- 12.40
Episode length: 14.20 +/- 18.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.2     |
|    mean_reward     | 9.07     |
| time/              |          |
|    total_timesteps | 441600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.61     |
|    ep_rew_mean     | 2.18     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1104     |
|    time_elapsed    | 100468   |
|    total_timesteps | 441600   |
---------------------------------
Progress update from timestep 441700: Updated shield, loss is 0.03382408618927002
Progress update from timestep 441800: Updated shield, loss is 0.035361018031835556
Eval num_timesteps=441800, episode_reward=21.73 +/- 15.17
Episode length: 32.20 +/- 21.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.2         |
|    mean_reward          | 21.7         |
| time/                   |              |
|    total_timesteps      | 441800       |
| train/                  |              |
|    approx_kl            | 0.0001838479 |
|    clip_fraction        | 0.0096       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0358      |
|    explained_variance   | 0.163        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.462        |
|    n_updates            | 11040        |
|    policy_gradient_loss | -0.00125     |
|    value_loss           | 1.44         |
------------------------------------------
Progress update from timestep 441900: Updated shield, loss is 0.03857076168060303
Progress update from timestep 442000: Updated shield, loss is 0.03393667936325073
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 442000: Saved shield_loss.csv. 
Eval num_timesteps=442000, episode_reward=11.44 +/- 12.18
Episode length: 17.00 +/- 16.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | 11.4     |
| time/              |          |
|    total_timesteps | 442000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.03     |
|    ep_rew_mean     | 2.51     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1105     |
|    time_elapsed    | 100551   |
|    total_timesteps | 442000   |
---------------------------------
Progress update from timestep 442100: Updated shield, loss is 0.033110249787569046
Progress update from timestep 442200: Updated shield, loss is 0.04012392461299896
Eval num_timesteps=442200, episode_reward=14.94 +/- 15.98
Episode length: 22.20 +/- 22.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22.2         |
|    mean_reward          | 14.9         |
| time/                   |              |
|    total_timesteps      | 442200       |
| train/                  |              |
|    approx_kl            | 0.0003226229 |
|    clip_fraction        | 0.0103       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0332      |
|    explained_variance   | 0.197        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.68         |
|    n_updates            | 11050        |
|    policy_gradient_loss | -0.0014      |
|    value_loss           | 1.77         |
------------------------------------------
Progress update from timestep 442300: Updated shield, loss is 0.035245880484580994
Progress update from timestep 442400: Updated shield, loss is 0.04191744327545166
Eval num_timesteps=442400, episode_reward=17.65 +/- 14.77
Episode length: 25.60 +/- 20.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 17.7     |
| time/              |          |
|    total_timesteps | 442400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.52     |
|    ep_rew_mean     | 2.13     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1106     |
|    time_elapsed    | 100640   |
|    total_timesteps | 442400   |
---------------------------------
Progress update from timestep 442500: Updated shield, loss is 0.04749505594372749
Progress update from timestep 442600: Updated shield, loss is 0.0439455509185791
Eval num_timesteps=442600, episode_reward=14.96 +/- 15.96
Episode length: 22.20 +/- 22.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.2          |
|    mean_reward          | 15            |
| time/                   |               |
|    total_timesteps      | 442600        |
| train/                  |               |
|    approx_kl            | 0.00039791616 |
|    clip_fraction        | 0.00513       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0142       |
|    explained_variance   | 0.145         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.327         |
|    n_updates            | 11060         |
|    policy_gradient_loss | -0.00055      |
|    value_loss           | 1.04          |
-------------------------------------------
Progress update from timestep 442700: Updated shield, loss is 0.027901658788323402
Progress update from timestep 442800: Updated shield, loss is 0.03387122228741646
Eval num_timesteps=442800, episode_reward=11.69 +/- 12.83
Episode length: 17.00 +/- 17.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | 11.7     |
| time/              |          |
|    total_timesteps | 442800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.01     |
|    ep_rew_mean     | 2.54     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1107     |
|    time_elapsed    | 100723   |
|    total_timesteps | 442800   |
---------------------------------
Progress update from timestep 442900: Updated shield, loss is 0.0287641454488039
Progress update from timestep 443000: Updated shield, loss is 0.033061202615499496
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 443000: Saved shield_loss.csv. 
Eval num_timesteps=443000, episode_reward=20.22 +/- 12.74
Episode length: 29.00 +/- 17.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 29            |
|    mean_reward          | 20.2          |
| time/                   |               |
|    total_timesteps      | 443000        |
| train/                  |               |
|    approx_kl            | 1.5042488e-05 |
|    clip_fraction        | 0.00179       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00498      |
|    explained_variance   | 0.0675        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.511         |
|    n_updates            | 11070         |
|    policy_gradient_loss | -0.00018      |
|    value_loss           | 1.69          |
-------------------------------------------
Progress update from timestep 443100: Updated shield, loss is 0.03792789205908775
Progress update from timestep 443200: Updated shield, loss is 0.033105459064245224
Eval num_timesteps=443200, episode_reward=9.38 +/- 13.71
Episode length: 13.60 +/- 18.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.6     |
|    mean_reward     | 9.38     |
| time/              |          |
|    total_timesteps | 443200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.89     |
|    ep_rew_mean     | 2.46     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1108     |
|    time_elapsed    | 100824   |
|    total_timesteps | 443200   |
---------------------------------
Progress update from timestep 443300: Updated shield, loss is 0.032385047525167465
Progress update from timestep 443400: Updated shield, loss is 0.032902251929044724
Eval num_timesteps=443400, episode_reward=12.53 +/- 11.78
Episode length: 18.60 +/- 16.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.6         |
|    mean_reward          | 12.5         |
| time/                   |              |
|    total_timesteps      | 443400       |
| train/                  |              |
|    approx_kl            | 0.0008093677 |
|    clip_fraction        | 0.0228       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0277      |
|    explained_variance   | 0.0817       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.906        |
|    n_updates            | 11080        |
|    policy_gradient_loss | -0.00213     |
|    value_loss           | 1.77         |
------------------------------------------
Progress update from timestep 443500: Updated shield, loss is 0.04593151435256004
Progress update from timestep 443600: Updated shield, loss is 0.03510202467441559
Eval num_timesteps=443600, episode_reward=12.77 +/- 13.74
Episode length: 18.60 +/- 18.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 12.8     |
| time/              |          |
|    total_timesteps | 443600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.91     |
|    ep_rew_mean     | 2.39     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1109     |
|    time_elapsed    | 100905   |
|    total_timesteps | 443600   |
---------------------------------
Progress update from timestep 443700: Updated shield, loss is 0.03830980882048607
Progress update from timestep 443800: Updated shield, loss is 0.03283580020070076
Eval num_timesteps=443800, episode_reward=21.64 +/- 13.10
Episode length: 32.20 +/- 18.79
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 32.2           |
|    mean_reward          | 21.6           |
| time/                   |                |
|    total_timesteps      | 443800         |
| train/                  |                |
|    approx_kl            | 0.000100620746 |
|    clip_fraction        | 0.00424        |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.00831       |
|    explained_variance   | 0.0787         |
|    learning_rate        | 0.0001         |
|    loss                 | 0.504          |
|    n_updates            | 11090          |
|    policy_gradient_loss | -0.000337      |
|    value_loss           | 1.69           |
--------------------------------------------
Progress update from timestep 443900: Updated shield, loss is 0.04155666008591652
Progress update from timestep 444000: Updated shield, loss is 0.029321733862161636
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 444000: Saved shield_loss.csv. 
Eval num_timesteps=444000, episode_reward=22.33 +/- 15.40
Episode length: 32.40 +/- 21.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.4     |
|    mean_reward     | 22.3     |
| time/              |          |
|    total_timesteps | 444000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.94     |
|    ep_rew_mean     | 2.45     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1110     |
|    time_elapsed    | 101025   |
|    total_timesteps | 444000   |
---------------------------------
Progress update from timestep 444100: Updated shield, loss is 0.03886409476399422
Progress update from timestep 444200: Updated shield, loss is 0.039429981261491776
Eval num_timesteps=444200, episode_reward=10.14 +/- 13.34
Episode length: 14.60 +/- 17.79
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.6          |
|    mean_reward          | 10.1          |
| time/                   |               |
|    total_timesteps      | 444200        |
| train/                  |               |
|    approx_kl            | 0.00074193126 |
|    clip_fraction        | 0.0143        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0286       |
|    explained_variance   | 0.163         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.489         |
|    n_updates            | 11100         |
|    policy_gradient_loss | -0.00198      |
|    value_loss           | 1.29          |
-------------------------------------------
Progress update from timestep 444300: Updated shield, loss is 0.038446731865406036
Progress update from timestep 444400: Updated shield, loss is 0.029952462762594223
Eval num_timesteps=444400, episode_reward=9.73 +/- 11.83
Episode length: 15.20 +/- 17.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | 9.73     |
| time/              |          |
|    total_timesteps | 444400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.19     |
|    ep_rew_mean     | 2.55     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1111     |
|    time_elapsed    | 101106   |
|    total_timesteps | 444400   |
---------------------------------
Progress update from timestep 444500: Updated shield, loss is 0.04190453886985779
Progress update from timestep 444600: Updated shield, loss is 0.04012710601091385
Eval num_timesteps=444600, episode_reward=13.52 +/- 12.63
Episode length: 19.60 +/- 16.81
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 19.6           |
|    mean_reward          | 13.5           |
| time/                   |                |
|    total_timesteps      | 444600         |
| train/                  |                |
|    approx_kl            | -1.7059522e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.000378      |
|    explained_variance   | 0.038          |
|    learning_rate        | 0.0001         |
|    loss                 | 1.38           |
|    n_updates            | 11110          |
|    policy_gradient_loss | 8.79e-08       |
|    value_loss           | 2.06           |
--------------------------------------------
Progress update from timestep 444700: Updated shield, loss is 0.04013717547059059
Progress update from timestep 444800: Updated shield, loss is 0.03363732621073723
Eval num_timesteps=444800, episode_reward=21.37 +/- 13.46
Episode length: 30.80 +/- 18.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.8     |
|    mean_reward     | 21.4     |
| time/              |          |
|    total_timesteps | 444800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.85     |
|    ep_rew_mean     | 3.13     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1112     |
|    time_elapsed    | 101201   |
|    total_timesteps | 444800   |
---------------------------------
Progress update from timestep 444900: Updated shield, loss is 0.03208261728286743
Progress update from timestep 445000: Updated shield, loss is 0.0339726097881794
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 445000: Saved shield_loss.csv. 
Eval num_timesteps=445000, episode_reward=23.56 +/- 14.25
Episode length: 34.00 +/- 20.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34            |
|    mean_reward          | 23.6          |
| time/                   |               |
|    total_timesteps      | 445000        |
| train/                  |               |
|    approx_kl            | 0.00028844082 |
|    clip_fraction        | 0.0125        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0249       |
|    explained_variance   | 0.167         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.872         |
|    n_updates            | 11120         |
|    policy_gradient_loss | -0.00171      |
|    value_loss           | 1.89          |
-------------------------------------------
Progress update from timestep 445100: Updated shield, loss is 0.034389983862638474
Progress update from timestep 445200: Updated shield, loss is 0.03035692311823368
Eval num_timesteps=445200, episode_reward=20.69 +/- 12.24
Episode length: 30.00 +/- 16.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 20.7     |
| time/              |          |
|    total_timesteps | 445200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.36     |
|    ep_rew_mean     | 2.78     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1113     |
|    time_elapsed    | 101290   |
|    total_timesteps | 445200   |
---------------------------------
Progress update from timestep 445300: Updated shield, loss is 0.03466220945119858
Progress update from timestep 445400: Updated shield, loss is 0.03880522772669792
Eval num_timesteps=445400, episode_reward=3.62 +/- 2.39
Episode length: 6.00 +/- 3.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 6            |
|    mean_reward          | 3.62         |
| time/                   |              |
|    total_timesteps      | 445400       |
| train/                  |              |
|    approx_kl            | 0.0008958616 |
|    clip_fraction        | 0.0121       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0331      |
|    explained_variance   | 0.153        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.986        |
|    n_updates            | 11130        |
|    policy_gradient_loss | -0.00178     |
|    value_loss           | 1.77         |
------------------------------------------
Progress update from timestep 445500: Updated shield, loss is 0.03236646577715874
Progress update from timestep 445600: Updated shield, loss is 0.03989415243268013
Eval num_timesteps=445600, episode_reward=4.87 +/- 2.62
Episode length: 8.00 +/- 3.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8        |
|    mean_reward     | 4.87     |
| time/              |          |
|    total_timesteps | 445600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.1      |
|    ep_rew_mean     | 2.52     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1114     |
|    time_elapsed    | 101343   |
|    total_timesteps | 445600   |
---------------------------------
Progress update from timestep 445700: Updated shield, loss is 0.03841174766421318
Progress update from timestep 445800: Updated shield, loss is 0.05004774034023285
Eval num_timesteps=445800, episode_reward=23.68 +/- 13.92
Episode length: 34.40 +/- 19.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.4          |
|    mean_reward          | 23.7          |
| time/                   |               |
|    total_timesteps      | 445800        |
| train/                  |               |
|    approx_kl            | 0.00024895646 |
|    clip_fraction        | 0.00625       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.025        |
|    explained_variance   | 0.148         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.71          |
|    n_updates            | 11140         |
|    policy_gradient_loss | -0.000616     |
|    value_loss           | 1.51          |
-------------------------------------------
Progress update from timestep 445900: Updated shield, loss is 0.036124128848314285
Progress update from timestep 446000: Updated shield, loss is 0.03862754628062248
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 446000: Saved shield_loss.csv. 
Eval num_timesteps=446000, episode_reward=19.97 +/- 13.74
Episode length: 28.40 +/- 18.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.4     |
|    mean_reward     | 20       |
| time/              |          |
|    total_timesteps | 446000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.26     |
|    ep_rew_mean     | 2.75     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1115     |
|    time_elapsed    | 101431   |
|    total_timesteps | 446000   |
---------------------------------
Progress update from timestep 446100: Updated shield, loss is 0.03954863175749779
Progress update from timestep 446200: Updated shield, loss is 0.03914869576692581
Eval num_timesteps=446200, episode_reward=22.56 +/- 15.55
Episode length: 32.40 +/- 21.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32.4          |
|    mean_reward          | 22.6          |
| time/                   |               |
|    total_timesteps      | 446200        |
| train/                  |               |
|    approx_kl            | 0.00016237401 |
|    clip_fraction        | 0.00804       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0164       |
|    explained_variance   | 0.153         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.14          |
|    n_updates            | 11150         |
|    policy_gradient_loss | -0.000681     |
|    value_loss           | 2.14          |
-------------------------------------------
Progress update from timestep 446300: Updated shield, loss is 0.03222557529807091
Progress update from timestep 446400: Updated shield, loss is 0.028379056602716446
Eval num_timesteps=446400, episode_reward=28.55 +/- 11.86
Episode length: 41.60 +/- 16.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 28.6     |
| time/              |          |
|    total_timesteps | 446400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.02     |
|    ep_rew_mean     | 2.53     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1116     |
|    time_elapsed    | 101517   |
|    total_timesteps | 446400   |
---------------------------------
Progress update from timestep 446500: Updated shield, loss is 0.027733495458960533
Progress update from timestep 446600: Updated shield, loss is 0.03955652192234993
Eval num_timesteps=446600, episode_reward=12.43 +/- 12.48
Episode length: 18.20 +/- 17.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 18.2          |
|    mean_reward          | 12.4          |
| time/                   |               |
|    total_timesteps      | 446600        |
| train/                  |               |
|    approx_kl            | 0.00038790065 |
|    clip_fraction        | 0.0112        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0196       |
|    explained_variance   | 0.0943        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.26          |
|    n_updates            | 11160         |
|    policy_gradient_loss | -0.00127      |
|    value_loss           | 1.88          |
-------------------------------------------
Progress update from timestep 446700: Updated shield, loss is 0.02792191505432129
Progress update from timestep 446800: Updated shield, loss is 0.03577647730708122
Eval num_timesteps=446800, episode_reward=12.21 +/- 12.13
Episode length: 18.40 +/- 17.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.4     |
|    mean_reward     | 12.2     |
| time/              |          |
|    total_timesteps | 446800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.52     |
|    ep_rew_mean     | 2.09     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1117     |
|    time_elapsed    | 101600   |
|    total_timesteps | 446800   |
---------------------------------
Progress update from timestep 446900: Updated shield, loss is 0.03523306921124458
Progress update from timestep 447000: Updated shield, loss is 0.03499716520309448
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 447000: Saved shield_loss.csv. 
Eval num_timesteps=447000, episode_reward=13.23 +/- 11.97
Episode length: 19.00 +/- 15.85
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 19            |
|    mean_reward          | 13.2          |
| time/                   |               |
|    total_timesteps      | 447000        |
| train/                  |               |
|    approx_kl            | 0.00014198803 |
|    clip_fraction        | 0.00737       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0196       |
|    explained_variance   | 0.271         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.38          |
|    n_updates            | 11170         |
|    policy_gradient_loss | -0.00149      |
|    value_loss           | 1.04          |
-------------------------------------------
Progress update from timestep 447100: Updated shield, loss is 0.04143591225147247
Progress update from timestep 447200: Updated shield, loss is 0.0274957325309515
Eval num_timesteps=447200, episode_reward=11.56 +/- 12.30
Episode length: 17.00 +/- 16.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | 11.6     |
| time/              |          |
|    total_timesteps | 447200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.35     |
|    ep_rew_mean     | 2.81     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1118     |
|    time_elapsed    | 101683   |
|    total_timesteps | 447200   |
---------------------------------
Progress update from timestep 447300: Updated shield, loss is 0.0321967713534832
Progress update from timestep 447400: Updated shield, loss is 0.03220941498875618
Eval num_timesteps=447400, episode_reward=26.27 +/- 11.02
Episode length: 38.00 +/- 15.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 38           |
|    mean_reward          | 26.3         |
| time/                   |              |
|    total_timesteps      | 447400       |
| train/                  |              |
|    approx_kl            | 0.0002075359 |
|    clip_fraction        | 0.00603      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0357      |
|    explained_variance   | 0.148        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.624        |
|    n_updates            | 11180        |
|    policy_gradient_loss | -0.000566    |
|    value_loss           | 1.91         |
------------------------------------------
Progress update from timestep 447500: Updated shield, loss is 0.03410413861274719
Progress update from timestep 447600: Updated shield, loss is 0.043719056993722916
Eval num_timesteps=447600, episode_reward=5.70 +/- 4.40
Episode length: 8.80 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8.8      |
|    mean_reward     | 5.7      |
| time/              |          |
|    total_timesteps | 447600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.23     |
|    ep_rew_mean     | 2.71     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1119     |
|    time_elapsed    | 101773   |
|    total_timesteps | 447600   |
---------------------------------
Progress update from timestep 447700: Updated shield, loss is 0.027887029573321342
Progress update from timestep 447800: Updated shield, loss is 0.04077234864234924
Eval num_timesteps=447800, episode_reward=15.11 +/- 15.91
Episode length: 22.40 +/- 22.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.4          |
|    mean_reward          | 15.1          |
| time/                   |               |
|    total_timesteps      | 447800        |
| train/                  |               |
|    approx_kl            | 0.00022591479 |
|    clip_fraction        | 0.00536       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0222       |
|    explained_variance   | 0.238         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.768         |
|    n_updates            | 11190         |
|    policy_gradient_loss | -0.000554     |
|    value_loss           | 1.31          |
-------------------------------------------
Progress update from timestep 447900: Updated shield, loss is 0.03749735280871391
Progress update from timestep 448000: Updated shield, loss is 0.042874861508607864
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 448000: Saved shield_loss.csv. 
Eval num_timesteps=448000, episode_reward=27.74 +/- 13.48
Episode length: 40.40 +/- 19.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.4     |
|    mean_reward     | 27.7     |
| time/              |          |
|    total_timesteps | 448000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.2      |
|    ep_rew_mean     | 2.72     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1120     |
|    time_elapsed    | 101877   |
|    total_timesteps | 448000   |
---------------------------------
Progress update from timestep 448100: Updated shield, loss is 0.036567628383636475
Progress update from timestep 448200: Updated shield, loss is 0.03587029129266739
Eval num_timesteps=448200, episode_reward=10.44 +/- 11.96
Episode length: 16.20 +/- 17.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.2          |
|    mean_reward          | 10.4          |
| time/                   |               |
|    total_timesteps      | 448200        |
| train/                  |               |
|    approx_kl            | 0.00026820303 |
|    clip_fraction        | 0.00603       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0137       |
|    explained_variance   | 0.262         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.946         |
|    n_updates            | 11200         |
|    policy_gradient_loss | -0.00129      |
|    value_loss           | 1.36          |
-------------------------------------------
Progress update from timestep 448300: Updated shield, loss is 0.03664015978574753
Progress update from timestep 448400: Updated shield, loss is 0.03470613807439804
Eval num_timesteps=448400, episode_reward=22.17 +/- 16.01
Episode length: 31.80 +/- 22.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.8     |
|    mean_reward     | 22.2     |
| time/              |          |
|    total_timesteps | 448400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.08     |
|    ep_rew_mean     | 2.65     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1121     |
|    time_elapsed    | 101958   |
|    total_timesteps | 448400   |
---------------------------------
Progress update from timestep 448500: Updated shield, loss is 0.03788747265934944
Progress update from timestep 448600: Updated shield, loss is 0.044571928679943085
Eval num_timesteps=448600, episode_reward=16.07 +/- 15.95
Episode length: 23.20 +/- 21.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.2         |
|    mean_reward          | 16.1         |
| time/                   |              |
|    total_timesteps      | 448600       |
| train/                  |              |
|    approx_kl            | 0.0019408565 |
|    clip_fraction        | 0.0129       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0301      |
|    explained_variance   | 0.0855       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.974        |
|    n_updates            | 11210        |
|    policy_gradient_loss | -0.00123     |
|    value_loss           | 2.15         |
------------------------------------------
Progress update from timestep 448700: Updated shield, loss is 0.033861298114061356
Progress update from timestep 448800: Updated shield, loss is 0.0418686605989933
Eval num_timesteps=448800, episode_reward=3.61 +/- 2.71
Episode length: 6.00 +/- 3.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6        |
|    mean_reward     | 3.61     |
| time/              |          |
|    total_timesteps | 448800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.79     |
|    ep_rew_mean     | 2.37     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1122     |
|    time_elapsed    | 102025   |
|    total_timesteps | 448800   |
---------------------------------
Progress update from timestep 448900: Updated shield, loss is 0.030639050528407097
Progress update from timestep 449000: Updated shield, loss is 0.03460056334733963
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 449000: Saved shield_loss.csv. 
Eval num_timesteps=449000, episode_reward=22.01 +/- 17.09
Episode length: 31.00 +/- 23.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 31           |
|    mean_reward          | 22           |
| time/                   |              |
|    total_timesteps      | 449000       |
| train/                  |              |
|    approx_kl            | 0.0001468014 |
|    clip_fraction        | 0.00692      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0262      |
|    explained_variance   | 0.23         |
|    learning_rate        | 0.0001       |
|    loss                 | 1.04         |
|    n_updates            | 11220        |
|    policy_gradient_loss | -0.000692    |
|    value_loss           | 1.93         |
------------------------------------------
Progress update from timestep 449100: Updated shield, loss is 0.03275669738650322
Progress update from timestep 449200: Updated shield, loss is 0.028951803222298622
Eval num_timesteps=449200, episode_reward=22.80 +/- 15.40
Episode length: 32.80 +/- 21.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 22.8     |
| time/              |          |
|    total_timesteps | 449200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.23     |
|    ep_rew_mean     | 2.69     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1123     |
|    time_elapsed    | 102111   |
|    total_timesteps | 449200   |
---------------------------------
Progress update from timestep 449300: Updated shield, loss is 0.0461055226624012
Progress update from timestep 449400: Updated shield, loss is 0.031871821731328964
Eval num_timesteps=449400, episode_reward=18.11 +/- 14.39
Episode length: 26.00 +/- 19.73
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26            |
|    mean_reward          | 18.1          |
| time/                   |               |
|    total_timesteps      | 449400        |
| train/                  |               |
|    approx_kl            | 6.0681254e-05 |
|    clip_fraction        | 0.00246       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00585      |
|    explained_variance   | 0.341         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.25          |
|    n_updates            | 11230         |
|    policy_gradient_loss | -0.000524     |
|    value_loss           | 2.13          |
-------------------------------------------
Progress update from timestep 449500: Updated shield, loss is 0.03133745864033699
Progress update from timestep 449600: Updated shield, loss is 0.035982657223939896
Eval num_timesteps=449600, episode_reward=17.39 +/- 15.15
Episode length: 25.00 +/- 20.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 449600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.38     |
|    ep_rew_mean     | 2.83     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1124     |
|    time_elapsed    | 102197   |
|    total_timesteps | 449600   |
---------------------------------
Progress update from timestep 449700: Updated shield, loss is 0.02988455258309841
Progress update from timestep 449800: Updated shield, loss is 0.030100159347057343
Eval num_timesteps=449800, episode_reward=13.40 +/- 10.53
Episode length: 20.40 +/- 15.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 20.4          |
|    mean_reward          | 13.4          |
| time/                   |               |
|    total_timesteps      | 449800        |
| train/                  |               |
|    approx_kl            | 0.00017401062 |
|    clip_fraction        | 0.00513       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0127       |
|    explained_variance   | 0.102         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.405         |
|    n_updates            | 11240         |
|    policy_gradient_loss | -0.000421     |
|    value_loss           | 1.65          |
-------------------------------------------
Progress update from timestep 449900: Updated shield, loss is 0.027338869869709015
Progress update from timestep 450000: Updated shield, loss is 0.029737455770373344
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 450000: Saved shield_loss.csv. 
Eval num_timesteps=450000, episode_reward=11.93 +/- 12.38
Episode length: 17.40 +/- 17.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 11.9     |
| time/              |          |
|    total_timesteps | 450000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.36     |
|    ep_rew_mean     | 2.77     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1125     |
|    time_elapsed    | 102283   |
|    total_timesteps | 450000   |
---------------------------------
Progress update from timestep 450100: Updated shield, loss is 0.03453657403588295
Progress update from timestep 450200: Updated shield, loss is 0.03819267451763153
Eval num_timesteps=450200, episode_reward=29.09 +/- 11.92
Episode length: 41.80 +/- 16.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 41.8          |
|    mean_reward          | 29.1          |
| time/                   |               |
|    total_timesteps      | 450200        |
| train/                  |               |
|    approx_kl            | 0.00014824208 |
|    clip_fraction        | 0.00469       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0103       |
|    explained_variance   | 0.357         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.562         |
|    n_updates            | 11250         |
|    policy_gradient_loss | -0.000278     |
|    value_loss           | 1.31          |
-------------------------------------------
Progress update from timestep 450300: Updated shield, loss is 0.03603649511933327
Progress update from timestep 450400: Updated shield, loss is 0.03357783704996109
Eval num_timesteps=450400, episode_reward=11.73 +/- 10.92
Episode length: 18.20 +/- 16.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | 11.7     |
| time/              |          |
|    total_timesteps | 450400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.44     |
|    ep_rew_mean     | 2.83     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1126     |
|    time_elapsed    | 102383   |
|    total_timesteps | 450400   |
---------------------------------
Progress update from timestep 450500: Updated shield, loss is 0.03195096179842949
Progress update from timestep 450600: Updated shield, loss is 0.029925301671028137
Eval num_timesteps=450600, episode_reward=22.91 +/- 15.20
Episode length: 33.00 +/- 21.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33            |
|    mean_reward          | 22.9          |
| time/                   |               |
|    total_timesteps      | 450600        |
| train/                  |               |
|    approx_kl            | 0.00014694444 |
|    clip_fraction        | 0.00357       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00683      |
|    explained_variance   | 0.0813        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.24          |
|    n_updates            | 11260         |
|    policy_gradient_loss | -0.000313     |
|    value_loss           | 1.77          |
-------------------------------------------
Progress update from timestep 450700: Updated shield, loss is 0.03976137936115265
Progress update from timestep 450800: Updated shield, loss is 0.043295204639434814
Eval num_timesteps=450800, episode_reward=15.12 +/- 16.72
Episode length: 21.80 +/- 23.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | 15.1     |
| time/              |          |
|    total_timesteps | 450800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.06     |
|    ep_rew_mean     | 2.54     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1127     |
|    time_elapsed    | 102466   |
|    total_timesteps | 450800   |
---------------------------------
Progress update from timestep 450900: Updated shield, loss is 0.03744762763381004
Progress update from timestep 451000: Updated shield, loss is 0.037499990314245224
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 451000: Saved shield_loss.csv. 
Eval num_timesteps=451000, episode_reward=5.48 +/- 3.37
Episode length: 8.40 +/- 4.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 8.4           |
|    mean_reward          | 5.48          |
| time/                   |               |
|    total_timesteps      | 451000        |
| train/                  |               |
|    approx_kl            | 0.00021980987 |
|    clip_fraction        | 0.00647       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0177       |
|    explained_variance   | 0.163         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.503         |
|    n_updates            | 11270         |
|    policy_gradient_loss | -0.000924     |
|    value_loss           | 1.35          |
-------------------------------------------
Progress update from timestep 451100: Updated shield, loss is 0.03462325781583786
Progress update from timestep 451200: Updated shield, loss is 0.03258531540632248
Eval num_timesteps=451200, episode_reward=12.22 +/- 12.80
Episode length: 17.60 +/- 17.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | 12.2     |
| time/              |          |
|    total_timesteps | 451200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.3      |
|    ep_rew_mean     | 2.7      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1128     |
|    time_elapsed    | 102535   |
|    total_timesteps | 451200   |
---------------------------------
Progress update from timestep 451300: Updated shield, loss is 0.04025759547948837
Progress update from timestep 451400: Updated shield, loss is 0.04511266201734543
Eval num_timesteps=451400, episode_reward=10.25 +/- 12.41
Episode length: 15.60 +/- 17.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.6          |
|    mean_reward          | 10.3          |
| time/                   |               |
|    total_timesteps      | 451400        |
| train/                  |               |
|    approx_kl            | 0.00026755195 |
|    clip_fraction        | 0.00625       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0349       |
|    explained_variance   | 0.282         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.631         |
|    n_updates            | 11280         |
|    policy_gradient_loss | -0.00116      |
|    value_loss           | 2.2           |
-------------------------------------------
Progress update from timestep 451500: Updated shield, loss is 0.037988606840372086
Progress update from timestep 451600: Updated shield, loss is 0.04483766853809357
Eval num_timesteps=451600, episode_reward=28.94 +/- 12.77
Episode length: 41.20 +/- 17.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | 28.9     |
| time/              |          |
|    total_timesteps | 451600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.41     |
|    ep_rew_mean     | 2.78     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1129     |
|    time_elapsed    | 102636   |
|    total_timesteps | 451600   |
---------------------------------
Progress update from timestep 451700: Updated shield, loss is 0.032993946224451065
Progress update from timestep 451800: Updated shield, loss is 0.03669873997569084
Eval num_timesteps=451800, episode_reward=13.20 +/- 12.15
Episode length: 19.20 +/- 16.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 19.2          |
|    mean_reward          | 13.2          |
| time/                   |               |
|    total_timesteps      | 451800        |
| train/                  |               |
|    approx_kl            | 0.00024846307 |
|    clip_fraction        | 0.0067        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0246       |
|    explained_variance   | 0.248         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.4           |
|    n_updates            | 11290         |
|    policy_gradient_loss | -0.00104      |
|    value_loss           | 1.64          |
-------------------------------------------
Progress update from timestep 451900: Updated shield, loss is 0.04583171010017395
Progress update from timestep 452000: Updated shield, loss is 0.04065081849694252
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 452000: Saved shield_loss.csv. 
Eval num_timesteps=452000, episode_reward=21.96 +/- 13.97
Episode length: 33.20 +/- 20.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | 22       |
| time/              |          |
|    total_timesteps | 452000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.14     |
|    ep_rew_mean     | 2.58     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1130     |
|    time_elapsed    | 102729   |
|    total_timesteps | 452000   |
---------------------------------
Progress update from timestep 452100: Updated shield, loss is 0.03888789564371109
Progress update from timestep 452200: Updated shield, loss is 0.041378043591976166
Eval num_timesteps=452200, episode_reward=11.20 +/- 12.80
Episode length: 16.20 +/- 16.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.2          |
|    mean_reward          | 11.2          |
| time/                   |               |
|    total_timesteps      | 452200        |
| train/                  |               |
|    approx_kl            | 0.00021820025 |
|    clip_fraction        | 0.00826       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0434       |
|    explained_variance   | 0.0807        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.619         |
|    n_updates            | 11300         |
|    policy_gradient_loss | -0.00124      |
|    value_loss           | 1.85          |
-------------------------------------------
Progress update from timestep 452300: Updated shield, loss is 0.044088173657655716
Progress update from timestep 452400: Updated shield, loss is 0.037147652357816696
Eval num_timesteps=452400, episode_reward=18.29 +/- 13.48
Episode length: 27.00 +/- 19.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27       |
|    mean_reward     | 18.3     |
| time/              |          |
|    total_timesteps | 452400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.06     |
|    ep_rew_mean     | 2.53     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1131     |
|    time_elapsed    | 102816   |
|    total_timesteps | 452400   |
---------------------------------
Progress update from timestep 452500: Updated shield, loss is 0.03468022495508194
Progress update from timestep 452600: Updated shield, loss is 0.04068884998559952
Eval num_timesteps=452600, episode_reward=9.77 +/- 12.54
Episode length: 15.00 +/- 17.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15            |
|    mean_reward          | 9.77          |
| time/                   |               |
|    total_timesteps      | 452600        |
| train/                  |               |
|    approx_kl            | 0.00023469664 |
|    clip_fraction        | 0.00871       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.017        |
|    explained_variance   | 0.00594       |
|    learning_rate        | 0.0001        |
|    loss                 | 0.635         |
|    n_updates            | 11310         |
|    policy_gradient_loss | -0.000922     |
|    value_loss           | 2.32          |
-------------------------------------------
Progress update from timestep 452700: Updated shield, loss is 0.03446882963180542
Progress update from timestep 452800: Updated shield, loss is 0.035903967916965485
Eval num_timesteps=452800, episode_reward=17.30 +/- 14.95
Episode length: 24.80 +/- 20.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 17.3     |
| time/              |          |
|    total_timesteps | 452800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.63     |
|    ep_rew_mean     | 2.96     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1132     |
|    time_elapsed    | 102901   |
|    total_timesteps | 452800   |
---------------------------------
Progress update from timestep 452900: Updated shield, loss is 0.03585047647356987
Progress update from timestep 453000: Updated shield, loss is 0.04783695936203003
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 453000: Saved shield_loss.csv. 
Eval num_timesteps=453000, episode_reward=19.57 +/- 12.89
Episode length: 28.60 +/- 18.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 28.6         |
|    mean_reward          | 19.6         |
| time/                   |              |
|    total_timesteps      | 453000       |
| train/                  |              |
|    approx_kl            | 9.764507e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00943     |
|    explained_variance   | 0.244        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.451        |
|    n_updates            | 11320        |
|    policy_gradient_loss | -0.000163    |
|    value_loss           | 1.31         |
------------------------------------------
Progress update from timestep 453100: Updated shield, loss is 0.039754852652549744
Progress update from timestep 453200: Updated shield, loss is 0.03828498721122742
Eval num_timesteps=453200, episode_reward=17.58 +/- 14.96
Episode length: 25.20 +/- 20.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 453200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.61     |
|    ep_rew_mean     | 2.96     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1133     |
|    time_elapsed    | 102990   |
|    total_timesteps | 453200   |
---------------------------------
Progress update from timestep 453300: Updated shield, loss is 0.03791239485144615
Progress update from timestep 453400: Updated shield, loss is 0.041260261088609695
Eval num_timesteps=453400, episode_reward=10.14 +/- 11.62
Episode length: 15.60 +/- 17.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.6          |
|    mean_reward          | 10.1          |
| time/                   |               |
|    total_timesteps      | 453400        |
| train/                  |               |
|    approx_kl            | 0.00048712757 |
|    clip_fraction        | 0.0107        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0269       |
|    explained_variance   | 0.18          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.688         |
|    n_updates            | 11330         |
|    policy_gradient_loss | -0.0013       |
|    value_loss           | 1.91          |
-------------------------------------------
Progress update from timestep 453500: Updated shield, loss is 0.04283146560192108
Progress update from timestep 453600: Updated shield, loss is 0.03237699344754219
Eval num_timesteps=453600, episode_reward=16.15 +/- 16.37
Episode length: 23.00 +/- 22.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 16.1     |
| time/              |          |
|    total_timesteps | 453600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.24     |
|    ep_rew_mean     | 2.64     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1134     |
|    time_elapsed    | 103072   |
|    total_timesteps | 453600   |
---------------------------------
Progress update from timestep 453700: Updated shield, loss is 0.038741059601306915
Progress update from timestep 453800: Updated shield, loss is 0.03985755145549774
Eval num_timesteps=453800, episode_reward=23.49 +/- 13.94
Episode length: 34.00 +/- 19.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34            |
|    mean_reward          | 23.5          |
| time/                   |               |
|    total_timesteps      | 453800        |
| train/                  |               |
|    approx_kl            | 1.6922853e-05 |
|    clip_fraction        | 0.000893      |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00451      |
|    explained_variance   | 0.285         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.998         |
|    n_updates            | 11340         |
|    policy_gradient_loss | -0.000365     |
|    value_loss           | 1.85          |
-------------------------------------------
Progress update from timestep 453900: Updated shield, loss is 0.03559780865907669
Progress update from timestep 454000: Updated shield, loss is 0.03745587542653084
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 454000: Saved shield_loss.csv. 
Eval num_timesteps=454000, episode_reward=17.06 +/- 10.34
Episode length: 25.40 +/- 14.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | 17.1     |
| time/              |          |
|    total_timesteps | 454000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.08     |
|    ep_rew_mean     | 2.54     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1135     |
|    time_elapsed    | 103158   |
|    total_timesteps | 454000   |
---------------------------------
Progress update from timestep 454100: Updated shield, loss is 0.03693520277738571
Progress update from timestep 454200: Updated shield, loss is 0.046756308525800705
Eval num_timesteps=454200, episode_reward=19.20 +/- 12.70
Episode length: 28.40 +/- 17.98
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 28.4          |
|    mean_reward          | 19.2          |
| time/                   |               |
|    total_timesteps      | 454200        |
| train/                  |               |
|    approx_kl            | 0.00032386277 |
|    clip_fraction        | 0.00357       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0141       |
|    explained_variance   | 0.121         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.564         |
|    n_updates            | 11350         |
|    policy_gradient_loss | -0.000553     |
|    value_loss           | 1.93          |
-------------------------------------------
Progress update from timestep 454300: Updated shield, loss is 0.04082491248846054
Progress update from timestep 454400: Updated shield, loss is 0.036458756774663925
Eval num_timesteps=454400, episode_reward=19.21 +/- 13.80
Episode length: 28.20 +/- 19.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.2     |
|    mean_reward     | 19.2     |
| time/              |          |
|    total_timesteps | 454400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.77     |
|    ep_rew_mean     | 2.25     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1136     |
|    time_elapsed    | 103242   |
|    total_timesteps | 454400   |
---------------------------------
Progress update from timestep 454500: Updated shield, loss is 0.03291682153940201
Progress update from timestep 454600: Updated shield, loss is 0.03106050379574299
Eval num_timesteps=454600, episode_reward=4.11 +/- 1.93
Episode length: 6.60 +/- 2.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 6.6         |
|    mean_reward          | 4.11        |
| time/                   |             |
|    total_timesteps      | 454600      |
| train/                  |             |
|    approx_kl            | 0.000691515 |
|    clip_fraction        | 0.0138      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0246     |
|    explained_variance   | 0.149       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.401       |
|    n_updates            | 11360       |
|    policy_gradient_loss | -0.00137    |
|    value_loss           | 1.52        |
-----------------------------------------
Progress update from timestep 454700: Updated shield, loss is 0.03751321882009506
Progress update from timestep 454800: Updated shield, loss is 0.03516656532883644
Eval num_timesteps=454800, episode_reward=17.42 +/- 15.27
Episode length: 25.20 +/- 20.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 454800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.69     |
|    ep_rew_mean     | 3        |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1137     |
|    time_elapsed    | 103306   |
|    total_timesteps | 454800   |
---------------------------------
Progress update from timestep 454900: Updated shield, loss is 0.042365506291389465
Progress update from timestep 455000: Updated shield, loss is 0.04098154604434967
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 455000: Saved shield_loss.csv. 
Eval num_timesteps=455000, episode_reward=21.63 +/- 15.31
Episode length: 32.00 +/- 22.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32            |
|    mean_reward          | 21.6          |
| time/                   |               |
|    total_timesteps      | 455000        |
| train/                  |               |
|    approx_kl            | 0.00020622948 |
|    clip_fraction        | 0.00848       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0184       |
|    explained_variance   | 0.29          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.373         |
|    n_updates            | 11370         |
|    policy_gradient_loss | -0.000838     |
|    value_loss           | 1.76          |
-------------------------------------------
Progress update from timestep 455100: Updated shield, loss is 0.03555987402796745
Progress update from timestep 455200: Updated shield, loss is 0.033369820564985275
Eval num_timesteps=455200, episode_reward=27.54 +/- 12.77
Episode length: 40.80 +/- 18.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.8     |
|    mean_reward     | 27.5     |
| time/              |          |
|    total_timesteps | 455200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.1      |
|    ep_rew_mean     | 2.6      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1138     |
|    time_elapsed    | 103408   |
|    total_timesteps | 455200   |
---------------------------------
Progress update from timestep 455300: Updated shield, loss is 0.038600459694862366
Progress update from timestep 455400: Updated shield, loss is 0.03651929274201393
Eval num_timesteps=455400, episode_reward=2.95 +/- 1.84
Episode length: 5.20 +/- 2.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 5.2          |
|    mean_reward          | 2.95         |
| time/                   |              |
|    total_timesteps      | 455400       |
| train/                  |              |
|    approx_kl            | 1.738955e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00706     |
|    explained_variance   | 0.28         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.851        |
|    n_updates            | 11380        |
|    policy_gradient_loss | -0.000101    |
|    value_loss           | 1.91         |
------------------------------------------
Progress update from timestep 455500: Updated shield, loss is 0.03201891481876373
Progress update from timestep 455600: Updated shield, loss is 0.03419525921344757
Eval num_timesteps=455600, episode_reward=31.32 +/- 9.18
Episode length: 44.00 +/- 12.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44       |
|    mean_reward     | 31.3     |
| time/              |          |
|    total_timesteps | 455600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.71     |
|    ep_rew_mean     | 3.01     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1139     |
|    time_elapsed    | 103483   |
|    total_timesteps | 455600   |
---------------------------------
Progress update from timestep 455700: Updated shield, loss is 0.02952524647116661
Progress update from timestep 455800: Updated shield, loss is 0.031550850719213486
Eval num_timesteps=455800, episode_reward=26.36 +/- 13.59
Episode length: 36.80 +/- 17.86
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.8          |
|    mean_reward          | 26.4          |
| time/                   |               |
|    total_timesteps      | 455800        |
| train/                  |               |
|    approx_kl            | 0.00015742864 |
|    clip_fraction        | 0.00179       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00657      |
|    explained_variance   | 0.323         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.15          |
|    n_updates            | 11390         |
|    policy_gradient_loss | -0.000314     |
|    value_loss           | 1.94          |
-------------------------------------------
Progress update from timestep 455900: Updated shield, loss is 0.03512538969516754
Progress update from timestep 456000: Updated shield, loss is 0.031271763145923615
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 456000: Saved shield_loss.csv. 
Eval num_timesteps=456000, episode_reward=11.41 +/- 13.21
Episode length: 17.40 +/- 18.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 11.4     |
| time/              |          |
|    total_timesteps | 456000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.59     |
|    ep_rew_mean     | 2.95     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1140     |
|    time_elapsed    | 103586   |
|    total_timesteps | 456000   |
---------------------------------
Progress update from timestep 456100: Updated shield, loss is 0.03471348062157631
Progress update from timestep 456200: Updated shield, loss is 0.03237617388367653
Eval num_timesteps=456200, episode_reward=16.23 +/- 16.39
Episode length: 23.00 +/- 22.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23           |
|    mean_reward          | 16.2         |
| time/                   |              |
|    total_timesteps      | 456200       |
| train/                  |              |
|    approx_kl            | 4.035679e-05 |
|    clip_fraction        | 0.00134      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00408     |
|    explained_variance   | 0.208        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.34         |
|    n_updates            | 11400        |
|    policy_gradient_loss | -0.000244    |
|    value_loss           | 2.07         |
------------------------------------------
Progress update from timestep 456300: Updated shield, loss is 0.025009168311953545
Progress update from timestep 456400: Updated shield, loss is 0.041829753667116165
Eval num_timesteps=456400, episode_reward=18.41 +/- 14.93
Episode length: 26.20 +/- 20.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | 18.4     |
| time/              |          |
|    total_timesteps | 456400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.35     |
|    ep_rew_mean     | 2.77     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1141     |
|    time_elapsed    | 103706   |
|    total_timesteps | 456400   |
---------------------------------
Progress update from timestep 456500: Updated shield, loss is 0.025027088820934296
Progress update from timestep 456600: Updated shield, loss is 0.029106570407748222
Eval num_timesteps=456600, episode_reward=19.09 +/- 13.06
Episode length: 27.80 +/- 18.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 27.8          |
|    mean_reward          | 19.1          |
| time/                   |               |
|    total_timesteps      | 456600        |
| train/                  |               |
|    approx_kl            | -2.980869e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.000568     |
|    explained_variance   | 0.195         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.492         |
|    n_updates            | 11410         |
|    policy_gradient_loss | -9.75e-08     |
|    value_loss           | 1.67          |
-------------------------------------------
Progress update from timestep 456700: Updated shield, loss is 0.029864991083741188
Progress update from timestep 456800: Updated shield, loss is 0.034058187156915665
Eval num_timesteps=456800, episode_reward=16.68 +/- 14.89
Episode length: 24.80 +/- 21.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 456800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.95     |
|    ep_rew_mean     | 2.43     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1142     |
|    time_elapsed    | 103809   |
|    total_timesteps | 456800   |
---------------------------------
Progress update from timestep 456900: Updated shield, loss is 0.030560050159692764
Progress update from timestep 457000: Updated shield, loss is 0.03562123700976372
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 457000: Saved shield_loss.csv. 
Eval num_timesteps=457000, episode_reward=18.00 +/- 13.14
Episode length: 27.20 +/- 19.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 27.2         |
|    mean_reward          | 18           |
| time/                   |              |
|    total_timesteps      | 457000       |
| train/                  |              |
|    approx_kl            | 0.0017188095 |
|    clip_fraction        | 0.0109       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0148      |
|    explained_variance   | 0.184        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.605        |
|    n_updates            | 11420        |
|    policy_gradient_loss | -0.00161     |
|    value_loss           | 1.55         |
------------------------------------------
Progress update from timestep 457100: Updated shield, loss is 0.04573965445160866
Progress update from timestep 457200: Updated shield, loss is 0.03600494563579559
Eval num_timesteps=457200, episode_reward=22.30 +/- 15.82
Episode length: 32.00 +/- 22.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32       |
|    mean_reward     | 22.3     |
| time/              |          |
|    total_timesteps | 457200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.55     |
|    ep_rew_mean     | 2.87     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1143     |
|    time_elapsed    | 103891   |
|    total_timesteps | 457200   |
---------------------------------
Progress update from timestep 457300: Updated shield, loss is 0.031626440584659576
Progress update from timestep 457400: Updated shield, loss is 0.04824705794453621
Eval num_timesteps=457400, episode_reward=9.77 +/- 13.50
Episode length: 14.20 +/- 17.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.2         |
|    mean_reward          | 9.77         |
| time/                   |              |
|    total_timesteps      | 457400       |
| train/                  |              |
|    approx_kl            | 7.096905e-05 |
|    clip_fraction        | 0.00558      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00507     |
|    explained_variance   | 0.186        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.57         |
|    n_updates            | 11430        |
|    policy_gradient_loss | -0.000619    |
|    value_loss           | 2.23         |
------------------------------------------
Progress update from timestep 457500: Updated shield, loss is 0.03962957486510277
Progress update from timestep 457600: Updated shield, loss is 0.03435727581381798
Eval num_timesteps=457600, episode_reward=10.68 +/- 12.51
Episode length: 15.80 +/- 17.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | 10.7     |
| time/              |          |
|    total_timesteps | 457600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.73     |
|    ep_rew_mean     | 2.97     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1144     |
|    time_elapsed    | 103975   |
|    total_timesteps | 457600   |
---------------------------------
Progress update from timestep 457700: Updated shield, loss is 0.028339948505163193
Progress update from timestep 457800: Updated shield, loss is 0.033293675631284714
Eval num_timesteps=457800, episode_reward=8.86 +/- 12.84
Episode length: 13.60 +/- 18.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 13.6          |
|    mean_reward          | 8.86          |
| time/                   |               |
|    total_timesteps      | 457800        |
| train/                  |               |
|    approx_kl            | 0.00015978418 |
|    clip_fraction        | 0.00313       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00981      |
|    explained_variance   | 0.121         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.962         |
|    n_updates            | 11440         |
|    policy_gradient_loss | -0.000419     |
|    value_loss           | 1.95          |
-------------------------------------------
Progress update from timestep 457900: Updated shield, loss is 0.04305703938007355
Progress update from timestep 458000: Updated shield, loss is 0.028624851256608963
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 458000: Saved shield_loss.csv. 
Eval num_timesteps=458000, episode_reward=23.40 +/- 14.95
Episode length: 33.20 +/- 20.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | 23.4     |
| time/              |          |
|    total_timesteps | 458000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.75     |
|    ep_rew_mean     | 2.3      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1145     |
|    time_elapsed    | 104059   |
|    total_timesteps | 458000   |
---------------------------------
Progress update from timestep 458100: Updated shield, loss is 0.03740845248103142
Progress update from timestep 458200: Updated shield, loss is 0.04059503972530365
Eval num_timesteps=458200, episode_reward=14.05 +/- 11.26
Episode length: 21.00 +/- 15.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 21            |
|    mean_reward          | 14.1          |
| time/                   |               |
|    total_timesteps      | 458200        |
| train/                  |               |
|    approx_kl            | 0.00028664805 |
|    clip_fraction        | 0.00379       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0106       |
|    explained_variance   | 0.205         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.866         |
|    n_updates            | 11450         |
|    policy_gradient_loss | -0.000699     |
|    value_loss           | 1.32          |
-------------------------------------------
Progress update from timestep 458300: Updated shield, loss is 0.03195377439260483
Progress update from timestep 458400: Updated shield, loss is 0.04111986979842186
Eval num_timesteps=458400, episode_reward=24.54 +/- 13.46
Episode length: 35.00 +/- 18.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 24.5     |
| time/              |          |
|    total_timesteps | 458400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.35     |
|    ep_rew_mean     | 2.8      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1146     |
|    time_elapsed    | 104160   |
|    total_timesteps | 458400   |
---------------------------------
Progress update from timestep 458500: Updated shield, loss is 0.03760414198040962
Progress update from timestep 458600: Updated shield, loss is 0.04547392949461937
Eval num_timesteps=458600, episode_reward=4.13 +/- 4.01
Episode length: 6.60 +/- 5.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 6.6           |
|    mean_reward          | 4.13          |
| time/                   |               |
|    total_timesteps      | 458600        |
| train/                  |               |
|    approx_kl            | 1.6343786e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00461      |
|    explained_variance   | 0.207         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.606         |
|    n_updates            | 11460         |
|    policy_gradient_loss | -0.000118     |
|    value_loss           | 1.78          |
-------------------------------------------
Progress update from timestep 458700: Updated shield, loss is 0.03950759023427963
Progress update from timestep 458800: Updated shield, loss is 0.03637896850705147
Eval num_timesteps=458800, episode_reward=16.02 +/- 15.19
Episode length: 23.60 +/- 21.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 458800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.8      |
|    ep_rew_mean     | 2.29     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1147     |
|    time_elapsed    | 104228   |
|    total_timesteps | 458800   |
---------------------------------
Progress update from timestep 458900: Updated shield, loss is 0.03945162892341614
Progress update from timestep 459000: Updated shield, loss is 0.03747188299894333
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 459000: Saved shield_loss.csv. 
Eval num_timesteps=459000, episode_reward=29.85 +/- 11.48
Episode length: 42.20 +/- 15.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.2          |
|    mean_reward          | 29.9          |
| time/                   |               |
|    total_timesteps      | 459000        |
| train/                  |               |
|    approx_kl            | 0.00022583439 |
|    clip_fraction        | 0.0107        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0268       |
|    explained_variance   | 0.207         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.17          |
|    n_updates            | 11470         |
|    policy_gradient_loss | -0.00151      |
|    value_loss           | 1.56          |
-------------------------------------------
Progress update from timestep 459100: Updated shield, loss is 0.03319646045565605
Progress update from timestep 459200: Updated shield, loss is 0.04508068040013313
Eval num_timesteps=459200, episode_reward=22.90 +/- 16.02
Episode length: 32.20 +/- 21.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 22.9     |
| time/              |          |
|    total_timesteps | 459200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.9      |
|    ep_rew_mean     | 2.45     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1148     |
|    time_elapsed    | 104329   |
|    total_timesteps | 459200   |
---------------------------------
Progress update from timestep 459300: Updated shield, loss is 0.028614094480872154
Progress update from timestep 459400: Updated shield, loss is 0.03686635568737984
Eval num_timesteps=459400, episode_reward=19.66 +/- 13.48
Episode length: 28.20 +/- 18.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 28.2          |
|    mean_reward          | 19.7          |
| time/                   |               |
|    total_timesteps      | 459400        |
| train/                  |               |
|    approx_kl            | 0.00019305552 |
|    clip_fraction        | 0.00826       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0124       |
|    explained_variance   | 0.0745        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.985         |
|    n_updates            | 11480         |
|    policy_gradient_loss | -0.000899     |
|    value_loss           | 1.44          |
-------------------------------------------
Progress update from timestep 459500: Updated shield, loss is 0.037103280425071716
Progress update from timestep 459600: Updated shield, loss is 0.03515331447124481
Eval num_timesteps=459600, episode_reward=18.10 +/- 13.50
Episode length: 26.60 +/- 19.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.6     |
|    mean_reward     | 18.1     |
| time/              |          |
|    total_timesteps | 459600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.03     |
|    ep_rew_mean     | 2.45     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1149     |
|    time_elapsed    | 104419   |
|    total_timesteps | 459600   |
---------------------------------
Progress update from timestep 459700: Updated shield, loss is 0.04045792296528816
Progress update from timestep 459800: Updated shield, loss is 0.03809603303670883
Eval num_timesteps=459800, episode_reward=17.55 +/- 15.79
Episode length: 24.60 +/- 20.98
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.6          |
|    mean_reward          | 17.5          |
| time/                   |               |
|    total_timesteps      | 459800        |
| train/                  |               |
|    approx_kl            | 0.00023954776 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00838      |
|    explained_variance   | 0.11          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.563         |
|    n_updates            | 11490         |
|    policy_gradient_loss | -0.000274     |
|    value_loss           | 1.37          |
-------------------------------------------
Progress update from timestep 459900: Updated shield, loss is 0.04543224722146988
Progress update from timestep 460000: Updated shield, loss is 0.043642379343509674
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 460000: Saved shield_loss.csv. 
Eval num_timesteps=460000, episode_reward=10.60 +/- 11.88
Episode length: 16.20 +/- 17.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | 10.6     |
| time/              |          |
|    total_timesteps | 460000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.88     |
|    ep_rew_mean     | 2.39     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1150     |
|    time_elapsed    | 104506   |
|    total_timesteps | 460000   |
---------------------------------
Progress update from timestep 460100: Updated shield, loss is 0.04299613833427429
Progress update from timestep 460200: Updated shield, loss is 0.03654266521334648
Eval num_timesteps=460200, episode_reward=5.17 +/- 3.55
Episode length: 8.40 +/- 5.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 8.4           |
|    mean_reward          | 5.17          |
| time/                   |               |
|    total_timesteps      | 460200        |
| train/                  |               |
|    approx_kl            | 0.00043964197 |
|    clip_fraction        | 0.00737       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0262       |
|    explained_variance   | 0.256         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.246         |
|    n_updates            | 11500         |
|    policy_gradient_loss | -0.00109      |
|    value_loss           | 1.08          |
-------------------------------------------
Progress update from timestep 460300: Updated shield, loss is 0.03318333253264427
Progress update from timestep 460400: Updated shield, loss is 0.04336666315793991
Eval num_timesteps=460400, episode_reward=21.52 +/- 13.73
Episode length: 32.40 +/- 20.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.4     |
|    mean_reward     | 21.5     |
| time/              |          |
|    total_timesteps | 460400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.15     |
|    ep_rew_mean     | 2.52     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1151     |
|    time_elapsed    | 104594   |
|    total_timesteps | 460400   |
---------------------------------
Progress update from timestep 460500: Updated shield, loss is 0.03981918469071388
Progress update from timestep 460600: Updated shield, loss is 0.03188185766339302
Eval num_timesteps=460600, episode_reward=8.54 +/- 12.49
Episode length: 13.40 +/- 18.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 13.4          |
|    mean_reward          | 8.54          |
| time/                   |               |
|    total_timesteps      | 460600        |
| train/                  |               |
|    approx_kl            | -4.487057e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.000397     |
|    explained_variance   | 0.161         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.578         |
|    n_updates            | 11510         |
|    policy_gradient_loss | -6.49e-08     |
|    value_loss           | 1.34          |
-------------------------------------------
Progress update from timestep 460700: Updated shield, loss is 0.030434362590312958
Progress update from timestep 460800: Updated shield, loss is 0.03271261975169182
Eval num_timesteps=460800, episode_reward=18.85 +/- 13.84
Episode length: 27.20 +/- 19.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.2     |
|    mean_reward     | 18.8     |
| time/              |          |
|    total_timesteps | 460800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.99     |
|    ep_rew_mean     | 2.42     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1152     |
|    time_elapsed    | 104686   |
|    total_timesteps | 460800   |
---------------------------------
Progress update from timestep 460900: Updated shield, loss is 0.03234761953353882
Progress update from timestep 461000: Updated shield, loss is 0.04080735519528389
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 461000: Saved shield_loss.csv. 
Eval num_timesteps=461000, episode_reward=8.69 +/- 14.02
Episode length: 12.80 +/- 18.63
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 12.8          |
|    mean_reward          | 8.69          |
| time/                   |               |
|    total_timesteps      | 461000        |
| train/                  |               |
|    approx_kl            | 0.00032033861 |
|    clip_fraction        | 0.00513       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00558      |
|    explained_variance   | 0.104         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.17          |
|    n_updates            | 11520         |
|    policy_gradient_loss | -0.000828     |
|    value_loss           | 2.15          |
-------------------------------------------
Progress update from timestep 461100: Updated shield, loss is 0.044731080532073975
Progress update from timestep 461200: Updated shield, loss is 0.028752228245139122
Eval num_timesteps=461200, episode_reward=13.57 +/- 11.55
Episode length: 20.00 +/- 15.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20       |
|    mean_reward     | 13.6     |
| time/              |          |
|    total_timesteps | 461200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.89     |
|    ep_rew_mean     | 3.14     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1153     |
|    time_elapsed    | 104769   |
|    total_timesteps | 461200   |
---------------------------------
Progress update from timestep 461300: Updated shield, loss is 0.02833065576851368
Progress update from timestep 461400: Updated shield, loss is 0.03715917095541954
Eval num_timesteps=461400, episode_reward=5.00 +/- 3.38
Episode length: 8.20 +/- 5.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 8.2           |
|    mean_reward          | 5             |
| time/                   |               |
|    total_timesteps      | 461400        |
| train/                  |               |
|    approx_kl            | 5.1366373e-05 |
|    clip_fraction        | 0.00268       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00899      |
|    explained_variance   | 0.388         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.398         |
|    n_updates            | 11530         |
|    policy_gradient_loss | -0.000319     |
|    value_loss           | 1.63          |
-------------------------------------------
Progress update from timestep 461500: Updated shield, loss is 0.03888643532991409
Progress update from timestep 461600: Updated shield, loss is 0.030065789818763733
Eval num_timesteps=461600, episode_reward=11.81 +/- 11.71
Episode length: 18.00 +/- 17.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | 11.8     |
| time/              |          |
|    total_timesteps | 461600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.05     |
|    ep_rew_mean     | 3.26     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1154     |
|    time_elapsed    | 104836   |
|    total_timesteps | 461600   |
---------------------------------
Progress update from timestep 461700: Updated shield, loss is 0.039381321519613266
Progress update from timestep 461800: Updated shield, loss is 0.03830857202410698
Eval num_timesteps=461800, episode_reward=29.02 +/- 10.38
Episode length: 42.60 +/- 14.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.6          |
|    mean_reward          | 29            |
| time/                   |               |
|    total_timesteps      | 461800        |
| train/                  |               |
|    approx_kl            | 2.2903609e-05 |
|    clip_fraction        | 0.000893      |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00626      |
|    explained_variance   | 0.229         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.12          |
|    n_updates            | 11540         |
|    policy_gradient_loss | -0.000126     |
|    value_loss           | 2.71          |
-------------------------------------------
Progress update from timestep 461900: Updated shield, loss is 0.037807054817676544
Progress update from timestep 462000: Updated shield, loss is 0.033125970512628555
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 462000: Saved shield_loss.csv. 
Eval num_timesteps=462000, episode_reward=10.52 +/- 1.09
Episode length: 16.00 +/- 1.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | 10.5     |
| time/              |          |
|    total_timesteps | 462000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.63     |
|    ep_rew_mean     | 2.95     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1155     |
|    time_elapsed    | 104928   |
|    total_timesteps | 462000   |
---------------------------------
Progress update from timestep 462100: Updated shield, loss is 0.03760559484362602
Progress update from timestep 462200: Updated shield, loss is 0.03354121372103691
Eval num_timesteps=462200, episode_reward=15.73 +/- 15.77
Episode length: 23.00 +/- 22.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23            |
|    mean_reward          | 15.7          |
| time/                   |               |
|    total_timesteps      | 462200        |
| train/                  |               |
|    approx_kl            | 0.00023583454 |
|    clip_fraction        | 0.0103        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0256       |
|    explained_variance   | 0.286         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.955         |
|    n_updates            | 11550         |
|    policy_gradient_loss | -0.00166      |
|    value_loss           | 2.24          |
-------------------------------------------
Progress update from timestep 462300: Updated shield, loss is 0.039633359760046005
Progress update from timestep 462400: Updated shield, loss is 0.034690771251916885
Eval num_timesteps=462400, episode_reward=28.19 +/- 13.73
Episode length: 40.40 +/- 19.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.4     |
|    mean_reward     | 28.2     |
| time/              |          |
|    total_timesteps | 462400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4        |
|    ep_rew_mean     | 2.44     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1156     |
|    time_elapsed    | 105009   |
|    total_timesteps | 462400   |
---------------------------------
Progress update from timestep 462500: Updated shield, loss is 0.03786645457148552
Progress update from timestep 462600: Updated shield, loss is 0.04853806272149086
Eval num_timesteps=462600, episode_reward=11.40 +/- 12.35
Episode length: 16.80 +/- 16.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.8          |
|    mean_reward          | 11.4          |
| time/                   |               |
|    total_timesteps      | 462600        |
| train/                  |               |
|    approx_kl            | 0.00013406154 |
|    clip_fraction        | 0.00491       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0135       |
|    explained_variance   | 0.155         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.605         |
|    n_updates            | 11560         |
|    policy_gradient_loss | -0.00072      |
|    value_loss           | 1.83          |
-------------------------------------------
Progress update from timestep 462700: Updated shield, loss is 0.03399710729718208
Progress update from timestep 462800: Updated shield, loss is 0.03989306464791298
Eval num_timesteps=462800, episode_reward=22.92 +/- 15.25
Episode length: 32.80 +/- 21.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 22.9     |
| time/              |          |
|    total_timesteps | 462800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.62     |
|    ep_rew_mean     | 2.19     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1157     |
|    time_elapsed    | 105090   |
|    total_timesteps | 462800   |
---------------------------------
Progress update from timestep 462900: Updated shield, loss is 0.03997385874390602
Progress update from timestep 463000: Updated shield, loss is 0.031429123133420944
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 463000: Saved shield_loss.csv. 
Eval num_timesteps=463000, episode_reward=7.08 +/- 4.79
Episode length: 11.20 +/- 7.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 11.2        |
|    mean_reward          | 7.08        |
| time/                   |             |
|    total_timesteps      | 463000      |
| train/                  |             |
|    approx_kl            | 0.000278938 |
|    clip_fraction        | 0.01        |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0175     |
|    explained_variance   | 0.0301      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.647       |
|    n_updates            | 11570       |
|    policy_gradient_loss | -0.00114    |
|    value_loss           | 1.46        |
-----------------------------------------
Progress update from timestep 463100: Updated shield, loss is 0.04012787714600563
Progress update from timestep 463200: Updated shield, loss is 0.043881651014089584
Eval num_timesteps=463200, episode_reward=10.89 +/- 11.80
Episode length: 16.40 +/- 16.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | 10.9     |
| time/              |          |
|    total_timesteps | 463200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.65     |
|    ep_rew_mean     | 2.18     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1158     |
|    time_elapsed    | 105160   |
|    total_timesteps | 463200   |
---------------------------------
Progress update from timestep 463300: Updated shield, loss is 0.03272051736712456
Progress update from timestep 463400: Updated shield, loss is 0.03481244295835495
Eval num_timesteps=463400, episode_reward=10.99 +/- 12.64
Episode length: 16.20 +/- 17.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.2         |
|    mean_reward          | 11           |
| time/                   |              |
|    total_timesteps      | 463400       |
| train/                  |              |
|    approx_kl            | 0.0007987915 |
|    clip_fraction        | 0.00871      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0121      |
|    explained_variance   | 0.174        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.826        |
|    n_updates            | 11580        |
|    policy_gradient_loss | -0.000893    |
|    value_loss           | 1.48         |
------------------------------------------
Progress update from timestep 463500: Updated shield, loss is 0.02870267815887928
Progress update from timestep 463600: Updated shield, loss is 0.03181585296988487
Eval num_timesteps=463600, episode_reward=16.83 +/- 14.08
Episode length: 25.40 +/- 20.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 463600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.18     |
|    ep_rew_mean     | 2.72     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1159     |
|    time_elapsed    | 105240   |
|    total_timesteps | 463600   |
---------------------------------
Progress update from timestep 463700: Updated shield, loss is 0.04433898255228996
Progress update from timestep 463800: Updated shield, loss is 0.03532421588897705
Eval num_timesteps=463800, episode_reward=17.48 +/- 14.75
Episode length: 25.60 +/- 20.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.6         |
|    mean_reward          | 17.5         |
| time/                   |              |
|    total_timesteps      | 463800       |
| train/                  |              |
|    approx_kl            | 6.351977e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00487     |
|    explained_variance   | 0.0482       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.844        |
|    n_updates            | 11590        |
|    policy_gradient_loss | -1.93e-05    |
|    value_loss           | 1.96         |
------------------------------------------
Progress update from timestep 463900: Updated shield, loss is 0.030775457620620728
Progress update from timestep 464000: Updated shield, loss is 0.035991620272397995
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 464000: Saved shield_loss.csv. 
Eval num_timesteps=464000, episode_reward=13.42 +/- 12.50
Episode length: 19.60 +/- 17.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 13.4     |
| time/              |          |
|    total_timesteps | 464000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.97     |
|    ep_rew_mean     | 2.48     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1160     |
|    time_elapsed    | 105325   |
|    total_timesteps | 464000   |
---------------------------------
Progress update from timestep 464100: Updated shield, loss is 0.04679100587964058
Progress update from timestep 464200: Updated shield, loss is 0.04568856209516525
Eval num_timesteps=464200, episode_reward=22.96 +/- 14.79
Episode length: 33.20 +/- 20.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.2          |
|    mean_reward          | 23            |
| time/                   |               |
|    total_timesteps      | 464200        |
| train/                  |               |
|    approx_kl            | 0.00020515836 |
|    clip_fraction        | 0.00848       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0212       |
|    explained_variance   | 0.205         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.688         |
|    n_updates            | 11600         |
|    policy_gradient_loss | -0.000987     |
|    value_loss           | 1.92          |
-------------------------------------------
Progress update from timestep 464300: Updated shield, loss is 0.046503763645887375
Progress update from timestep 464400: Updated shield, loss is 0.0359298512339592
Eval num_timesteps=464400, episode_reward=23.75 +/- 15.86
Episode length: 32.80 +/- 21.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 23.8     |
| time/              |          |
|    total_timesteps | 464400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.55     |
|    ep_rew_mean     | 2.91     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1161     |
|    time_elapsed    | 105426   |
|    total_timesteps | 464400   |
---------------------------------
Progress update from timestep 464500: Updated shield, loss is 0.03476376086473465
Progress update from timestep 464600: Updated shield, loss is 0.041358962655067444
Eval num_timesteps=464600, episode_reward=10.50 +/- 11.94
Episode length: 16.00 +/- 17.63
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16            |
|    mean_reward          | 10.5          |
| time/                   |               |
|    total_timesteps      | 464600        |
| train/                  |               |
|    approx_kl            | 0.00028521504 |
|    clip_fraction        | 0.0138        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0227       |
|    explained_variance   | 0.129         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.04          |
|    n_updates            | 11610         |
|    policy_gradient_loss | -0.00161      |
|    value_loss           | 2.4           |
-------------------------------------------
Progress update from timestep 464700: Updated shield, loss is 0.03594956547021866
Progress update from timestep 464800: Updated shield, loss is 0.033657874912023544
Eval num_timesteps=464800, episode_reward=22.27 +/- 15.89
Episode length: 32.00 +/- 22.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32       |
|    mean_reward     | 22.3     |
| time/              |          |
|    total_timesteps | 464800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.2      |
|    ep_rew_mean     | 2.72     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1162     |
|    time_elapsed    | 105508   |
|    total_timesteps | 464800   |
---------------------------------
Progress update from timestep 464900: Updated shield, loss is 0.042753513902425766
Progress update from timestep 465000: Updated shield, loss is 0.03396333009004593
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 465000: Saved shield_loss.csv. 
Eval num_timesteps=465000, episode_reward=18.70 +/- 13.61
Episode length: 28.00 +/- 19.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 28            |
|    mean_reward          | 18.7          |
| time/                   |               |
|    total_timesteps      | 465000        |
| train/                  |               |
|    approx_kl            | 0.00023143033 |
|    clip_fraction        | 0.00893       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0342       |
|    explained_variance   | 0.4           |
|    learning_rate        | 0.0001        |
|    loss                 | 0.981         |
|    n_updates            | 11620         |
|    policy_gradient_loss | -0.00155      |
|    value_loss           | 2.37          |
-------------------------------------------
Progress update from timestep 465100: Updated shield, loss is 0.03932325914502144
Progress update from timestep 465200: Updated shield, loss is 0.038553327322006226
Eval num_timesteps=465200, episode_reward=22.32 +/- 16.29
Episode length: 31.80 +/- 22.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.8     |
|    mean_reward     | 22.3     |
| time/              |          |
|    total_timesteps | 465200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.87     |
|    ep_rew_mean     | 2.41     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1163     |
|    time_elapsed    | 105619   |
|    total_timesteps | 465200   |
---------------------------------
Progress update from timestep 465300: Updated shield, loss is 0.032933540642261505
Progress update from timestep 465400: Updated shield, loss is 0.040409281849861145
Eval num_timesteps=465400, episode_reward=23.41 +/- 13.26
Episode length: 34.60 +/- 19.07
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 34.6           |
|    mean_reward          | 23.4           |
| time/                   |                |
|    total_timesteps      | 465400         |
| train/                  |                |
|    approx_kl            | -2.0924874e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.000747      |
|    explained_variance   | 0.0951         |
|    learning_rate        | 0.0001         |
|    loss                 | 1.06           |
|    n_updates            | 11630          |
|    policy_gradient_loss | -3.02e-08      |
|    value_loss           | 1.52           |
--------------------------------------------
Progress update from timestep 465500: Updated shield, loss is 0.04372594878077507
Progress update from timestep 465600: Updated shield, loss is 0.04072161763906479
Eval num_timesteps=465600, episode_reward=14.60 +/- 13.49
Episode length: 21.40 +/- 18.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 14.6     |
| time/              |          |
|    total_timesteps | 465600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.24     |
|    ep_rew_mean     | 2.69     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1164     |
|    time_elapsed    | 105700   |
|    total_timesteps | 465600   |
---------------------------------
Progress update from timestep 465700: Updated shield, loss is 0.04729105904698372
Progress update from timestep 465800: Updated shield, loss is 0.04011638090014458
Eval num_timesteps=465800, episode_reward=19.30 +/- 13.39
Episode length: 28.20 +/- 18.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 28.2          |
|    mean_reward          | 19.3          |
| time/                   |               |
|    total_timesteps      | 465800        |
| train/                  |               |
|    approx_kl            | 0.00016118369 |
|    clip_fraction        | 0.00603       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00893      |
|    explained_variance   | 0.154         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.449         |
|    n_updates            | 11640         |
|    policy_gradient_loss | -0.00034      |
|    value_loss           | 2.26          |
-------------------------------------------
Progress update from timestep 465900: Updated shield, loss is 0.033486854285001755
Progress update from timestep 466000: Updated shield, loss is 0.039340365678071976
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 466000: Saved shield_loss.csv. 
Eval num_timesteps=466000, episode_reward=18.46 +/- 14.93
Episode length: 26.20 +/- 20.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | 18.5     |
| time/              |          |
|    total_timesteps | 466000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.33     |
|    ep_rew_mean     | 2.77     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1165     |
|    time_elapsed    | 105787   |
|    total_timesteps | 466000   |
---------------------------------
Progress update from timestep 466100: Updated shield, loss is 0.03907471150159836
Progress update from timestep 466200: Updated shield, loss is 0.03694472461938858
Eval num_timesteps=466200, episode_reward=11.63 +/- 12.13
Episode length: 17.60 +/- 17.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.6         |
|    mean_reward          | 11.6         |
| time/                   |              |
|    total_timesteps      | 466200       |
| train/                  |              |
|    approx_kl            | 0.0006944872 |
|    clip_fraction        | 0.00804      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0171      |
|    explained_variance   | 0.139        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.601        |
|    n_updates            | 11650        |
|    policy_gradient_loss | -0.000956    |
|    value_loss           | 1.61         |
------------------------------------------
Progress update from timestep 466300: Updated shield, loss is 0.03704283758997917
Progress update from timestep 466400: Updated shield, loss is 0.033391665667295456
Eval num_timesteps=466400, episode_reward=29.91 +/- 10.83
Episode length: 42.60 +/- 14.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.6     |
|    mean_reward     | 29.9     |
| time/              |          |
|    total_timesteps | 466400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.29     |
|    ep_rew_mean     | 2.73     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1166     |
|    time_elapsed    | 105872   |
|    total_timesteps | 466400   |
---------------------------------
Progress update from timestep 466500: Updated shield, loss is 0.04145104065537453
Progress update from timestep 466600: Updated shield, loss is 0.06956568360328674
Eval num_timesteps=466600, episode_reward=8.72 +/- 12.45
Episode length: 13.60 +/- 18.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 13.6          |
|    mean_reward          | 8.72          |
| time/                   |               |
|    total_timesteps      | 466600        |
| train/                  |               |
|    approx_kl            | 0.00046348045 |
|    clip_fraction        | 0.0185        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.037        |
|    explained_variance   | 0.227         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.614         |
|    n_updates            | 11660         |
|    policy_gradient_loss | -0.000905     |
|    value_loss           | 1.37          |
-------------------------------------------
Progress update from timestep 466700: Updated shield, loss is 0.04689391329884529
Progress update from timestep 466800: Updated shield, loss is 0.07585106045007706
Eval num_timesteps=466800, episode_reward=15.18 +/- 12.38
Episode length: 21.60 +/- 16.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 15.2     |
| time/              |          |
|    total_timesteps | 466800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.67     |
|    ep_rew_mean     | 2.96     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1167     |
|    time_elapsed    | 105953   |
|    total_timesteps | 466800   |
---------------------------------
Progress update from timestep 466900: Updated shield, loss is 0.04507169872522354
Progress update from timestep 467000: Updated shield, loss is 0.05989908427000046
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 467000: Saved shield_loss.csv. 
Eval num_timesteps=467000, episode_reward=23.54 +/- 13.88
Episode length: 34.20 +/- 19.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.2          |
|    mean_reward          | 23.5          |
| time/                   |               |
|    total_timesteps      | 467000        |
| train/                  |               |
|    approx_kl            | 0.00022460996 |
|    clip_fraction        | 0.00603       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0196       |
|    explained_variance   | 0.213         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.553         |
|    n_updates            | 11670         |
|    policy_gradient_loss | -0.00065      |
|    value_loss           | 1.55          |
-------------------------------------------
Progress update from timestep 467100: Updated shield, loss is 0.057635512202978134
Progress update from timestep 467200: Updated shield, loss is 0.029387541115283966
Eval num_timesteps=467200, episode_reward=22.35 +/- 15.03
Episode length: 32.80 +/- 21.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 22.3     |
| time/              |          |
|    total_timesteps | 467200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.43     |
|    ep_rew_mean     | 2.04     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1168     |
|    time_elapsed    | 106043   |
|    total_timesteps | 467200   |
---------------------------------
Progress update from timestep 467300: Updated shield, loss is 0.06301674991846085
Progress update from timestep 467400: Updated shield, loss is 0.044547852128744125
Eval num_timesteps=467400, episode_reward=16.25 +/- 14.54
Episode length: 24.40 +/- 21.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.4          |
|    mean_reward          | 16.2          |
| time/                   |               |
|    total_timesteps      | 467400        |
| train/                  |               |
|    approx_kl            | 0.00050388754 |
|    clip_fraction        | 0.0219        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0384       |
|    explained_variance   | 0.281         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.41          |
|    n_updates            | 11680         |
|    policy_gradient_loss | -0.00223      |
|    value_loss           | 0.807         |
-------------------------------------------
Progress update from timestep 467500: Updated shield, loss is 0.03846386447548866
Progress update from timestep 467600: Updated shield, loss is 0.05370224639773369
Eval num_timesteps=467600, episode_reward=17.73 +/- 14.93
Episode length: 25.60 +/- 20.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 17.7     |
| time/              |          |
|    total_timesteps | 467600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.44     |
|    ep_rew_mean     | 2.8      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1169     |
|    time_elapsed    | 106149   |
|    total_timesteps | 467600   |
---------------------------------
Progress update from timestep 467700: Updated shield, loss is 0.03456611558794975
Progress update from timestep 467800: Updated shield, loss is 0.0747843086719513
Eval num_timesteps=467800, episode_reward=9.65 +/- 12.50
Episode length: 14.60 +/- 17.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.6          |
|    mean_reward          | 9.65          |
| time/                   |               |
|    total_timesteps      | 467800        |
| train/                  |               |
|    approx_kl            | 3.1097396e-05 |
|    clip_fraction        | 0.00112       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0075       |
|    explained_variance   | 0.22          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.593         |
|    n_updates            | 11690         |
|    policy_gradient_loss | -0.000127     |
|    value_loss           | 1.51          |
-------------------------------------------
Progress update from timestep 467900: Updated shield, loss is 0.041514672338962555
Progress update from timestep 468000: Updated shield, loss is 0.03508791700005531
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 468000: Saved shield_loss.csv. 
Eval num_timesteps=468000, episode_reward=14.14 +/- 11.90
Episode length: 20.40 +/- 16.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 14.1     |
| time/              |          |
|    total_timesteps | 468000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.7      |
|    ep_rew_mean     | 2.29     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1170     |
|    time_elapsed    | 106231   |
|    total_timesteps | 468000   |
---------------------------------
Progress update from timestep 468100: Updated shield, loss is 0.05011080205440521
Progress update from timestep 468200: Updated shield, loss is 0.0336618572473526
Eval num_timesteps=468200, episode_reward=13.24 +/- 6.27
Episode length: 19.40 +/- 8.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 19.4         |
|    mean_reward          | 13.2         |
| time/                   |              |
|    total_timesteps      | 468200       |
| train/                  |              |
|    approx_kl            | 9.554251e-05 |
|    clip_fraction        | 0.00424      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0163      |
|    explained_variance   | 0.231        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.658        |
|    n_updates            | 11700        |
|    policy_gradient_loss | -0.0004      |
|    value_loss           | 1.28         |
------------------------------------------
Progress update from timestep 468300: Updated shield, loss is 0.04791036248207092
Progress update from timestep 468400: Updated shield, loss is 0.04435937851667404
Eval num_timesteps=468400, episode_reward=16.76 +/- 15.03
Episode length: 24.40 +/- 21.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 468400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.59     |
|    ep_rew_mean     | 2.2      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1171     |
|    time_elapsed    | 106309   |
|    total_timesteps | 468400   |
---------------------------------
Progress update from timestep 468500: Updated shield, loss is 0.04726162180304527
Progress update from timestep 468600: Updated shield, loss is 0.03857113793492317
Eval num_timesteps=468600, episode_reward=15.23 +/- 15.75
Episode length: 22.60 +/- 22.43
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.6          |
|    mean_reward          | 15.2          |
| time/                   |               |
|    total_timesteps      | 468600        |
| train/                  |               |
|    approx_kl            | 0.00016522047 |
|    clip_fraction        | 0.00402       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00709      |
|    explained_variance   | 0.269         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.302         |
|    n_updates            | 11710         |
|    policy_gradient_loss | -0.000357     |
|    value_loss           | 1.17          |
-------------------------------------------
Progress update from timestep 468700: Updated shield, loss is 0.04250672459602356
Progress update from timestep 468800: Updated shield, loss is 0.04092375189065933
Eval num_timesteps=468800, episode_reward=23.98 +/- 13.30
Episode length: 35.00 +/- 18.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 24       |
| time/              |          |
|    total_timesteps | 468800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.32     |
|    ep_rew_mean     | 2.73     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1172     |
|    time_elapsed    | 106409   |
|    total_timesteps | 468800   |
---------------------------------
Progress update from timestep 468900: Updated shield, loss is 0.03262042999267578
Progress update from timestep 469000: Updated shield, loss is 0.04084033519029617
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 469000: Saved shield_loss.csv. 
Eval num_timesteps=469000, episode_reward=16.97 +/- 14.12
Episode length: 25.60 +/- 20.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.6         |
|    mean_reward          | 17           |
| time/                   |              |
|    total_timesteps      | 469000       |
| train/                  |              |
|    approx_kl            | 0.0003354249 |
|    clip_fraction        | 0.0103       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0116      |
|    explained_variance   | 0.0283       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.727        |
|    n_updates            | 11720        |
|    policy_gradient_loss | -0.000952    |
|    value_loss           | 2.53         |
------------------------------------------
Progress update from timestep 469100: Updated shield, loss is 0.03100111335515976
Progress update from timestep 469200: Updated shield, loss is 0.03969168663024902
Eval num_timesteps=469200, episode_reward=4.95 +/- 2.25
Episode length: 8.00 +/- 3.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8        |
|    mean_reward     | 4.95     |
| time/              |          |
|    total_timesteps | 469200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.31     |
|    ep_rew_mean     | 2.66     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1173     |
|    time_elapsed    | 106476   |
|    total_timesteps | 469200   |
---------------------------------
Progress update from timestep 469300: Updated shield, loss is 0.03360449895262718
Progress update from timestep 469400: Updated shield, loss is 0.04102303087711334
Eval num_timesteps=469400, episode_reward=10.21 +/- 13.96
Episode length: 14.80 +/- 18.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.8          |
|    mean_reward          | 10.2          |
| time/                   |               |
|    total_timesteps      | 469400        |
| train/                  |               |
|    approx_kl            | 3.3974768e-05 |
|    clip_fraction        | 0.00134       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0113       |
|    explained_variance   | 0.261         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.611         |
|    n_updates            | 11730         |
|    policy_gradient_loss | -0.000426     |
|    value_loss           | 1.94          |
-------------------------------------------
Progress update from timestep 469500: Updated shield, loss is 0.029901809990406036
Progress update from timestep 469600: Updated shield, loss is 0.034662920981645584
Eval num_timesteps=469600, episode_reward=9.38 +/- 12.81
Episode length: 14.40 +/- 18.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.4     |
|    mean_reward     | 9.38     |
| time/              |          |
|    total_timesteps | 469600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.12     |
|    ep_rew_mean     | 2.61     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1174     |
|    time_elapsed    | 106563   |
|    total_timesteps | 469600   |
---------------------------------
Progress update from timestep 469700: Updated shield, loss is 0.04227688908576965
Progress update from timestep 469800: Updated shield, loss is 0.040897708386182785
Eval num_timesteps=469800, episode_reward=23.80 +/- 14.45
Episode length: 33.80 +/- 19.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.8         |
|    mean_reward          | 23.8         |
| time/                   |              |
|    total_timesteps      | 469800       |
| train/                  |              |
|    approx_kl            | 0.0003338061 |
|    clip_fraction        | 0.00804      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0142      |
|    explained_variance   | 0.295        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.591        |
|    n_updates            | 11740        |
|    policy_gradient_loss | -0.00071     |
|    value_loss           | 1.44         |
------------------------------------------
Progress update from timestep 469900: Updated shield, loss is 0.029667314141988754
Progress update from timestep 470000: Updated shield, loss is 0.03787551075220108
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 470000: Saved shield_loss.csv. 
Eval num_timesteps=470000, episode_reward=24.41 +/- 12.67
Episode length: 35.60 +/- 18.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 24.4     |
| time/              |          |
|    total_timesteps | 470000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.5      |
|    ep_rew_mean     | 2.07     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1175     |
|    time_elapsed    | 106655   |
|    total_timesteps | 470000   |
---------------------------------
Progress update from timestep 470100: Updated shield, loss is 0.04651361703872681
Progress update from timestep 470200: Updated shield, loss is 0.034942734986543655
Eval num_timesteps=470200, episode_reward=16.88 +/- 14.29
Episode length: 25.20 +/- 20.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.2         |
|    mean_reward          | 16.9         |
| time/                   |              |
|    total_timesteps      | 470200       |
| train/                  |              |
|    approx_kl            | 0.0013087267 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0243      |
|    explained_variance   | 0.233        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.539        |
|    n_updates            | 11750        |
|    policy_gradient_loss | -0.00163     |
|    value_loss           | 1.07         |
------------------------------------------
Progress update from timestep 470300: Updated shield, loss is 0.03422980755567551
Progress update from timestep 470400: Updated shield, loss is 0.03734145313501358
Eval num_timesteps=470400, episode_reward=16.52 +/- 15.19
Episode length: 24.20 +/- 21.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 470400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.38     |
|    ep_rew_mean     | 2.73     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1176     |
|    time_elapsed    | 106735   |
|    total_timesteps | 470400   |
---------------------------------
Progress update from timestep 470500: Updated shield, loss is 0.04611373320221901
Progress update from timestep 470600: Updated shield, loss is 0.039013367146253586
Eval num_timesteps=470600, episode_reward=5.49 +/- 2.63
Episode length: 8.60 +/- 3.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 8.6           |
|    mean_reward          | 5.49          |
| time/                   |               |
|    total_timesteps      | 470600        |
| train/                  |               |
|    approx_kl            | 0.00018286999 |
|    clip_fraction        | 0.00737       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0258       |
|    explained_variance   | 0.363         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.975         |
|    n_updates            | 11760         |
|    policy_gradient_loss | -0.00141      |
|    value_loss           | 1.81          |
-------------------------------------------
Progress update from timestep 470700: Updated shield, loss is 0.03395076468586922
Progress update from timestep 470800: Updated shield, loss is 0.034780535846948624
Eval num_timesteps=470800, episode_reward=29.58 +/- 12.05
Episode length: 41.80 +/- 16.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 29.6     |
| time/              |          |
|    total_timesteps | 470800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.12     |
|    ep_rew_mean     | 2.6      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1177     |
|    time_elapsed    | 106808   |
|    total_timesteps | 470800   |
---------------------------------
Progress update from timestep 470900: Updated shield, loss is 0.02750483900308609
Progress update from timestep 471000: Updated shield, loss is 0.0316091924905777
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 471000: Saved shield_loss.csv. 
Eval num_timesteps=471000, episode_reward=4.60 +/- 2.12
Episode length: 7.40 +/- 2.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 7.4           |
|    mean_reward          | 4.6           |
| time/                   |               |
|    total_timesteps      | 471000        |
| train/                  |               |
|    approx_kl            | -1.357096e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.000419     |
|    explained_variance   | 0.169         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.13          |
|    n_updates            | 11770         |
|    policy_gradient_loss | -4.05e-08     |
|    value_loss           | 1.48          |
-------------------------------------------
Progress update from timestep 471100: Updated shield, loss is 0.03778510168194771
Progress update from timestep 471200: Updated shield, loss is 0.03683719038963318
Eval num_timesteps=471200, episode_reward=3.20 +/- 3.01
Episode length: 5.40 +/- 4.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5.4      |
|    mean_reward     | 3.2      |
| time/              |          |
|    total_timesteps | 471200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.17     |
|    ep_rew_mean     | 2.58     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1178     |
|    time_elapsed    | 106861   |
|    total_timesteps | 471200   |
---------------------------------
Progress update from timestep 471300: Updated shield, loss is 0.03799938037991524
Progress update from timestep 471400: Updated shield, loss is 0.03874653950333595
Eval num_timesteps=471400, episode_reward=14.85 +/- 15.59
Episode length: 22.40 +/- 22.57
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.4          |
|    mean_reward          | 14.9          |
| time/                   |               |
|    total_timesteps      | 471400        |
| train/                  |               |
|    approx_kl            | 3.9933315e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.000559     |
|    explained_variance   | 0.0368        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.816         |
|    n_updates            | 11780         |
|    policy_gradient_loss | -8.02e-08     |
|    value_loss           | 2.03          |
-------------------------------------------
Progress update from timestep 471500: Updated shield, loss is 0.038724031299352646
Progress update from timestep 471600: Updated shield, loss is 0.041020967066287994
Eval num_timesteps=471600, episode_reward=14.76 +/- 15.22
Episode length: 22.60 +/- 22.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 14.8     |
| time/              |          |
|    total_timesteps | 471600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.48     |
|    ep_rew_mean     | 2.83     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1179     |
|    time_elapsed    | 106942   |
|    total_timesteps | 471600   |
---------------------------------
Progress update from timestep 471700: Updated shield, loss is 0.03449484333395958
Progress update from timestep 471800: Updated shield, loss is 0.03412556275725365
Eval num_timesteps=471800, episode_reward=10.64 +/- 11.63
Episode length: 16.40 +/- 17.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.4          |
|    mean_reward          | 10.6          |
| time/                   |               |
|    total_timesteps      | 471800        |
| train/                  |               |
|    approx_kl            | -7.743698e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.000682     |
|    explained_variance   | 0.196         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.577         |
|    n_updates            | 11790         |
|    policy_gradient_loss | -3.75e-08     |
|    value_loss           | 1.34          |
-------------------------------------------
Progress update from timestep 471900: Updated shield, loss is 0.04098372533917427
Progress update from timestep 472000: Updated shield, loss is 0.04285258427262306
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 472000: Saved shield_loss.csv. 
Eval num_timesteps=472000, episode_reward=13.01 +/- 11.93
Episode length: 19.00 +/- 15.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 13       |
| time/              |          |
|    total_timesteps | 472000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.96     |
|    ep_rew_mean     | 3.17     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1180     |
|    time_elapsed    | 107028   |
|    total_timesteps | 472000   |
---------------------------------
Progress update from timestep 472100: Updated shield, loss is 0.03335611894726753
Progress update from timestep 472200: Updated shield, loss is 0.03631967678666115
Eval num_timesteps=472200, episode_reward=23.85 +/- 13.04
Episode length: 34.80 +/- 18.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.8        |
|    mean_reward          | 23.8        |
| time/                   |             |
|    total_timesteps      | 472200      |
| train/                  |             |
|    approx_kl            | 0.000586283 |
|    clip_fraction        | 0.00804     |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0118     |
|    explained_variance   | 0.245       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.791       |
|    n_updates            | 11800       |
|    policy_gradient_loss | -0.000917   |
|    value_loss           | 1.93        |
-----------------------------------------
Progress update from timestep 472300: Updated shield, loss is 0.040458135306835175
Progress update from timestep 472400: Updated shield, loss is 0.032968178391456604
Eval num_timesteps=472400, episode_reward=8.89 +/- 8.22
Episode length: 13.80 +/- 12.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.8     |
|    mean_reward     | 8.89     |
| time/              |          |
|    total_timesteps | 472400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.69     |
|    ep_rew_mean     | 3.07     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1181     |
|    time_elapsed    | 107110   |
|    total_timesteps | 472400   |
---------------------------------
Progress update from timestep 472500: Updated shield, loss is 0.03427083045244217
Progress update from timestep 472600: Updated shield, loss is 0.04696793854236603
Eval num_timesteps=472600, episode_reward=22.36 +/- 14.49
Episode length: 33.00 +/- 20.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33            |
|    mean_reward          | 22.4          |
| time/                   |               |
|    total_timesteps      | 472600        |
| train/                  |               |
|    approx_kl            | 0.00058811496 |
|    clip_fraction        | 0.00871       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0369       |
|    explained_variance   | 0.194         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.525         |
|    n_updates            | 11810         |
|    policy_gradient_loss | -0.00101      |
|    value_loss           | 2.17          |
-------------------------------------------
Progress update from timestep 472700: Updated shield, loss is 0.039857566356658936
Progress update from timestep 472800: Updated shield, loss is 0.03372820094227791
Eval num_timesteps=472800, episode_reward=11.67 +/- 12.99
Episode length: 17.60 +/- 18.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | 11.7     |
| time/              |          |
|    total_timesteps | 472800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.75     |
|    ep_rew_mean     | 2.32     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1182     |
|    time_elapsed    | 107195   |
|    total_timesteps | 472800   |
---------------------------------
Progress update from timestep 472900: Updated shield, loss is 0.04756776615977287
Progress update from timestep 473000: Updated shield, loss is 0.039666756987571716
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 473000: Saved shield_loss.csv. 
Eval num_timesteps=473000, episode_reward=17.33 +/- 13.15
Episode length: 26.40 +/- 19.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.4         |
|    mean_reward          | 17.3         |
| time/                   |              |
|    total_timesteps      | 473000       |
| train/                  |              |
|    approx_kl            | 0.0011044302 |
|    clip_fraction        | 0.0152       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0302      |
|    explained_variance   | 0.106        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.89         |
|    n_updates            | 11820        |
|    policy_gradient_loss | -0.00126     |
|    value_loss           | 1.81         |
------------------------------------------
Progress update from timestep 473100: Updated shield, loss is 0.03306081146001816
Progress update from timestep 473200: Updated shield, loss is 0.03889894858002663
Eval num_timesteps=473200, episode_reward=27.81 +/- 12.77
Episode length: 40.80 +/- 18.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.8     |
|    mean_reward     | 27.8     |
| time/              |          |
|    total_timesteps | 473200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.54     |
|    ep_rew_mean     | 2.88     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1183     |
|    time_elapsed    | 107282   |
|    total_timesteps | 473200   |
---------------------------------
Progress update from timestep 473300: Updated shield, loss is 0.041971612721681595
Progress update from timestep 473400: Updated shield, loss is 0.03099387139081955
Eval num_timesteps=473400, episode_reward=10.71 +/- 12.60
Episode length: 16.00 +/- 17.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16            |
|    mean_reward          | 10.7          |
| time/                   |               |
|    total_timesteps      | 473400        |
| train/                  |               |
|    approx_kl            | 0.00041291412 |
|    clip_fraction        | 0.00871       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0202       |
|    explained_variance   | 0.267         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.652         |
|    n_updates            | 11830         |
|    policy_gradient_loss | -0.0012       |
|    value_loss           | 1.33          |
-------------------------------------------
Progress update from timestep 473500: Updated shield, loss is 0.03234536200761795
Progress update from timestep 473600: Updated shield, loss is 0.03941122815012932
Eval num_timesteps=473600, episode_reward=29.14 +/- 12.35
Episode length: 41.40 +/- 17.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | 29.1     |
| time/              |          |
|    total_timesteps | 473600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.18     |
|    ep_rew_mean     | 2.63     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1184     |
|    time_elapsed    | 107366   |
|    total_timesteps | 473600   |
---------------------------------
Progress update from timestep 473700: Updated shield, loss is 0.030748993158340454
Progress update from timestep 473800: Updated shield, loss is 0.03971799090504646
Eval num_timesteps=473800, episode_reward=17.23 +/- 13.82
Episode length: 26.00 +/- 19.91
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 26             |
|    mean_reward          | 17.2           |
| time/                   |                |
|    total_timesteps      | 473800         |
| train/                  |                |
|    approx_kl            | 0.000118056276 |
|    clip_fraction        | 0.00357        |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.00768       |
|    explained_variance   | 0.207          |
|    learning_rate        | 0.0001         |
|    loss                 | 0.46           |
|    n_updates            | 11840          |
|    policy_gradient_loss | -0.000207      |
|    value_loss           | 1.16           |
--------------------------------------------
Progress update from timestep 473900: Updated shield, loss is 0.04563980549573898
Progress update from timestep 474000: Updated shield, loss is 0.04262092337012291
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 474000: Saved shield_loss.csv. 
Eval num_timesteps=474000, episode_reward=7.14 +/- 4.15
Episode length: 11.20 +/- 6.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 11.2     |
|    mean_reward     | 7.14     |
| time/              |          |
|    total_timesteps | 474000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.08     |
|    ep_rew_mean     | 2.54     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1185     |
|    time_elapsed    | 107443   |
|    total_timesteps | 474000   |
---------------------------------
Progress update from timestep 474100: Updated shield, loss is 0.03966151922941208
Progress update from timestep 474200: Updated shield, loss is 0.04010821133852005
Eval num_timesteps=474200, episode_reward=18.37 +/- 14.35
Episode length: 26.40 +/- 19.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.4          |
|    mean_reward          | 18.4          |
| time/                   |               |
|    total_timesteps      | 474200        |
| train/                  |               |
|    approx_kl            | 9.1375565e-05 |
|    clip_fraction        | 0.00446       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0176       |
|    explained_variance   | 0.144         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.27          |
|    n_updates            | 11850         |
|    policy_gradient_loss | -0.000407     |
|    value_loss           | 1.53          |
-------------------------------------------
Progress update from timestep 474300: Updated shield, loss is 0.044352900236845016
Progress update from timestep 474400: Updated shield, loss is 0.03960670903325081
Eval num_timesteps=474400, episode_reward=6.21 +/- 3.90
Episode length: 10.00 +/- 5.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 6.21     |
| time/              |          |
|    total_timesteps | 474400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.07     |
|    ep_rew_mean     | 2.54     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1186     |
|    time_elapsed    | 107520   |
|    total_timesteps | 474400   |
---------------------------------
Progress update from timestep 474500: Updated shield, loss is 0.030991770327091217
Progress update from timestep 474600: Updated shield, loss is 0.037658728659152985
Eval num_timesteps=474600, episode_reward=27.49 +/- 13.43
Episode length: 40.40 +/- 19.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 40.4         |
|    mean_reward          | 27.5         |
| time/                   |              |
|    total_timesteps      | 474600       |
| train/                  |              |
|    approx_kl            | 0.0005010806 |
|    clip_fraction        | 0.0114       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0211      |
|    explained_variance   | 0.249        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.656        |
|    n_updates            | 11860        |
|    policy_gradient_loss | -0.000805    |
|    value_loss           | 1.4          |
------------------------------------------
Progress update from timestep 474700: Updated shield, loss is 0.04513799026608467
Progress update from timestep 474800: Updated shield, loss is 0.0458984449505806
Eval num_timesteps=474800, episode_reward=9.94 +/- 13.14
Episode length: 14.80 +/- 18.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 9.94     |
| time/              |          |
|    total_timesteps | 474800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.27     |
|    ep_rew_mean     | 2.65     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1187     |
|    time_elapsed    | 107626   |
|    total_timesteps | 474800   |
---------------------------------
Progress update from timestep 474900: Updated shield, loss is 0.033334944397211075
Progress update from timestep 475000: Updated shield, loss is 0.033570751547813416
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 475000: Saved shield_loss.csv. 
Eval num_timesteps=475000, episode_reward=19.99 +/- 13.16
Episode length: 28.80 +/- 17.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 28.8          |
|    mean_reward          | 20            |
| time/                   |               |
|    total_timesteps      | 475000        |
| train/                  |               |
|    approx_kl            | 0.00030103794 |
|    clip_fraction        | 0.0136        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0218       |
|    explained_variance   | 0.192         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.673         |
|    n_updates            | 11870         |
|    policy_gradient_loss | -0.000847     |
|    value_loss           | 1.43          |
-------------------------------------------
Progress update from timestep 475100: Updated shield, loss is 0.043662477284669876
Progress update from timestep 475200: Updated shield, loss is 0.03599123656749725
Eval num_timesteps=475200, episode_reward=29.72 +/- 11.21
Episode length: 42.40 +/- 15.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 29.7     |
| time/              |          |
|    total_timesteps | 475200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.68     |
|    ep_rew_mean     | 2.97     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1188     |
|    time_elapsed    | 107711   |
|    total_timesteps | 475200   |
---------------------------------
Progress update from timestep 475300: Updated shield, loss is 0.040096480399370193
Progress update from timestep 475400: Updated shield, loss is 0.0333729051053524
Eval num_timesteps=475400, episode_reward=20.30 +/- 10.89
Episode length: 30.40 +/- 16.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 30.4          |
|    mean_reward          | 20.3          |
| time/                   |               |
|    total_timesteps      | 475400        |
| train/                  |               |
|    approx_kl            | 0.00014433412 |
|    clip_fraction        | 0.00536       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00583      |
|    explained_variance   | 0.105         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.28          |
|    n_updates            | 11880         |
|    policy_gradient_loss | -0.000577     |
|    value_loss           | 1.61          |
-------------------------------------------
Progress update from timestep 475500: Updated shield, loss is 0.04147421941161156
Progress update from timestep 475600: Updated shield, loss is 0.03815707564353943
Eval num_timesteps=475600, episode_reward=16.94 +/- 13.92
Episode length: 25.40 +/- 20.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 475600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.06     |
|    ep_rew_mean     | 2.47     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1189     |
|    time_elapsed    | 107803   |
|    total_timesteps | 475600   |
---------------------------------
Progress update from timestep 475700: Updated shield, loss is 0.03139874339103699
Progress update from timestep 475800: Updated shield, loss is 0.032163020223379135
Eval num_timesteps=475800, episode_reward=23.73 +/- 15.03
Episode length: 33.40 +/- 20.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.4          |
|    mean_reward          | 23.7          |
| time/                   |               |
|    total_timesteps      | 475800        |
| train/                  |               |
|    approx_kl            | 0.00019039153 |
|    clip_fraction        | 0.00536       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0158       |
|    explained_variance   | 0.161         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.27          |
|    n_updates            | 11890         |
|    policy_gradient_loss | -0.000417     |
|    value_loss           | 1.85          |
-------------------------------------------
Progress update from timestep 475900: Updated shield, loss is 0.03999963402748108
Progress update from timestep 476000: Updated shield, loss is 0.03989637270569801
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 476000: Saved shield_loss.csv. 
Eval num_timesteps=476000, episode_reward=21.97 +/- 15.38
Episode length: 32.20 +/- 21.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 22       |
| time/              |          |
|    total_timesteps | 476000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.78     |
|    ep_rew_mean     | 3.07     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1190     |
|    time_elapsed    | 107887   |
|    total_timesteps | 476000   |
---------------------------------
Progress update from timestep 476100: Updated shield, loss is 0.037995595484972
Progress update from timestep 476200: Updated shield, loss is 0.04424392804503441
Eval num_timesteps=476200, episode_reward=33.35 +/- 4.05
Episode length: 47.60 +/- 4.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 47.6          |
|    mean_reward          | 33.4          |
| time/                   |               |
|    total_timesteps      | 476200        |
| train/                  |               |
|    approx_kl            | 0.00036624525 |
|    clip_fraction        | 0.0121        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0151       |
|    explained_variance   | 0.137         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.638         |
|    n_updates            | 11900         |
|    policy_gradient_loss | -0.00113      |
|    value_loss           | 2.05          |
-------------------------------------------
Progress update from timestep 476300: Updated shield, loss is 0.03352232649922371
Progress update from timestep 476400: Updated shield, loss is 0.04145897552371025
Eval num_timesteps=476400, episode_reward=28.92 +/- 12.82
Episode length: 41.20 +/- 17.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | 28.9     |
| time/              |          |
|    total_timesteps | 476400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.91     |
|    ep_rew_mean     | 2.41     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1191     |
|    time_elapsed    | 107990   |
|    total_timesteps | 476400   |
---------------------------------
Progress update from timestep 476500: Updated shield, loss is 0.031694356352090836
Progress update from timestep 476600: Updated shield, loss is 0.04363316670060158
Eval num_timesteps=476600, episode_reward=11.77 +/- 10.83
Episode length: 18.20 +/- 15.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.2         |
|    mean_reward          | 11.8         |
| time/                   |              |
|    total_timesteps      | 476600       |
| train/                  |              |
|    approx_kl            | 0.0006161923 |
|    clip_fraction        | 0.00781      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00692     |
|    explained_variance   | 0.232        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.713        |
|    n_updates            | 11910        |
|    policy_gradient_loss | -0.000932    |
|    value_loss           | 1.47         |
------------------------------------------
Progress update from timestep 476700: Updated shield, loss is 0.03544849157333374
Progress update from timestep 476800: Updated shield, loss is 0.03614845499396324
Eval num_timesteps=476800, episode_reward=23.50 +/- 16.18
Episode length: 32.40 +/- 21.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.4     |
|    mean_reward     | 23.5     |
| time/              |          |
|    total_timesteps | 476800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.03     |
|    ep_rew_mean     | 2.55     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1192     |
|    time_elapsed    | 108071   |
|    total_timesteps | 476800   |
---------------------------------
Progress update from timestep 476900: Updated shield, loss is 0.04024216905236244
Progress update from timestep 477000: Updated shield, loss is 0.038850706070661545
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 477000: Saved shield_loss.csv. 
Eval num_timesteps=477000, episode_reward=21.61 +/- 14.19
Episode length: 31.60 +/- 20.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 31.6        |
|    mean_reward          | 21.6        |
| time/                   |             |
|    total_timesteps      | 477000      |
| train/                  |             |
|    approx_kl            | 0.001212899 |
|    clip_fraction        | 0.0219      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0344     |
|    explained_variance   | 0.216       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.918       |
|    n_updates            | 11920       |
|    policy_gradient_loss | -0.00228    |
|    value_loss           | 1.78        |
-----------------------------------------
Progress update from timestep 477100: Updated shield, loss is 0.036827921867370605
Progress update from timestep 477200: Updated shield, loss is 0.038899898529052734
Eval num_timesteps=477200, episode_reward=16.41 +/- 15.01
Episode length: 24.40 +/- 21.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 16.4     |
| time/              |          |
|    total_timesteps | 477200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.3      |
|    ep_rew_mean     | 2.78     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1193     |
|    time_elapsed    | 108177   |
|    total_timesteps | 477200   |
---------------------------------
Progress update from timestep 477300: Updated shield, loss is 0.0465305857360363
Progress update from timestep 477400: Updated shield, loss is 0.04014115035533905
Eval num_timesteps=477400, episode_reward=11.82 +/- 12.03
Episode length: 17.60 +/- 16.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17.6          |
|    mean_reward          | 11.8          |
| time/                   |               |
|    total_timesteps      | 477400        |
| train/                  |               |
|    approx_kl            | 0.00014625068 |
|    clip_fraction        | 0.00379       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0144       |
|    explained_variance   | 0.152         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.781         |
|    n_updates            | 11930         |
|    policy_gradient_loss | -0.000831     |
|    value_loss           | 1.83          |
-------------------------------------------
Progress update from timestep 477500: Updated shield, loss is 0.03477368503808975
Progress update from timestep 477600: Updated shield, loss is 0.035561297088861465
Eval num_timesteps=477600, episode_reward=22.58 +/- 12.56
Episode length: 33.20 +/- 17.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | 22.6     |
| time/              |          |
|    total_timesteps | 477600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.28     |
|    ep_rew_mean     | 2.72     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1194     |
|    time_elapsed    | 108273   |
|    total_timesteps | 477600   |
---------------------------------
Progress update from timestep 477700: Updated shield, loss is 0.04171227291226387
Progress update from timestep 477800: Updated shield, loss is 0.03786129131913185
Eval num_timesteps=477800, episode_reward=10.97 +/- 11.90
Episode length: 16.80 +/- 16.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.8         |
|    mean_reward          | 11           |
| time/                   |              |
|    total_timesteps      | 477800       |
| train/                  |              |
|    approx_kl            | 0.0005823508 |
|    clip_fraction        | 0.0163       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0393      |
|    explained_variance   | 0.161        |
|    learning_rate        | 0.0001       |
|    loss                 | 1            |
|    n_updates            | 11940        |
|    policy_gradient_loss | -0.00175     |
|    value_loss           | 1.57         |
------------------------------------------
Progress update from timestep 477900: Updated shield, loss is 0.0450892336666584
Progress update from timestep 478000: Updated shield, loss is 0.05469828471541405
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 478000: Saved shield_loss.csv. 
Eval num_timesteps=478000, episode_reward=17.03 +/- 15.33
Episode length: 24.60 +/- 20.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 478000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.77     |
|    ep_rew_mean     | 3.07     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1195     |
|    time_elapsed    | 108378   |
|    total_timesteps | 478000   |
---------------------------------
Progress update from timestep 478100: Updated shield, loss is 0.03611794859170914
Progress update from timestep 478200: Updated shield, loss is 0.03678838908672333
Eval num_timesteps=478200, episode_reward=21.25 +/- 13.19
Episode length: 30.40 +/- 17.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 30.4          |
|    mean_reward          | 21.2          |
| time/                   |               |
|    total_timesteps      | 478200        |
| train/                  |               |
|    approx_kl            | 8.4600026e-05 |
|    clip_fraction        | 0.00536       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0123       |
|    explained_variance   | 0.117         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.26          |
|    n_updates            | 11950         |
|    policy_gradient_loss | -0.000402     |
|    value_loss           | 2.54          |
-------------------------------------------
Progress update from timestep 478300: Updated shield, loss is 0.046765949577093124
Progress update from timestep 478400: Updated shield, loss is 0.03961103409528732
Eval num_timesteps=478400, episode_reward=9.93 +/- 12.48
Episode length: 15.00 +/- 17.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | 9.93     |
| time/              |          |
|    total_timesteps | 478400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.02     |
|    ep_rew_mean     | 2.5      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1196     |
|    time_elapsed    | 108470   |
|    total_timesteps | 478400   |
---------------------------------
Progress update from timestep 478500: Updated shield, loss is 0.03480900079011917
Progress update from timestep 478600: Updated shield, loss is 0.030486740171909332
Eval num_timesteps=478600, episode_reward=3.72 +/- 3.34
Episode length: 6.40 +/- 4.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 6.4           |
|    mean_reward          | 3.72          |
| time/                   |               |
|    total_timesteps      | 478600        |
| train/                  |               |
|    approx_kl            | 0.00014565024 |
|    clip_fraction        | 0.0116        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0217       |
|    explained_variance   | 0.398         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.622         |
|    n_updates            | 11960         |
|    policy_gradient_loss | -0.00117      |
|    value_loss           | 1.74          |
-------------------------------------------
Progress update from timestep 478700: Updated shield, loss is 0.04586753994226456
Progress update from timestep 478800: Updated shield, loss is 0.0387129932641983
Eval num_timesteps=478800, episode_reward=7.25 +/- 5.18
Episode length: 11.60 +/- 7.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 11.6     |
|    mean_reward     | 7.25     |
| time/              |          |
|    total_timesteps | 478800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.13     |
|    ep_rew_mean     | 2.63     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1197     |
|    time_elapsed    | 108529   |
|    total_timesteps | 478800   |
---------------------------------
Progress update from timestep 478900: Updated shield, loss is 0.035530172288417816
Progress update from timestep 479000: Updated shield, loss is 0.03493521735072136
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 479000: Saved shield_loss.csv. 
Eval num_timesteps=479000, episode_reward=17.45 +/- 14.38
Episode length: 25.60 +/- 19.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.6          |
|    mean_reward          | 17.4          |
| time/                   |               |
|    total_timesteps      | 479000        |
| train/                  |               |
|    approx_kl            | 0.00014080659 |
|    clip_fraction        | 0.00625       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0182       |
|    explained_variance   | 0.16          |
|    learning_rate        | 0.0001        |
|    loss                 | 1.21          |
|    n_updates            | 11970         |
|    policy_gradient_loss | -0.000464     |
|    value_loss           | 1.86          |
-------------------------------------------
Progress update from timestep 479100: Updated shield, loss is 0.03799980506300926
Progress update from timestep 479200: Updated shield, loss is 0.038380689918994904
Eval num_timesteps=479200, episode_reward=16.50 +/- 13.88
Episode length: 25.00 +/- 20.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 479200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.84     |
|    ep_rew_mean     | 2.4      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1198     |
|    time_elapsed    | 108616   |
|    total_timesteps | 479200   |
---------------------------------
Progress update from timestep 479300: Updated shield, loss is 0.03147940710186958
Progress update from timestep 479400: Updated shield, loss is 0.04137923941016197
Eval num_timesteps=479400, episode_reward=15.62 +/- 15.40
Episode length: 23.20 +/- 21.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.2          |
|    mean_reward          | 15.6          |
| time/                   |               |
|    total_timesteps      | 479400        |
| train/                  |               |
|    approx_kl            | 4.7875154e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.000695     |
|    explained_variance   | 0.103         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.536         |
|    n_updates            | 11980         |
|    policy_gradient_loss | -1.81e-07     |
|    value_loss           | 1.98          |
-------------------------------------------
Progress update from timestep 479500: Updated shield, loss is 0.03185972943902016
Progress update from timestep 479600: Updated shield, loss is 0.04003044217824936
Eval num_timesteps=479600, episode_reward=11.39 +/- 12.85
Episode length: 16.40 +/- 17.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | 11.4     |
| time/              |          |
|    total_timesteps | 479600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.93     |
|    ep_rew_mean     | 2.45     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1199     |
|    time_elapsed    | 108702   |
|    total_timesteps | 479600   |
---------------------------------
Progress update from timestep 479700: Updated shield, loss is 0.03763760253787041
Progress update from timestep 479800: Updated shield, loss is 0.0317145437002182
Eval num_timesteps=479800, episode_reward=16.00 +/- 16.06
Episode length: 23.20 +/- 21.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.2         |
|    mean_reward          | 16           |
| time/                   |              |
|    total_timesteps      | 479800       |
| train/                  |              |
|    approx_kl            | 0.0003902924 |
|    clip_fraction        | 0.00937      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0156      |
|    explained_variance   | 0.175        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.07         |
|    n_updates            | 11990        |
|    policy_gradient_loss | -0.0008      |
|    value_loss           | 1.66         |
------------------------------------------
Progress update from timestep 479900: Updated shield, loss is 0.036869823932647705
Progress update from timestep 480000: Updated shield, loss is 0.0407998189330101
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 480000: Saved shield_loss.csv. 
Eval num_timesteps=480000, episode_reward=21.48 +/- 11.09
Episode length: 31.40 +/- 15.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.4     |
|    mean_reward     | 21.5     |
| time/              |          |
|    total_timesteps | 480000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.28     |
|    ep_rew_mean     | 2.73     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1200     |
|    time_elapsed    | 108787   |
|    total_timesteps | 480000   |
---------------------------------
Progress update from timestep 480100: Updated shield, loss is 0.03864002227783203
Progress update from timestep 480200: Updated shield, loss is 0.0386592373251915
Eval num_timesteps=480200, episode_reward=20.77 +/- 13.16
Episode length: 29.40 +/- 17.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29.4         |
|    mean_reward          | 20.8         |
| time/                   |              |
|    total_timesteps      | 480200       |
| train/                  |              |
|    approx_kl            | 7.599952e-05 |
|    clip_fraction        | 0.00603      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00997     |
|    explained_variance   | 0.0856       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.862        |
|    n_updates            | 12000        |
|    policy_gradient_loss | -0.000662    |
|    value_loss           | 1.71         |
------------------------------------------
Progress update from timestep 480300: Updated shield, loss is 0.02957472763955593
Progress update from timestep 480400: Updated shield, loss is 0.03617684915661812
Eval num_timesteps=480400, episode_reward=17.49 +/- 13.65
Episode length: 26.60 +/- 20.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.6     |
|    mean_reward     | 17.5     |
| time/              |          |
|    total_timesteps | 480400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.82     |
|    ep_rew_mean     | 2.39     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1201     |
|    time_elapsed    | 108872   |
|    total_timesteps | 480400   |
---------------------------------
Progress update from timestep 480500: Updated shield, loss is 0.030186789110302925
Progress update from timestep 480600: Updated shield, loss is 0.030316224321722984
Eval num_timesteps=480600, episode_reward=12.32 +/- 12.47
Episode length: 18.20 +/- 17.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.2         |
|    mean_reward          | 12.3         |
| time/                   |              |
|    total_timesteps      | 480600       |
| train/                  |              |
|    approx_kl            | 0.0005051638 |
|    clip_fraction        | 0.01         |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0121      |
|    explained_variance   | 0.0549       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.28         |
|    n_updates            | 12010        |
|    policy_gradient_loss | -0.000826    |
|    value_loss           | 1.88         |
------------------------------------------
Progress update from timestep 480700: Updated shield, loss is 0.02643768861889839
Progress update from timestep 480800: Updated shield, loss is 0.04028066620230675
Eval num_timesteps=480800, episode_reward=9.97 +/- 11.78
Episode length: 15.60 +/- 17.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 9.97     |
| time/              |          |
|    total_timesteps | 480800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.49     |
|    ep_rew_mean     | 2.85     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1202     |
|    time_elapsed    | 108960   |
|    total_timesteps | 480800   |
---------------------------------
Progress update from timestep 480900: Updated shield, loss is 0.038792893290519714
Progress update from timestep 481000: Updated shield, loss is 0.030741320922970772
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 481000: Saved shield_loss.csv. 
Eval num_timesteps=481000, episode_reward=22.43 +/- 14.78
Episode length: 33.00 +/- 20.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33            |
|    mean_reward          | 22.4          |
| time/                   |               |
|    total_timesteps      | 481000        |
| train/                  |               |
|    approx_kl            | 0.00032164744 |
|    clip_fraction        | 0.0138        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0231       |
|    explained_variance   | 0.279         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.569         |
|    n_updates            | 12020         |
|    policy_gradient_loss | -0.00149      |
|    value_loss           | 2.18          |
-------------------------------------------
Progress update from timestep 481100: Updated shield, loss is 0.04283488541841507
Progress update from timestep 481200: Updated shield, loss is 0.04256569221615791
Eval num_timesteps=481200, episode_reward=34.69 +/- 0.83
Episode length: 50.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 34.7     |
| time/              |          |
|    total_timesteps | 481200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.86     |
|    ep_rew_mean     | 3.16     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1203     |
|    time_elapsed    | 109061   |
|    total_timesteps | 481200   |
---------------------------------
Progress update from timestep 481300: Updated shield, loss is 0.038678593933582306
Progress update from timestep 481400: Updated shield, loss is 0.03553243353962898
Eval num_timesteps=481400, episode_reward=8.93 +/- 12.87
Episode length: 13.80 +/- 18.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 13.8          |
|    mean_reward          | 8.93          |
| time/                   |               |
|    total_timesteps      | 481400        |
| train/                  |               |
|    approx_kl            | 0.00045965775 |
|    clip_fraction        | 0.00804       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0155       |
|    explained_variance   | 0.132         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.781         |
|    n_updates            | 12030         |
|    policy_gradient_loss | -0.00118      |
|    value_loss           | 2.49          |
-------------------------------------------
Progress update from timestep 481500: Updated shield, loss is 0.032235320657491684
Progress update from timestep 481600: Updated shield, loss is 0.047389715909957886
Eval num_timesteps=481600, episode_reward=15.65 +/- 14.93
Episode length: 23.60 +/- 21.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 15.6     |
| time/              |          |
|    total_timesteps | 481600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.56     |
|    ep_rew_mean     | 2.16     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1204     |
|    time_elapsed    | 109143   |
|    total_timesteps | 481600   |
---------------------------------
Progress update from timestep 481700: Updated shield, loss is 0.044081468135118484
Progress update from timestep 481800: Updated shield, loss is 0.035428475588560104
Eval num_timesteps=481800, episode_reward=30.52 +/- 8.56
Episode length: 44.00 +/- 12.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 44           |
|    mean_reward          | 30.5         |
| time/                   |              |
|    total_timesteps      | 481800       |
| train/                  |              |
|    approx_kl            | 0.0003200138 |
|    clip_fraction        | 0.00826      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0161      |
|    explained_variance   | 0.191        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.354        |
|    n_updates            | 12040        |
|    policy_gradient_loss | -0.000874    |
|    value_loss           | 1.29         |
------------------------------------------
Progress update from timestep 481900: Updated shield, loss is 0.03954191133379936
Progress update from timestep 482000: Updated shield, loss is 0.04311627149581909
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 482000: Saved shield_loss.csv. 
Eval num_timesteps=482000, episode_reward=18.34 +/- 13.65
Episode length: 27.20 +/- 19.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.2     |
|    mean_reward     | 18.3     |
| time/              |          |
|    total_timesteps | 482000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.77     |
|    ep_rew_mean     | 2.34     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1205     |
|    time_elapsed    | 109233   |
|    total_timesteps | 482000   |
---------------------------------
Progress update from timestep 482100: Updated shield, loss is 0.042921021580696106
Progress update from timestep 482200: Updated shield, loss is 0.03743845224380493
Eval num_timesteps=482200, episode_reward=23.09 +/- 14.42
Episode length: 33.60 +/- 20.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.6          |
|    mean_reward          | 23.1          |
| time/                   |               |
|    total_timesteps      | 482200        |
| train/                  |               |
|    approx_kl            | 0.00033009477 |
|    clip_fraction        | 0.00781       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0128       |
|    explained_variance   | 0.101         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.743         |
|    n_updates            | 12050         |
|    policy_gradient_loss | -0.000891     |
|    value_loss           | 1.94          |
-------------------------------------------
Progress update from timestep 482300: Updated shield, loss is 0.0347854420542717
Progress update from timestep 482400: Updated shield, loss is 0.029639415442943573
Eval num_timesteps=482400, episode_reward=10.74 +/- 12.98
Episode length: 15.60 +/- 17.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 10.7     |
| time/              |          |
|    total_timesteps | 482400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.05     |
|    ep_rew_mean     | 2.51     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1206     |
|    time_elapsed    | 109316   |
|    total_timesteps | 482400   |
---------------------------------
Progress update from timestep 482500: Updated shield, loss is 0.050170764327049255
Progress update from timestep 482600: Updated shield, loss is 0.03895416855812073
Eval num_timesteps=482600, episode_reward=23.09 +/- 15.76
Episode length: 32.60 +/- 21.33
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 32.6           |
|    mean_reward          | 23.1           |
| time/                   |                |
|    total_timesteps      | 482600         |
| train/                  |                |
|    approx_kl            | -1.0816491e-11 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.000712      |
|    explained_variance   | 0.0801         |
|    learning_rate        | 0.0001         |
|    loss                 | 1.4            |
|    n_updates            | 12060          |
|    policy_gradient_loss | -1.84e-07      |
|    value_loss           | 1.5            |
--------------------------------------------
Progress update from timestep 482700: Updated shield, loss is 0.04865604639053345
Progress update from timestep 482800: Updated shield, loss is 0.03692929446697235
Eval num_timesteps=482800, episode_reward=4.63 +/- 2.90
Episode length: 7.40 +/- 4.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7.4      |
|    mean_reward     | 4.63     |
| time/              |          |
|    total_timesteps | 482800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.15     |
|    ep_rew_mean     | 2.57     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1207     |
|    time_elapsed    | 109406   |
|    total_timesteps | 482800   |
---------------------------------
Progress update from timestep 482900: Updated shield, loss is 0.03325912728905678
Progress update from timestep 483000: Updated shield, loss is 0.03510244935750961
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 483000: Saved shield_loss.csv. 
Eval num_timesteps=483000, episode_reward=9.96 +/- 12.90
Episode length: 14.80 +/- 17.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.8          |
|    mean_reward          | 9.96          |
| time/                   |               |
|    total_timesteps      | 483000        |
| train/                  |               |
|    approx_kl            | 0.00012788454 |
|    clip_fraction        | 0.0029        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0115       |
|    explained_variance   | 0.172         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.673         |
|    n_updates            | 12070         |
|    policy_gradient_loss | -0.000422     |
|    value_loss           | 2.19          |
-------------------------------------------
Progress update from timestep 483100: Updated shield, loss is 0.03335678577423096
Progress update from timestep 483200: Updated shield, loss is 0.03525194153189659
Eval num_timesteps=483200, episode_reward=9.16 +/- 12.24
Episode length: 14.40 +/- 18.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.4     |
|    mean_reward     | 9.16     |
| time/              |          |
|    total_timesteps | 483200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.14     |
|    ep_rew_mean     | 2.7      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1208     |
|    time_elapsed    | 109487   |
|    total_timesteps | 483200   |
---------------------------------
Progress update from timestep 483300: Updated shield, loss is 0.04269646853208542
Progress update from timestep 483400: Updated shield, loss is 0.035763755440711975
Eval num_timesteps=483400, episode_reward=19.02 +/- 13.61
Episode length: 27.40 +/- 18.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 27.4          |
|    mean_reward          | 19            |
| time/                   |               |
|    total_timesteps      | 483400        |
| train/                  |               |
|    approx_kl            | 0.00031037498 |
|    clip_fraction        | 0.0096        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.021        |
|    explained_variance   | 0.165         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.75          |
|    n_updates            | 12080         |
|    policy_gradient_loss | -0.00101      |
|    value_loss           | 2.18          |
-------------------------------------------
Progress update from timestep 483500: Updated shield, loss is 0.037209127098321915
Progress update from timestep 483600: Updated shield, loss is 0.038662925362586975
Eval num_timesteps=483600, episode_reward=16.72 +/- 13.65
Episode length: 25.20 +/- 20.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 483600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.57     |
|    ep_rew_mean     | 2.97     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1209     |
|    time_elapsed    | 109572   |
|    total_timesteps | 483600   |
---------------------------------
Progress update from timestep 483700: Updated shield, loss is 0.03497638553380966
Progress update from timestep 483800: Updated shield, loss is 0.04451751708984375
Eval num_timesteps=483800, episode_reward=24.77 +/- 14.92
Episode length: 34.40 +/- 19.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.4          |
|    mean_reward          | 24.8          |
| time/                   |               |
|    total_timesteps      | 483800        |
| train/                  |               |
|    approx_kl            | 0.00010440278 |
|    clip_fraction        | 0.0029        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.011        |
|    explained_variance   | 0.285         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.42          |
|    n_updates            | 12090         |
|    policy_gradient_loss | -0.000381     |
|    value_loss           | 2.5           |
-------------------------------------------
Progress update from timestep 483900: Updated shield, loss is 0.03729313611984253
Progress update from timestep 484000: Updated shield, loss is 0.03204823657870293
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 484000: Saved shield_loss.csv. 
Eval num_timesteps=484000, episode_reward=28.43 +/- 13.23
Episode length: 40.80 +/- 18.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.8     |
|    mean_reward     | 28.4     |
| time/              |          |
|    total_timesteps | 484000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.16     |
|    ep_rew_mean     | 2.64     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1210     |
|    time_elapsed    | 109675   |
|    total_timesteps | 484000   |
---------------------------------
Progress update from timestep 484100: Updated shield, loss is 0.03587077558040619
Progress update from timestep 484200: Updated shield, loss is 0.03829680010676384
Eval num_timesteps=484200, episode_reward=22.95 +/- 15.03
Episode length: 33.00 +/- 20.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33            |
|    mean_reward          | 23            |
| time/                   |               |
|    total_timesteps      | 484200        |
| train/                  |               |
|    approx_kl            | 0.00042352447 |
|    clip_fraction        | 0.0156        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0234       |
|    explained_variance   | 0.241         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.985         |
|    n_updates            | 12100         |
|    policy_gradient_loss | -0.00187      |
|    value_loss           | 1.55          |
-------------------------------------------
Progress update from timestep 484300: Updated shield, loss is 0.029398595914244652
Progress update from timestep 484400: Updated shield, loss is 0.03778577595949173
Eval num_timesteps=484400, episode_reward=16.11 +/- 14.59
Episode length: 24.20 +/- 21.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 16.1     |
| time/              |          |
|    total_timesteps | 484400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.19     |
|    ep_rew_mean     | 2.62     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1211     |
|    time_elapsed    | 109777   |
|    total_timesteps | 484400   |
---------------------------------
Progress update from timestep 484500: Updated shield, loss is 0.0391831137239933
Progress update from timestep 484600: Updated shield, loss is 0.03900327533483505
Eval num_timesteps=484600, episode_reward=13.19 +/- 11.20
Episode length: 19.80 +/- 15.85
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 19.8          |
|    mean_reward          | 13.2          |
| time/                   |               |
|    total_timesteps      | 484600        |
| train/                  |               |
|    approx_kl            | 0.00027011117 |
|    clip_fraction        | 0.00491       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00672      |
|    explained_variance   | 0.195         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.556         |
|    n_updates            | 12110         |
|    policy_gradient_loss | -0.000783     |
|    value_loss           | 2.04          |
-------------------------------------------
Progress update from timestep 484700: Updated shield, loss is 0.03408567234873772
Progress update from timestep 484800: Updated shield, loss is 0.032571498304605484
Eval num_timesteps=484800, episode_reward=15.20 +/- 16.66
Episode length: 22.00 +/- 22.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22       |
|    mean_reward     | 15.2     |
| time/              |          |
|    total_timesteps | 484800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.41     |
|    ep_rew_mean     | 2.74     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1212     |
|    time_elapsed    | 109860   |
|    total_timesteps | 484800   |
---------------------------------
Progress update from timestep 484900: Updated shield, loss is 0.04314759373664856
Progress update from timestep 485000: Updated shield, loss is 0.033865638077259064
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 485000: Saved shield_loss.csv. 
Eval num_timesteps=485000, episode_reward=22.72 +/- 15.32
Episode length: 32.60 +/- 21.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32.6          |
|    mean_reward          | 22.7          |
| time/                   |               |
|    total_timesteps      | 485000        |
| train/                  |               |
|    approx_kl            | 0.00033450063 |
|    clip_fraction        | 0.00871       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0198       |
|    explained_variance   | 0.193         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.437         |
|    n_updates            | 12120         |
|    policy_gradient_loss | -0.00088      |
|    value_loss           | 1.24          |
-------------------------------------------
Progress update from timestep 485100: Updated shield, loss is 0.03606098145246506
Progress update from timestep 485200: Updated shield, loss is 0.04238021746277809
Eval num_timesteps=485200, episode_reward=18.25 +/- 14.47
Episode length: 27.00 +/- 20.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27       |
|    mean_reward     | 18.3     |
| time/              |          |
|    total_timesteps | 485200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.39     |
|    ep_rew_mean     | 2.06     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1213     |
|    time_elapsed    | 109944   |
|    total_timesteps | 485200   |
---------------------------------
Progress update from timestep 485300: Updated shield, loss is 0.04109112173318863
Progress update from timestep 485400: Updated shield, loss is 0.04770153388381004
Eval num_timesteps=485400, episode_reward=17.22 +/- 14.39
Episode length: 25.40 +/- 20.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.4          |
|    mean_reward          | 17.2          |
| time/                   |               |
|    total_timesteps      | 485400        |
| train/                  |               |
|    approx_kl            | 0.00019147772 |
|    clip_fraction        | 0.00558       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0142       |
|    explained_variance   | 0.0631        |
|    learning_rate        | 0.0001        |
|    loss                 | 1             |
|    n_updates            | 12130         |
|    policy_gradient_loss | -0.000822     |
|    value_loss           | 1.59          |
-------------------------------------------
Progress update from timestep 485500: Updated shield, loss is 0.03902985155582428
Progress update from timestep 485600: Updated shield, loss is 0.03545210510492325
Eval num_timesteps=485600, episode_reward=21.81 +/- 16.92
Episode length: 31.00 +/- 23.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31       |
|    mean_reward     | 21.8     |
| time/              |          |
|    total_timesteps | 485600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.8      |
|    ep_rew_mean     | 2.35     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1214     |
|    time_elapsed    | 110047   |
|    total_timesteps | 485600   |
---------------------------------
Progress update from timestep 485700: Updated shield, loss is 0.03479776531457901
Progress update from timestep 485800: Updated shield, loss is 0.037884294986724854
Eval num_timesteps=485800, episode_reward=23.63 +/- 14.19
Episode length: 34.00 +/- 19.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34            |
|    mean_reward          | 23.6          |
| time/                   |               |
|    total_timesteps      | 485800        |
| train/                  |               |
|    approx_kl            | 0.00074629823 |
|    clip_fraction        | 0.0158        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0239       |
|    explained_variance   | 0.139         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.788         |
|    n_updates            | 12140         |
|    policy_gradient_loss | -0.00125      |
|    value_loss           | 1.51          |
-------------------------------------------
Progress update from timestep 485900: Updated shield, loss is 0.033587608486413956
Progress update from timestep 486000: Updated shield, loss is 0.03698991611599922
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 486000: Saved shield_loss.csv. 
Eval num_timesteps=486000, episode_reward=17.76 +/- 14.18
Episode length: 25.80 +/- 19.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | 17.8     |
| time/              |          |
|    total_timesteps | 486000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.37     |
|    ep_rew_mean     | 2.81     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1215     |
|    time_elapsed    | 110150   |
|    total_timesteps | 486000   |
---------------------------------
Progress update from timestep 486100: Updated shield, loss is 0.040623076260089874
Progress update from timestep 486200: Updated shield, loss is 0.03625968098640442
Eval num_timesteps=486200, episode_reward=6.94 +/- 4.41
Episode length: 11.00 +/- 6.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 11            |
|    mean_reward          | 6.94          |
| time/                   |               |
|    total_timesteps      | 486200        |
| train/                  |               |
|    approx_kl            | 0.00024190839 |
|    clip_fraction        | 0.00536       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00898      |
|    explained_variance   | 0.136         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.16          |
|    n_updates            | 12150         |
|    policy_gradient_loss | -0.000972     |
|    value_loss           | 2.8           |
-------------------------------------------
Progress update from timestep 486300: Updated shield, loss is 0.03728453442454338
Progress update from timestep 486400: Updated shield, loss is 0.034410182386636734
Eval num_timesteps=486400, episode_reward=23.08 +/- 13.18
Episode length: 34.60 +/- 19.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 23.1     |
| time/              |          |
|    total_timesteps | 486400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.87     |
|    ep_rew_mean     | 2.41     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1216     |
|    time_elapsed    | 110230   |
|    total_timesteps | 486400   |
---------------------------------
Progress update from timestep 486500: Updated shield, loss is 0.039453957229852676
Progress update from timestep 486600: Updated shield, loss is 0.040963370352983475
Eval num_timesteps=486600, episode_reward=17.00 +/- 15.89
Episode length: 24.20 +/- 21.43
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.2          |
|    mean_reward          | 17            |
| time/                   |               |
|    total_timesteps      | 486600        |
| train/                  |               |
|    approx_kl            | 0.00012047855 |
|    clip_fraction        | 0.00536       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.013        |
|    explained_variance   | 0.0572        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.447         |
|    n_updates            | 12160         |
|    policy_gradient_loss | -0.000546     |
|    value_loss           | 1.41          |
-------------------------------------------
Progress update from timestep 486700: Updated shield, loss is 0.048368193209171295
Progress update from timestep 486800: Updated shield, loss is 0.046833720058202744
Eval num_timesteps=486800, episode_reward=16.88 +/- 14.47
Episode length: 24.80 +/- 20.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 486800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.01     |
|    ep_rew_mean     | 2.47     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1217     |
|    time_elapsed    | 110316   |
|    total_timesteps | 486800   |
---------------------------------
Progress update from timestep 486900: Updated shield, loss is 0.03293191269040108
Progress update from timestep 487000: Updated shield, loss is 0.03345329686999321
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 487000: Saved shield_loss.csv. 
Eval num_timesteps=487000, episode_reward=23.36 +/- 14.86
Episode length: 33.40 +/- 20.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.4          |
|    mean_reward          | 23.4          |
| time/                   |               |
|    total_timesteps      | 487000        |
| train/                  |               |
|    approx_kl            | 0.00014584755 |
|    clip_fraction        | 0.00402       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0201       |
|    explained_variance   | 0.187         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.794         |
|    n_updates            | 12170         |
|    policy_gradient_loss | -0.000619     |
|    value_loss           | 1.34          |
-------------------------------------------
Progress update from timestep 487100: Updated shield, loss is 0.042814139276742935
Progress update from timestep 487200: Updated shield, loss is 0.03720256686210632
Eval num_timesteps=487200, episode_reward=17.40 +/- 14.88
Episode length: 25.20 +/- 20.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 487200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.24     |
|    ep_rew_mean     | 2.65     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1218     |
|    time_elapsed    | 110401   |
|    total_timesteps | 487200   |
---------------------------------
Progress update from timestep 487300: Updated shield, loss is 0.032401539385318756
Progress update from timestep 487400: Updated shield, loss is 0.03774289786815643
Eval num_timesteps=487400, episode_reward=3.57 +/- 0.64
Episode length: 6.00 +/- 0.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 6             |
|    mean_reward          | 3.57          |
| time/                   |               |
|    total_timesteps      | 487400        |
| train/                  |               |
|    approx_kl            | 0.00057336816 |
|    clip_fraction        | 0.00402       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0112       |
|    explained_variance   | 0.0658        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.335         |
|    n_updates            | 12180         |
|    policy_gradient_loss | -0.000379     |
|    value_loss           | 1.68          |
-------------------------------------------
Progress update from timestep 487500: Updated shield, loss is 0.036915235221385956
Progress update from timestep 487600: Updated shield, loss is 0.03694283217191696
Eval num_timesteps=487600, episode_reward=29.89 +/- 10.28
Episode length: 43.00 +/- 14.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43       |
|    mean_reward     | 29.9     |
| time/              |          |
|    total_timesteps | 487600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.34     |
|    ep_rew_mean     | 2.7      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1219     |
|    time_elapsed    | 110472   |
|    total_timesteps | 487600   |
---------------------------------
Progress update from timestep 487700: Updated shield, loss is 0.03480159491300583
Progress update from timestep 487800: Updated shield, loss is 0.05046539381146431
Eval num_timesteps=487800, episode_reward=10.41 +/- 11.91
Episode length: 16.00 +/- 17.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | 10.4        |
| time/                   |             |
|    total_timesteps      | 487800      |
| train/                  |             |
|    approx_kl            | 0.001811537 |
|    clip_fraction        | 0.0205      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0276     |
|    explained_variance   | 0.267       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.693       |
|    n_updates            | 12190       |
|    policy_gradient_loss | -0.00163    |
|    value_loss           | 1.31        |
-----------------------------------------
Progress update from timestep 487900: Updated shield, loss is 0.03071267530322075
Progress update from timestep 488000: Updated shield, loss is 0.04151776060461998
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 488000: Saved shield_loss.csv. 
Eval num_timesteps=488000, episode_reward=23.20 +/- 16.09
Episode length: 32.40 +/- 21.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.4     |
|    mean_reward     | 23.2     |
| time/              |          |
|    total_timesteps | 488000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.72     |
|    ep_rew_mean     | 3.01     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1220     |
|    time_elapsed    | 110572   |
|    total_timesteps | 488000   |
---------------------------------
Progress update from timestep 488100: Updated shield, loss is 0.05500558018684387
Progress update from timestep 488200: Updated shield, loss is 0.032234709709882736
Eval num_timesteps=488200, episode_reward=18.53 +/- 14.17
Episode length: 26.60 +/- 19.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.6          |
|    mean_reward          | 18.5          |
| time/                   |               |
|    total_timesteps      | 488200        |
| train/                  |               |
|    approx_kl            | 0.00033345656 |
|    clip_fraction        | 0.01          |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0202       |
|    explained_variance   | 0.14          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.947         |
|    n_updates            | 12200         |
|    policy_gradient_loss | -0.00122      |
|    value_loss           | 2.49          |
-------------------------------------------
Progress update from timestep 488300: Updated shield, loss is 0.03817460313439369
Progress update from timestep 488400: Updated shield, loss is 0.04989442229270935
Eval num_timesteps=488400, episode_reward=22.91 +/- 13.72
Episode length: 33.80 +/- 19.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 22.9     |
| time/              |          |
|    total_timesteps | 488400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.18     |
|    ep_rew_mean     | 2.66     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1221     |
|    time_elapsed    | 110659   |
|    total_timesteps | 488400   |
---------------------------------
Progress update from timestep 488500: Updated shield, loss is 0.03575839102268219
Progress update from timestep 488600: Updated shield, loss is 0.047710515558719635
Eval num_timesteps=488600, episode_reward=14.18 +/- 11.24
Episode length: 20.80 +/- 15.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 20.8          |
|    mean_reward          | 14.2          |
| time/                   |               |
|    total_timesteps      | 488600        |
| train/                  |               |
|    approx_kl            | 0.00023790855 |
|    clip_fraction        | 0.00759       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0099       |
|    explained_variance   | 0.157         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.955         |
|    n_updates            | 12210         |
|    policy_gradient_loss | -0.000873     |
|    value_loss           | 1.53          |
-------------------------------------------
Progress update from timestep 488700: Updated shield, loss is 0.06275591999292374
Progress update from timestep 488800: Updated shield, loss is 0.035424068570137024
Eval num_timesteps=488800, episode_reward=18.37 +/- 13.92
Episode length: 26.80 +/- 19.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.8     |
|    mean_reward     | 18.4     |
| time/              |          |
|    total_timesteps | 488800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.01     |
|    ep_rew_mean     | 2.5      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1222     |
|    time_elapsed    | 110750   |
|    total_timesteps | 488800   |
---------------------------------
Progress update from timestep 488900: Updated shield, loss is 0.09018666297197342
Progress update from timestep 489000: Updated shield, loss is 0.046961698681116104
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 489000: Saved shield_loss.csv. 
Eval num_timesteps=489000, episode_reward=24.58 +/- 13.23
Episode length: 35.40 +/- 18.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.4         |
|    mean_reward          | 24.6         |
| time/                   |              |
|    total_timesteps      | 489000       |
| train/                  |              |
|    approx_kl            | 9.843079e-05 |
|    clip_fraction        | 0.00313      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00886     |
|    explained_variance   | 0.29         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.815        |
|    n_updates            | 12220        |
|    policy_gradient_loss | -0.00108     |
|    value_loss           | 1.62         |
------------------------------------------
Progress update from timestep 489100: Updated shield, loss is 0.03968951851129532
Progress update from timestep 489200: Updated shield, loss is 0.05583973973989487
Eval num_timesteps=489200, episode_reward=21.53 +/- 14.99
Episode length: 32.20 +/- 21.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 21.5     |
| time/              |          |
|    total_timesteps | 489200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.23     |
|    ep_rew_mean     | 2.69     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1223     |
|    time_elapsed    | 110870   |
|    total_timesteps | 489200   |
---------------------------------
Progress update from timestep 489300: Updated shield, loss is 0.047803234308958054
Progress update from timestep 489400: Updated shield, loss is 0.045051611959934235
Eval num_timesteps=489400, episode_reward=15.00 +/- 15.47
Episode length: 22.60 +/- 22.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.6          |
|    mean_reward          | 15            |
| time/                   |               |
|    total_timesteps      | 489400        |
| train/                  |               |
|    approx_kl            | 0.00011115714 |
|    clip_fraction        | 0.00402       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.013        |
|    explained_variance   | 0.196         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.504         |
|    n_updates            | 12230         |
|    policy_gradient_loss | -0.000488     |
|    value_loss           | 1.44          |
-------------------------------------------
Progress update from timestep 489500: Updated shield, loss is 0.04757741466164589
Progress update from timestep 489600: Updated shield, loss is 0.04320591315627098
Eval num_timesteps=489600, episode_reward=7.08 +/- 2.11
Episode length: 11.00 +/- 3.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 11       |
|    mean_reward     | 7.08     |
| time/              |          |
|    total_timesteps | 489600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.79     |
|    ep_rew_mean     | 2.29     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1224     |
|    time_elapsed    | 110938   |
|    total_timesteps | 489600   |
---------------------------------
Progress update from timestep 489700: Updated shield, loss is 0.039064500480890274
Progress update from timestep 489800: Updated shield, loss is 0.03471982106566429
Eval num_timesteps=489800, episode_reward=12.37 +/- 12.51
Episode length: 17.80 +/- 16.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.8         |
|    mean_reward          | 12.4         |
| time/                   |              |
|    total_timesteps      | 489800       |
| train/                  |              |
|    approx_kl            | 0.0004091788 |
|    clip_fraction        | 0.00625      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0114      |
|    explained_variance   | 0.215        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.662        |
|    n_updates            | 12240        |
|    policy_gradient_loss | -0.000377    |
|    value_loss           | 1.05         |
------------------------------------------
Progress update from timestep 489900: Updated shield, loss is 0.04126677289605141
Progress update from timestep 490000: Updated shield, loss is 0.04035352170467377
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 490000: Saved shield_loss.csv. 
Eval num_timesteps=490000, episode_reward=16.98 +/- 14.67
Episode length: 25.00 +/- 20.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 490000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.85     |
|    ep_rew_mean     | 2.38     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1225     |
|    time_elapsed    | 111022   |
|    total_timesteps | 490000   |
---------------------------------
Progress update from timestep 490100: Updated shield, loss is 0.0577775277197361
Progress update from timestep 490200: Updated shield, loss is 0.03958992287516594
Eval num_timesteps=490200, episode_reward=11.69 +/- 12.76
Episode length: 17.00 +/- 16.83
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17            |
|    mean_reward          | 11.7          |
| time/                   |               |
|    total_timesteps      | 490200        |
| train/                  |               |
|    approx_kl            | 0.00030441635 |
|    clip_fraction        | 0.00536       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00666      |
|    explained_variance   | 0.177         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.579         |
|    n_updates            | 12250         |
|    policy_gradient_loss | -0.000737     |
|    value_loss           | 1.27          |
-------------------------------------------
Progress update from timestep 490300: Updated shield, loss is 0.036301035434007645
Progress update from timestep 490400: Updated shield, loss is 0.045017655938863754
Eval num_timesteps=490400, episode_reward=6.19 +/- 4.80
Episode length: 9.80 +/- 7.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.8      |
|    mean_reward     | 6.19     |
| time/              |          |
|    total_timesteps | 490400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.69     |
|    ep_rew_mean     | 2.96     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1226     |
|    time_elapsed    | 111092   |
|    total_timesteps | 490400   |
---------------------------------
Progress update from timestep 490500: Updated shield, loss is 0.03263932466506958
Progress update from timestep 490600: Updated shield, loss is 0.044506512582302094
Eval num_timesteps=490600, episode_reward=4.60 +/- 2.42
Episode length: 7.40 +/- 3.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 7.4          |
|    mean_reward          | 4.6          |
| time/                   |              |
|    total_timesteps      | 490600       |
| train/                  |              |
|    approx_kl            | 4.993417e-05 |
|    clip_fraction        | 0.00246      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0104      |
|    explained_variance   | 0.213        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.421        |
|    n_updates            | 12260        |
|    policy_gradient_loss | -0.00074     |
|    value_loss           | 1.78         |
------------------------------------------
Progress update from timestep 490700: Updated shield, loss is 0.03527286276221275
Progress update from timestep 490800: Updated shield, loss is 0.04364138841629028
Eval num_timesteps=490800, episode_reward=5.05 +/- 4.65
Episode length: 8.20 +/- 7.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8.2      |
|    mean_reward     | 5.05     |
| time/              |          |
|    total_timesteps | 490800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.18     |
|    ep_rew_mean     | 2.68     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1227     |
|    time_elapsed    | 111150   |
|    total_timesteps | 490800   |
---------------------------------
Progress update from timestep 490900: Updated shield, loss is 0.03381500020623207
Progress update from timestep 491000: Updated shield, loss is 0.0319034680724144
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 491000: Saved shield_loss.csv. 
Eval num_timesteps=491000, episode_reward=29.49 +/- 12.76
Episode length: 41.40 +/- 17.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.4         |
|    mean_reward          | 29.5         |
| time/                   |              |
|    total_timesteps      | 491000       |
| train/                  |              |
|    approx_kl            | 7.653581e-05 |
|    clip_fraction        | 0.0029       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0138      |
|    explained_variance   | 0.15         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.82         |
|    n_updates            | 12270        |
|    policy_gradient_loss | -0.000311    |
|    value_loss           | 1.8          |
------------------------------------------
Progress update from timestep 491100: Updated shield, loss is 0.03149263933300972
Progress update from timestep 491200: Updated shield, loss is 0.03692364692687988
Eval num_timesteps=491200, episode_reward=24.50 +/- 14.95
Episode length: 33.80 +/- 19.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 24.5     |
| time/              |          |
|    total_timesteps | 491200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.09     |
|    ep_rew_mean     | 2.61     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1228     |
|    time_elapsed    | 111237   |
|    total_timesteps | 491200   |
---------------------------------
Progress update from timestep 491300: Updated shield, loss is 0.03997109457850456
Progress update from timestep 491400: Updated shield, loss is 0.03599569573998451
Eval num_timesteps=491400, episode_reward=28.63 +/- 13.94
Episode length: 40.40 +/- 19.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 40.4          |
|    mean_reward          | 28.6          |
| time/                   |               |
|    total_timesteps      | 491400        |
| train/                  |               |
|    approx_kl            | 3.1482105e-05 |
|    clip_fraction        | 0.00179       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00696      |
|    explained_variance   | 0.183         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.345         |
|    n_updates            | 12280         |
|    policy_gradient_loss | -0.000521     |
|    value_loss           | 1.42          |
-------------------------------------------
Progress update from timestep 491500: Updated shield, loss is 0.037873316556215286
Progress update from timestep 491600: Updated shield, loss is 0.03183621168136597
Eval num_timesteps=491600, episode_reward=23.92 +/- 13.23
Episode length: 35.00 +/- 18.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 23.9     |
| time/              |          |
|    total_timesteps | 491600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.09     |
|    ep_rew_mean     | 2.56     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1229     |
|    time_elapsed    | 111345   |
|    total_timesteps | 491600   |
---------------------------------
Progress update from timestep 491700: Updated shield, loss is 0.04408836364746094
Progress update from timestep 491800: Updated shield, loss is 0.03401634097099304
Eval num_timesteps=491800, episode_reward=16.75 +/- 15.11
Episode length: 24.60 +/- 21.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.6         |
|    mean_reward          | 16.7         |
| time/                   |              |
|    total_timesteps      | 491800       |
| train/                  |              |
|    approx_kl            | 0.0007108287 |
|    clip_fraction        | 0.0103       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0172      |
|    explained_variance   | 0.179        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.551        |
|    n_updates            | 12290        |
|    policy_gradient_loss | -0.000924    |
|    value_loss           | 1.24         |
------------------------------------------
Progress update from timestep 491900: Updated shield, loss is 0.030281176790595055
Progress update from timestep 492000: Updated shield, loss is 0.029563691467046738
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 492000: Saved shield_loss.csv. 
Eval num_timesteps=492000, episode_reward=21.97 +/- 15.83
Episode length: 31.80 +/- 22.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.8     |
|    mean_reward     | 22       |
| time/              |          |
|    total_timesteps | 492000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.98     |
|    ep_rew_mean     | 2.52     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1230     |
|    time_elapsed    | 111447   |
|    total_timesteps | 492000   |
---------------------------------
Progress update from timestep 492100: Updated shield, loss is 0.03194546699523926
Progress update from timestep 492200: Updated shield, loss is 0.03959031403064728
Eval num_timesteps=492200, episode_reward=16.30 +/- 14.97
Episode length: 24.20 +/- 21.25
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.2          |
|    mean_reward          | 16.3          |
| time/                   |               |
|    total_timesteps      | 492200        |
| train/                  |               |
|    approx_kl            | 0.00026542583 |
|    clip_fraction        | 0.00781       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00645      |
|    explained_variance   | 0.146         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.65          |
|    n_updates            | 12300         |
|    policy_gradient_loss | -0.000972     |
|    value_loss           | 1.88          |
-------------------------------------------
Progress update from timestep 492300: Updated shield, loss is 0.03150711581110954
Progress update from timestep 492400: Updated shield, loss is 0.03521963208913803
Eval num_timesteps=492400, episode_reward=4.04 +/- 4.80
Episode length: 6.60 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6.6      |
|    mean_reward     | 4.04     |
| time/              |          |
|    total_timesteps | 492400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.85     |
|    ep_rew_mean     | 2.34     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1231     |
|    time_elapsed    | 111516   |
|    total_timesteps | 492400   |
---------------------------------
Progress update from timestep 492500: Updated shield, loss is 0.031489014625549316
Progress update from timestep 492600: Updated shield, loss is 0.02476326748728752
Eval num_timesteps=492600, episode_reward=28.99 +/- 13.77
Episode length: 40.60 +/- 18.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 40.6         |
|    mean_reward          | 29           |
| time/                   |              |
|    total_timesteps      | 492600       |
| train/                  |              |
|    approx_kl            | 0.0002994368 |
|    clip_fraction        | 0.00647      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0107      |
|    explained_variance   | 0.219        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.674        |
|    n_updates            | 12310        |
|    policy_gradient_loss | -0.00031     |
|    value_loss           | 1.43         |
------------------------------------------
Progress update from timestep 492700: Updated shield, loss is 0.03375539556145668
Progress update from timestep 492800: Updated shield, loss is 0.037380289286375046
Eval num_timesteps=492800, episode_reward=10.54 +/- 13.15
Episode length: 15.20 +/- 17.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | 10.5     |
| time/              |          |
|    total_timesteps | 492800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.88     |
|    ep_rew_mean     | 2.42     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1232     |
|    time_elapsed    | 111597   |
|    total_timesteps | 492800   |
---------------------------------
Progress update from timestep 492900: Updated shield, loss is 0.026898667216300964
Progress update from timestep 493000: Updated shield, loss is 0.033944837749004364
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 493000: Saved shield_loss.csv. 
Eval num_timesteps=493000, episode_reward=29.22 +/- 12.22
Episode length: 41.60 +/- 16.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 41.6          |
|    mean_reward          | 29.2          |
| time/                   |               |
|    total_timesteps      | 493000        |
| train/                  |               |
|    approx_kl            | 9.8528704e-05 |
|    clip_fraction        | 0.00313       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00578      |
|    explained_variance   | 0.259         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.341         |
|    n_updates            | 12320         |
|    policy_gradient_loss | -0.00039      |
|    value_loss           | 1.49          |
-------------------------------------------
Progress update from timestep 493100: Updated shield, loss is 0.04526674002408981
Progress update from timestep 493200: Updated shield, loss is 0.031696151942014694
Eval num_timesteps=493200, episode_reward=29.74 +/- 10.63
Episode length: 42.60 +/- 14.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.6     |
|    mean_reward     | 29.7     |
| time/              |          |
|    total_timesteps | 493200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.88     |
|    ep_rew_mean     | 2.41     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1233     |
|    time_elapsed    | 111701   |
|    total_timesteps | 493200   |
---------------------------------
Progress update from timestep 493300: Updated shield, loss is 0.02507978305220604
Progress update from timestep 493400: Updated shield, loss is 0.032868556678295135
Eval num_timesteps=493400, episode_reward=9.82 +/- 13.52
Episode length: 14.40 +/- 17.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.4          |
|    mean_reward          | 9.82          |
| time/                   |               |
|    total_timesteps      | 493400        |
| train/                  |               |
|    approx_kl            | 0.00012208801 |
|    clip_fraction        | 0.00402       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0187       |
|    explained_variance   | 0.118         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.98          |
|    n_updates            | 12330         |
|    policy_gradient_loss | -0.000856     |
|    value_loss           | 1.81          |
-------------------------------------------
Progress update from timestep 493500: Updated shield, loss is 0.027303040027618408
Progress update from timestep 493600: Updated shield, loss is 0.03940863907337189
Eval num_timesteps=493600, episode_reward=7.89 +/- 5.25
Episode length: 12.00 +/- 7.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 12       |
|    mean_reward     | 7.89     |
| time/              |          |
|    total_timesteps | 493600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.02     |
|    ep_rew_mean     | 2.54     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1234     |
|    time_elapsed    | 111773   |
|    total_timesteps | 493600   |
---------------------------------
Progress update from timestep 493700: Updated shield, loss is 0.027930723503232002
Progress update from timestep 493800: Updated shield, loss is 0.03660047799348831
Eval num_timesteps=493800, episode_reward=8.60 +/- 12.42
Episode length: 13.40 +/- 18.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 13.4          |
|    mean_reward          | 8.6           |
| time/                   |               |
|    total_timesteps      | 493800        |
| train/                  |               |
|    approx_kl            | 0.00018608577 |
|    clip_fraction        | 0.00357       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0102       |
|    explained_variance   | 0.232         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.662         |
|    n_updates            | 12340         |
|    policy_gradient_loss | -0.000447     |
|    value_loss           | 1.54          |
-------------------------------------------
Progress update from timestep 493900: Updated shield, loss is 0.036365240812301636
Progress update from timestep 494000: Updated shield, loss is 0.02698115073144436
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 494000: Saved shield_loss.csv. 
Eval num_timesteps=494000, episode_reward=12.14 +/- 11.74
Episode length: 18.20 +/- 16.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | 12.1     |
| time/              |          |
|    total_timesteps | 494000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.13     |
|    ep_rew_mean     | 2.59     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1235     |
|    time_elapsed    | 111854   |
|    total_timesteps | 494000   |
---------------------------------
Progress update from timestep 494100: Updated shield, loss is 0.03563426062464714
Progress update from timestep 494200: Updated shield, loss is 0.037257447838783264
Eval num_timesteps=494200, episode_reward=23.60 +/- 14.73
Episode length: 33.60 +/- 20.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.6        |
|    mean_reward          | 23.6        |
| time/                   |             |
|    total_timesteps      | 494200      |
| train/                  |             |
|    approx_kl            | 0.002150889 |
|    clip_fraction        | 0.0165      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0178     |
|    explained_variance   | 0.257       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.58        |
|    n_updates            | 12350       |
|    policy_gradient_loss | -0.00206    |
|    value_loss           | 1.28        |
-----------------------------------------
Progress update from timestep 494300: Updated shield, loss is 0.027975982055068016
Progress update from timestep 494400: Updated shield, loss is 0.037076685577631
Eval num_timesteps=494400, episode_reward=30.35 +/- 11.61
Episode length: 42.20 +/- 15.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 30.3     |
| time/              |          |
|    total_timesteps | 494400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.08     |
|    ep_rew_mean     | 2.59     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1236     |
|    time_elapsed    | 111957   |
|    total_timesteps | 494400   |
---------------------------------
Progress update from timestep 494500: Updated shield, loss is 0.03767585754394531
Progress update from timestep 494600: Updated shield, loss is 0.03843824937939644
Eval num_timesteps=494600, episode_reward=19.78 +/- 14.17
Episode length: 28.40 +/- 19.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 28.4          |
|    mean_reward          | 19.8          |
| time/                   |               |
|    total_timesteps      | 494600        |
| train/                  |               |
|    approx_kl            | 0.00019498194 |
|    clip_fraction        | 0.00625       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0163       |
|    explained_variance   | 0.245         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.876         |
|    n_updates            | 12360         |
|    policy_gradient_loss | -0.000591     |
|    value_loss           | 1.38          |
-------------------------------------------
Progress update from timestep 494700: Updated shield, loss is 0.03471152111887932
Progress update from timestep 494800: Updated shield, loss is 0.04368414357304573
Eval num_timesteps=494800, episode_reward=6.37 +/- 3.37
Episode length: 10.00 +/- 4.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 6.37     |
| time/              |          |
|    total_timesteps | 494800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.16     |
|    ep_rew_mean     | 2.59     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1237     |
|    time_elapsed    | 112024   |
|    total_timesteps | 494800   |
---------------------------------
Progress update from timestep 494900: Updated shield, loss is 0.04372427985072136
Progress update from timestep 495000: Updated shield, loss is 0.03231625631451607
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 495000: Saved shield_loss.csv. 
Eval num_timesteps=495000, episode_reward=29.00 +/- 12.64
Episode length: 41.20 +/- 17.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.2         |
|    mean_reward          | 29           |
| time/                   |              |
|    total_timesteps      | 495000       |
| train/                  |              |
|    approx_kl            | 4.646662e-05 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0111      |
|    explained_variance   | 0.165        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.279        |
|    n_updates            | 12370        |
|    policy_gradient_loss | -0.000567    |
|    value_loss           | 1.24         |
------------------------------------------
Progress update from timestep 495100: Updated shield, loss is 0.03254318609833717
Progress update from timestep 495200: Updated shield, loss is 0.0400504507124424
Eval num_timesteps=495200, episode_reward=9.83 +/- 5.78
Episode length: 15.00 +/- 8.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | 9.83     |
| time/              |          |
|    total_timesteps | 495200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.78     |
|    ep_rew_mean     | 2.34     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1238     |
|    time_elapsed    | 112122   |
|    total_timesteps | 495200   |
---------------------------------
Progress update from timestep 495300: Updated shield, loss is 0.03239688277244568
Progress update from timestep 495400: Updated shield, loss is 0.03460979461669922
Eval num_timesteps=495400, episode_reward=22.45 +/- 15.21
Episode length: 32.60 +/- 21.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32.6          |
|    mean_reward          | 22.4          |
| time/                   |               |
|    total_timesteps      | 495400        |
| train/                  |               |
|    approx_kl            | 0.00094666093 |
|    clip_fraction        | 0.00402       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00484      |
|    explained_variance   | 0.201         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.547         |
|    n_updates            | 12380         |
|    policy_gradient_loss | -0.00041      |
|    value_loss           | 1.34          |
-------------------------------------------
Progress update from timestep 495500: Updated shield, loss is 0.043168097734451294
Progress update from timestep 495600: Updated shield, loss is 0.046300582587718964
Eval num_timesteps=495600, episode_reward=6.87 +/- 4.61
Episode length: 10.80 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10.8     |
|    mean_reward     | 6.87     |
| time/              |          |
|    total_timesteps | 495600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.18     |
|    ep_rew_mean     | 2.65     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1239     |
|    time_elapsed    | 112211   |
|    total_timesteps | 495600   |
---------------------------------
Progress update from timestep 495700: Updated shield, loss is 0.02950698882341385
Progress update from timestep 495800: Updated shield, loss is 0.03408519923686981
Eval num_timesteps=495800, episode_reward=16.32 +/- 14.84
Episode length: 24.20 +/- 21.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.2          |
|    mean_reward          | 16.3          |
| time/                   |               |
|    total_timesteps      | 495800        |
| train/                  |               |
|    approx_kl            | 0.00021910707 |
|    clip_fraction        | 0.00536       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00966      |
|    explained_variance   | 0.034         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.548         |
|    n_updates            | 12390         |
|    policy_gradient_loss | -0.000987     |
|    value_loss           | 1.78          |
-------------------------------------------
Progress update from timestep 495900: Updated shield, loss is 0.033896367996931076
Progress update from timestep 496000: Updated shield, loss is 0.030530031770467758
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 496000: Saved shield_loss.csv. 
Eval num_timesteps=496000, episode_reward=17.29 +/- 14.12
Episode length: 25.60 +/- 20.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 17.3     |
| time/              |          |
|    total_timesteps | 496000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.32     |
|    ep_rew_mean     | 2.77     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1240     |
|    time_elapsed    | 112294   |
|    total_timesteps | 496000   |
---------------------------------
Progress update from timestep 496100: Updated shield, loss is 0.028912516310811043
Progress update from timestep 496200: Updated shield, loss is 0.04364690184593201
Eval num_timesteps=496200, episode_reward=7.21 +/- 2.48
Episode length: 11.40 +/- 3.50
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 11.4           |
|    mean_reward          | 7.21           |
| time/                   |                |
|    total_timesteps      | 496200         |
| train/                  |                |
|    approx_kl            | 0.000109566296 |
|    clip_fraction        | 0.00491        |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.0153        |
|    explained_variance   | 0.238          |
|    learning_rate        | 0.0001         |
|    loss                 | 1.08           |
|    n_updates            | 12400          |
|    policy_gradient_loss | -0.000403      |
|    value_loss           | 1.37           |
--------------------------------------------
Progress update from timestep 496300: Updated shield, loss is 0.04041305184364319
Progress update from timestep 496400: Updated shield, loss is 0.0274631530046463
Eval num_timesteps=496400, episode_reward=20.86 +/- 12.98
Episode length: 30.20 +/- 18.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 20.9     |
| time/              |          |
|    total_timesteps | 496400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.95     |
|    ep_rew_mean     | 2.44     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1241     |
|    time_elapsed    | 112366   |
|    total_timesteps | 496400   |
---------------------------------
Progress update from timestep 496500: Updated shield, loss is 0.03797685727477074
Progress update from timestep 496600: Updated shield, loss is 0.0408453531563282
Eval num_timesteps=496600, episode_reward=22.79 +/- 15.79
Episode length: 32.40 +/- 21.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.4         |
|    mean_reward          | 22.8         |
| time/                   |              |
|    total_timesteps      | 496600       |
| train/                  |              |
|    approx_kl            | 0.0001747786 |
|    clip_fraction        | 0.00446      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0112      |
|    explained_variance   | 0.259        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.696        |
|    n_updates            | 12410        |
|    policy_gradient_loss | -0.000493    |
|    value_loss           | 1.28         |
------------------------------------------
Progress update from timestep 496700: Updated shield, loss is 0.032667916268110275
Progress update from timestep 496800: Updated shield, loss is 0.028332894667983055
Eval num_timesteps=496800, episode_reward=15.96 +/- 14.49
Episode length: 24.40 +/- 21.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 496800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.34     |
|    ep_rew_mean     | 2.64     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1242     |
|    time_elapsed    | 112457   |
|    total_timesteps | 496800   |
---------------------------------
Progress update from timestep 496900: Updated shield, loss is 0.03560524806380272
Progress update from timestep 497000: Updated shield, loss is 0.0340307354927063
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 497000: Saved shield_loss.csv. 
Eval num_timesteps=497000, episode_reward=21.72 +/- 16.08
Episode length: 31.60 +/- 22.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 31.6          |
|    mean_reward          | 21.7          |
| time/                   |               |
|    total_timesteps      | 497000        |
| train/                  |               |
|    approx_kl            | 0.00021505778 |
|    clip_fraction        | 0.00558       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00979      |
|    explained_variance   | 0.229         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.626         |
|    n_updates            | 12420         |
|    policy_gradient_loss | -0.000426     |
|    value_loss           | 2.05          |
-------------------------------------------
Progress update from timestep 497100: Updated shield, loss is 0.036601100116968155
Progress update from timestep 497200: Updated shield, loss is 0.029676858335733414
Eval num_timesteps=497200, episode_reward=16.29 +/- 16.69
Episode length: 23.00 +/- 22.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 16.3     |
| time/              |          |
|    total_timesteps | 497200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.81     |
|    ep_rew_mean     | 2.3      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1243     |
|    time_elapsed    | 112541   |
|    total_timesteps | 497200   |
---------------------------------
Progress update from timestep 497300: Updated shield, loss is 0.029342303052544594
Progress update from timestep 497400: Updated shield, loss is 0.02913103997707367
Eval num_timesteps=497400, episode_reward=16.47 +/- 14.76
Episode length: 24.40 +/- 20.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.4         |
|    mean_reward          | 16.5         |
| time/                   |              |
|    total_timesteps      | 497400       |
| train/                  |              |
|    approx_kl            | 0.0007996276 |
|    clip_fraction        | 0.0154       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0306      |
|    explained_variance   | 0.259        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.797        |
|    n_updates            | 12430        |
|    policy_gradient_loss | -0.00158     |
|    value_loss           | 1.43         |
------------------------------------------
Progress update from timestep 497500: Updated shield, loss is 0.043744247406721115
Progress update from timestep 497600: Updated shield, loss is 0.03647526353597641
Eval num_timesteps=497600, episode_reward=31.70 +/- 7.29
Episode length: 45.20 +/- 9.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.2     |
|    mean_reward     | 31.7     |
| time/              |          |
|    total_timesteps | 497600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.49     |
|    ep_rew_mean     | 2.84     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1244     |
|    time_elapsed    | 112642   |
|    total_timesteps | 497600   |
---------------------------------
Progress update from timestep 497700: Updated shield, loss is 0.03372797742486
Progress update from timestep 497800: Updated shield, loss is 0.038817282766103745
Eval num_timesteps=497800, episode_reward=16.59 +/- 15.56
Episode length: 24.00 +/- 21.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24           |
|    mean_reward          | 16.6         |
| time/                   |              |
|    total_timesteps      | 497800       |
| train/                  |              |
|    approx_kl            | 0.0011074471 |
|    clip_fraction        | 0.0127       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0301      |
|    explained_variance   | 0.364        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.23         |
|    n_updates            | 12440        |
|    policy_gradient_loss | -0.00163     |
|    value_loss           | 2.32         |
------------------------------------------
Progress update from timestep 497900: Updated shield, loss is 0.04155239462852478
Progress update from timestep 498000: Updated shield, loss is 0.042633168399333954
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 498000: Saved shield_loss.csv. 
Eval num_timesteps=498000, episode_reward=25.13 +/- 11.94
Episode length: 36.20 +/- 16.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 25.1     |
| time/              |          |
|    total_timesteps | 498000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.36     |
|    ep_rew_mean     | 2.76     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1245     |
|    time_elapsed    | 112722   |
|    total_timesteps | 498000   |
---------------------------------
Progress update from timestep 498100: Updated shield, loss is 0.03526471182703972
Progress update from timestep 498200: Updated shield, loss is 0.04629326984286308
Eval num_timesteps=498200, episode_reward=17.31 +/- 14.02
Episode length: 25.60 +/- 19.93
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.6          |
|    mean_reward          | 17.3          |
| time/                   |               |
|    total_timesteps      | 498200        |
| train/                  |               |
|    approx_kl            | 0.00043688042 |
|    clip_fraction        | 0.0103        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0205       |
|    explained_variance   | 0.312         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.757         |
|    n_updates            | 12450         |
|    policy_gradient_loss | -0.000936     |
|    value_loss           | 1.55          |
-------------------------------------------
Progress update from timestep 498300: Updated shield, loss is 0.03341934084892273
Progress update from timestep 498400: Updated shield, loss is 0.02892795391380787
Eval num_timesteps=498400, episode_reward=11.08 +/- 12.44
Episode length: 16.40 +/- 17.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | 11.1     |
| time/              |          |
|    total_timesteps | 498400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.14     |
|    ep_rew_mean     | 2.6      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1246     |
|    time_elapsed    | 112802   |
|    total_timesteps | 498400   |
---------------------------------
Progress update from timestep 498500: Updated shield, loss is 0.03778659924864769
Progress update from timestep 498600: Updated shield, loss is 0.039420273154973984
Eval num_timesteps=498600, episode_reward=9.73 +/- 11.95
Episode length: 15.20 +/- 17.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.2          |
|    mean_reward          | 9.73          |
| time/                   |               |
|    total_timesteps      | 498600        |
| train/                  |               |
|    approx_kl            | 4.7468515e-05 |
|    clip_fraction        | 0.00112       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00632      |
|    explained_variance   | 0.0324        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.643         |
|    n_updates            | 12460         |
|    policy_gradient_loss | -0.000205     |
|    value_loss           | 1.73          |
-------------------------------------------
Progress update from timestep 498700: Updated shield, loss is 0.04318023845553398
Progress update from timestep 498800: Updated shield, loss is 0.04047350957989693
Eval num_timesteps=498800, episode_reward=9.61 +/- 12.63
Episode length: 14.60 +/- 17.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 9.61     |
| time/              |          |
|    total_timesteps | 498800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.31     |
|    ep_rew_mean     | 2.76     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1247     |
|    time_elapsed    | 112886   |
|    total_timesteps | 498800   |
---------------------------------
Progress update from timestep 498900: Updated shield, loss is 0.03891870751976967
Progress update from timestep 499000: Updated shield, loss is 0.03768886998295784
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 499000: Saved shield_loss.csv. 
Eval num_timesteps=499000, episode_reward=13.39 +/- 10.78
Episode length: 20.00 +/- 15.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 20            |
|    mean_reward          | 13.4          |
| time/                   |               |
|    total_timesteps      | 499000        |
| train/                  |               |
|    approx_kl            | 0.00040357988 |
|    clip_fraction        | 0.00491       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00964      |
|    explained_variance   | 0.0945        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.748         |
|    n_updates            | 12470         |
|    policy_gradient_loss | -0.000759     |
|    value_loss           | 2.07          |
-------------------------------------------
Progress update from timestep 499100: Updated shield, loss is 0.03262483328580856
Progress update from timestep 499200: Updated shield, loss is 0.040565408766269684
Eval num_timesteps=499200, episode_reward=4.92 +/- 3.47
Episode length: 8.00 +/- 4.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8        |
|    mean_reward     | 4.92     |
| time/              |          |
|    total_timesteps | 499200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.01     |
|    ep_rew_mean     | 2.45     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1248     |
|    time_elapsed    | 112962   |
|    total_timesteps | 499200   |
---------------------------------
Progress update from timestep 499300: Updated shield, loss is 0.030890384688973427
Progress update from timestep 499400: Updated shield, loss is 0.04514872655272484
Eval num_timesteps=499400, episode_reward=9.29 +/- 12.07
Episode length: 14.40 +/- 17.85
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.4          |
|    mean_reward          | 9.29          |
| time/                   |               |
|    total_timesteps      | 499400        |
| train/                  |               |
|    approx_kl            | 0.00057313184 |
|    clip_fraction        | 0.0118        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0219       |
|    explained_variance   | 0.177         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.647         |
|    n_updates            | 12480         |
|    policy_gradient_loss | -0.00142      |
|    value_loss           | 1.72          |
-------------------------------------------
Progress update from timestep 499500: Updated shield, loss is 0.0377705916762352
Progress update from timestep 499600: Updated shield, loss is 0.04399293288588524
Eval num_timesteps=499600, episode_reward=18.20 +/- 15.58
Episode length: 25.40 +/- 20.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | 18.2     |
| time/              |          |
|    total_timesteps | 499600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.35     |
|    ep_rew_mean     | 2.72     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1249     |
|    time_elapsed    | 113042   |
|    total_timesteps | 499600   |
---------------------------------
Progress update from timestep 499700: Updated shield, loss is 0.045552801340818405
Progress update from timestep 499800: Updated shield, loss is 0.0452483631670475
Eval num_timesteps=499800, episode_reward=19.13 +/- 14.22
Episode length: 27.40 +/- 19.55
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 27.4          |
|    mean_reward          | 19.1          |
| time/                   |               |
|    total_timesteps      | 499800        |
| train/                  |               |
|    approx_kl            | 0.00054893794 |
|    clip_fraction        | 0.00804       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0106       |
|    explained_variance   | 0.18          |
|    learning_rate        | 0.0001        |
|    loss                 | 1.47          |
|    n_updates            | 12490         |
|    policy_gradient_loss | -0.00089      |
|    value_loss           | 2.58          |
-------------------------------------------
Progress update from timestep 499900: Updated shield, loss is 0.04006598889827728
Progress update from timestep 500000: Updated shield, loss is 0.03745676577091217
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 500000: Saved shield_loss.csv. 
Eval num_timesteps=500000, episode_reward=17.10 +/- 14.91
Episode length: 25.00 +/- 20.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | 17.1     |
| time/              |          |
|    total_timesteps | 500000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.67     |
|    ep_rew_mean     | 3.04     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1250     |
|    time_elapsed    | 113130   |
|    total_timesteps | 500000   |
---------------------------------
Progress update from timestep 500100: Updated shield, loss is 0.03207229822874069
Progress update from timestep 500200: Updated shield, loss is 0.03361450508236885
Eval num_timesteps=500200, episode_reward=16.93 +/- 14.74
Episode length: 25.60 +/- 21.28
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.6          |
|    mean_reward          | 16.9          |
| time/                   |               |
|    total_timesteps      | 500200        |
| train/                  |               |
|    approx_kl            | 0.00041886955 |
|    clip_fraction        | 0.0158        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0243       |
|    explained_variance   | 0.0963        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.861         |
|    n_updates            | 12500         |
|    policy_gradient_loss | -0.00132      |
|    value_loss           | 2.02          |
-------------------------------------------
Progress update from timestep 500300: Updated shield, loss is 0.04059920459985733
Progress update from timestep 500400: Updated shield, loss is 0.03496013209223747
Eval num_timesteps=500400, episode_reward=12.21 +/- 11.53
Episode length: 18.20 +/- 16.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | 12.2     |
| time/              |          |
|    total_timesteps | 500400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.27     |
|    ep_rew_mean     | 2.74     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1251     |
|    time_elapsed    | 113230   |
|    total_timesteps | 500400   |
---------------------------------
Progress update from timestep 500500: Updated shield, loss is 0.04261093959212303
Progress update from timestep 500600: Updated shield, loss is 0.04507158324122429
Eval num_timesteps=500600, episode_reward=21.56 +/- 16.76
Episode length: 31.00 +/- 23.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 31           |
|    mean_reward          | 21.6         |
| time/                   |              |
|    total_timesteps      | 500600       |
| train/                  |              |
|    approx_kl            | 0.0004108946 |
|    clip_fraction        | 0.0107       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0156      |
|    explained_variance   | 0.242        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.765        |
|    n_updates            | 12510        |
|    policy_gradient_loss | -0.00174     |
|    value_loss           | 1.97         |
------------------------------------------
Progress update from timestep 500700: Updated shield, loss is 0.03888801112771034
Progress update from timestep 500800: Updated shield, loss is 0.029996370896697044
Eval num_timesteps=500800, episode_reward=30.90 +/- 9.45
Episode length: 43.60 +/- 12.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.6     |
|    mean_reward     | 30.9     |
| time/              |          |
|    total_timesteps | 500800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.01     |
|    ep_rew_mean     | 2.52     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1252     |
|    time_elapsed    | 113349   |
|    total_timesteps | 500800   |
---------------------------------
Progress update from timestep 500900: Updated shield, loss is 0.03683320805430412
Progress update from timestep 501000: Updated shield, loss is 0.03685647249221802
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 501000: Saved shield_loss.csv. 
Eval num_timesteps=501000, episode_reward=11.07 +/- 12.86
Episode length: 16.60 +/- 17.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.6          |
|    mean_reward          | 11.1          |
| time/                   |               |
|    total_timesteps      | 501000        |
| train/                  |               |
|    approx_kl            | 0.00036669575 |
|    clip_fraction        | 0.0107        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.029        |
|    explained_variance   | 0.0709        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.916         |
|    n_updates            | 12520         |
|    policy_gradient_loss | -0.00141      |
|    value_loss           | 1.68          |
-------------------------------------------
Progress update from timestep 501100: Updated shield, loss is 0.031739719212055206
Progress update from timestep 501200: Updated shield, loss is 0.0325298048555851
Eval num_timesteps=501200, episode_reward=13.63 +/- 12.63
Episode length: 20.20 +/- 16.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.2     |
|    mean_reward     | 13.6     |
| time/              |          |
|    total_timesteps | 501200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4        |
|    ep_rew_mean     | 2.43     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1253     |
|    time_elapsed    | 113440   |
|    total_timesteps | 501200   |
---------------------------------
Progress update from timestep 501300: Updated shield, loss is 0.03840138390660286
Progress update from timestep 501400: Updated shield, loss is 0.042964547872543335
Eval num_timesteps=501400, episode_reward=15.93 +/- 16.08
Episode length: 23.00 +/- 22.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23           |
|    mean_reward          | 15.9         |
| time/                   |              |
|    total_timesteps      | 501400       |
| train/                  |              |
|    approx_kl            | 1.825161e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0002      |
|    explained_variance   | 0.126        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.23         |
|    n_updates            | 12530        |
|    policy_gradient_loss | -5.61e-08    |
|    value_loss           | 1.47         |
------------------------------------------
Progress update from timestep 501500: Updated shield, loss is 0.04597078263759613
Progress update from timestep 501600: Updated shield, loss is 0.031708624213933945
Eval num_timesteps=501600, episode_reward=20.61 +/- 13.58
Episode length: 29.20 +/- 18.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.2     |
|    mean_reward     | 20.6     |
| time/              |          |
|    total_timesteps | 501600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.33     |
|    ep_rew_mean     | 2.72     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1254     |
|    time_elapsed    | 113522   |
|    total_timesteps | 501600   |
---------------------------------
Progress update from timestep 501700: Updated shield, loss is 0.03512633591890335
Progress update from timestep 501800: Updated shield, loss is 0.03963427618145943
Eval num_timesteps=501800, episode_reward=17.71 +/- 15.07
Episode length: 25.20 +/- 20.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.2          |
|    mean_reward          | 17.7          |
| time/                   |               |
|    total_timesteps      | 501800        |
| train/                  |               |
|    approx_kl            | 5.0353152e-05 |
|    clip_fraction        | 0.00268       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0116       |
|    explained_variance   | 0.233         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.652         |
|    n_updates            | 12540         |
|    policy_gradient_loss | -0.000286     |
|    value_loss           | 1.37          |
-------------------------------------------
Progress update from timestep 501900: Updated shield, loss is 0.0306665301322937
Progress update from timestep 502000: Updated shield, loss is 0.0412348136305809
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 502000: Saved shield_loss.csv. 
Eval num_timesteps=502000, episode_reward=17.65 +/- 13.34
Episode length: 26.40 +/- 19.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.4     |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 502000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.91     |
|    ep_rew_mean     | 2.45     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1255     |
|    time_elapsed    | 113627   |
|    total_timesteps | 502000   |
---------------------------------
Progress update from timestep 502100: Updated shield, loss is 0.037130843847990036
Progress update from timestep 502200: Updated shield, loss is 0.033476367592811584
Eval num_timesteps=502200, episode_reward=18.24 +/- 14.62
Episode length: 25.80 +/- 19.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.8          |
|    mean_reward          | 18.2          |
| time/                   |               |
|    total_timesteps      | 502200        |
| train/                  |               |
|    approx_kl            | 0.00074405025 |
|    clip_fraction        | 0.0112        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0255       |
|    explained_variance   | 0.13          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.742         |
|    n_updates            | 12550         |
|    policy_gradient_loss | -0.00119      |
|    value_loss           | 1.89          |
-------------------------------------------
Progress update from timestep 502300: Updated shield, loss is 0.04138492792844772
Progress update from timestep 502400: Updated shield, loss is 0.030871668830513954
Eval num_timesteps=502400, episode_reward=18.45 +/- 13.29
Episode length: 27.40 +/- 18.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.4     |
|    mean_reward     | 18.4     |
| time/              |          |
|    total_timesteps | 502400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.02     |
|    ep_rew_mean     | 2.58     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1256     |
|    time_elapsed    | 113711   |
|    total_timesteps | 502400   |
---------------------------------
Progress update from timestep 502500: Updated shield, loss is 0.03368113562464714
Progress update from timestep 502600: Updated shield, loss is 0.036620449274778366
Eval num_timesteps=502600, episode_reward=17.21 +/- 14.12
Episode length: 25.40 +/- 20.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.4          |
|    mean_reward          | 17.2          |
| time/                   |               |
|    total_timesteps      | 502600        |
| train/                  |               |
|    approx_kl            | 0.00032440206 |
|    clip_fraction        | 0.0134        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0205       |
|    explained_variance   | 0.0413        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.926         |
|    n_updates            | 12560         |
|    policy_gradient_loss | -0.00148      |
|    value_loss           | 2.27          |
-------------------------------------------
Progress update from timestep 502700: Updated shield, loss is 0.04156484082341194
Progress update from timestep 502800: Updated shield, loss is 0.043675199151039124
Eval num_timesteps=502800, episode_reward=27.30 +/- 13.27
Episode length: 40.40 +/- 19.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.4     |
|    mean_reward     | 27.3     |
| time/              |          |
|    total_timesteps | 502800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.04     |
|    ep_rew_mean     | 2.51     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1257     |
|    time_elapsed    | 113816   |
|    total_timesteps | 502800   |
---------------------------------
Progress update from timestep 502900: Updated shield, loss is 0.030502712354063988
Progress update from timestep 503000: Updated shield, loss is 0.03507881239056587
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 503000: Saved shield_loss.csv. 
Eval num_timesteps=503000, episode_reward=20.63 +/- 13.90
Episode length: 29.80 +/- 19.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29.8         |
|    mean_reward          | 20.6         |
| time/                   |              |
|    total_timesteps      | 503000       |
| train/                  |              |
|    approx_kl            | 0.0004466984 |
|    clip_fraction        | 0.0161       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.033       |
|    explained_variance   | 0.17         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.288        |
|    n_updates            | 12570        |
|    policy_gradient_loss | -0.00134     |
|    value_loss           | 1.57         |
------------------------------------------
Progress update from timestep 503100: Updated shield, loss is 0.03757800906896591
Progress update from timestep 503200: Updated shield, loss is 0.03398989140987396
Eval num_timesteps=503200, episode_reward=28.28 +/- 12.41
Episode length: 41.20 +/- 17.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | 28.3     |
| time/              |          |
|    total_timesteps | 503200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.34     |
|    ep_rew_mean     | 2.73     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1258     |
|    time_elapsed    | 113929   |
|    total_timesteps | 503200   |
---------------------------------
Progress update from timestep 503300: Updated shield, loss is 0.03870279714465141
Progress update from timestep 503400: Updated shield, loss is 0.03189912810921669
Eval num_timesteps=503400, episode_reward=13.41 +/- 10.25
Episode length: 20.40 +/- 15.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 20.4         |
|    mean_reward          | 13.4         |
| time/                   |              |
|    total_timesteps      | 503400       |
| train/                  |              |
|    approx_kl            | 8.976843e-05 |
|    clip_fraction        | 0.00424      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.016       |
|    explained_variance   | 0.15         |
|    learning_rate        | 0.0001       |
|    loss                 | 1.07         |
|    n_updates            | 12580        |
|    policy_gradient_loss | -0.00097     |
|    value_loss           | 1.85         |
------------------------------------------
Progress update from timestep 503500: Updated shield, loss is 0.03791385889053345
Progress update from timestep 503600: Updated shield, loss is 0.034227486699819565
Eval num_timesteps=503600, episode_reward=4.42 +/- 2.86
Episode length: 7.20 +/- 4.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7.2      |
|    mean_reward     | 4.42     |
| time/              |          |
|    total_timesteps | 503600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.2      |
|    ep_rew_mean     | 2.58     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1259     |
|    time_elapsed    | 113997   |
|    total_timesteps | 503600   |
---------------------------------
Progress update from timestep 503700: Updated shield, loss is 0.036904651671648026
Progress update from timestep 503800: Updated shield, loss is 0.03180379047989845
Eval num_timesteps=503800, episode_reward=15.00 +/- 10.81
Episode length: 22.40 +/- 15.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.4          |
|    mean_reward          | 15            |
| time/                   |               |
|    total_timesteps      | 503800        |
| train/                  |               |
|    approx_kl            | 0.00032472538 |
|    clip_fraction        | 0.0067        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00865      |
|    explained_variance   | 0.252         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.319         |
|    n_updates            | 12590         |
|    policy_gradient_loss | -0.000474     |
|    value_loss           | 1.41          |
-------------------------------------------
Progress update from timestep 503900: Updated shield, loss is 0.037562720477581024
Progress update from timestep 504000: Updated shield, loss is 0.03257562592625618
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 504000: Saved shield_loss.csv. 
Eval num_timesteps=504000, episode_reward=28.38 +/- 13.87
Episode length: 40.40 +/- 19.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.4     |
|    mean_reward     | 28.4     |
| time/              |          |
|    total_timesteps | 504000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.37     |
|    ep_rew_mean     | 2.71     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1260     |
|    time_elapsed    | 114098   |
|    total_timesteps | 504000   |
---------------------------------
Progress update from timestep 504100: Updated shield, loss is 0.032101958990097046
Progress update from timestep 504200: Updated shield, loss is 0.032253459095954895
Eval num_timesteps=504200, episode_reward=12.11 +/- 12.48
Episode length: 18.00 +/- 17.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 18            |
|    mean_reward          | 12.1          |
| time/                   |               |
|    total_timesteps      | 504200        |
| train/                  |               |
|    approx_kl            | 0.00015199557 |
|    clip_fraction        | 0.00714       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0128       |
|    explained_variance   | 0.138         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.473         |
|    n_updates            | 12600         |
|    policy_gradient_loss | -0.00126      |
|    value_loss           | 1.72          |
-------------------------------------------
Progress update from timestep 504300: Updated shield, loss is 0.04123249277472496
Progress update from timestep 504400: Updated shield, loss is 0.0367780402302742
Eval num_timesteps=504400, episode_reward=5.34 +/- 4.06
Episode length: 8.60 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8.6      |
|    mean_reward     | 5.34     |
| time/              |          |
|    total_timesteps | 504400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.57     |
|    ep_rew_mean     | 2.92     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1261     |
|    time_elapsed    | 114170   |
|    total_timesteps | 504400   |
---------------------------------
Progress update from timestep 504500: Updated shield, loss is 0.03924994170665741
Progress update from timestep 504600: Updated shield, loss is 0.041988909244537354
Eval num_timesteps=504600, episode_reward=30.60 +/- 8.93
Episode length: 43.80 +/- 12.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 43.8          |
|    mean_reward          | 30.6          |
| time/                   |               |
|    total_timesteps      | 504600        |
| train/                  |               |
|    approx_kl            | 4.1771425e-05 |
|    clip_fraction        | 0.00313       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00906      |
|    explained_variance   | 0.236         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.12          |
|    n_updates            | 12610         |
|    policy_gradient_loss | -0.000244     |
|    value_loss           | 1.75          |
-------------------------------------------
Progress update from timestep 504700: Updated shield, loss is 0.03861244395375252
Progress update from timestep 504800: Updated shield, loss is 0.038249190896749496
Eval num_timesteps=504800, episode_reward=10.93 +/- 12.63
Episode length: 16.40 +/- 17.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | 10.9     |
| time/              |          |
|    total_timesteps | 504800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.88     |
|    ep_rew_mean     | 2.4      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1262     |
|    time_elapsed    | 114259   |
|    total_timesteps | 504800   |
---------------------------------
Progress update from timestep 504900: Updated shield, loss is 0.028922704979777336
Progress update from timestep 505000: Updated shield, loss is 0.03898727893829346
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 505000: Saved shield_loss.csv. 
Eval num_timesteps=505000, episode_reward=13.82 +/- 11.14
Episode length: 20.20 +/- 15.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 20.2          |
|    mean_reward          | 13.8          |
| time/                   |               |
|    total_timesteps      | 505000        |
| train/                  |               |
|    approx_kl            | 0.00011833008 |
|    clip_fraction        | 0.01          |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0188       |
|    explained_variance   | 0.201         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.633         |
|    n_updates            | 12620         |
|    policy_gradient_loss | -0.000725     |
|    value_loss           | 1.25          |
-------------------------------------------
Progress update from timestep 505100: Updated shield, loss is 0.03796426206827164
Progress update from timestep 505200: Updated shield, loss is 0.02766234055161476
Eval num_timesteps=505200, episode_reward=12.64 +/- 11.12
Episode length: 19.00 +/- 15.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 12.6     |
| time/              |          |
|    total_timesteps | 505200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.2      |
|    ep_rew_mean     | 2.65     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1263     |
|    time_elapsed    | 114348   |
|    total_timesteps | 505200   |
---------------------------------
Progress update from timestep 505300: Updated shield, loss is 0.03789393603801727
Progress update from timestep 505400: Updated shield, loss is 0.036918893456459045
Eval num_timesteps=505400, episode_reward=15.97 +/- 15.56
Episode length: 23.40 +/- 21.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.4         |
|    mean_reward          | 16           |
| time/                   |              |
|    total_timesteps      | 505400       |
| train/                  |              |
|    approx_kl            | 0.0023475843 |
|    clip_fraction        | 0.0214       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0423      |
|    explained_variance   | 0.156        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.423        |
|    n_updates            | 12630        |
|    policy_gradient_loss | -0.00144     |
|    value_loss           | 1.27         |
------------------------------------------
Progress update from timestep 505500: Updated shield, loss is 0.029545903205871582
Progress update from timestep 505600: Updated shield, loss is 0.03194877877831459
Eval num_timesteps=505600, episode_reward=28.11 +/- 12.20
Episode length: 41.20 +/- 17.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | 28.1     |
| time/              |          |
|    total_timesteps | 505600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.9      |
|    ep_rew_mean     | 2.37     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1264     |
|    time_elapsed    | 114431   |
|    total_timesteps | 505600   |
---------------------------------
Progress update from timestep 505700: Updated shield, loss is 0.04829663783311844
Progress update from timestep 505800: Updated shield, loss is 0.041746579110622406
Eval num_timesteps=505800, episode_reward=28.61 +/- 12.36
Episode length: 41.40 +/- 17.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.4         |
|    mean_reward          | 28.6         |
| time/                   |              |
|    total_timesteps      | 505800       |
| train/                  |              |
|    approx_kl            | 6.339169e-05 |
|    clip_fraction        | 0.00402      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0151      |
|    explained_variance   | 0.148        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.78         |
|    n_updates            | 12640        |
|    policy_gradient_loss | -0.000357    |
|    value_loss           | 1.51         |
------------------------------------------
Progress update from timestep 505900: Updated shield, loss is 0.04958401247859001
Progress update from timestep 506000: Updated shield, loss is 0.03347371518611908
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 506000: Saved shield_loss.csv. 
Eval num_timesteps=506000, episode_reward=17.90 +/- 14.99
Episode length: 25.80 +/- 20.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 506000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.24     |
|    ep_rew_mean     | 2.65     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1265     |
|    time_elapsed    | 114517   |
|    total_timesteps | 506000   |
---------------------------------
Progress update from timestep 506100: Updated shield, loss is 0.04312187805771828
Progress update from timestep 506200: Updated shield, loss is 0.04675576463341713
Eval num_timesteps=506200, episode_reward=11.16 +/- 11.86
Episode length: 17.00 +/- 16.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17           |
|    mean_reward          | 11.2         |
| time/                   |              |
|    total_timesteps      | 506200       |
| train/                  |              |
|    approx_kl            | 0.0005409277 |
|    clip_fraction        | 0.0112       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0275      |
|    explained_variance   | 0.293        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.801        |
|    n_updates            | 12650        |
|    policy_gradient_loss | -0.0013      |
|    value_loss           | 1.56         |
------------------------------------------
Progress update from timestep 506300: Updated shield, loss is 0.044715337455272675
Progress update from timestep 506400: Updated shield, loss is 0.04514814540743828
Eval num_timesteps=506400, episode_reward=21.90 +/- 16.31
Episode length: 31.40 +/- 22.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.4     |
|    mean_reward     | 21.9     |
| time/              |          |
|    total_timesteps | 506400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.25     |
|    ep_rew_mean     | 2.72     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1266     |
|    time_elapsed    | 114602   |
|    total_timesteps | 506400   |
---------------------------------
Progress update from timestep 506500: Updated shield, loss is 0.03978050872683525
Progress update from timestep 506600: Updated shield, loss is 0.043524082750082016
Eval num_timesteps=506600, episode_reward=7.17 +/- 6.33
Episode length: 11.40 +/- 9.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 7.17          |
| time/                   |               |
|    total_timesteps      | 506600        |
| train/                  |               |
|    approx_kl            | 0.00052882906 |
|    clip_fraction        | 0.0103        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0225       |
|    explained_variance   | 0.244         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.675         |
|    n_updates            | 12660         |
|    policy_gradient_loss | -0.00117      |
|    value_loss           | 1.32          |
-------------------------------------------
Progress update from timestep 506700: Updated shield, loss is 0.03715578466653824
Progress update from timestep 506800: Updated shield, loss is 0.042069002985954285
Eval num_timesteps=506800, episode_reward=17.84 +/- 14.74
Episode length: 25.60 +/- 20.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 17.8     |
| time/              |          |
|    total_timesteps | 506800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.06     |
|    ep_rew_mean     | 2.47     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1267     |
|    time_elapsed    | 114679   |
|    total_timesteps | 506800   |
---------------------------------
Progress update from timestep 506900: Updated shield, loss is 0.04016454145312309
Progress update from timestep 507000: Updated shield, loss is 0.04692048206925392
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 507000: Saved shield_loss.csv. 
Eval num_timesteps=507000, episode_reward=22.45 +/- 14.75
Episode length: 32.80 +/- 21.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.8         |
|    mean_reward          | 22.4         |
| time/                   |              |
|    total_timesteps      | 507000       |
| train/                  |              |
|    approx_kl            | 8.107496e-11 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.000672    |
|    explained_variance   | 0.0477       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.709        |
|    n_updates            | 12670        |
|    policy_gradient_loss | -3.95e-07    |
|    value_loss           | 1.51         |
------------------------------------------
Progress update from timestep 507100: Updated shield, loss is 0.04874827712774277
Progress update from timestep 507200: Updated shield, loss is 0.04536660760641098
Eval num_timesteps=507200, episode_reward=2.92 +/- 2.37
Episode length: 5.20 +/- 3.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5.2      |
|    mean_reward     | 2.92     |
| time/              |          |
|    total_timesteps | 507200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.04     |
|    ep_rew_mean     | 2.47     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1268     |
|    time_elapsed    | 114765   |
|    total_timesteps | 507200   |
---------------------------------
Progress update from timestep 507300: Updated shield, loss is 0.03468769043684006
Progress update from timestep 507400: Updated shield, loss is 0.039196085184812546
Eval num_timesteps=507400, episode_reward=17.04 +/- 16.15
Episode length: 24.00 +/- 21.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24           |
|    mean_reward          | 17           |
| time/                   |              |
|    total_timesteps      | 507400       |
| train/                  |              |
|    approx_kl            | 0.0010641564 |
|    clip_fraction        | 0.0257       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0262      |
|    explained_variance   | 0.279        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.609        |
|    n_updates            | 12680        |
|    policy_gradient_loss | -0.00194     |
|    value_loss           | 1.14         |
------------------------------------------
Progress update from timestep 507500: Updated shield, loss is 0.04139838367700577
Progress update from timestep 507600: Updated shield, loss is 0.029182840138673782
Eval num_timesteps=507600, episode_reward=14.51 +/- 11.40
Episode length: 20.80 +/- 15.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.8     |
|    mean_reward     | 14.5     |
| time/              |          |
|    total_timesteps | 507600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.97     |
|    ep_rew_mean     | 2.49     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1269     |
|    time_elapsed    | 114846   |
|    total_timesteps | 507600   |
---------------------------------
Progress update from timestep 507700: Updated shield, loss is 0.03089994005858898
Progress update from timestep 507800: Updated shield, loss is 0.04908259958028793
Eval num_timesteps=507800, episode_reward=2.37 +/- 1.20
Episode length: 4.40 +/- 1.74
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 4.4            |
|    mean_reward          | 2.37           |
| time/                   |                |
|    total_timesteps      | 507800         |
| train/                  |                |
|    approx_kl            | 0.000109336455 |
|    clip_fraction        | 0.00536        |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.0117        |
|    explained_variance   | 0.17           |
|    learning_rate        | 0.0001         |
|    loss                 | 0.597          |
|    n_updates            | 12690          |
|    policy_gradient_loss | -0.000361      |
|    value_loss           | 1.25           |
--------------------------------------------
Progress update from timestep 507900: Updated shield, loss is 0.04241424426436424
Progress update from timestep 508000: Updated shield, loss is 0.03528926149010658
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 508000: Saved shield_loss.csv. 
Eval num_timesteps=508000, episode_reward=10.40 +/- 12.12
Episode length: 15.80 +/- 17.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | 10.4     |
| time/              |          |
|    total_timesteps | 508000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.3      |
|    ep_rew_mean     | 2.68     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1270     |
|    time_elapsed    | 114911   |
|    total_timesteps | 508000   |
---------------------------------
Progress update from timestep 508100: Updated shield, loss is 0.029361100867390633
Progress update from timestep 508200: Updated shield, loss is 0.03423014655709267
Eval num_timesteps=508200, episode_reward=5.52 +/- 5.09
Episode length: 8.80 +/- 7.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 8.8           |
|    mean_reward          | 5.52          |
| time/                   |               |
|    total_timesteps      | 508200        |
| train/                  |               |
|    approx_kl            | 0.00036962845 |
|    clip_fraction        | 0.0067        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0188       |
|    explained_variance   | 0.29          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.507         |
|    n_updates            | 12700         |
|    policy_gradient_loss | -0.00114      |
|    value_loss           | 1.6           |
-------------------------------------------
Progress update from timestep 508300: Updated shield, loss is 0.035924576222896576
Progress update from timestep 508400: Updated shield, loss is 0.041829269379377365
Eval num_timesteps=508400, episode_reward=15.68 +/- 16.73
Episode length: 22.40 +/- 22.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 15.7     |
| time/              |          |
|    total_timesteps | 508400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.17     |
|    ep_rew_mean     | 2.6      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1271     |
|    time_elapsed    | 114985   |
|    total_timesteps | 508400   |
---------------------------------
Progress update from timestep 508500: Updated shield, loss is 0.035077858716249466
Progress update from timestep 508600: Updated shield, loss is 0.031531959772109985
Eval num_timesteps=508600, episode_reward=15.51 +/- 16.86
Episode length: 22.00 +/- 22.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22           |
|    mean_reward          | 15.5         |
| time/                   |              |
|    total_timesteps      | 508600       |
| train/                  |              |
|    approx_kl            | 0.0002392578 |
|    clip_fraction        | 0.00313      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00593     |
|    explained_variance   | 0.185        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.691        |
|    n_updates            | 12710        |
|    policy_gradient_loss | -0.000715    |
|    value_loss           | 1.46         |
------------------------------------------
Progress update from timestep 508700: Updated shield, loss is 0.02934212237596512
Progress update from timestep 508800: Updated shield, loss is 0.037432700395584106
Eval num_timesteps=508800, episode_reward=14.56 +/- 12.10
Episode length: 21.20 +/- 16.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.2     |
|    mean_reward     | 14.6     |
| time/              |          |
|    total_timesteps | 508800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.12     |
|    ep_rew_mean     | 2.55     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1272     |
|    time_elapsed    | 115066   |
|    total_timesteps | 508800   |
---------------------------------
Progress update from timestep 508900: Updated shield, loss is 0.035062193870544434
Progress update from timestep 509000: Updated shield, loss is 0.03261348977684975
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 509000: Saved shield_loss.csv. 
Eval num_timesteps=509000, episode_reward=8.45 +/- 6.44
Episode length: 13.00 +/- 9.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 13            |
|    mean_reward          | 8.45          |
| time/                   |               |
|    total_timesteps      | 509000        |
| train/                  |               |
|    approx_kl            | 2.2374903e-05 |
|    clip_fraction        | 0.00112       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00459      |
|    explained_variance   | 0.199         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.845         |
|    n_updates            | 12720         |
|    policy_gradient_loss | -0.000549     |
|    value_loss           | 2.05          |
-------------------------------------------
Progress update from timestep 509100: Updated shield, loss is 0.03734682500362396
Progress update from timestep 509200: Updated shield, loss is 0.028455914929509163
Eval num_timesteps=509200, episode_reward=17.25 +/- 14.27
Episode length: 25.60 +/- 20.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 17.2     |
| time/              |          |
|    total_timesteps | 509200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.38     |
|    ep_rew_mean     | 2.79     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1273     |
|    time_elapsed    | 115146   |
|    total_timesteps | 509200   |
---------------------------------
Progress update from timestep 509300: Updated shield, loss is 0.03619014844298363
Progress update from timestep 509400: Updated shield, loss is 0.036649469286203384
Eval num_timesteps=509400, episode_reward=22.57 +/- 14.72
Episode length: 33.20 +/- 20.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.2          |
|    mean_reward          | 22.6          |
| time/                   |               |
|    total_timesteps      | 509400        |
| train/                  |               |
|    approx_kl            | 1.7786573e-05 |
|    clip_fraction        | 0.00223       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0054       |
|    explained_variance   | 0.281         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.438         |
|    n_updates            | 12730         |
|    policy_gradient_loss | -0.000325     |
|    value_loss           | 1.48          |
-------------------------------------------
Progress update from timestep 509500: Updated shield, loss is 0.03427752107381821
Progress update from timestep 509600: Updated shield, loss is 0.026858458295464516
Eval num_timesteps=509600, episode_reward=28.85 +/- 12.96
Episode length: 41.00 +/- 18.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41       |
|    mean_reward     | 28.8     |
| time/              |          |
|    total_timesteps | 509600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.15     |
|    ep_rew_mean     | 2.57     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1274     |
|    time_elapsed    | 115267   |
|    total_timesteps | 509600   |
---------------------------------
Progress update from timestep 509700: Updated shield, loss is 0.03387747332453728
Progress update from timestep 509800: Updated shield, loss is 0.039972227066755295
Eval num_timesteps=509800, episode_reward=12.61 +/- 10.98
Episode length: 19.00 +/- 15.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 19            |
|    mean_reward          | 12.6          |
| time/                   |               |
|    total_timesteps      | 509800        |
| train/                  |               |
|    approx_kl            | 0.00011776237 |
|    clip_fraction        | 0.00603       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0122       |
|    explained_variance   | 0.154         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.01          |
|    n_updates            | 12740         |
|    policy_gradient_loss | -0.000588     |
|    value_loss           | 1.75          |
-------------------------------------------
Progress update from timestep 509900: Updated shield, loss is 0.024788925424218178
Progress update from timestep 510000: Updated shield, loss is 0.03316453844308853
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 510000: Saved shield_loss.csv. 
Eval num_timesteps=510000, episode_reward=21.46 +/- 12.16
Episode length: 30.80 +/- 16.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.8     |
|    mean_reward     | 21.5     |
| time/              |          |
|    total_timesteps | 510000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.1      |
|    ep_rew_mean     | 2.52     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1275     |
|    time_elapsed    | 115358   |
|    total_timesteps | 510000   |
---------------------------------
Progress update from timestep 510100: Updated shield, loss is 0.03225788474082947
Progress update from timestep 510200: Updated shield, loss is 0.03532066196203232
Eval num_timesteps=510200, episode_reward=7.41 +/- 2.14
Episode length: 11.40 +/- 3.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 11.4         |
|    mean_reward          | 7.41         |
| time/                   |              |
|    total_timesteps      | 510200       |
| train/                  |              |
|    approx_kl            | 0.0006231196 |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0267      |
|    explained_variance   | 0.192        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.513        |
|    n_updates            | 12750        |
|    policy_gradient_loss | -0.00115     |
|    value_loss           | 1.74         |
------------------------------------------
Progress update from timestep 510300: Updated shield, loss is 0.033013708889484406
Progress update from timestep 510400: Updated shield, loss is 0.040459658950567245
Eval num_timesteps=510400, episode_reward=23.51 +/- 14.27
Episode length: 34.00 +/- 20.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 23.5     |
| time/              |          |
|    total_timesteps | 510400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.93     |
|    ep_rew_mean     | 2.42     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1276     |
|    time_elapsed    | 115450   |
|    total_timesteps | 510400   |
---------------------------------
Progress update from timestep 510500: Updated shield, loss is 0.029298577457666397
Progress update from timestep 510600: Updated shield, loss is 0.034742116928100586
Eval num_timesteps=510600, episode_reward=14.26 +/- 10.05
Episode length: 21.80 +/- 14.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 21.8          |
|    mean_reward          | 14.3          |
| time/                   |               |
|    total_timesteps      | 510600        |
| train/                  |               |
|    approx_kl            | 0.00024539704 |
|    clip_fraction        | 0.00871       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0241       |
|    explained_variance   | 0.0751        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.355         |
|    n_updates            | 12760         |
|    policy_gradient_loss | -0.00123      |
|    value_loss           | 1.41          |
-------------------------------------------
Progress update from timestep 510700: Updated shield, loss is 0.03326807916164398
Progress update from timestep 510800: Updated shield, loss is 0.03791314363479614
Eval num_timesteps=510800, episode_reward=14.45 +/- 10.75
Episode length: 21.40 +/- 15.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 14.5     |
| time/              |          |
|    total_timesteps | 510800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.84     |
|    ep_rew_mean     | 3.17     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1277     |
|    time_elapsed    | 115531   |
|    total_timesteps | 510800   |
---------------------------------
Progress update from timestep 510900: Updated shield, loss is 0.03797677531838417
Progress update from timestep 511000: Updated shield, loss is 0.037649016827344894
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 511000: Saved shield_loss.csv. 
Eval num_timesteps=511000, episode_reward=30.31 +/- 8.94
Episode length: 43.60 +/- 12.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 43.6          |
|    mean_reward          | 30.3          |
| time/                   |               |
|    total_timesteps      | 511000        |
| train/                  |               |
|    approx_kl            | 2.4059369e-05 |
|    clip_fraction        | 0.0029        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00725      |
|    explained_variance   | 0.274         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.917         |
|    n_updates            | 12770         |
|    policy_gradient_loss | -0.000291     |
|    value_loss           | 1.9           |
-------------------------------------------
Progress update from timestep 511100: Updated shield, loss is 0.03473011031746864
Progress update from timestep 511200: Updated shield, loss is 0.03737596422433853
Eval num_timesteps=511200, episode_reward=15.55 +/- 15.16
Episode length: 23.40 +/- 21.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 15.5     |
| time/              |          |
|    total_timesteps | 511200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.99     |
|    ep_rew_mean     | 2.54     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1278     |
|    time_elapsed    | 115619   |
|    total_timesteps | 511200   |
---------------------------------
Progress update from timestep 511300: Updated shield, loss is 0.03474465012550354
Progress update from timestep 511400: Updated shield, loss is 0.0300168227404356
Eval num_timesteps=511400, episode_reward=4.19 +/- 1.17
Episode length: 6.80 +/- 1.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 6.8          |
|    mean_reward          | 4.19         |
| time/                   |              |
|    total_timesteps      | 511400       |
| train/                  |              |
|    approx_kl            | 0.0017580279 |
|    clip_fraction        | 0.0121       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0226      |
|    explained_variance   | 0.332        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.374        |
|    n_updates            | 12780        |
|    policy_gradient_loss | -0.00217     |
|    value_loss           | 1.5          |
------------------------------------------
Progress update from timestep 511500: Updated shield, loss is 0.031828828155994415
Progress update from timestep 511600: Updated shield, loss is 0.04021208733320236
Eval num_timesteps=511600, episode_reward=9.64 +/- 13.00
Episode length: 14.40 +/- 17.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.4     |
|    mean_reward     | 9.64     |
| time/              |          |
|    total_timesteps | 511600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.23     |
|    ep_rew_mean     | 2.73     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1279     |
|    time_elapsed    | 115688   |
|    total_timesteps | 511600   |
---------------------------------
Progress update from timestep 511700: Updated shield, loss is 0.04093453660607338
Progress update from timestep 511800: Updated shield, loss is 0.03293055668473244
Eval num_timesteps=511800, episode_reward=16.09 +/- 15.96
Episode length: 23.20 +/- 21.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.2         |
|    mean_reward          | 16.1         |
| time/                   |              |
|    total_timesteps      | 511800       |
| train/                  |              |
|    approx_kl            | 0.0008509646 |
|    clip_fraction        | 0.0154       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0301      |
|    explained_variance   | 0.194        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.765        |
|    n_updates            | 12790        |
|    policy_gradient_loss | -0.00195     |
|    value_loss           | 1.72         |
------------------------------------------
Progress update from timestep 511900: Updated shield, loss is 0.04408205300569534
Progress update from timestep 512000: Updated shield, loss is 0.03799213469028473
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 512000: Saved shield_loss.csv. 
Eval num_timesteps=512000, episode_reward=23.41 +/- 14.53
Episode length: 33.60 +/- 20.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 23.4     |
| time/              |          |
|    total_timesteps | 512000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.6      |
|    ep_rew_mean     | 2.95     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1280     |
|    time_elapsed    | 115774   |
|    total_timesteps | 512000   |
---------------------------------
Progress update from timestep 512100: Updated shield, loss is 0.03658914938569069
Progress update from timestep 512200: Updated shield, loss is 0.04494275897741318
Eval num_timesteps=512200, episode_reward=18.57 +/- 14.17
Episode length: 26.80 +/- 19.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.8          |
|    mean_reward          | 18.6          |
| time/                   |               |
|    total_timesteps      | 512200        |
| train/                  |               |
|    approx_kl            | 0.00018937301 |
|    clip_fraction        | 0.00424       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0211       |
|    explained_variance   | 0.304         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.741         |
|    n_updates            | 12800         |
|    policy_gradient_loss | -0.00051      |
|    value_loss           | 2.04          |
-------------------------------------------
Progress update from timestep 512300: Updated shield, loss is 0.03331592306494713
Progress update from timestep 512400: Updated shield, loss is 0.04245523735880852
Eval num_timesteps=512400, episode_reward=16.34 +/- 15.41
Episode length: 23.80 +/- 21.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | 16.3     |
| time/              |          |
|    total_timesteps | 512400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.79     |
|    ep_rew_mean     | 3.17     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1281     |
|    time_elapsed    | 115861   |
|    total_timesteps | 512400   |
---------------------------------
Progress update from timestep 512500: Updated shield, loss is 0.034532323479652405
Progress update from timestep 512600: Updated shield, loss is 0.038625266402959824
Eval num_timesteps=512600, episode_reward=15.93 +/- 13.94
Episode length: 22.80 +/- 18.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22.8         |
|    mean_reward          | 15.9         |
| time/                   |              |
|    total_timesteps      | 512600       |
| train/                  |              |
|    approx_kl            | 6.558905e-05 |
|    clip_fraction        | 0.00246      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0105      |
|    explained_variance   | 0.214        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.728        |
|    n_updates            | 12810        |
|    policy_gradient_loss | -0.000276    |
|    value_loss           | 2.14         |
------------------------------------------
Progress update from timestep 512700: Updated shield, loss is 0.03635400906205177
Progress update from timestep 512800: Updated shield, loss is 0.0342283695936203
Eval num_timesteps=512800, episode_reward=16.41 +/- 14.90
Episode length: 24.20 +/- 21.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 16.4     |
| time/              |          |
|    total_timesteps | 512800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.46     |
|    ep_rew_mean     | 2.76     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1282     |
|    time_elapsed    | 115963   |
|    total_timesteps | 512800   |
---------------------------------
Progress update from timestep 512900: Updated shield, loss is 0.043580226600170135
Progress update from timestep 513000: Updated shield, loss is 0.033072762191295624
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 513000: Saved shield_loss.csv. 
Eval num_timesteps=513000, episode_reward=17.69 +/- 14.68
Episode length: 25.40 +/- 20.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.4          |
|    mean_reward          | 17.7          |
| time/                   |               |
|    total_timesteps      | 513000        |
| train/                  |               |
|    approx_kl            | 0.00017105624 |
|    clip_fraction        | 0.00826       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.014        |
|    explained_variance   | 0.0873        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.463         |
|    n_updates            | 12820         |
|    policy_gradient_loss | -0.00106      |
|    value_loss           | 1.59          |
-------------------------------------------
Progress update from timestep 513100: Updated shield, loss is 0.037538643926382065
Progress update from timestep 513200: Updated shield, loss is 0.03861427307128906
Eval num_timesteps=513200, episode_reward=20.36 +/- 12.74
Episode length: 29.60 +/- 17.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.6     |
|    mean_reward     | 20.4     |
| time/              |          |
|    total_timesteps | 513200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.15     |
|    ep_rew_mean     | 2.58     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1283     |
|    time_elapsed    | 116056   |
|    total_timesteps | 513200   |
---------------------------------
Progress update from timestep 513300: Updated shield, loss is 0.04739835858345032
Progress update from timestep 513400: Updated shield, loss is 0.03623468056321144
Eval num_timesteps=513400, episode_reward=12.81 +/- 11.89
Episode length: 19.40 +/- 17.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 19.4          |
|    mean_reward          | 12.8          |
| time/                   |               |
|    total_timesteps      | 513400        |
| train/                  |               |
|    approx_kl            | 0.00016729734 |
|    clip_fraction        | 0.00223       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0188       |
|    explained_variance   | 0.0969        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.742         |
|    n_updates            | 12830         |
|    policy_gradient_loss | -0.000346     |
|    value_loss           | 1.46          |
-------------------------------------------
Progress update from timestep 513500: Updated shield, loss is 0.034013982862234116
Progress update from timestep 513600: Updated shield, loss is 0.03466053679585457
Eval num_timesteps=513600, episode_reward=18.53 +/- 13.88
Episode length: 27.20 +/- 19.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.2     |
|    mean_reward     | 18.5     |
| time/              |          |
|    total_timesteps | 513600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.3      |
|    ep_rew_mean     | 2.69     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1284     |
|    time_elapsed    | 116142   |
|    total_timesteps | 513600   |
---------------------------------
Progress update from timestep 513700: Updated shield, loss is 0.04259912669658661
Progress update from timestep 513800: Updated shield, loss is 0.03781416267156601
Eval num_timesteps=513800, episode_reward=22.78 +/- 13.46
Episode length: 34.20 +/- 19.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.2          |
|    mean_reward          | 22.8          |
| time/                   |               |
|    total_timesteps      | 513800        |
| train/                  |               |
|    approx_kl            | 3.2871737e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.000351     |
|    explained_variance   | 0.193         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.532         |
|    n_updates            | 12840         |
|    policy_gradient_loss | -4.92e-08     |
|    value_loss           | 1.5           |
-------------------------------------------
Progress update from timestep 513900: Updated shield, loss is 0.04476068913936615
Progress update from timestep 514000: Updated shield, loss is 0.03786074370145798
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 514000: Saved shield_loss.csv. 
Eval num_timesteps=514000, episode_reward=9.33 +/- 13.15
Episode length: 14.00 +/- 18.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14       |
|    mean_reward     | 9.33     |
| time/              |          |
|    total_timesteps | 514000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.55     |
|    ep_rew_mean     | 2.85     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1285     |
|    time_elapsed    | 116227   |
|    total_timesteps | 514000   |
---------------------------------
Progress update from timestep 514100: Updated shield, loss is 0.03755801543593407
Progress update from timestep 514200: Updated shield, loss is 0.03459027409553528
Eval num_timesteps=514200, episode_reward=22.61 +/- 14.56
Episode length: 33.20 +/- 20.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.2         |
|    mean_reward          | 22.6         |
| time/                   |              |
|    total_timesteps      | 514200       |
| train/                  |              |
|    approx_kl            | 0.0005958042 |
|    clip_fraction        | 0.019        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0443      |
|    explained_variance   | 0.342        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.36         |
|    n_updates            | 12850        |
|    policy_gradient_loss | -0.0028      |
|    value_loss           | 1.99         |
------------------------------------------
Progress update from timestep 514300: Updated shield, loss is 0.030200323089957237
Progress update from timestep 514400: Updated shield, loss is 0.021494580432772636
Eval num_timesteps=514400, episode_reward=12.91 +/- 11.43
Episode length: 18.60 +/- 15.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 12.9     |
| time/              |          |
|    total_timesteps | 514400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.98     |
|    ep_rew_mean     | 3.21     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1286     |
|    time_elapsed    | 116312   |
|    total_timesteps | 514400   |
---------------------------------
Progress update from timestep 514500: Updated shield, loss is 0.034336917102336884
Progress update from timestep 514600: Updated shield, loss is 0.03715719282627106
Eval num_timesteps=514600, episode_reward=11.94 +/- 12.18
Episode length: 18.20 +/- 17.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 18.2          |
|    mean_reward          | 11.9          |
| time/                   |               |
|    total_timesteps      | 514600        |
| train/                  |               |
|    approx_kl            | 0.00027470154 |
|    clip_fraction        | 0.00558       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0116       |
|    explained_variance   | 0.153         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.908         |
|    n_updates            | 12860         |
|    policy_gradient_loss | -0.000355     |
|    value_loss           | 2.4           |
-------------------------------------------
Progress update from timestep 514700: Updated shield, loss is 0.028351271525025368
Progress update from timestep 514800: Updated shield, loss is 0.04660053551197052
Eval num_timesteps=514800, episode_reward=23.41 +/- 11.52
Episode length: 33.60 +/- 15.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 23.4     |
| time/              |          |
|    total_timesteps | 514800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.65     |
|    ep_rew_mean     | 2.99     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1287     |
|    time_elapsed    | 116400   |
|    total_timesteps | 514800   |
---------------------------------
Progress update from timestep 514900: Updated shield, loss is 0.03382498398423195
Progress update from timestep 515000: Updated shield, loss is 0.03861086815595627
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 515000: Saved shield_loss.csv. 
Eval num_timesteps=515000, episode_reward=25.24 +/- 11.33
Episode length: 37.00 +/- 16.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 37            |
|    mean_reward          | 25.2          |
| time/                   |               |
|    total_timesteps      | 515000        |
| train/                  |               |
|    approx_kl            | 0.00029004118 |
|    clip_fraction        | 0.0058        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0105       |
|    explained_variance   | 0.3           |
|    learning_rate        | 0.0001        |
|    loss                 | 1.32          |
|    n_updates            | 12870         |
|    policy_gradient_loss | -0.000841     |
|    value_loss           | 2.47          |
-------------------------------------------
Progress update from timestep 515100: Updated shield, loss is 0.035919949412345886
Progress update from timestep 515200: Updated shield, loss is 0.035460468381643295
Eval num_timesteps=515200, episode_reward=10.69 +/- 13.19
Episode length: 15.60 +/- 17.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 10.7     |
| time/              |          |
|    total_timesteps | 515200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.39     |
|    ep_rew_mean     | 2.8      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1288     |
|    time_elapsed    | 116495   |
|    total_timesteps | 515200   |
---------------------------------
Progress update from timestep 515300: Updated shield, loss is 0.03668438270688057
Progress update from timestep 515400: Updated shield, loss is 0.0274504441767931
Eval num_timesteps=515400, episode_reward=17.47 +/- 13.85
Episode length: 26.20 +/- 20.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.2         |
|    mean_reward          | 17.5         |
| time/                   |              |
|    total_timesteps      | 515400       |
| train/                  |              |
|    approx_kl            | 0.0005280092 |
|    clip_fraction        | 0.0109       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0299      |
|    explained_variance   | 0.102        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.377        |
|    n_updates            | 12880        |
|    policy_gradient_loss | -0.00105     |
|    value_loss           | 1.55         |
------------------------------------------
Progress update from timestep 515500: Updated shield, loss is 0.03786086291074753
Progress update from timestep 515600: Updated shield, loss is 0.03479888290166855
Eval num_timesteps=515600, episode_reward=17.25 +/- 14.68
Episode length: 25.20 +/- 20.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 17.2     |
| time/              |          |
|    total_timesteps | 515600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.39     |
|    ep_rew_mean     | 2.78     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1289     |
|    time_elapsed    | 116595   |
|    total_timesteps | 515600   |
---------------------------------
Progress update from timestep 515700: Updated shield, loss is 0.03433175012469292
Progress update from timestep 515800: Updated shield, loss is 0.03610081225633621
Eval num_timesteps=515800, episode_reward=16.89 +/- 14.38
Episode length: 24.80 +/- 20.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.8          |
|    mean_reward          | 16.9          |
| time/                   |               |
|    total_timesteps      | 515800        |
| train/                  |               |
|    approx_kl            | 0.00022053406 |
|    clip_fraction        | 0.00781       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0138       |
|    explained_variance   | 0.168         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.38          |
|    n_updates            | 12890         |
|    policy_gradient_loss | -0.000714     |
|    value_loss           | 2.02          |
-------------------------------------------
Progress update from timestep 515900: Updated shield, loss is 0.047601379454135895
Progress update from timestep 516000: Updated shield, loss is 0.033782120794057846
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 516000: Saved shield_loss.csv. 
Eval num_timesteps=516000, episode_reward=10.98 +/- 11.28
Episode length: 17.00 +/- 16.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | 11       |
| time/              |          |
|    total_timesteps | 516000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.81     |
|    ep_rew_mean     | 3.14     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1290     |
|    time_elapsed    | 116682   |
|    total_timesteps | 516000   |
---------------------------------
Progress update from timestep 516100: Updated shield, loss is 0.03220824897289276
Progress update from timestep 516200: Updated shield, loss is 0.03647986426949501
Eval num_timesteps=516200, episode_reward=18.57 +/- 13.77
Episode length: 27.20 +/- 19.14
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 27.2           |
|    mean_reward          | 18.6           |
| time/                   |                |
|    total_timesteps      | 516200         |
| train/                  |                |
|    approx_kl            | -7.8768735e-11 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.000429      |
|    explained_variance   | 0.232          |
|    learning_rate        | 0.0001         |
|    loss                 | 0.833          |
|    n_updates            | 12900          |
|    policy_gradient_loss | -3e-07         |
|    value_loss           | 2.12           |
--------------------------------------------
Progress update from timestep 516300: Updated shield, loss is 0.03729693219065666
Progress update from timestep 516400: Updated shield, loss is 0.03040982224047184
Eval num_timesteps=516400, episode_reward=8.24 +/- 12.61
Episode length: 13.00 +/- 18.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13       |
|    mean_reward     | 8.24     |
| time/              |          |
|    total_timesteps | 516400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.87     |
|    ep_rew_mean     | 3.11     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1291     |
|    time_elapsed    | 116763   |
|    total_timesteps | 516400   |
---------------------------------
Progress update from timestep 516500: Updated shield, loss is 0.027927327901124954
Progress update from timestep 516600: Updated shield, loss is 0.03793821856379509
Eval num_timesteps=516600, episode_reward=14.12 +/- 12.55
Episode length: 21.00 +/- 18.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 21            |
|    mean_reward          | 14.1          |
| time/                   |               |
|    total_timesteps      | 516600        |
| train/                  |               |
|    approx_kl            | 2.9971098e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.000483     |
|    explained_variance   | 0.092         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.515         |
|    n_updates            | 12910         |
|    policy_gradient_loss | -2.67e-08     |
|    value_loss           | 2.31          |
-------------------------------------------
Progress update from timestep 516700: Updated shield, loss is 0.03410034999251366
Progress update from timestep 516800: Updated shield, loss is 0.03692976012825966
Eval num_timesteps=516800, episode_reward=16.25 +/- 16.70
Episode length: 22.80 +/- 22.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 16.3     |
| time/              |          |
|    total_timesteps | 516800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.5      |
|    ep_rew_mean     | 2.79     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1292     |
|    time_elapsed    | 116845   |
|    total_timesteps | 516800   |
---------------------------------
Progress update from timestep 516900: Updated shield, loss is 0.033567093312740326
Progress update from timestep 517000: Updated shield, loss is 0.03682345151901245
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 517000: Saved shield_loss.csv. 
Eval num_timesteps=517000, episode_reward=21.93 +/- 15.40
Episode length: 32.20 +/- 21.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32.2          |
|    mean_reward          | 21.9          |
| time/                   |               |
|    total_timesteps      | 517000        |
| train/                  |               |
|    approx_kl            | 0.00050790573 |
|    clip_fraction        | 0.0114        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0183       |
|    explained_variance   | 0.0543        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.681         |
|    n_updates            | 12920         |
|    policy_gradient_loss | -0.00156      |
|    value_loss           | 1.98          |
-------------------------------------------
Progress update from timestep 517100: Updated shield, loss is 0.034325581043958664
Progress update from timestep 517200: Updated shield, loss is 0.0419902503490448
Eval num_timesteps=517200, episode_reward=4.22 +/- 2.66
Episode length: 7.20 +/- 3.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7.2      |
|    mean_reward     | 4.22     |
| time/              |          |
|    total_timesteps | 517200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.25     |
|    ep_rew_mean     | 2.68     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1293     |
|    time_elapsed    | 116931   |
|    total_timesteps | 517200   |
---------------------------------
Progress update from timestep 517300: Updated shield, loss is 0.03282983601093292
Progress update from timestep 517400: Updated shield, loss is 0.031075378879904747
Eval num_timesteps=517400, episode_reward=10.15 +/- 12.76
Episode length: 15.00 +/- 17.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15            |
|    mean_reward          | 10.2          |
| time/                   |               |
|    total_timesteps      | 517400        |
| train/                  |               |
|    approx_kl            | 2.4218694e-05 |
|    clip_fraction        | 0.00067       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00625      |
|    explained_variance   | 0.169         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.677         |
|    n_updates            | 12930         |
|    policy_gradient_loss | -0.000132     |
|    value_loss           | 2.2           |
-------------------------------------------
Progress update from timestep 517500: Updated shield, loss is 0.03507934510707855
Progress update from timestep 517600: Updated shield, loss is 0.028640151023864746
Eval num_timesteps=517600, episode_reward=23.08 +/- 13.97
Episode length: 33.80 +/- 19.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 23.1     |
| time/              |          |
|    total_timesteps | 517600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.42     |
|    ep_rew_mean     | 2.81     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1294     |
|    time_elapsed    | 117015   |
|    total_timesteps | 517600   |
---------------------------------
Progress update from timestep 517700: Updated shield, loss is 0.03222400322556496
Progress update from timestep 517800: Updated shield, loss is 0.03289002552628517
Eval num_timesteps=517800, episode_reward=22.65 +/- 14.97
Episode length: 33.00 +/- 20.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33            |
|    mean_reward          | 22.6          |
| time/                   |               |
|    total_timesteps      | 517800        |
| train/                  |               |
|    approx_kl            | 0.00010539116 |
|    clip_fraction        | 0.00491       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0137       |
|    explained_variance   | 0.146         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.802         |
|    n_updates            | 12940         |
|    policy_gradient_loss | -0.0011       |
|    value_loss           | 1.95          |
-------------------------------------------
Progress update from timestep 517900: Updated shield, loss is 0.04402510076761246
Progress update from timestep 518000: Updated shield, loss is 0.033467285335063934
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 518000: Saved shield_loss.csv. 
Eval num_timesteps=518000, episode_reward=15.21 +/- 13.41
Episode length: 22.00 +/- 18.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22       |
|    mean_reward     | 15.2     |
| time/              |          |
|    total_timesteps | 518000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.16     |
|    ep_rew_mean     | 2.61     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1295     |
|    time_elapsed    | 117098   |
|    total_timesteps | 518000   |
---------------------------------
Progress update from timestep 518100: Updated shield, loss is 0.023902803659439087
Progress update from timestep 518200: Updated shield, loss is 0.042782802134752274
Eval num_timesteps=518200, episode_reward=18.81 +/- 12.84
Episode length: 27.80 +/- 18.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 27.8          |
|    mean_reward          | 18.8          |
| time/                   |               |
|    total_timesteps      | 518200        |
| train/                  |               |
|    approx_kl            | 0.00055863505 |
|    clip_fraction        | 0.0136        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0356       |
|    explained_variance   | 0.185         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.713         |
|    n_updates            | 12950         |
|    policy_gradient_loss | -0.00166      |
|    value_loss           | 1.56          |
-------------------------------------------
Progress update from timestep 518300: Updated shield, loss is 0.029901880770921707
Progress update from timestep 518400: Updated shield, loss is 0.03260393813252449
Eval num_timesteps=518400, episode_reward=10.98 +/- 11.89
Episode length: 16.60 +/- 16.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | 11       |
| time/              |          |
|    total_timesteps | 518400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.97     |
|    ep_rew_mean     | 2.52     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1296     |
|    time_elapsed    | 117187   |
|    total_timesteps | 518400   |
---------------------------------
Progress update from timestep 518500: Updated shield, loss is 0.039663657546043396
Progress update from timestep 518600: Updated shield, loss is 0.046200282871723175
Eval num_timesteps=518600, episode_reward=9.13 +/- 13.29
Episode length: 13.60 +/- 18.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 13.6          |
|    mean_reward          | 9.13          |
| time/                   |               |
|    total_timesteps      | 518600        |
| train/                  |               |
|    approx_kl            | 0.00022133252 |
|    clip_fraction        | 0.00804       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0168       |
|    explained_variance   | 0.183         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.427         |
|    n_updates            | 12960         |
|    policy_gradient_loss | -0.00076      |
|    value_loss           | 1.33          |
-------------------------------------------
Progress update from timestep 518700: Updated shield, loss is 0.03456299751996994
Progress update from timestep 518800: Updated shield, loss is 0.028653806075453758
Eval num_timesteps=518800, episode_reward=17.41 +/- 14.95
Episode length: 25.20 +/- 20.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 518800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.5      |
|    ep_rew_mean     | 2.9      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1297     |
|    time_elapsed    | 117274   |
|    total_timesteps | 518800   |
---------------------------------
Progress update from timestep 518900: Updated shield, loss is 0.03272322192788124
Progress update from timestep 519000: Updated shield, loss is 0.030612217262387276
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 519000: Saved shield_loss.csv. 
Eval num_timesteps=519000, episode_reward=4.91 +/- 2.98
Episode length: 7.80 +/- 4.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 7.8          |
|    mean_reward          | 4.91         |
| time/                   |              |
|    total_timesteps      | 519000       |
| train/                  |              |
|    approx_kl            | 0.0006594436 |
|    clip_fraction        | 0.00603      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0198      |
|    explained_variance   | 0.154        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.56         |
|    n_updates            | 12970        |
|    policy_gradient_loss | -0.000819    |
|    value_loss           | 2.87         |
------------------------------------------
Progress update from timestep 519100: Updated shield, loss is 0.035566095262765884
Progress update from timestep 519200: Updated shield, loss is 0.038957156240940094
Eval num_timesteps=519200, episode_reward=12.98 +/- 11.58
Episode length: 19.60 +/- 16.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 13       |
| time/              |          |
|    total_timesteps | 519200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.15     |
|    ep_rew_mean     | 2.57     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1298     |
|    time_elapsed    | 117350   |
|    total_timesteps | 519200   |
---------------------------------
Progress update from timestep 519300: Updated shield, loss is 0.03258766978979111
Progress update from timestep 519400: Updated shield, loss is 0.0364377498626709
Eval num_timesteps=519400, episode_reward=23.34 +/- 15.50
Episode length: 33.00 +/- 20.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33            |
|    mean_reward          | 23.3          |
| time/                   |               |
|    total_timesteps      | 519400        |
| train/                  |               |
|    approx_kl            | -5.671349e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.000499     |
|    explained_variance   | 0.129         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.472         |
|    n_updates            | 12980         |
|    policy_gradient_loss | 5.76e-08      |
|    value_loss           | 1.65          |
-------------------------------------------
Progress update from timestep 519500: Updated shield, loss is 0.030740296468138695
Progress update from timestep 519600: Updated shield, loss is 0.03617360442876816
Eval num_timesteps=519600, episode_reward=18.16 +/- 15.28
Episode length: 26.40 +/- 21.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.4     |
|    mean_reward     | 18.2     |
| time/              |          |
|    total_timesteps | 519600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.59     |
|    ep_rew_mean     | 2.92     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1299     |
|    time_elapsed    | 117433   |
|    total_timesteps | 519600   |
---------------------------------
Progress update from timestep 519700: Updated shield, loss is 0.03279779851436615
Progress update from timestep 519800: Updated shield, loss is 0.03572893142700195
Eval num_timesteps=519800, episode_reward=15.37 +/- 15.22
Episode length: 23.20 +/- 21.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.2          |
|    mean_reward          | 15.4          |
| time/                   |               |
|    total_timesteps      | 519800        |
| train/                  |               |
|    approx_kl            | 1.2742671e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.000413     |
|    explained_variance   | 0.0552        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.21          |
|    n_updates            | 12990         |
|    policy_gradient_loss | -8.88e-08     |
|    value_loss           | 2.39          |
-------------------------------------------
Progress update from timestep 519900: Updated shield, loss is 0.03728955611586571
Progress update from timestep 520000: Updated shield, loss is 0.02578539401292801
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 520000: Saved shield_loss.csv. 
Eval num_timesteps=520000, episode_reward=24.41 +/- 13.28
Episode length: 35.00 +/- 18.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 24.4     |
| time/              |          |
|    total_timesteps | 520000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.22     |
|    ep_rew_mean     | 2.65     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1300     |
|    time_elapsed    | 117519   |
|    total_timesteps | 520000   |
---------------------------------
Progress update from timestep 520100: Updated shield, loss is 0.029340257868170738
Progress update from timestep 520200: Updated shield, loss is 0.033418990671634674
Eval num_timesteps=520200, episode_reward=4.65 +/- 1.87
Episode length: 7.80 +/- 2.93
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 7.8           |
|    mean_reward          | 4.65          |
| time/                   |               |
|    total_timesteps      | 520200        |
| train/                  |               |
|    approx_kl            | 5.5327215e-05 |
|    clip_fraction        | 0.00402       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0104       |
|    explained_variance   | 0.34          |
|    learning_rate        | 0.0001        |
|    loss                 | 1.35          |
|    n_updates            | 13000         |
|    policy_gradient_loss | -0.000246     |
|    value_loss           | 2.1           |
-------------------------------------------
Progress update from timestep 520300: Updated shield, loss is 0.04021794721484184
Progress update from timestep 520400: Updated shield, loss is 0.03143530711531639
Eval num_timesteps=520400, episode_reward=13.64 +/- 11.03
Episode length: 20.20 +/- 15.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.2     |
|    mean_reward     | 13.6     |
| time/              |          |
|    total_timesteps | 520400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.94     |
|    ep_rew_mean     | 2.45     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1301     |
|    time_elapsed    | 117585   |
|    total_timesteps | 520400   |
---------------------------------
Progress update from timestep 520500: Updated shield, loss is 0.037761952728033066
Progress update from timestep 520600: Updated shield, loss is 0.042713116854429245
Eval num_timesteps=520600, episode_reward=16.41 +/- 15.68
Episode length: 23.80 +/- 21.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.8          |
|    mean_reward          | 16.4          |
| time/                   |               |
|    total_timesteps      | 520600        |
| train/                  |               |
|    approx_kl            | 3.0595398e-05 |
|    clip_fraction        | 0.00112       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00549      |
|    explained_variance   | 0.214         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.893         |
|    n_updates            | 13010         |
|    policy_gradient_loss | -0.000145     |
|    value_loss           | 2.48          |
-------------------------------------------
Progress update from timestep 520700: Updated shield, loss is 0.04383032023906708
Progress update from timestep 520800: Updated shield, loss is 0.027973946183919907
Eval num_timesteps=520800, episode_reward=11.34 +/- 11.05
Episode length: 17.40 +/- 16.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 11.3     |
| time/              |          |
|    total_timesteps | 520800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.9      |
|    ep_rew_mean     | 2.41     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1302     |
|    time_elapsed    | 117672   |
|    total_timesteps | 520800   |
---------------------------------
Progress update from timestep 520900: Updated shield, loss is 0.03272242844104767
Progress update from timestep 521000: Updated shield, loss is 0.03526892140507698
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 521000: Saved shield_loss.csv. 
Eval num_timesteps=521000, episode_reward=16.07 +/- 15.59
Episode length: 23.60 +/- 21.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.6         |
|    mean_reward          | 16.1         |
| time/                   |              |
|    total_timesteps      | 521000       |
| train/                  |              |
|    approx_kl            | 0.0007655783 |
|    clip_fraction        | 0.0179       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0381      |
|    explained_variance   | 0.12         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.731        |
|    n_updates            | 13020        |
|    policy_gradient_loss | -0.00158     |
|    value_loss           | 1.51         |
------------------------------------------
Progress update from timestep 521100: Updated shield, loss is 0.03752167895436287
Progress update from timestep 521200: Updated shield, loss is 0.03962268307805061
Eval num_timesteps=521200, episode_reward=28.41 +/- 13.82
Episode length: 40.40 +/- 19.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.4     |
|    mean_reward     | 28.4     |
| time/              |          |
|    total_timesteps | 521200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.77     |
|    ep_rew_mean     | 3.1      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1303     |
|    time_elapsed    | 117791   |
|    total_timesteps | 521200   |
---------------------------------
Progress update from timestep 521300: Updated shield, loss is 0.03334784880280495
Progress update from timestep 521400: Updated shield, loss is 0.033023130148649216
Eval num_timesteps=521400, episode_reward=22.48 +/- 17.41
Episode length: 31.00 +/- 23.27
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 31             |
|    mean_reward          | 22.5           |
| time/                   |                |
|    total_timesteps      | 521400         |
| train/                  |                |
|    approx_kl            | 0.000115209084 |
|    clip_fraction        | 0.00826        |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.0151        |
|    explained_variance   | 0.167          |
|    learning_rate        | 0.0001         |
|    loss                 | 0.66           |
|    n_updates            | 13030          |
|    policy_gradient_loss | -0.000857      |
|    value_loss           | 1.93           |
--------------------------------------------
Progress update from timestep 521500: Updated shield, loss is 0.036368902772665024
Progress update from timestep 521600: Updated shield, loss is 0.03879203647375107
Eval num_timesteps=521600, episode_reward=13.45 +/- 12.75
Episode length: 19.20 +/- 16.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.2     |
|    mean_reward     | 13.4     |
| time/              |          |
|    total_timesteps | 521600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.42     |
|    ep_rew_mean     | 2.79     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1304     |
|    time_elapsed    | 117876   |
|    total_timesteps | 521600   |
---------------------------------
Progress update from timestep 521700: Updated shield, loss is 0.03564075380563736
Progress update from timestep 521800: Updated shield, loss is 0.027152245864272118
Eval num_timesteps=521800, episode_reward=6.28 +/- 3.53
Episode length: 9.80 +/- 5.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 9.8           |
|    mean_reward          | 6.28          |
| time/                   |               |
|    total_timesteps      | 521800        |
| train/                  |               |
|    approx_kl            | 0.00020126652 |
|    clip_fraction        | 0.00625       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0133       |
|    explained_variance   | 0.162         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.02          |
|    n_updates            | 13040         |
|    policy_gradient_loss | -0.00059      |
|    value_loss           | 1.59          |
-------------------------------------------
Progress update from timestep 521900: Updated shield, loss is 0.03527417033910751
Progress update from timestep 522000: Updated shield, loss is 0.03034036234021187
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 522000: Saved shield_loss.csv. 
Eval num_timesteps=522000, episode_reward=24.34 +/- 13.21
Episode length: 35.40 +/- 18.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 24.3     |
| time/              |          |
|    total_timesteps | 522000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.96     |
|    ep_rew_mean     | 2.51     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1305     |
|    time_elapsed    | 117964   |
|    total_timesteps | 522000   |
---------------------------------
Progress update from timestep 522100: Updated shield, loss is 0.03564148768782616
Progress update from timestep 522200: Updated shield, loss is 0.035948801785707474
Eval num_timesteps=522200, episode_reward=18.87 +/- 14.03
Episode length: 27.00 +/- 19.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 27          |
|    mean_reward          | 18.9        |
| time/                   |             |
|    total_timesteps      | 522200      |
| train/                  |             |
|    approx_kl            | 0.000658817 |
|    clip_fraction        | 0.0067      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.012      |
|    explained_variance   | 0.133       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.663       |
|    n_updates            | 13050       |
|    policy_gradient_loss | -0.00115    |
|    value_loss           | 1.65        |
-----------------------------------------
Progress update from timestep 522300: Updated shield, loss is 0.0394156277179718
Progress update from timestep 522400: Updated shield, loss is 0.029483525082468987
Eval num_timesteps=522400, episode_reward=17.88 +/- 12.80
Episode length: 27.20 +/- 18.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.2     |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 522400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.13     |
|    ep_rew_mean     | 2.6      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1306     |
|    time_elapsed    | 118073   |
|    total_timesteps | 522400   |
---------------------------------
Progress update from timestep 522500: Updated shield, loss is 0.037076905369758606
Progress update from timestep 522600: Updated shield, loss is 0.03468603640794754
Eval num_timesteps=522600, episode_reward=16.27 +/- 15.37
Episode length: 23.60 +/- 21.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.6          |
|    mean_reward          | 16.3          |
| time/                   |               |
|    total_timesteps      | 522600        |
| train/                  |               |
|    approx_kl            | 0.00010232501 |
|    clip_fraction        | 0.0029        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00515      |
|    explained_variance   | 0.261         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.987         |
|    n_updates            | 13060         |
|    policy_gradient_loss | -0.000353     |
|    value_loss           | 1.58          |
-------------------------------------------
Progress update from timestep 522700: Updated shield, loss is 0.038173794746398926
Progress update from timestep 522800: Updated shield, loss is 0.038508910685777664
Eval num_timesteps=522800, episode_reward=23.75 +/- 14.09
Episode length: 34.20 +/- 19.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 23.8     |
| time/              |          |
|    total_timesteps | 522800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.04     |
|    ep_rew_mean     | 3.34     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1307     |
|    time_elapsed    | 118175   |
|    total_timesteps | 522800   |
---------------------------------
Progress update from timestep 522900: Updated shield, loss is 0.02945694327354431
Progress update from timestep 523000: Updated shield, loss is 0.039859991520643234
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 523000: Saved shield_loss.csv. 
Eval num_timesteps=523000, episode_reward=22.61 +/- 15.51
Episode length: 32.40 +/- 21.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32.4          |
|    mean_reward          | 22.6          |
| time/                   |               |
|    total_timesteps      | 523000        |
| train/                  |               |
|    approx_kl            | 9.2609225e-05 |
|    clip_fraction        | 0.00379       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0225       |
|    explained_variance   | 0.243         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.13          |
|    n_updates            | 13070         |
|    policy_gradient_loss | -0.0003       |
|    value_loss           | 2.37          |
-------------------------------------------
Progress update from timestep 523100: Updated shield, loss is 0.03496987000107765
Progress update from timestep 523200: Updated shield, loss is 0.02903003618121147
Eval num_timesteps=523200, episode_reward=16.49 +/- 14.72
Episode length: 24.20 +/- 21.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 523200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.24     |
|    ep_rew_mean     | 2.64     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1308     |
|    time_elapsed    | 118278   |
|    total_timesteps | 523200   |
---------------------------------
Progress update from timestep 523300: Updated shield, loss is 0.03532684966921806
Progress update from timestep 523400: Updated shield, loss is 0.02845669724047184
Eval num_timesteps=523400, episode_reward=7.58 +/- 5.46
Episode length: 11.80 +/- 7.98
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 11.8          |
|    mean_reward          | 7.58          |
| time/                   |               |
|    total_timesteps      | 523400        |
| train/                  |               |
|    approx_kl            | 0.00039525397 |
|    clip_fraction        | 0.0125        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.018        |
|    explained_variance   | 0.103         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.783         |
|    n_updates            | 13080         |
|    policy_gradient_loss | -0.000985     |
|    value_loss           | 1.73          |
-------------------------------------------
Progress update from timestep 523500: Updated shield, loss is 0.03361828252673149
Progress update from timestep 523600: Updated shield, loss is 0.04563235491514206
Eval num_timesteps=523600, episode_reward=17.44 +/- 14.20
Episode length: 26.20 +/- 20.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 523600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.88     |
|    ep_rew_mean     | 2.44     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1309     |
|    time_elapsed    | 118350   |
|    total_timesteps | 523600   |
---------------------------------
Progress update from timestep 523700: Updated shield, loss is 0.038443759083747864
Progress update from timestep 523800: Updated shield, loss is 0.03775636851787567
Eval num_timesteps=523800, episode_reward=8.01 +/- 5.40
Episode length: 12.40 +/- 7.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 12.4          |
|    mean_reward          | 8.01          |
| time/                   |               |
|    total_timesteps      | 523800        |
| train/                  |               |
|    approx_kl            | 0.00034030914 |
|    clip_fraction        | 0.00491       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0149       |
|    explained_variance   | 0.144         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.477         |
|    n_updates            | 13090         |
|    policy_gradient_loss | -0.000409     |
|    value_loss           | 1.3           |
-------------------------------------------
Progress update from timestep 523900: Updated shield, loss is 0.030109623447060585
Progress update from timestep 524000: Updated shield, loss is 0.028587408363819122
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 524000: Saved shield_loss.csv. 
Eval num_timesteps=524000, episode_reward=9.33 +/- 13.15
Episode length: 13.80 +/- 18.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.8     |
|    mean_reward     | 9.33     |
| time/              |          |
|    total_timesteps | 524000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.84     |
|    ep_rew_mean     | 2.35     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1310     |
|    time_elapsed    | 118422   |
|    total_timesteps | 524000   |
---------------------------------
Progress update from timestep 524100: Updated shield, loss is 0.03534692898392677
Progress update from timestep 524200: Updated shield, loss is 0.023961620405316353
Eval num_timesteps=524200, episode_reward=10.21 +/- 12.74
Episode length: 15.20 +/- 17.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.2          |
|    mean_reward          | 10.2          |
| time/                   |               |
|    total_timesteps      | 524200        |
| train/                  |               |
|    approx_kl            | 0.00012597024 |
|    clip_fraction        | 0.00871       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0133       |
|    explained_variance   | 0.208         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.7           |
|    n_updates            | 13100         |
|    policy_gradient_loss | -0.000931     |
|    value_loss           | 1.28          |
-------------------------------------------
Progress update from timestep 524300: Updated shield, loss is 0.039763469249010086
Progress update from timestep 524400: Updated shield, loss is 0.0357540100812912
Eval num_timesteps=524400, episode_reward=12.31 +/- 12.08
Episode length: 18.20 +/- 16.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | 12.3     |
| time/              |          |
|    total_timesteps | 524400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.05     |
|    ep_rew_mean     | 2.6      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1311     |
|    time_elapsed    | 118503   |
|    total_timesteps | 524400   |
---------------------------------
Progress update from timestep 524500: Updated shield, loss is 0.03775549679994583
Progress update from timestep 524600: Updated shield, loss is 0.03377687558531761
Eval num_timesteps=524600, episode_reward=3.81 +/- 2.52
Episode length: 6.60 +/- 3.83
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 6.6           |
|    mean_reward          | 3.81          |
| time/                   |               |
|    total_timesteps      | 524600        |
| train/                  |               |
|    approx_kl            | 3.0347685e-05 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.000301     |
|    explained_variance   | 0.177         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.907         |
|    n_updates            | 13110         |
|    policy_gradient_loss | -7.11e-05     |
|    value_loss           | 1.8           |
-------------------------------------------
Progress update from timestep 524700: Updated shield, loss is 0.028897257521748543
Progress update from timestep 524800: Updated shield, loss is 0.03747488558292389
Eval num_timesteps=524800, episode_reward=11.68 +/- 11.03
Episode length: 18.00 +/- 16.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | 11.7     |
| time/              |          |
|    total_timesteps | 524800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.51     |
|    ep_rew_mean     | 2.91     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1312     |
|    time_elapsed    | 118570   |
|    total_timesteps | 524800   |
---------------------------------
Progress update from timestep 524900: Updated shield, loss is 0.04047836735844612
Progress update from timestep 525000: Updated shield, loss is 0.03783984109759331
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 525000: Saved shield_loss.csv. 
Eval num_timesteps=525000, episode_reward=16.70 +/- 14.69
Episode length: 24.80 +/- 20.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.8          |
|    mean_reward          | 16.7          |
| time/                   |               |
|    total_timesteps      | 525000        |
| train/                  |               |
|    approx_kl            | 0.00017918104 |
|    clip_fraction        | 0.00223       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0144       |
|    explained_variance   | 0.135         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.571         |
|    n_updates            | 13120         |
|    policy_gradient_loss | -0.000285     |
|    value_loss           | 1.72          |
-------------------------------------------
Progress update from timestep 525100: Updated shield, loss is 0.04000996798276901
Progress update from timestep 525200: Updated shield, loss is 0.0395582877099514
Eval num_timesteps=525200, episode_reward=11.41 +/- 12.07
Episode length: 17.00 +/- 17.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | 11.4     |
| time/              |          |
|    total_timesteps | 525200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.83     |
|    ep_rew_mean     | 2.39     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1313     |
|    time_elapsed    | 118656   |
|    total_timesteps | 525200   |
---------------------------------
Progress update from timestep 525300: Updated shield, loss is 0.033417537808418274
Progress update from timestep 525400: Updated shield, loss is 0.03842777758836746
Eval num_timesteps=525400, episode_reward=5.52 +/- 3.40
Episode length: 8.80 +/- 4.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 8.8          |
|    mean_reward          | 5.52         |
| time/                   |              |
|    total_timesteps      | 525400       |
| train/                  |              |
|    approx_kl            | 0.0002521008 |
|    clip_fraction        | 0.00781      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0223      |
|    explained_variance   | 0.182        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.01         |
|    n_updates            | 13130        |
|    policy_gradient_loss | -0.000648    |
|    value_loss           | 1.6          |
------------------------------------------
Progress update from timestep 525500: Updated shield, loss is 0.04235073924064636
Progress update from timestep 525600: Updated shield, loss is 0.038944076746702194
Eval num_timesteps=525600, episode_reward=23.18 +/- 14.28
Episode length: 34.00 +/- 20.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 23.2     |
| time/              |          |
|    total_timesteps | 525600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.43     |
|    ep_rew_mean     | 2.81     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1314     |
|    time_elapsed    | 118723   |
|    total_timesteps | 525600   |
---------------------------------
Progress update from timestep 525700: Updated shield, loss is 0.039888136088848114
Progress update from timestep 525800: Updated shield, loss is 0.02975022979080677
Eval num_timesteps=525800, episode_reward=12.16 +/- 13.12
Episode length: 17.40 +/- 17.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.4         |
|    mean_reward          | 12.2         |
| time/                   |              |
|    total_timesteps      | 525800       |
| train/                  |              |
|    approx_kl            | 7.499717e-05 |
|    clip_fraction        | 0.00402      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0115      |
|    explained_variance   | 0.381        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.626        |
|    n_updates            | 13140        |
|    policy_gradient_loss | -0.000413    |
|    value_loss           | 1.69         |
------------------------------------------
Progress update from timestep 525900: Updated shield, loss is 0.03584490343928337
Progress update from timestep 526000: Updated shield, loss is 0.03922087699174881
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 526000: Saved shield_loss.csv. 
Eval num_timesteps=526000, episode_reward=12.54 +/- 11.09
Episode length: 18.60 +/- 15.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 12.5     |
| time/              |          |
|    total_timesteps | 526000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.63     |
|    ep_rew_mean     | 2.19     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1315     |
|    time_elapsed    | 118813   |
|    total_timesteps | 526000   |
---------------------------------
Progress update from timestep 526100: Updated shield, loss is 0.03923584520816803
Progress update from timestep 526200: Updated shield, loss is 0.04045484587550163
Eval num_timesteps=526200, episode_reward=27.96 +/- 13.58
Episode length: 40.40 +/- 19.20
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 40.4           |
|    mean_reward          | 28             |
| time/                   |                |
|    total_timesteps      | 526200         |
| train/                  |                |
|    approx_kl            | -1.2356136e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.000517      |
|    explained_variance   | 0.197          |
|    learning_rate        | 0.0001         |
|    loss                 | 0.445          |
|    n_updates            | 13150          |
|    policy_gradient_loss | -1.52e-07      |
|    value_loss           | 1.11           |
--------------------------------------------
Progress update from timestep 526300: Updated shield, loss is 0.03373900428414345
Progress update from timestep 526400: Updated shield, loss is 0.03413775563240051
Eval num_timesteps=526400, episode_reward=22.82 +/- 14.73
Episode length: 33.00 +/- 20.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 22.8     |
| time/              |          |
|    total_timesteps | 526400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.24     |
|    ep_rew_mean     | 2.68     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1316     |
|    time_elapsed    | 118933   |
|    total_timesteps | 526400   |
---------------------------------
Progress update from timestep 526500: Updated shield, loss is 0.03381695970892906
Progress update from timestep 526600: Updated shield, loss is 0.0356748104095459
Eval num_timesteps=526600, episode_reward=10.11 +/- 12.86
Episode length: 15.20 +/- 17.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.2         |
|    mean_reward          | 10.1         |
| time/                   |              |
|    total_timesteps      | 526600       |
| train/                  |              |
|    approx_kl            | 0.0020220235 |
|    clip_fraction        | 0.0105       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0103      |
|    explained_variance   | 0.212        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.936        |
|    n_updates            | 13160        |
|    policy_gradient_loss | -0.00144     |
|    value_loss           | 1.2          |
------------------------------------------
Progress update from timestep 526700: Updated shield, loss is 0.03979175165295601
Progress update from timestep 526800: Updated shield, loss is 0.04201332479715347
Eval num_timesteps=526800, episode_reward=10.49 +/- 11.80
Episode length: 16.20 +/- 17.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | 10.5     |
| time/              |          |
|    total_timesteps | 526800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.01     |
|    ep_rew_mean     | 2.55     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1317     |
|    time_elapsed    | 119015   |
|    total_timesteps | 526800   |
---------------------------------
Progress update from timestep 526900: Updated shield, loss is 0.0329582579433918
Progress update from timestep 527000: Updated shield, loss is 0.030054274946451187
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 527000: Saved shield_loss.csv. 
Eval num_timesteps=527000, episode_reward=16.55 +/- 15.32
Episode length: 24.20 +/- 21.36
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 24.2           |
|    mean_reward          | 16.5           |
| time/                   |                |
|    total_timesteps      | 527000         |
| train/                  |                |
|    approx_kl            | -2.4036645e-12 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.000111      |
|    explained_variance   | 0.326          |
|    learning_rate        | 0.0001         |
|    loss                 | 0.205          |
|    n_updates            | 13170          |
|    policy_gradient_loss | -8.59e-09      |
|    value_loss           | 1.12           |
--------------------------------------------
Progress update from timestep 527100: Updated shield, loss is 0.04290615767240524
Progress update from timestep 527200: Updated shield, loss is 0.0407266803085804
Eval num_timesteps=527200, episode_reward=17.04 +/- 16.17
Episode length: 23.80 +/- 21.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 527200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.88     |
|    ep_rew_mean     | 2.37     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1318     |
|    time_elapsed    | 119097   |
|    total_timesteps | 527200   |
---------------------------------
Progress update from timestep 527300: Updated shield, loss is 0.03082575462758541
Progress update from timestep 527400: Updated shield, loss is 0.03948814421892166
Eval num_timesteps=527400, episode_reward=16.06 +/- 16.40
Episode length: 22.80 +/- 22.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.8          |
|    mean_reward          | 16.1          |
| time/                   |               |
|    total_timesteps      | 527400        |
| train/                  |               |
|    approx_kl            | 1.1660813e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0035       |
|    explained_variance   | 0.186         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.351         |
|    n_updates            | 13180         |
|    policy_gradient_loss | -0.00015      |
|    value_loss           | 1.17          |
-------------------------------------------
Progress update from timestep 527500: Updated shield, loss is 0.033341117203235626
Progress update from timestep 527600: Updated shield, loss is 0.03655685856938362
Eval num_timesteps=527600, episode_reward=12.42 +/- 11.63
Episode length: 19.00 +/- 16.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 12.4     |
| time/              |          |
|    total_timesteps | 527600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.75     |
|    ep_rew_mean     | 2.33     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1319     |
|    time_elapsed    | 119181   |
|    total_timesteps | 527600   |
---------------------------------
Progress update from timestep 527700: Updated shield, loss is 0.03856400027871132
Progress update from timestep 527800: Updated shield, loss is 0.03602086752653122
Eval num_timesteps=527800, episode_reward=11.73 +/- 11.47
Episode length: 17.60 +/- 16.30
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17.6          |
|    mean_reward          | 11.7          |
| time/                   |               |
|    total_timesteps      | 527800        |
| train/                  |               |
|    approx_kl            | 0.00025044702 |
|    clip_fraction        | 0.0109        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0118       |
|    explained_variance   | 0.00484       |
|    learning_rate        | 0.0001        |
|    loss                 | 0.841         |
|    n_updates            | 13190         |
|    policy_gradient_loss | -0.000985     |
|    value_loss           | 1.43          |
-------------------------------------------
Progress update from timestep 527900: Updated shield, loss is 0.03463084250688553
Progress update from timestep 528000: Updated shield, loss is 0.04026900976896286
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 528000: Saved shield_loss.csv. 
Eval num_timesteps=528000, episode_reward=21.75 +/- 16.52
Episode length: 31.20 +/- 23.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.2     |
|    mean_reward     | 21.8     |
| time/              |          |
|    total_timesteps | 528000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.56     |
|    ep_rew_mean     | 2.13     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1320     |
|    time_elapsed    | 119265   |
|    total_timesteps | 528000   |
---------------------------------
Progress update from timestep 528100: Updated shield, loss is 0.029214927926659584
Progress update from timestep 528200: Updated shield, loss is 0.03900614008307457
Eval num_timesteps=528200, episode_reward=22.32 +/- 14.67
Episode length: 33.00 +/- 21.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33            |
|    mean_reward          | 22.3          |
| time/                   |               |
|    total_timesteps      | 528200        |
| train/                  |               |
|    approx_kl            | 0.00070463115 |
|    clip_fraction        | 0.0116        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0171       |
|    explained_variance   | 0.0956        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.65          |
|    n_updates            | 13200         |
|    policy_gradient_loss | -0.00103      |
|    value_loss           | 1.23          |
-------------------------------------------
Progress update from timestep 528300: Updated shield, loss is 0.03412970155477524
Progress update from timestep 528400: Updated shield, loss is 0.043156228959560394
Eval num_timesteps=528400, episode_reward=10.54 +/- 13.29
Episode length: 15.40 +/- 17.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | 10.5     |
| time/              |          |
|    total_timesteps | 528400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.08     |
|    ep_rew_mean     | 2.54     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1321     |
|    time_elapsed    | 119347   |
|    total_timesteps | 528400   |
---------------------------------
Progress update from timestep 528500: Updated shield, loss is 0.0354652926325798
Progress update from timestep 528600: Updated shield, loss is 0.032426342368125916
Eval num_timesteps=528600, episode_reward=23.72 +/- 14.92
Episode length: 33.60 +/- 20.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.6        |
|    mean_reward          | 23.7        |
| time/                   |             |
|    total_timesteps      | 528600      |
| train/                  |             |
|    approx_kl            | 0.004515667 |
|    clip_fraction        | 0.00848     |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0104     |
|    explained_variance   | 0.201       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.786       |
|    n_updates            | 13210       |
|    policy_gradient_loss | -0.000405   |
|    value_loss           | 1.95        |
-----------------------------------------
Progress update from timestep 528700: Updated shield, loss is 0.02815680205821991
Progress update from timestep 528800: Updated shield, loss is 0.03737950325012207
Eval num_timesteps=528800, episode_reward=4.45 +/- 2.92
Episode length: 7.20 +/- 4.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7.2      |
|    mean_reward     | 4.45     |
| time/              |          |
|    total_timesteps | 528800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.73     |
|    ep_rew_mean     | 2.3      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1322     |
|    time_elapsed    | 119420   |
|    total_timesteps | 528800   |
---------------------------------
Progress update from timestep 528900: Updated shield, loss is 0.02881905995309353
Progress update from timestep 529000: Updated shield, loss is 0.039517149329185486
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 529000: Saved shield_loss.csv. 
Eval num_timesteps=529000, episode_reward=21.50 +/- 15.95
Episode length: 31.60 +/- 22.57
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 31.6          |
|    mean_reward          | 21.5          |
| time/                   |               |
|    total_timesteps      | 529000        |
| train/                  |               |
|    approx_kl            | 0.00045958013 |
|    clip_fraction        | 0.00759       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0104       |
|    explained_variance   | 0.219         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.673         |
|    n_updates            | 13220         |
|    policy_gradient_loss | -0.001        |
|    value_loss           | 1.55          |
-------------------------------------------
Progress update from timestep 529100: Updated shield, loss is 0.0344555489718914
Progress update from timestep 529200: Updated shield, loss is 0.03427756205201149
Eval num_timesteps=529200, episode_reward=17.72 +/- 15.67
Episode length: 25.00 +/- 20.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | 17.7     |
| time/              |          |
|    total_timesteps | 529200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.14     |
|    ep_rew_mean     | 2.61     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1323     |
|    time_elapsed    | 119508   |
|    total_timesteps | 529200   |
---------------------------------
Progress update from timestep 529300: Updated shield, loss is 0.03758411109447479
Progress update from timestep 529400: Updated shield, loss is 0.04522961005568504
Eval num_timesteps=529400, episode_reward=18.21 +/- 14.17
Episode length: 27.40 +/- 20.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 27.4          |
|    mean_reward          | 18.2          |
| time/                   |               |
|    total_timesteps      | 529400        |
| train/                  |               |
|    approx_kl            | 4.4832017e-05 |
|    clip_fraction        | 0.000446      |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00749      |
|    explained_variance   | 0.139         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.811         |
|    n_updates            | 13230         |
|    policy_gradient_loss | -0.000313     |
|    value_loss           | 1.57          |
-------------------------------------------
Progress update from timestep 529500: Updated shield, loss is 0.03685588389635086
Progress update from timestep 529600: Updated shield, loss is 0.03850390762090683
Eval num_timesteps=529600, episode_reward=10.71 +/- 11.67
Episode length: 16.60 +/- 17.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | 10.7     |
| time/              |          |
|    total_timesteps | 529600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.05     |
|    ep_rew_mean     | 2.55     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1324     |
|    time_elapsed    | 119610   |
|    total_timesteps | 529600   |
---------------------------------
Progress update from timestep 529700: Updated shield, loss is 0.02957041934132576
Progress update from timestep 529800: Updated shield, loss is 0.028047680854797363
Eval num_timesteps=529800, episode_reward=17.77 +/- 14.30
Episode length: 26.00 +/- 19.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26           |
|    mean_reward          | 17.8         |
| time/                   |              |
|    total_timesteps      | 529800       |
| train/                  |              |
|    approx_kl            | 0.0001137835 |
|    clip_fraction        | 0.00692      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0104      |
|    explained_variance   | 0.171        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.764        |
|    n_updates            | 13240        |
|    policy_gradient_loss | -0.000879    |
|    value_loss           | 1.27         |
------------------------------------------
Progress update from timestep 529900: Updated shield, loss is 0.03411562740802765
Progress update from timestep 530000: Updated shield, loss is 0.034974612295627594
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 530000: Saved shield_loss.csv. 
Eval num_timesteps=530000, episode_reward=15.05 +/- 11.48
Episode length: 21.80 +/- 15.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | 15.1     |
| time/              |          |
|    total_timesteps | 530000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.4      |
|    ep_rew_mean     | 2.86     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1325     |
|    time_elapsed    | 119694   |
|    total_timesteps | 530000   |
---------------------------------
Progress update from timestep 530100: Updated shield, loss is 0.04040035977959633
Progress update from timestep 530200: Updated shield, loss is 0.03932739794254303
Eval num_timesteps=530200, episode_reward=22.87 +/- 15.17
Episode length: 33.00 +/- 20.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33           |
|    mean_reward          | 22.9         |
| time/                   |              |
|    total_timesteps      | 530200       |
| train/                  |              |
|    approx_kl            | 0.0009780814 |
|    clip_fraction        | 0.0219       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0349      |
|    explained_variance   | 0.234        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.49         |
|    n_updates            | 13250        |
|    policy_gradient_loss | -0.00182     |
|    value_loss           | 2.04         |
------------------------------------------
Progress update from timestep 530300: Updated shield, loss is 0.04002319276332855
Progress update from timestep 530400: Updated shield, loss is 0.03568540886044502
Eval num_timesteps=530400, episode_reward=30.09 +/- 9.93
Episode length: 43.00 +/- 14.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43       |
|    mean_reward     | 30.1     |
| time/              |          |
|    total_timesteps | 530400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.73     |
|    ep_rew_mean     | 3.05     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1326     |
|    time_elapsed    | 119783   |
|    total_timesteps | 530400   |
---------------------------------
Progress update from timestep 530500: Updated shield, loss is 0.03221381828188896
Progress update from timestep 530600: Updated shield, loss is 0.03258788213133812
Eval num_timesteps=530600, episode_reward=16.53 +/- 14.39
Episode length: 24.80 +/- 20.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.8          |
|    mean_reward          | 16.5          |
| time/                   |               |
|    total_timesteps      | 530600        |
| train/                  |               |
|    approx_kl            | 0.00020316716 |
|    clip_fraction        | 0.00647       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0144       |
|    explained_variance   | 0.174         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.89          |
|    n_updates            | 13260         |
|    policy_gradient_loss | -0.000659     |
|    value_loss           | 2.47          |
-------------------------------------------
Progress update from timestep 530700: Updated shield, loss is 0.03408656641840935
Progress update from timestep 530800: Updated shield, loss is 0.029435742646455765
Eval num_timesteps=530800, episode_reward=23.63 +/- 13.77
Episode length: 34.40 +/- 19.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 23.6     |
| time/              |          |
|    total_timesteps | 530800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.24     |
|    ep_rew_mean     | 2.63     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1327     |
|    time_elapsed    | 119868   |
|    total_timesteps | 530800   |
---------------------------------
Progress update from timestep 530900: Updated shield, loss is 0.038305576890707016
Progress update from timestep 531000: Updated shield, loss is 0.037036702036857605
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 531000: Saved shield_loss.csv. 
Eval num_timesteps=531000, episode_reward=22.69 +/- 14.47
Episode length: 33.20 +/- 20.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.2          |
|    mean_reward          | 22.7          |
| time/                   |               |
|    total_timesteps      | 531000        |
| train/                  |               |
|    approx_kl            | 0.00020819003 |
|    clip_fraction        | 0.00692       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0112       |
|    explained_variance   | 0.306         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.47          |
|    n_updates            | 13270         |
|    policy_gradient_loss | -0.00117      |
|    value_loss           | 2.28          |
-------------------------------------------
Progress update from timestep 531100: Updated shield, loss is 0.03906930238008499
Progress update from timestep 531200: Updated shield, loss is 0.04352928325533867
Eval num_timesteps=531200, episode_reward=24.61 +/- 14.12
Episode length: 34.80 +/- 19.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 24.6     |
| time/              |          |
|    total_timesteps | 531200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.25     |
|    ep_rew_mean     | 2.68     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1328     |
|    time_elapsed    | 119970   |
|    total_timesteps | 531200   |
---------------------------------
Progress update from timestep 531300: Updated shield, loss is 0.03431835398077965
Progress update from timestep 531400: Updated shield, loss is 0.034232884645462036
Eval num_timesteps=531400, episode_reward=15.56 +/- 13.10
Episode length: 22.60 +/- 17.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22.6         |
|    mean_reward          | 15.6         |
| time/                   |              |
|    total_timesteps      | 531400       |
| train/                  |              |
|    approx_kl            | 0.0001764823 |
|    clip_fraction        | 0.00335      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0149      |
|    explained_variance   | 0.384        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.783        |
|    n_updates            | 13280        |
|    policy_gradient_loss | -0.000649    |
|    value_loss           | 1.66         |
------------------------------------------
Progress update from timestep 531500: Updated shield, loss is 0.02987976185977459
Progress update from timestep 531600: Updated shield, loss is 0.03638371825218201
Eval num_timesteps=531600, episode_reward=18.41 +/- 13.34
Episode length: 27.40 +/- 19.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.4     |
|    mean_reward     | 18.4     |
| time/              |          |
|    total_timesteps | 531600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.82     |
|    ep_rew_mean     | 2.36     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1329     |
|    time_elapsed    | 120053   |
|    total_timesteps | 531600   |
---------------------------------
Progress update from timestep 531700: Updated shield, loss is 0.03925217688083649
Progress update from timestep 531800: Updated shield, loss is 0.029759302735328674
Eval num_timesteps=531800, episode_reward=22.95 +/- 15.06
Episode length: 33.00 +/- 20.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33            |
|    mean_reward          | 23            |
| time/                   |               |
|    total_timesteps      | 531800        |
| train/                  |               |
|    approx_kl            | 0.00027762444 |
|    clip_fraction        | 0.0152        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0304       |
|    explained_variance   | 0.206         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.93          |
|    n_updates            | 13290         |
|    policy_gradient_loss | -0.00118      |
|    value_loss           | 2.55          |
-------------------------------------------
Progress update from timestep 531900: Updated shield, loss is 0.031733203679323196
Progress update from timestep 532000: Updated shield, loss is 0.03852451220154762
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 532000: Saved shield_loss.csv. 
Eval num_timesteps=532000, episode_reward=9.45 +/- 12.06
Episode length: 14.60 +/- 17.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 9.45     |
| time/              |          |
|    total_timesteps | 532000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.85     |
|    ep_rew_mean     | 3.16     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1330     |
|    time_elapsed    | 120140   |
|    total_timesteps | 532000   |
---------------------------------
Progress update from timestep 532100: Updated shield, loss is 0.03656952455639839
Progress update from timestep 532200: Updated shield, loss is 0.03256077319383621
Eval num_timesteps=532200, episode_reward=12.26 +/- 12.00
Episode length: 18.00 +/- 16.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 18            |
|    mean_reward          | 12.3          |
| time/                   |               |
|    total_timesteps      | 532200        |
| train/                  |               |
|    approx_kl            | 0.00025237486 |
|    clip_fraction        | 0.0125        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0221       |
|    explained_variance   | 0.136         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.945         |
|    n_updates            | 13300         |
|    policy_gradient_loss | -0.00128      |
|    value_loss           | 2.22          |
-------------------------------------------
Progress update from timestep 532300: Updated shield, loss is 0.03531161695718765
Progress update from timestep 532400: Updated shield, loss is 0.033990923315286636
Eval num_timesteps=532400, episode_reward=7.08 +/- 2.81
Episode length: 11.20 +/- 3.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 11.2     |
|    mean_reward     | 7.08     |
| time/              |          |
|    total_timesteps | 532400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.15     |
|    ep_rew_mean     | 2.58     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1331     |
|    time_elapsed    | 120208   |
|    total_timesteps | 532400   |
---------------------------------
Progress update from timestep 532500: Updated shield, loss is 0.03214748576283455
Progress update from timestep 532600: Updated shield, loss is 0.0326148122549057
Eval num_timesteps=532600, episode_reward=5.53 +/- 2.81
Episode length: 8.80 +/- 4.02
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 8.8           |
|    mean_reward          | 5.53          |
| time/                   |               |
|    total_timesteps      | 532600        |
| train/                  |               |
|    approx_kl            | 0.00046999785 |
|    clip_fraction        | 0.00826       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0114       |
|    explained_variance   | 0.241         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.537         |
|    n_updates            | 13310         |
|    policy_gradient_loss | -0.000554     |
|    value_loss           | 1.42          |
-------------------------------------------
Progress update from timestep 532700: Updated shield, loss is 0.03367987647652626
Progress update from timestep 532800: Updated shield, loss is 0.031325563788414
Eval num_timesteps=532800, episode_reward=6.27 +/- 2.19
Episode length: 9.80 +/- 2.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.8      |
|    mean_reward     | 6.27     |
| time/              |          |
|    total_timesteps | 532800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.96     |
|    ep_rew_mean     | 2.5      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1332     |
|    time_elapsed    | 120267   |
|    total_timesteps | 532800   |
---------------------------------
Progress update from timestep 532900: Updated shield, loss is 0.03600342944264412
Progress update from timestep 533000: Updated shield, loss is 0.036725033074617386
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 533000: Saved shield_loss.csv. 
Eval num_timesteps=533000, episode_reward=29.51 +/- 11.10
Episode length: 42.20 +/- 15.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.2        |
|    mean_reward          | 29.5        |
| time/                   |             |
|    total_timesteps      | 533000      |
| train/                  |             |
|    approx_kl            | 0.000641741 |
|    clip_fraction        | 0.00379     |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0143     |
|    explained_variance   | 0.174       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.56        |
|    n_updates            | 13320       |
|    policy_gradient_loss | -0.00119    |
|    value_loss           | 1.61        |
-----------------------------------------
Progress update from timestep 533100: Updated shield, loss is 0.0332409031689167
Progress update from timestep 533200: Updated shield, loss is 0.03834834694862366
Eval num_timesteps=533200, episode_reward=3.41 +/- 2.63
Episode length: 5.80 +/- 3.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5.8      |
|    mean_reward     | 3.41     |
| time/              |          |
|    total_timesteps | 533200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.18     |
|    ep_rew_mean     | 2.69     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1333     |
|    time_elapsed    | 120352   |
|    total_timesteps | 533200   |
---------------------------------
Progress update from timestep 533300: Updated shield, loss is 0.03406677022576332
Progress update from timestep 533400: Updated shield, loss is 0.03689853101968765
Eval num_timesteps=533400, episode_reward=13.69 +/- 11.32
Episode length: 20.20 +/- 15.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 20.2         |
|    mean_reward          | 13.7         |
| time/                   |              |
|    total_timesteps      | 533400       |
| train/                  |              |
|    approx_kl            | 4.324495e-05 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00406     |
|    explained_variance   | 0.373        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.02         |
|    n_updates            | 13330        |
|    policy_gradient_loss | -0.000297    |
|    value_loss           | 2.17         |
------------------------------------------
Progress update from timestep 533500: Updated shield, loss is 0.03804231807589531
Progress update from timestep 533600: Updated shield, loss is 0.035043153911828995
Eval num_timesteps=533600, episode_reward=16.37 +/- 16.22
Episode length: 23.20 +/- 21.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 16.4     |
| time/              |          |
|    total_timesteps | 533600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.2      |
|    ep_rew_mean     | 2.61     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1334     |
|    time_elapsed    | 120434   |
|    total_timesteps | 533600   |
---------------------------------
Progress update from timestep 533700: Updated shield, loss is 0.04316770285367966
Progress update from timestep 533800: Updated shield, loss is 0.032361019402742386
Eval num_timesteps=533800, episode_reward=4.42 +/- 1.26
Episode length: 7.40 +/- 1.85
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 7.4           |
|    mean_reward          | 4.42          |
| time/                   |               |
|    total_timesteps      | 533800        |
| train/                  |               |
|    approx_kl            | 0.00019874783 |
|    clip_fraction        | 0.00603       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0139       |
|    explained_variance   | 0.214         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.913         |
|    n_updates            | 13340         |
|    policy_gradient_loss | -0.000668     |
|    value_loss           | 1.89          |
-------------------------------------------
Progress update from timestep 533900: Updated shield, loss is 0.03336135670542717
Progress update from timestep 534000: Updated shield, loss is 0.04210400953888893
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 534000: Saved shield_loss.csv. 
Eval num_timesteps=534000, episode_reward=4.55 +/- 1.92
Episode length: 7.60 +/- 2.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7.6      |
|    mean_reward     | 4.55     |
| time/              |          |
|    total_timesteps | 534000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.76     |
|    ep_rew_mean     | 2.32     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1335     |
|    time_elapsed    | 120489   |
|    total_timesteps | 534000   |
---------------------------------
Progress update from timestep 534100: Updated shield, loss is 0.03879975900053978
Progress update from timestep 534200: Updated shield, loss is 0.0428662970662117
Eval num_timesteps=534200, episode_reward=17.72 +/- 14.73
Episode length: 25.40 +/- 20.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.4          |
|    mean_reward          | 17.7          |
| time/                   |               |
|    total_timesteps      | 534200        |
| train/                  |               |
|    approx_kl            | 3.1853082e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00191      |
|    explained_variance   | 0.253         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.264         |
|    n_updates            | 13350         |
|    policy_gradient_loss | -1.25e-05     |
|    value_loss           | 1.27          |
-------------------------------------------
Progress update from timestep 534300: Updated shield, loss is 0.033279381692409515
Progress update from timestep 534400: Updated shield, loss is 0.033349018543958664
Eval num_timesteps=534400, episode_reward=13.32 +/- 11.65
Episode length: 20.00 +/- 17.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20       |
|    mean_reward     | 13.3     |
| time/              |          |
|    total_timesteps | 534400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.33     |
|    ep_rew_mean     | 2.7      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1336     |
|    time_elapsed    | 120572   |
|    total_timesteps | 534400   |
---------------------------------
Progress update from timestep 534500: Updated shield, loss is 0.03553091734647751
Progress update from timestep 534600: Updated shield, loss is 0.05132678523659706
Eval num_timesteps=534600, episode_reward=6.19 +/- 2.05
Episode length: 9.40 +/- 2.73
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 9.4           |
|    mean_reward          | 6.19          |
| time/                   |               |
|    total_timesteps      | 534600        |
| train/                  |               |
|    approx_kl            | 0.00011538997 |
|    clip_fraction        | 0.00513       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0135       |
|    explained_variance   | 0.21          |
|    learning_rate        | 0.0001        |
|    loss                 | 1.01          |
|    n_updates            | 13360         |
|    policy_gradient_loss | -0.000457     |
|    value_loss           | 1.98          |
-------------------------------------------
Progress update from timestep 534700: Updated shield, loss is 0.04450332745909691
Progress update from timestep 534800: Updated shield, loss is 0.04278814047574997
Eval num_timesteps=534800, episode_reward=3.96 +/- 2.97
Episode length: 6.80 +/- 4.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6.8      |
|    mean_reward     | 3.96     |
| time/              |          |
|    total_timesteps | 534800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.11     |
|    ep_rew_mean     | 2.53     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1337     |
|    time_elapsed    | 120629   |
|    total_timesteps | 534800   |
---------------------------------
Progress update from timestep 534900: Updated shield, loss is 0.029346473515033722
Progress update from timestep 535000: Updated shield, loss is 0.03163764625787735
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 535000: Saved shield_loss.csv. 
Eval num_timesteps=535000, episode_reward=4.95 +/- 3.38
Episode length: 8.00 +/- 4.98
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 8             |
|    mean_reward          | 4.95          |
| time/                   |               |
|    total_timesteps      | 535000        |
| train/                  |               |
|    approx_kl            | 1.0991893e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.000382     |
|    explained_variance   | 0.248         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.423         |
|    n_updates            | 13370         |
|    policy_gradient_loss | -3.2e-08      |
|    value_loss           | 1.45          |
-------------------------------------------
Progress update from timestep 535100: Updated shield, loss is 0.03599750995635986
Progress update from timestep 535200: Updated shield, loss is 0.03718359395861626
Eval num_timesteps=535200, episode_reward=18.54 +/- 14.93
Episode length: 26.20 +/- 19.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | 18.5     |
| time/              |          |
|    total_timesteps | 535200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.38     |
|    ep_rew_mean     | 2.74     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1338     |
|    time_elapsed    | 120696   |
|    total_timesteps | 535200   |
---------------------------------
Progress update from timestep 535300: Updated shield, loss is 0.03734663128852844
Progress update from timestep 535400: Updated shield, loss is 0.03456830978393555
Eval num_timesteps=535400, episode_reward=22.34 +/- 15.77
Episode length: 32.20 +/- 21.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32.2          |
|    mean_reward          | 22.3          |
| time/                   |               |
|    total_timesteps      | 535400        |
| train/                  |               |
|    approx_kl            | -4.186274e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.000387     |
|    explained_variance   | 0.179         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.926         |
|    n_updates            | 13380         |
|    policy_gradient_loss | -6.05e-08     |
|    value_loss           | 1.66          |
-------------------------------------------
Progress update from timestep 535500: Updated shield, loss is 0.032723989337682724
Progress update from timestep 535600: Updated shield, loss is 0.04294267296791077
Eval num_timesteps=535600, episode_reward=15.15 +/- 16.28
Episode length: 22.20 +/- 22.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 15.1     |
| time/              |          |
|    total_timesteps | 535600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.2      |
|    ep_rew_mean     | 2.65     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1339     |
|    time_elapsed    | 120797   |
|    total_timesteps | 535600   |
---------------------------------
Progress update from timestep 535700: Updated shield, loss is 0.040494002401828766
Progress update from timestep 535800: Updated shield, loss is 0.038607049733400345
Eval num_timesteps=535800, episode_reward=15.80 +/- 16.19
Episode length: 22.80 +/- 22.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.8          |
|    mean_reward          | 15.8          |
| time/                   |               |
|    total_timesteps      | 535800        |
| train/                  |               |
|    approx_kl            | 3.3052514e-05 |
|    clip_fraction        | 0.00134       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00699      |
|    explained_variance   | 0.171         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.12          |
|    n_updates            | 13390         |
|    policy_gradient_loss | -8.02e-05     |
|    value_loss           | 1.69          |
-------------------------------------------
Progress update from timestep 535900: Updated shield, loss is 0.028680158779025078
Progress update from timestep 536000: Updated shield, loss is 0.036792293190956116
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 536000: Saved shield_loss.csv. 
Eval num_timesteps=536000, episode_reward=18.46 +/- 14.46
Episode length: 26.60 +/- 19.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.6     |
|    mean_reward     | 18.5     |
| time/              |          |
|    total_timesteps | 536000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.31     |
|    ep_rew_mean     | 2.75     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1340     |
|    time_elapsed    | 120879   |
|    total_timesteps | 536000   |
---------------------------------
Progress update from timestep 536100: Updated shield, loss is 0.03132450580596924
Progress update from timestep 536200: Updated shield, loss is 0.028642568737268448
Eval num_timesteps=536200, episode_reward=11.75 +/- 11.27
Episode length: 18.00 +/- 16.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 18            |
|    mean_reward          | 11.7          |
| time/                   |               |
|    total_timesteps      | 536200        |
| train/                  |               |
|    approx_kl            | 8.9166024e-05 |
|    clip_fraction        | 0.00737       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0138       |
|    explained_variance   | 0.256         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.494         |
|    n_updates            | 13400         |
|    policy_gradient_loss | -0.00123      |
|    value_loss           | 1.89          |
-------------------------------------------
Progress update from timestep 536300: Updated shield, loss is 0.037679754197597504
Progress update from timestep 536400: Updated shield, loss is 0.03610045462846756
Eval num_timesteps=536400, episode_reward=18.78 +/- 14.09
Episode length: 27.00 +/- 19.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27       |
|    mean_reward     | 18.8     |
| time/              |          |
|    total_timesteps | 536400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.12     |
|    ep_rew_mean     | 2.54     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1341     |
|    time_elapsed    | 120962   |
|    total_timesteps | 536400   |
---------------------------------
Progress update from timestep 536500: Updated shield, loss is 0.037221167236566544
Progress update from timestep 536600: Updated shield, loss is 0.032392874360084534
Eval num_timesteps=536600, episode_reward=13.47 +/- 12.13
Episode length: 19.80 +/- 17.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 19.8          |
|    mean_reward          | 13.5          |
| time/                   |               |
|    total_timesteps      | 536600        |
| train/                  |               |
|    approx_kl            | 0.00022211681 |
|    clip_fraction        | 0.00357       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0175       |
|    explained_variance   | 0.333         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.559         |
|    n_updates            | 13410         |
|    policy_gradient_loss | -0.000585     |
|    value_loss           | 2.15          |
-------------------------------------------
Progress update from timestep 536700: Updated shield, loss is 0.03158685192465782
Progress update from timestep 536800: Updated shield, loss is 0.030708635225892067
Eval num_timesteps=536800, episode_reward=29.00 +/- 12.11
Episode length: 41.60 +/- 16.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 29       |
| time/              |          |
|    total_timesteps | 536800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.04     |
|    ep_rew_mean     | 2.49     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1342     |
|    time_elapsed    | 121062   |
|    total_timesteps | 536800   |
---------------------------------
Progress update from timestep 536900: Updated shield, loss is 0.03552849963307381
Progress update from timestep 537000: Updated shield, loss is 0.03020688146352768
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 537000: Saved shield_loss.csv. 
Eval num_timesteps=537000, episode_reward=12.09 +/- 13.30
Episode length: 17.40 +/- 17.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17.4          |
|    mean_reward          | 12.1          |
| time/                   |               |
|    total_timesteps      | 537000        |
| train/                  |               |
|    approx_kl            | 0.00010238407 |
|    clip_fraction        | 0.00603       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0163       |
|    explained_variance   | 0.198         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.547         |
|    n_updates            | 13420         |
|    policy_gradient_loss | -0.000554     |
|    value_loss           | 1.74          |
-------------------------------------------
Progress update from timestep 537100: Updated shield, loss is 0.033741943538188934
Progress update from timestep 537200: Updated shield, loss is 0.04588833823800087
Eval num_timesteps=537200, episode_reward=26.48 +/- 11.90
Episode length: 37.20 +/- 15.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.2     |
|    mean_reward     | 26.5     |
| time/              |          |
|    total_timesteps | 537200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.49     |
|    ep_rew_mean     | 2.07     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1343     |
|    time_elapsed    | 121148   |
|    total_timesteps | 537200   |
---------------------------------
Progress update from timestep 537300: Updated shield, loss is 0.03360924497246742
Progress update from timestep 537400: Updated shield, loss is 0.036993835121393204
Eval num_timesteps=537400, episode_reward=21.25 +/- 16.25
Episode length: 31.20 +/- 23.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 31.2          |
|    mean_reward          | 21.2          |
| time/                   |               |
|    total_timesteps      | 537400        |
| train/                  |               |
|    approx_kl            | 0.00010685913 |
|    clip_fraction        | 0.00759       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00986      |
|    explained_variance   | 0.0899        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.224         |
|    n_updates            | 13430         |
|    policy_gradient_loss | -0.000616     |
|    value_loss           | 1.12          |
-------------------------------------------
Progress update from timestep 537500: Updated shield, loss is 0.05072221904993057
Progress update from timestep 537600: Updated shield, loss is 0.04125514626502991
Eval num_timesteps=537600, episode_reward=23.16 +/- 14.41
Episode length: 33.60 +/- 20.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 23.2     |
| time/              |          |
|    total_timesteps | 537600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.14     |
|    ep_rew_mean     | 2.6      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1344     |
|    time_elapsed    | 121232   |
|    total_timesteps | 537600   |
---------------------------------
Progress update from timestep 537700: Updated shield, loss is 0.03432334586977959
Progress update from timestep 537800: Updated shield, loss is 0.0334649421274662
Eval num_timesteps=537800, episode_reward=4.56 +/- 2.06
Episode length: 7.40 +/- 2.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 7.4          |
|    mean_reward          | 4.56         |
| time/                   |              |
|    total_timesteps      | 537800       |
| train/                  |              |
|    approx_kl            | 6.382227e-05 |
|    clip_fraction        | 0.00424      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00924     |
|    explained_variance   | 0.207        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.05         |
|    n_updates            | 13440        |
|    policy_gradient_loss | -0.000726    |
|    value_loss           | 1.7          |
------------------------------------------
Progress update from timestep 537900: Updated shield, loss is 0.027291033416986465
Progress update from timestep 538000: Updated shield, loss is 0.030541613698005676
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 538000: Saved shield_loss.csv. 
Eval num_timesteps=538000, episode_reward=23.98 +/- 13.27
Episode length: 35.20 +/- 18.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 24       |
| time/              |          |
|    total_timesteps | 538000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.79     |
|    ep_rew_mean     | 2.37     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1345     |
|    time_elapsed    | 121321   |
|    total_timesteps | 538000   |
---------------------------------
Progress update from timestep 538100: Updated shield, loss is 0.03481558337807655
Progress update from timestep 538200: Updated shield, loss is 0.037054020911455154
Eval num_timesteps=538200, episode_reward=22.58 +/- 14.80
Episode length: 33.20 +/- 20.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.2         |
|    mean_reward          | 22.6         |
| time/                   |              |
|    total_timesteps      | 538200       |
| train/                  |              |
|    approx_kl            | 0.0002016065 |
|    clip_fraction        | 0.0058       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00894     |
|    explained_variance   | 0.245        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.937        |
|    n_updates            | 13450        |
|    policy_gradient_loss | -0.000369    |
|    value_loss           | 1.14         |
------------------------------------------
Progress update from timestep 538300: Updated shield, loss is 0.03317912295460701
Progress update from timestep 538400: Updated shield, loss is 0.0294808279722929
Eval num_timesteps=538400, episode_reward=22.20 +/- 15.54
Episode length: 32.20 +/- 21.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 22.2     |
| time/              |          |
|    total_timesteps | 538400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.86     |
|    ep_rew_mean     | 2.43     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1346     |
|    time_elapsed    | 121426   |
|    total_timesteps | 538400   |
---------------------------------
Progress update from timestep 538500: Updated shield, loss is 0.034155216068029404
Progress update from timestep 538600: Updated shield, loss is 0.042170312255620956
Eval num_timesteps=538600, episode_reward=15.78 +/- 15.73
Episode length: 23.20 +/- 21.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.2         |
|    mean_reward          | 15.8         |
| time/                   |              |
|    total_timesteps      | 538600       |
| train/                  |              |
|    approx_kl            | 0.0003890555 |
|    clip_fraction        | 0.00781      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0186      |
|    explained_variance   | 0.262        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.763        |
|    n_updates            | 13460        |
|    policy_gradient_loss | -0.00106     |
|    value_loss           | 1.92         |
------------------------------------------
Progress update from timestep 538700: Updated shield, loss is 0.03423019126057625
Progress update from timestep 538800: Updated shield, loss is 0.03431158512830734
Eval num_timesteps=538800, episode_reward=35.14 +/- 1.33
Episode length: 50.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 35.1     |
| time/              |          |
|    total_timesteps | 538800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.43     |
|    ep_rew_mean     | 2.06     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1347     |
|    time_elapsed    | 121529   |
|    total_timesteps | 538800   |
---------------------------------
Progress update from timestep 538900: Updated shield, loss is 0.037738166749477386
Progress update from timestep 539000: Updated shield, loss is 0.03794625401496887
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 539000: Saved shield_loss.csv. 
Eval num_timesteps=539000, episode_reward=17.05 +/- 15.61
Episode length: 24.20 +/- 21.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.2         |
|    mean_reward          | 17           |
| time/                   |              |
|    total_timesteps      | 539000       |
| train/                  |              |
|    approx_kl            | 0.0013117546 |
|    clip_fraction        | 0.017        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0211      |
|    explained_variance   | 0.114        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.95         |
|    n_updates            | 13470        |
|    policy_gradient_loss | -0.00169     |
|    value_loss           | 1.27         |
------------------------------------------
Progress update from timestep 539100: Updated shield, loss is 0.04229443892836571
Progress update from timestep 539200: Updated shield, loss is 0.044187095016241074
Eval num_timesteps=539200, episode_reward=11.73 +/- 10.98
Episode length: 17.80 +/- 16.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | 11.7     |
| time/              |          |
|    total_timesteps | 539200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.15     |
|    ep_rew_mean     | 2.62     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1348     |
|    time_elapsed    | 121613   |
|    total_timesteps | 539200   |
---------------------------------
Progress update from timestep 539300: Updated shield, loss is 0.03209869936108589
Progress update from timestep 539400: Updated shield, loss is 0.03690977022051811
Eval num_timesteps=539400, episode_reward=5.19 +/- 1.85
Episode length: 8.40 +/- 2.73
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 8.4           |
|    mean_reward          | 5.19          |
| time/                   |               |
|    total_timesteps      | 539400        |
| train/                  |               |
|    approx_kl            | 0.00061737315 |
|    clip_fraction        | 0.0156        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0251       |
|    explained_variance   | 0.19          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.649         |
|    n_updates            | 13480         |
|    policy_gradient_loss | -0.00121      |
|    value_loss           | 1.76          |
-------------------------------------------
Progress update from timestep 539500: Updated shield, loss is 0.03716390207409859
Progress update from timestep 539600: Updated shield, loss is 0.036433976143598557
Eval num_timesteps=539600, episode_reward=5.77 +/- 4.07
Episode length: 9.40 +/- 6.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.4      |
|    mean_reward     | 5.77     |
| time/              |          |
|    total_timesteps | 539600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.05     |
|    ep_rew_mean     | 2.5      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1349     |
|    time_elapsed    | 121669   |
|    total_timesteps | 539600   |
---------------------------------
Progress update from timestep 539700: Updated shield, loss is 0.028188008815050125
Progress update from timestep 539800: Updated shield, loss is 0.03298695385456085
Eval num_timesteps=539800, episode_reward=22.88 +/- 14.27
Episode length: 33.40 +/- 20.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.4        |
|    mean_reward          | 22.9        |
| time/                   |             |
|    total_timesteps      | 539800      |
| train/                  |             |
|    approx_kl            | 0.000104077 |
|    clip_fraction        | 0.00625     |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0123     |
|    explained_variance   | 0.318       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.4         |
|    n_updates            | 13490       |
|    policy_gradient_loss | -0.00024    |
|    value_loss           | 1.71        |
-----------------------------------------
Progress update from timestep 539900: Updated shield, loss is 0.03359192982316017
Progress update from timestep 540000: Updated shield, loss is 0.030217589810490608
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 540000: Saved shield_loss.csv. 
Eval num_timesteps=540000, episode_reward=28.78 +/- 13.63
Episode length: 40.60 +/- 18.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.6     |
|    mean_reward     | 28.8     |
| time/              |          |
|    total_timesteps | 540000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.66     |
|    ep_rew_mean     | 2.21     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1350     |
|    time_elapsed    | 121772   |
|    total_timesteps | 540000   |
---------------------------------
Progress update from timestep 540100: Updated shield, loss is 0.031763214617967606
Progress update from timestep 540200: Updated shield, loss is 0.04083285853266716
Eval num_timesteps=540200, episode_reward=24.29 +/- 13.54
Episode length: 34.80 +/- 18.79
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.8          |
|    mean_reward          | 24.3          |
| time/                   |               |
|    total_timesteps      | 540200        |
| train/                  |               |
|    approx_kl            | 0.00065369107 |
|    clip_fraction        | 0.0174        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0331       |
|    explained_variance   | 0.317         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.341         |
|    n_updates            | 13500         |
|    policy_gradient_loss | -0.00217      |
|    value_loss           | 1.1           |
-------------------------------------------
Progress update from timestep 540300: Updated shield, loss is 0.03725587576627731
Progress update from timestep 540400: Updated shield, loss is 0.03993203490972519
Eval num_timesteps=540400, episode_reward=10.16 +/- 11.81
Episode length: 15.60 +/- 17.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 10.2     |
| time/              |          |
|    total_timesteps | 540400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.5      |
|    ep_rew_mean     | 2.88     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1351     |
|    time_elapsed    | 121861   |
|    total_timesteps | 540400   |
---------------------------------
Progress update from timestep 540500: Updated shield, loss is 0.02930448018014431
Progress update from timestep 540600: Updated shield, loss is 0.02869599498808384
Eval num_timesteps=540600, episode_reward=14.96 +/- 15.03
Episode length: 22.80 +/- 22.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22.8         |
|    mean_reward          | 15           |
| time/                   |              |
|    total_timesteps      | 540600       |
| train/                  |              |
|    approx_kl            | 0.0007363089 |
|    clip_fraction        | 0.0141       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0253      |
|    explained_variance   | 0.268        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.305        |
|    n_updates            | 13510        |
|    policy_gradient_loss | -0.00114     |
|    value_loss           | 1.48         |
------------------------------------------
Progress update from timestep 540700: Updated shield, loss is 0.03557471185922623
Progress update from timestep 540800: Updated shield, loss is 0.03391135483980179
Eval num_timesteps=540800, episode_reward=11.31 +/- 12.37
Episode length: 16.80 +/- 16.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | 11.3     |
| time/              |          |
|    total_timesteps | 540800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.62     |
|    ep_rew_mean     | 2.19     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1352     |
|    time_elapsed    | 121944   |
|    total_timesteps | 540800   |
---------------------------------
Progress update from timestep 540900: Updated shield, loss is 0.028650963678956032
Progress update from timestep 541000: Updated shield, loss is 0.030180439352989197
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 541000: Saved shield_loss.csv. 
Eval num_timesteps=541000, episode_reward=16.04 +/- 15.62
Episode length: 23.60 +/- 21.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.6         |
|    mean_reward          | 16           |
| time/                   |              |
|    total_timesteps      | 541000       |
| train/                  |              |
|    approx_kl            | 0.0014719352 |
|    clip_fraction        | 0.0205       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0323      |
|    explained_variance   | 0.209        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.72         |
|    n_updates            | 13520        |
|    policy_gradient_loss | -0.00177     |
|    value_loss           | 1.38         |
------------------------------------------
Progress update from timestep 541100: Updated shield, loss is 0.04250514507293701
Progress update from timestep 541200: Updated shield, loss is 0.03237947076559067
Eval num_timesteps=541200, episode_reward=10.29 +/- 12.33
Episode length: 15.60 +/- 17.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 10.3     |
| time/              |          |
|    total_timesteps | 541200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.12     |
|    ep_rew_mean     | 2.6      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1353     |
|    time_elapsed    | 122029   |
|    total_timesteps | 541200   |
---------------------------------
Progress update from timestep 541300: Updated shield, loss is 0.044154226779937744
Progress update from timestep 541400: Updated shield, loss is 0.03697996214032173
Eval num_timesteps=541400, episode_reward=10.91 +/- 12.77
Episode length: 16.40 +/- 17.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | 10.9         |
| time/                   |              |
|    total_timesteps      | 541400       |
| train/                  |              |
|    approx_kl            | 0.0004174418 |
|    clip_fraction        | 0.00937      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0217      |
|    explained_variance   | 0.109        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.359        |
|    n_updates            | 13530        |
|    policy_gradient_loss | -0.000896    |
|    value_loss           | 1.46         |
------------------------------------------
Progress update from timestep 541500: Updated shield, loss is 0.03148288652300835
Progress update from timestep 541600: Updated shield, loss is 0.03908408060669899
Eval num_timesteps=541600, episode_reward=8.96 +/- 13.52
Episode length: 13.40 +/- 18.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.4     |
|    mean_reward     | 8.96     |
| time/              |          |
|    total_timesteps | 541600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.07     |
|    ep_rew_mean     | 2.49     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1354     |
|    time_elapsed    | 122116   |
|    total_timesteps | 541600   |
---------------------------------
Progress update from timestep 541700: Updated shield, loss is 0.04005172476172447
Progress update from timestep 541800: Updated shield, loss is 0.041142430156469345
Eval num_timesteps=541800, episode_reward=23.47 +/- 15.75
Episode length: 32.80 +/- 21.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32.8          |
|    mean_reward          | 23.5          |
| time/                   |               |
|    total_timesteps      | 541800        |
| train/                  |               |
|    approx_kl            | 5.6206678e-05 |
|    clip_fraction        | 0.000893      |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00618      |
|    explained_variance   | 0.257         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.732         |
|    n_updates            | 13540         |
|    policy_gradient_loss | -0.000194     |
|    value_loss           | 1.15          |
-------------------------------------------
Progress update from timestep 541900: Updated shield, loss is 0.0339416079223156
Progress update from timestep 542000: Updated shield, loss is 0.027398578822612762
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 542000: Saved shield_loss.csv. 
Eval num_timesteps=542000, episode_reward=20.17 +/- 12.09
Episode length: 30.00 +/- 16.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 20.2     |
| time/              |          |
|    total_timesteps | 542000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.04     |
|    ep_rew_mean     | 2.49     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1355     |
|    time_elapsed    | 122208   |
|    total_timesteps | 542000   |
---------------------------------
Progress update from timestep 542100: Updated shield, loss is 0.03466916084289551
Progress update from timestep 542200: Updated shield, loss is 0.04117332398891449
Eval num_timesteps=542200, episode_reward=17.80 +/- 14.88
Episode length: 25.80 +/- 20.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.8         |
|    mean_reward          | 17.8         |
| time/                   |              |
|    total_timesteps      | 542200       |
| train/                  |              |
|    approx_kl            | 6.548004e-05 |
|    clip_fraction        | 0.00223      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0122      |
|    explained_variance   | 0.096        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.638        |
|    n_updates            | 13550        |
|    policy_gradient_loss | -0.000635    |
|    value_loss           | 1.44         |
------------------------------------------
Progress update from timestep 542300: Updated shield, loss is 0.03643201291561127
Progress update from timestep 542400: Updated shield, loss is 0.039068546146154404
Eval num_timesteps=542400, episode_reward=9.66 +/- 13.02
Episode length: 14.20 +/- 17.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.2     |
|    mean_reward     | 9.66     |
| time/              |          |
|    total_timesteps | 542400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.95     |
|    ep_rew_mean     | 2.4      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1356     |
|    time_elapsed    | 122290   |
|    total_timesteps | 542400   |
---------------------------------
Progress update from timestep 542500: Updated shield, loss is 0.03853893280029297
Progress update from timestep 542600: Updated shield, loss is 0.034058257937431335
Eval num_timesteps=542600, episode_reward=22.33 +/- 15.41
Episode length: 32.40 +/- 21.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.4         |
|    mean_reward          | 22.3         |
| time/                   |              |
|    total_timesteps      | 542600       |
| train/                  |              |
|    approx_kl            | 7.860633e-12 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.000304    |
|    explained_variance   | 0.242        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.24         |
|    n_updates            | 13560        |
|    policy_gradient_loss | 7.33e-09     |
|    value_loss           | 1.45         |
------------------------------------------
Progress update from timestep 542700: Updated shield, loss is 0.02897464856505394
Progress update from timestep 542800: Updated shield, loss is 0.03631828352808952
Eval num_timesteps=542800, episode_reward=17.89 +/- 14.12
Episode length: 26.20 +/- 19.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 542800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.31     |
|    ep_rew_mean     | 2.66     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1357     |
|    time_elapsed    | 122372   |
|    total_timesteps | 542800   |
---------------------------------
Progress update from timestep 542900: Updated shield, loss is 0.030314596369862556
Progress update from timestep 543000: Updated shield, loss is 0.03023119643330574
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 543000: Saved shield_loss.csv. 
Eval num_timesteps=543000, episode_reward=17.75 +/- 14.27
Episode length: 25.80 +/- 20.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.8          |
|    mean_reward          | 17.7          |
| time/                   |               |
|    total_timesteps      | 543000        |
| train/                  |               |
|    approx_kl            | 5.8554295e-05 |
|    clip_fraction        | 0.000893      |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00546      |
|    explained_variance   | 0.237         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.44          |
|    n_updates            | 13570         |
|    policy_gradient_loss | -6.5e-05      |
|    value_loss           | 1.38          |
-------------------------------------------
Progress update from timestep 543100: Updated shield, loss is 0.03974556922912598
Progress update from timestep 543200: Updated shield, loss is 0.034477271139621735
Eval num_timesteps=543200, episode_reward=16.62 +/- 16.45
Episode length: 23.40 +/- 21.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 543200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.45     |
|    ep_rew_mean     | 2.81     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1358     |
|    time_elapsed    | 122462   |
|    total_timesteps | 543200   |
---------------------------------
Progress update from timestep 543300: Updated shield, loss is 0.0371122732758522
Progress update from timestep 543400: Updated shield, loss is 0.02721579372882843
Eval num_timesteps=543400, episode_reward=19.79 +/- 13.05
Episode length: 28.40 +/- 17.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 28.4          |
|    mean_reward          | 19.8          |
| time/                   |               |
|    total_timesteps      | 543400        |
| train/                  |               |
|    approx_kl            | -9.263853e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.000462     |
|    explained_variance   | 0.223         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.484         |
|    n_updates            | 13580         |
|    policy_gradient_loss | 3.14e-08      |
|    value_loss           | 1.64          |
-------------------------------------------
Progress update from timestep 543500: Updated shield, loss is 0.041557326912879944
Progress update from timestep 543600: Updated shield, loss is 0.03506430983543396
Eval num_timesteps=543600, episode_reward=5.37 +/- 2.96
Episode length: 8.60 +/- 4.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8.6      |
|    mean_reward     | 5.37     |
| time/              |          |
|    total_timesteps | 543600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.41     |
|    ep_rew_mean     | 2.78     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1359     |
|    time_elapsed    | 122535   |
|    total_timesteps | 543600   |
---------------------------------
Progress update from timestep 543700: Updated shield, loss is 0.03670458495616913
Progress update from timestep 543800: Updated shield, loss is 0.031912487000226974
Eval num_timesteps=543800, episode_reward=16.42 +/- 15.20
Episode length: 24.00 +/- 21.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24           |
|    mean_reward          | 16.4         |
| time/                   |              |
|    total_timesteps      | 543800       |
| train/                  |              |
|    approx_kl            | 9.974948e-05 |
|    clip_fraction        | 0.00335      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0112      |
|    explained_variance   | 0.175        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.769        |
|    n_updates            | 13590        |
|    policy_gradient_loss | -0.000397    |
|    value_loss           | 2.25         |
------------------------------------------
Progress update from timestep 543900: Updated shield, loss is 0.0313759371638298
Progress update from timestep 544000: Updated shield, loss is 0.03380707651376724
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 544000: Saved shield_loss.csv. 
Eval num_timesteps=544000, episode_reward=10.68 +/- 12.18
Episode length: 16.40 +/- 17.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | 10.7     |
| time/              |          |
|    total_timesteps | 544000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.79     |
|    ep_rew_mean     | 2.31     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1360     |
|    time_elapsed    | 122620   |
|    total_timesteps | 544000   |
---------------------------------
Progress update from timestep 544100: Updated shield, loss is 0.030158651992678642
Progress update from timestep 544200: Updated shield, loss is 0.03654089942574501
Eval num_timesteps=544200, episode_reward=23.78 +/- 13.59
Episode length: 34.40 +/- 19.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.4          |
|    mean_reward          | 23.8          |
| time/                   |               |
|    total_timesteps      | 544200        |
| train/                  |               |
|    approx_kl            | 5.6310717e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.000305     |
|    explained_variance   | 0.24          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.58          |
|    n_updates            | 13600         |
|    policy_gradient_loss | -2.05e-07     |
|    value_loss           | 1.39          |
-------------------------------------------
Progress update from timestep 544300: Updated shield, loss is 0.029698790982365608
Progress update from timestep 544400: Updated shield, loss is 0.02579432912170887
Eval num_timesteps=544400, episode_reward=15.84 +/- 16.23
Episode length: 22.80 +/- 22.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 15.8     |
| time/              |          |
|    total_timesteps | 544400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.04     |
|    ep_rew_mean     | 2.57     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1361     |
|    time_elapsed    | 122706   |
|    total_timesteps | 544400   |
---------------------------------
Progress update from timestep 544500: Updated shield, loss is 0.03115661069750786
Progress update from timestep 544600: Updated shield, loss is 0.039660293608903885
Eval num_timesteps=544600, episode_reward=19.07 +/- 13.26
Episode length: 28.00 +/- 18.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 28            |
|    mean_reward          | 19.1          |
| time/                   |               |
|    total_timesteps      | 544600        |
| train/                  |               |
|    approx_kl            | 0.00019992252 |
|    clip_fraction        | 0.0129        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0146       |
|    explained_variance   | 0.157         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.837         |
|    n_updates            | 13610         |
|    policy_gradient_loss | -0.000833     |
|    value_loss           | 1.48          |
-------------------------------------------
Progress update from timestep 544700: Updated shield, loss is 0.031731247901916504
Progress update from timestep 544800: Updated shield, loss is 0.038640569895505905
Eval num_timesteps=544800, episode_reward=17.33 +/- 14.57
Episode length: 25.20 +/- 20.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 17.3     |
| time/              |          |
|    total_timesteps | 544800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.61     |
|    ep_rew_mean     | 3.03     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1362     |
|    time_elapsed    | 122794   |
|    total_timesteps | 544800   |
---------------------------------
Progress update from timestep 544900: Updated shield, loss is 0.02909729816019535
Progress update from timestep 545000: Updated shield, loss is 0.03101983107626438
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 545000: Saved shield_loss.csv. 
Eval num_timesteps=545000, episode_reward=22.58 +/- 15.49
Episode length: 32.40 +/- 21.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.4         |
|    mean_reward          | 22.6         |
| time/                   |              |
|    total_timesteps      | 545000       |
| train/                  |              |
|    approx_kl            | 8.047661e-05 |
|    clip_fraction        | 0.00179      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00491     |
|    explained_variance   | 0.176        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.26         |
|    n_updates            | 13620        |
|    policy_gradient_loss | -0.000184    |
|    value_loss           | 2            |
------------------------------------------
Progress update from timestep 545100: Updated shield, loss is 0.03860394284129143
Progress update from timestep 545200: Updated shield, loss is 0.03760143369436264
Eval num_timesteps=545200, episode_reward=18.17 +/- 14.82
Episode length: 26.00 +/- 19.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | 18.2     |
| time/              |          |
|    total_timesteps | 545200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.41     |
|    ep_rew_mean     | 2.86     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1363     |
|    time_elapsed    | 122876   |
|    total_timesteps | 545200   |
---------------------------------
Progress update from timestep 545300: Updated shield, loss is 0.0339447520673275
Progress update from timestep 545400: Updated shield, loss is 0.032008230686187744
Eval num_timesteps=545400, episode_reward=11.78 +/- 11.48
Episode length: 17.40 +/- 16.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.4         |
|    mean_reward          | 11.8         |
| time/                   |              |
|    total_timesteps      | 545400       |
| train/                  |              |
|    approx_kl            | 5.248926e-06 |
|    clip_fraction        | 0.000223     |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0046      |
|    explained_variance   | 0.376        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.758        |
|    n_updates            | 13630        |
|    policy_gradient_loss | -6.58e-05    |
|    value_loss           | 1.68         |
------------------------------------------
Progress update from timestep 545500: Updated shield, loss is 0.032625146210193634
Progress update from timestep 545600: Updated shield, loss is 0.02759886346757412
Eval num_timesteps=545600, episode_reward=4.96 +/- 3.22
Episode length: 7.80 +/- 4.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7.8      |
|    mean_reward     | 4.96     |
| time/              |          |
|    total_timesteps | 545600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.05     |
|    ep_rew_mean     | 2.52     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1364     |
|    time_elapsed    | 122945   |
|    total_timesteps | 545600   |
---------------------------------
Progress update from timestep 545700: Updated shield, loss is 0.03411087766289711
Progress update from timestep 545800: Updated shield, loss is 0.03264104202389717
Eval num_timesteps=545800, episode_reward=11.53 +/- 12.19
Episode length: 17.00 +/- 16.73
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17            |
|    mean_reward          | 11.5          |
| time/                   |               |
|    total_timesteps      | 545800        |
| train/                  |               |
|    approx_kl            | 1.2809361e-05 |
|    clip_fraction        | 0.000446      |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00422      |
|    explained_variance   | 0.183         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.737         |
|    n_updates            | 13640         |
|    policy_gradient_loss | -6.02e-05     |
|    value_loss           | 1.48          |
-------------------------------------------
Progress update from timestep 545900: Updated shield, loss is 0.02653319388628006
Progress update from timestep 546000: Updated shield, loss is 0.03910032659769058
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 546000: Saved shield_loss.csv. 
Eval num_timesteps=546000, episode_reward=20.96 +/- 16.56
Episode length: 30.80 +/- 23.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.8     |
|    mean_reward     | 21       |
| time/              |          |
|    total_timesteps | 546000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.69     |
|    ep_rew_mean     | 2.27     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1365     |
|    time_elapsed    | 123028   |
|    total_timesteps | 546000   |
---------------------------------
Progress update from timestep 546100: Updated shield, loss is 0.03812474384903908
Progress update from timestep 546200: Updated shield, loss is 0.0287480391561985
Eval num_timesteps=546200, episode_reward=18.29 +/- 13.17
Episode length: 27.80 +/- 19.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 27.8          |
|    mean_reward          | 18.3          |
| time/                   |               |
|    total_timesteps      | 546200        |
| train/                  |               |
|    approx_kl            | 0.00031221713 |
|    clip_fraction        | 0.0109        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0173       |
|    explained_variance   | 0.204         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.82          |
|    n_updates            | 13650         |
|    policy_gradient_loss | -0.0014       |
|    value_loss           | 1.55          |
-------------------------------------------
Progress update from timestep 546300: Updated shield, loss is 0.03790256381034851
Progress update from timestep 546400: Updated shield, loss is 0.02358637936413288
Eval num_timesteps=546400, episode_reward=11.91 +/- 12.06
Episode length: 17.80 +/- 17.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | 11.9     |
| time/              |          |
|    total_timesteps | 546400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.78     |
|    ep_rew_mean     | 2.34     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1366     |
|    time_elapsed    | 123111   |
|    total_timesteps | 546400   |
---------------------------------
Progress update from timestep 546500: Updated shield, loss is 0.03640887141227722
Progress update from timestep 546600: Updated shield, loss is 0.040523432195186615
Eval num_timesteps=546600, episode_reward=11.22 +/- 12.53
Episode length: 16.60 +/- 17.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.6          |
|    mean_reward          | 11.2          |
| time/                   |               |
|    total_timesteps      | 546600        |
| train/                  |               |
|    approx_kl            | 0.00016394975 |
|    clip_fraction        | 0.0105        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.018        |
|    explained_variance   | 0.148         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.615         |
|    n_updates            | 13660         |
|    policy_gradient_loss | -0.00091      |
|    value_loss           | 1.08          |
-------------------------------------------
Progress update from timestep 546700: Updated shield, loss is 0.026236144825816154
Progress update from timestep 546800: Updated shield, loss is 0.036361269652843475
Eval num_timesteps=546800, episode_reward=11.47 +/- 11.66
Episode length: 17.20 +/- 16.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | 11.5     |
| time/              |          |
|    total_timesteps | 546800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.4      |
|    ep_rew_mean     | 2.86     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1367     |
|    time_elapsed    | 123197   |
|    total_timesteps | 546800   |
---------------------------------
Progress update from timestep 546900: Updated shield, loss is 0.03864268213510513
Progress update from timestep 547000: Updated shield, loss is 0.043087221682071686
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 547000: Saved shield_loss.csv. 
Eval num_timesteps=547000, episode_reward=5.89 +/- 1.82
Episode length: 9.60 +/- 2.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 9.6          |
|    mean_reward          | 5.89         |
| time/                   |              |
|    total_timesteps      | 547000       |
| train/                  |              |
|    approx_kl            | 8.053868e-05 |
|    clip_fraction        | 0.0029       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00782     |
|    explained_variance   | 0.23         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.585        |
|    n_updates            | 13670        |
|    policy_gradient_loss | -0.000384    |
|    value_loss           | 2.58         |
------------------------------------------
Progress update from timestep 547100: Updated shield, loss is 0.03545594587922096
Progress update from timestep 547200: Updated shield, loss is 0.03576214984059334
Eval num_timesteps=547200, episode_reward=10.40 +/- 11.61
Episode length: 16.20 +/- 17.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | 10.4     |
| time/              |          |
|    total_timesteps | 547200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.13     |
|    ep_rew_mean     | 2.59     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1368     |
|    time_elapsed    | 123270   |
|    total_timesteps | 547200   |
---------------------------------
Progress update from timestep 547300: Updated shield, loss is 0.034393906593322754
Progress update from timestep 547400: Updated shield, loss is 0.03356098383665085
Eval num_timesteps=547400, episode_reward=12.23 +/- 12.28
Episode length: 18.20 +/- 16.83
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 18.2          |
|    mean_reward          | 12.2          |
| time/                   |               |
|    total_timesteps      | 547400        |
| train/                  |               |
|    approx_kl            | 2.4211642e-05 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00362      |
|    explained_variance   | 0.148         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.16          |
|    n_updates            | 13680         |
|    policy_gradient_loss | -9.25e-05     |
|    value_loss           | 1.74          |
-------------------------------------------
Progress update from timestep 547500: Updated shield, loss is 0.03734750300645828
Progress update from timestep 547600: Updated shield, loss is 0.04323038086295128
Eval num_timesteps=547600, episode_reward=9.16 +/- 5.02
Episode length: 13.80 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.8     |
|    mean_reward     | 9.16     |
| time/              |          |
|    total_timesteps | 547600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.22     |
|    ep_rew_mean     | 2.64     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1369     |
|    time_elapsed    | 123342   |
|    total_timesteps | 547600   |
---------------------------------
Progress update from timestep 547700: Updated shield, loss is 0.038321152329444885
Progress update from timestep 547800: Updated shield, loss is 0.02929225191473961
Eval num_timesteps=547800, episode_reward=22.29 +/- 15.45
Episode length: 32.40 +/- 21.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.4         |
|    mean_reward          | 22.3         |
| time/                   |              |
|    total_timesteps      | 547800       |
| train/                  |              |
|    approx_kl            | 3.757174e-05 |
|    clip_fraction        | 0.00134      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00467     |
|    explained_variance   | 0.246        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.814        |
|    n_updates            | 13690        |
|    policy_gradient_loss | -0.000322    |
|    value_loss           | 1.54         |
------------------------------------------
Progress update from timestep 547900: Updated shield, loss is 0.03660124912858009
Progress update from timestep 548000: Updated shield, loss is 0.027004431933164597
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 548000: Saved shield_loss.csv. 
Eval num_timesteps=548000, episode_reward=11.52 +/- 11.47
Episode length: 17.80 +/- 16.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | 11.5     |
| time/              |          |
|    total_timesteps | 548000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.9      |
|    ep_rew_mean     | 2.44     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1370     |
|    time_elapsed    | 123426   |
|    total_timesteps | 548000   |
---------------------------------
Progress update from timestep 548100: Updated shield, loss is 0.030451549217104912
Progress update from timestep 548200: Updated shield, loss is 0.03388068825006485
Eval num_timesteps=548200, episode_reward=9.89 +/- 12.87
Episode length: 14.80 +/- 17.63
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.8          |
|    mean_reward          | 9.89          |
| time/                   |               |
|    total_timesteps      | 548200        |
| train/                  |               |
|    approx_kl            | 3.7594614e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.000508     |
|    explained_variance   | 0.115         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.852         |
|    n_updates            | 13700         |
|    policy_gradient_loss | -5.27e-08     |
|    value_loss           | 1.41          |
-------------------------------------------
Progress update from timestep 548300: Updated shield, loss is 0.031568966805934906
Progress update from timestep 548400: Updated shield, loss is 0.029642123728990555
Eval num_timesteps=548400, episode_reward=19.37 +/- 13.26
Episode length: 28.20 +/- 17.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.2     |
|    mean_reward     | 19.4     |
| time/              |          |
|    total_timesteps | 548400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.07     |
|    ep_rew_mean     | 2.55     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1371     |
|    time_elapsed    | 123513   |
|    total_timesteps | 548400   |
---------------------------------
Progress update from timestep 548500: Updated shield, loss is 0.05281177535653114
Progress update from timestep 548600: Updated shield, loss is 0.044449709355831146
Eval num_timesteps=548600, episode_reward=7.18 +/- 3.01
Episode length: 11.20 +/- 4.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 7.18          |
| time/                   |               |
|    total_timesteps      | 548600        |
| train/                  |               |
|    approx_kl            | 5.3224583e-05 |
|    clip_fraction        | 0.00335       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0129       |
|    explained_variance   | 0.0686        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.42          |
|    n_updates            | 13710         |
|    policy_gradient_loss | -0.000766     |
|    value_loss           | 1.9           |
-------------------------------------------
Progress update from timestep 548700: Updated shield, loss is 0.04078388214111328
Progress update from timestep 548800: Updated shield, loss is 0.03411683812737465
Eval num_timesteps=548800, episode_reward=10.56 +/- 11.68
Episode length: 16.00 +/- 17.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | 10.6     |
| time/              |          |
|    total_timesteps | 548800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.97     |
|    ep_rew_mean     | 2.49     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1372     |
|    time_elapsed    | 123590   |
|    total_timesteps | 548800   |
---------------------------------
Progress update from timestep 548900: Updated shield, loss is 0.03309145197272301
Progress update from timestep 549000: Updated shield, loss is 0.03231020271778107
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 549000: Saved shield_loss.csv. 
Eval num_timesteps=549000, episode_reward=23.09 +/- 14.88
Episode length: 33.20 +/- 20.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.2         |
|    mean_reward          | 23.1         |
| time/                   |              |
|    total_timesteps      | 549000       |
| train/                  |              |
|    approx_kl            | 0.0006342837 |
|    clip_fraction        | 0.00625      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0223      |
|    explained_variance   | 0.167        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.05         |
|    n_updates            | 13720        |
|    policy_gradient_loss | -0.00119     |
|    value_loss           | 2.16         |
------------------------------------------
Progress update from timestep 549100: Updated shield, loss is 0.03885526582598686
Progress update from timestep 549200: Updated shield, loss is 0.039548519998788834
Eval num_timesteps=549200, episode_reward=11.74 +/- 11.73
Episode length: 17.60 +/- 16.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | 11.7     |
| time/              |          |
|    total_timesteps | 549200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.63     |
|    ep_rew_mean     | 2.2      |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1373     |
|    time_elapsed    | 123673   |
|    total_timesteps | 549200   |
---------------------------------
Progress update from timestep 549300: Updated shield, loss is 0.02917974628508091
Progress update from timestep 549400: Updated shield, loss is 0.05153637006878853
Eval num_timesteps=549400, episode_reward=5.46 +/- 4.43
Episode length: 8.80 +/- 6.43
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 8.8           |
|    mean_reward          | 5.46          |
| time/                   |               |
|    total_timesteps      | 549400        |
| train/                  |               |
|    approx_kl            | 7.5909284e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0036       |
|    explained_variance   | 0.154         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.926         |
|    n_updates            | 13730         |
|    policy_gradient_loss | -9.88e-06     |
|    value_loss           | 1.49          |
-------------------------------------------
Progress update from timestep 549500: Updated shield, loss is 0.04352971911430359
Progress update from timestep 549600: Updated shield, loss is 0.03936270996928215
Eval num_timesteps=549600, episode_reward=9.63 +/- 12.52
Episode length: 14.60 +/- 17.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 9.63     |
| time/              |          |
|    total_timesteps | 549600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.19     |
|    ep_rew_mean     | 2.63     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1374     |
|    time_elapsed    | 123743   |
|    total_timesteps | 549600   |
---------------------------------
Progress update from timestep 549700: Updated shield, loss is 0.03737282752990723
Progress update from timestep 549800: Updated shield, loss is 0.03739335387945175
Eval num_timesteps=549800, episode_reward=16.70 +/- 15.50
Episode length: 24.20 +/- 21.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.2         |
|    mean_reward          | 16.7         |
| time/                   |              |
|    total_timesteps      | 549800       |
| train/                  |              |
|    approx_kl            | 8.670239e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00246     |
|    explained_variance   | 0.152        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.757        |
|    n_updates            | 13740        |
|    policy_gradient_loss | -8.4e-06     |
|    value_loss           | 1.93         |
------------------------------------------
Progress update from timestep 549900: Updated shield, loss is 0.030399486422538757
Progress update from timestep 550000: Updated shield, loss is 0.03231535851955414
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 550000: Saved shield_loss.csv. 
Eval num_timesteps=550000, episode_reward=24.98 +/- 12.49
Episode length: 36.20 +/- 17.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 25       |
| time/              |          |
|    total_timesteps | 550000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.16     |
|    ep_rew_mean     | 2.61     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1375     |
|    time_elapsed    | 123830   |
|    total_timesteps | 550000   |
---------------------------------
Progress update from timestep 550100: Updated shield, loss is 0.03372599929571152
Progress update from timestep 550200: Updated shield, loss is 0.03475473076105118
Eval num_timesteps=550200, episode_reward=24.13 +/- 14.61
Episode length: 34.00 +/- 19.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34            |
|    mean_reward          | 24.1          |
| time/                   |               |
|    total_timesteps      | 550200        |
| train/                  |               |
|    approx_kl            | 0.00041473735 |
|    clip_fraction        | 0.0165        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0324       |
|    explained_variance   | 0.116         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.925         |
|    n_updates            | 13750         |
|    policy_gradient_loss | -0.00252      |
|    value_loss           | 2.34          |
-------------------------------------------
Progress update from timestep 550300: Updated shield, loss is 0.039551228284835815
Progress update from timestep 550400: Updated shield, loss is 0.03432968258857727
Eval num_timesteps=550400, episode_reward=6.06 +/- 4.72
Episode length: 9.40 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.4      |
|    mean_reward     | 6.06     |
| time/              |          |
|    total_timesteps | 550400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.83     |
|    ep_rew_mean     | 2.37     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1376     |
|    time_elapsed    | 123902   |
|    total_timesteps | 550400   |
---------------------------------
Progress update from timestep 550500: Updated shield, loss is 0.030270181596279144
Progress update from timestep 550600: Updated shield, loss is 0.031116535887122154
Eval num_timesteps=550600, episode_reward=26.67 +/- 11.91
Episode length: 38.20 +/- 16.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 38.2         |
|    mean_reward          | 26.7         |
| time/                   |              |
|    total_timesteps      | 550600       |
| train/                  |              |
|    approx_kl            | 0.0002583464 |
|    clip_fraction        | 0.00781      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0129      |
|    explained_variance   | 0.202        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.529        |
|    n_updates            | 13760        |
|    policy_gradient_loss | -0.000397    |
|    value_loss           | 1.22         |
------------------------------------------
Progress update from timestep 550700: Updated shield, loss is 0.034736525267362595
Progress update from timestep 550800: Updated shield, loss is 0.035018619149923325
Eval num_timesteps=550800, episode_reward=25.29 +/- 12.64
Episode length: 36.40 +/- 17.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 25.3     |
| time/              |          |
|    total_timesteps | 550800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.54     |
|    ep_rew_mean     | 2.94     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1377     |
|    time_elapsed    | 124014   |
|    total_timesteps | 550800   |
---------------------------------
Progress update from timestep 550900: Updated shield, loss is 0.025840235874056816
Progress update from timestep 551000: Updated shield, loss is 0.027417218312621117
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 551000: Saved shield_loss.csv. 
Eval num_timesteps=551000, episode_reward=10.26 +/- 12.48
Episode length: 15.40 +/- 17.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.4         |
|    mean_reward          | 10.3         |
| time/                   |              |
|    total_timesteps      | 551000       |
| train/                  |              |
|    approx_kl            | 0.0006731338 |
|    clip_fraction        | 0.0103       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0186      |
|    explained_variance   | 0.302        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.813        |
|    n_updates            | 13770        |
|    policy_gradient_loss | -0.000775    |
|    value_loss           | 1.76         |
------------------------------------------
Progress update from timestep 551100: Updated shield, loss is 0.029063403606414795
Progress update from timestep 551200: Updated shield, loss is 0.03165143355727196
Eval num_timesteps=551200, episode_reward=21.82 +/- 16.44
Episode length: 31.40 +/- 22.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.4     |
|    mean_reward     | 21.8     |
| time/              |          |
|    total_timesteps | 551200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.28     |
|    ep_rew_mean     | 2.66     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1378     |
|    time_elapsed    | 124097   |
|    total_timesteps | 551200   |
---------------------------------
Progress update from timestep 551300: Updated shield, loss is 0.03348775953054428
Progress update from timestep 551400: Updated shield, loss is 0.03784049674868584
Eval num_timesteps=551400, episode_reward=28.00 +/- 11.86
Episode length: 41.40 +/- 17.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.4         |
|    mean_reward          | 28           |
| time/                   |              |
|    total_timesteps      | 551400       |
| train/                  |              |
|    approx_kl            | 7.342709e-05 |
|    clip_fraction        | 0.00379      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0131      |
|    explained_variance   | 0.0416       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.436        |
|    n_updates            | 13780        |
|    policy_gradient_loss | -0.00027     |
|    value_loss           | 1.78         |
------------------------------------------
Progress update from timestep 551500: Updated shield, loss is 0.03778098523616791
Progress update from timestep 551600: Updated shield, loss is 0.0421898178756237
Eval num_timesteps=551600, episode_reward=16.71 +/- 15.97
Episode length: 23.80 +/- 21.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 551600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4        |
|    ep_rew_mean     | 2.49     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1379     |
|    time_elapsed    | 124198   |
|    total_timesteps | 551600   |
---------------------------------
Progress update from timestep 551700: Updated shield, loss is 0.03475015237927437
Progress update from timestep 551800: Updated shield, loss is 0.03443149849772453
Eval num_timesteps=551800, episode_reward=13.03 +/- 12.07
Episode length: 19.00 +/- 15.91
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 19            |
|    mean_reward          | 13            |
| time/                   |               |
|    total_timesteps      | 551800        |
| train/                  |               |
|    approx_kl            | 0.00095181726 |
|    clip_fraction        | 0.00826       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0146       |
|    explained_variance   | 0.159         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.09          |
|    n_updates            | 13790         |
|    policy_gradient_loss | -0.001        |
|    value_loss           | 2.03          |
-------------------------------------------
Progress update from timestep 551900: Updated shield, loss is 0.030308933928608894
Progress update from timestep 552000: Updated shield, loss is 0.035172026604413986
Loss log saved to models/ShieldPPO_run/logs/shield_loss.csv
Progress update from timestep 552000: Saved shield_loss.csv. 
Eval num_timesteps=552000, episode_reward=12.68 +/- 10.59
Episode length: 19.40 +/- 15.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 12.7     |
| time/              |          |
|    total_timesteps | 552000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.12     |
|    ep_rew_mean     | 2.58     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1380     |
|    time_elapsed    | 124279   |
|    total_timesteps | 552000   |
---------------------------------
Progress update from timestep 552100: Updated shield, loss is 0.03568771108984947
Progress update from timestep 552200: Updated shield, loss is 0.033640120178461075
Eval num_timesteps=552200, episode_reward=11.62 +/- 12.57
Episode length: 16.60 +/- 16.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.6          |
|    mean_reward          | 11.6          |
| time/                   |               |
|    total_timesteps      | 552200        |
| train/                  |               |
|    approx_kl            | 0.00034041665 |
|    clip_fraction        | 0.00647       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0151       |
|    explained_variance   | 0.206         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.821         |
|    n_updates            | 13800         |
|    policy_gradient_loss | -0.000868     |
|    value_loss           | 1.78          |
-------------------------------------------
Progress update from timestep 552300: Updated shield, loss is 0.03561098128557205
Progress update from timestep 552400: Updated shield, loss is 0.03141777589917183
Eval num_timesteps=552400, episode_reward=4.64 +/- 1.42
Episode length: 7.60 +/- 2.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7.6      |
|    mean_reward     | 4.64     |
| time/              |          |
|    total_timesteps | 552400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.09     |
|    ep_rew_mean     | 2.52     |
| time/              |          |
|    fps             | 4        |
|    iterations      | 1381     |
|    time_elapsed    | 124347   |
|    total_timesteps | 552400   |
---------------------------------
Progress update from timestep 552500: Updated shield, loss is 0.04044375568628311
Progress update from timestep 552600: Updated shield, loss is 0.036422353237867355
Eval num_timesteps=552600, episode_reward=9.88 +/- 11.97
Episode length: 15.40 +/- 17.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.4          |
|    mean_reward          | 9.88          |
| time/                   |               |
|    total_timesteps      | 552600        |
| train/                  |               |
|    approx_kl            | 4.8428145e-05 |
|    clip_fraction        | 0.00223       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00971      |
|    explained_variance   | 0.211         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.855         |
|    n_updates            | 13810         |
|    policy_gradient_loss | -0.00054      |
|    value_loss           | 1.63          |
-------------------------------------------
