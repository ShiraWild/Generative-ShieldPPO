SLURM_JOBID=2333150
SLURM_JOB_NODELIST=cs-cpu-04
Device is set up to cpu
Seed is set to 0
Logging to models/ppo_run/logs
Using cpu device
Eval num_timesteps=200, episode_reward=23.78 +/- 13.44
Episode length: 34.60 +/- 19.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 23.8     |
| time/              |          |
|    total_timesteps | 200      |
---------------------------------
New best mean reward!
Eval num_timesteps=400, episode_reward=11.94 +/- 12.60
Episode length: 17.40 +/- 16.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 11.9     |
| time/              |          |
|    total_timesteps | 400      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.8      |
|    ep_rew_mean     | 3.1      |
| time/              |          |
|    fps             | 2        |
|    iterations      | 1        |
|    time_elapsed    | 135      |
|    total_timesteps | 400      |
---------------------------------
Eval num_timesteps=600, episode_reward=8.23 +/- 12.65
Episode length: 13.00 +/- 18.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 13           |
|    mean_reward          | 8.23         |
| time/                   |              |
|    total_timesteps      | 600          |
| train/                  |              |
|    approx_kl            | 0.0033658363 |
|    clip_fraction        | 0.116        |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.61        |
|    explained_variance   | -0.129       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.11         |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00833     |
|    value_loss           | 2.34         |
------------------------------------------
Eval num_timesteps=800, episode_reward=15.65 +/- 15.89
Episode length: 22.80 +/- 22.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 15.6     |
| time/              |          |
|    total_timesteps | 800      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.83     |
|    ep_rew_mean     | 3.14     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 2        |
|    time_elapsed    | 241      |
|    total_timesteps | 800      |
---------------------------------
Eval num_timesteps=1000, episode_reward=16.28 +/- 15.85
Episode length: 23.60 +/- 21.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 23.6       |
|    mean_reward          | 16.3       |
| time/                   |            |
|    total_timesteps      | 1000       |
| train/                  |            |
|    approx_kl            | 0.00244998 |
|    clip_fraction        | 0.0797     |
|    clip_range           | 0.1        |
|    entropy_loss         | -1.6       |
|    explained_variance   | -0.00623   |
|    learning_rate        | 0.0001     |
|    loss                 | 1.51       |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.00748   |
|    value_loss           | 2.17       |
----------------------------------------
Eval num_timesteps=1200, episode_reward=15.91 +/- 16.58
Episode length: 22.60 +/- 22.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 15.9     |
| time/              |          |
|    total_timesteps | 1200     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.92     |
|    ep_rew_mean     | 3.22     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 3        |
|    time_elapsed    | 375      |
|    total_timesteps | 1200     |
---------------------------------
Eval num_timesteps=1400, episode_reward=6.05 +/- 4.69
Episode length: 10.00 +/- 7.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 10           |
|    mean_reward          | 6.05         |
| time/                   |              |
|    total_timesteps      | 1400         |
| train/                  |              |
|    approx_kl            | 0.0035329144 |
|    clip_fraction        | 0.176        |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.59        |
|    explained_variance   | -0.0306      |
|    learning_rate        | 0.0001       |
|    loss                 | 1.71         |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.0119      |
|    value_loss           | 2.79         |
------------------------------------------
Eval num_timesteps=1600, episode_reward=27.96 +/- 12.53
Episode length: 41.00 +/- 18.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41       |
|    mean_reward     | 28       |
| time/              |          |
|    total_timesteps | 1600     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.8      |
|    ep_rew_mean     | 3.13     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 4        |
|    time_elapsed    | 496      |
|    total_timesteps | 1600     |
---------------------------------
Eval num_timesteps=1800, episode_reward=19.93 +/- 12.09
Episode length: 29.40 +/- 17.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29.4         |
|    mean_reward          | 19.9         |
| time/                   |              |
|    total_timesteps      | 1800         |
| train/                  |              |
|    approx_kl            | 0.0039025028 |
|    clip_fraction        | 0.146        |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.58        |
|    explained_variance   | 0.005        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.954        |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.0107      |
|    value_loss           | 2.51         |
------------------------------------------
Eval num_timesteps=2000, episode_reward=10.72 +/- 12.26
Episode length: 16.00 +/- 17.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | 10.7     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.12     |
|    ep_rew_mean     | 3.22     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 5        |
|    time_elapsed    | 607      |
|    total_timesteps | 2000     |
---------------------------------
Eval num_timesteps=2200, episode_reward=16.42 +/- 16.60
Episode length: 23.00 +/- 22.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23           |
|    mean_reward          | 16.4         |
| time/                   |              |
|    total_timesteps      | 2200         |
| train/                  |              |
|    approx_kl            | 0.0043805116 |
|    clip_fraction        | 0.172        |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.55        |
|    explained_variance   | 0.00937      |
|    learning_rate        | 0.0001       |
|    loss                 | 1.28         |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.0126      |
|    value_loss           | 2.12         |
------------------------------------------
Eval num_timesteps=2400, episode_reward=10.20 +/- 13.38
Episode length: 15.00 +/- 17.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | 10.2     |
| time/              |          |
|    total_timesteps | 2400     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.81     |
|    ep_rew_mean     | 3.72     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 6        |
|    time_elapsed    | 717      |
|    total_timesteps | 2400     |
---------------------------------
Eval num_timesteps=2600, episode_reward=13.85 +/- 11.56
Episode length: 20.80 +/- 17.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 20.8        |
|    mean_reward          | 13.9        |
| time/                   |             |
|    total_timesteps      | 2600        |
| train/                  |             |
|    approx_kl            | 0.002992709 |
|    clip_fraction        | 0.0737      |
|    clip_range           | 0.1         |
|    entropy_loss         | -1.53       |
|    explained_variance   | -0.0768     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.48        |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00597    |
|    value_loss           | 3.78        |
-----------------------------------------
Eval num_timesteps=2800, episode_reward=14.37 +/- 10.96
Episode length: 22.20 +/- 16.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 14.4     |
| time/              |          |
|    total_timesteps | 2800     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 6.75     |
|    ep_rew_mean     | 4.37     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 7        |
|    time_elapsed    | 826      |
|    total_timesteps | 2800     |
---------------------------------
Eval num_timesteps=3000, episode_reward=18.48 +/- 13.63
Episode length: 27.40 +/- 19.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 27.4         |
|    mean_reward          | 18.5         |
| time/                   |              |
|    total_timesteps      | 3000         |
| train/                  |              |
|    approx_kl            | 0.0034113904 |
|    clip_fraction        | 0.204        |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.49        |
|    explained_variance   | 0.0525       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.8          |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.0127      |
|    value_loss           | 3.66         |
------------------------------------------
Eval num_timesteps=3200, episode_reward=16.33 +/- 15.44
Episode length: 23.80 +/- 21.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | 16.3     |
| time/              |          |
|    total_timesteps | 3200     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 7.54     |
|    ep_rew_mean     | 4.93     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 8        |
|    time_elapsed    | 933      |
|    total_timesteps | 3200     |
---------------------------------
Eval num_timesteps=3400, episode_reward=6.34 +/- 2.10
Episode length: 9.80 +/- 2.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 9.8          |
|    mean_reward          | 6.34         |
| time/                   |              |
|    total_timesteps      | 3400         |
| train/                  |              |
|    approx_kl            | 0.0029434806 |
|    clip_fraction        | 0.183        |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.46        |
|    explained_variance   | 0.132        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.54         |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.0122      |
|    value_loss           | 3.6          |
------------------------------------------
Eval num_timesteps=3600, episode_reward=28.53 +/- 12.48
Episode length: 41.20 +/- 17.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | 28.5     |
| time/              |          |
|    total_timesteps | 3600     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 8.31     |
|    ep_rew_mean     | 5.47     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 9        |
|    time_elapsed    | 1027     |
|    total_timesteps | 3600     |
---------------------------------
Eval num_timesteps=3800, episode_reward=9.55 +/- 12.53
Episode length: 14.60 +/- 17.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 14.6      |
|    mean_reward          | 9.55      |
| time/                   |           |
|    total_timesteps      | 3800      |
| train/                  |           |
|    approx_kl            | 0.0029342 |
|    clip_fraction        | 0.108     |
|    clip_range           | 0.1       |
|    entropy_loss         | -1.41     |
|    explained_variance   | -0.0742   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.32      |
|    n_updates            | 90        |
|    policy_gradient_loss | -0.00889  |
|    value_loss           | 3.38      |
---------------------------------------
Eval num_timesteps=4000, episode_reward=16.61 +/- 15.14
Episode length: 24.40 +/- 21.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 7.75     |
|    ep_rew_mean     | 5.03     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 10       |
|    time_elapsed    | 1139     |
|    total_timesteps | 4000     |
---------------------------------
Eval num_timesteps=4200, episode_reward=23.56 +/- 13.89
Episode length: 34.20 +/- 19.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.2         |
|    mean_reward          | 23.6         |
| time/                   |              |
|    total_timesteps      | 4200         |
| train/                  |              |
|    approx_kl            | 0.0012443068 |
|    clip_fraction        | 0.0638       |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.38        |
|    explained_variance   | 0.0392       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.68         |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.00598     |
|    value_loss           | 3.55         |
------------------------------------------
Eval num_timesteps=4400, episode_reward=15.46 +/- 16.48
Episode length: 22.40 +/- 22.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 15.5     |
| time/              |          |
|    total_timesteps | 4400     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 8.22     |
|    ep_rew_mean     | 5.42     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 11       |
|    time_elapsed    | 1277     |
|    total_timesteps | 4400     |
---------------------------------
Eval num_timesteps=4600, episode_reward=16.69 +/- 15.02
Episode length: 24.60 +/- 20.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 24.6        |
|    mean_reward          | 16.7        |
| time/                   |             |
|    total_timesteps      | 4600        |
| train/                  |             |
|    approx_kl            | 0.002766157 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.1         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.0994      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.906       |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 3.3         |
-----------------------------------------
Eval num_timesteps=4800, episode_reward=29.36 +/- 11.39
Episode length: 42.00 +/- 16.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 29.4     |
| time/              |          |
|    total_timesteps | 4800     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.58     |
|    ep_rew_mean     | 6.46     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 12       |
|    time_elapsed    | 1408     |
|    total_timesteps | 4800     |
---------------------------------
Eval num_timesteps=5000, episode_reward=12.65 +/- 12.32
Episode length: 18.40 +/- 16.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.4         |
|    mean_reward          | 12.6         |
| time/                   |              |
|    total_timesteps      | 5000         |
| train/                  |              |
|    approx_kl            | 0.0030762877 |
|    clip_fraction        | 0.0672       |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.31        |
|    explained_variance   | 0.103        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.86         |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00706     |
|    value_loss           | 3.85         |
------------------------------------------
Eval num_timesteps=5200, episode_reward=17.14 +/- 13.71
Episode length: 25.80 +/- 19.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | 17.1     |
| time/              |          |
|    total_timesteps | 5200     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.94     |
|    ep_rew_mean     | 6.72     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 13       |
|    time_elapsed    | 1525     |
|    total_timesteps | 5200     |
---------------------------------
Eval num_timesteps=5400, episode_reward=11.64 +/- 11.65
Episode length: 17.60 +/- 17.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.6        |
|    mean_reward          | 11.6        |
| time/                   |             |
|    total_timesteps      | 5400        |
| train/                  |             |
|    approx_kl            | 0.004491191 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.1         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.151       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.52        |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 3.62        |
-----------------------------------------
Eval num_timesteps=5600, episode_reward=13.07 +/- 12.73
Episode length: 18.80 +/- 16.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.8     |
|    mean_reward     | 13.1     |
| time/              |          |
|    total_timesteps | 5600     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.41     |
|    ep_rew_mean     | 6.23     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 14       |
|    time_elapsed    | 1635     |
|    total_timesteps | 5600     |
---------------------------------
Eval num_timesteps=5800, episode_reward=18.88 +/- 13.05
Episode length: 28.20 +/- 18.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 28.2         |
|    mean_reward          | 18.9         |
| time/                   |              |
|    total_timesteps      | 5800         |
| train/                  |              |
|    approx_kl            | 0.0022424965 |
|    clip_fraction        | 0.0393       |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.19        |
|    explained_variance   | 0.108        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.19         |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00349     |
|    value_loss           | 3.68         |
------------------------------------------
Eval num_timesteps=6000, episode_reward=23.12 +/- 14.05
Episode length: 34.00 +/- 19.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 23.1     |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.7      |
|    ep_rew_mean     | 6.4      |
| time/              |          |
|    fps             | 3        |
|    iterations      | 15       |
|    time_elapsed    | 1766     |
|    total_timesteps | 6000     |
---------------------------------
Eval num_timesteps=6200, episode_reward=22.93 +/- 14.18
Episode length: 33.60 +/- 20.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.6         |
|    mean_reward          | 22.9         |
| time/                   |              |
|    total_timesteps      | 6200         |
| train/                  |              |
|    approx_kl            | 0.0032680295 |
|    clip_fraction        | 0.0634       |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.14        |
|    explained_variance   | 0.14         |
|    learning_rate        | 0.0001       |
|    loss                 | 1.9          |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.00661     |
|    value_loss           | 3.89         |
------------------------------------------
Eval num_timesteps=6400, episode_reward=21.44 +/- 15.55
Episode length: 31.80 +/- 22.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.8     |
|    mean_reward     | 21.4     |
| time/              |          |
|    total_timesteps | 6400     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 11.5     |
|    ep_rew_mean     | 7.7      |
| time/              |          |
|    fps             | 3        |
|    iterations      | 16       |
|    time_elapsed    | 1900     |
|    total_timesteps | 6400     |
---------------------------------
Eval num_timesteps=6600, episode_reward=17.89 +/- 14.65
Episode length: 26.00 +/- 20.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26           |
|    mean_reward          | 17.9         |
| time/                   |              |
|    total_timesteps      | 6600         |
| train/                  |              |
|    approx_kl            | 0.0032141698 |
|    clip_fraction        | 0.0569       |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0.199        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.58         |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.00665     |
|    value_loss           | 3.68         |
------------------------------------------
Eval num_timesteps=6800, episode_reward=22.94 +/- 14.15
Episode length: 33.60 +/- 20.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 22.9     |
| time/              |          |
|    total_timesteps | 6800     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 12.9     |
|    ep_rew_mean     | 8.7      |
| time/              |          |
|    fps             | 3        |
|    iterations      | 17       |
|    time_elapsed    | 2012     |
|    total_timesteps | 6800     |
---------------------------------
Eval num_timesteps=7000, episode_reward=20.65 +/- 12.36
Episode length: 29.60 +/- 16.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29.6         |
|    mean_reward          | 20.7         |
| time/                   |              |
|    total_timesteps      | 7000         |
| train/                  |              |
|    approx_kl            | 0.0024807735 |
|    clip_fraction        | 0.0685       |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.01        |
|    explained_variance   | 0.0369       |
|    learning_rate        | 0.0001       |
|    loss                 | 2.43         |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.00573     |
|    value_loss           | 3.99         |
------------------------------------------
Eval num_timesteps=7200, episode_reward=28.34 +/- 11.17
Episode length: 42.00 +/- 16.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 28.3     |
| time/              |          |
|    total_timesteps | 7200     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 13.9     |
|    ep_rew_mean     | 9.38     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 18       |
|    time_elapsed    | 2153     |
|    total_timesteps | 7200     |
---------------------------------
Eval num_timesteps=7400, episode_reward=15.75 +/- 14.49
Episode length: 23.80 +/- 21.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.8         |
|    mean_reward          | 15.7         |
| time/                   |              |
|    total_timesteps      | 7400         |
| train/                  |              |
|    approx_kl            | 0.0018518678 |
|    clip_fraction        | 0.0424       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.955       |
|    explained_variance   | 0.142        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.3          |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.00481     |
|    value_loss           | 3.36         |
------------------------------------------
Eval num_timesteps=7600, episode_reward=8.57 +/- 13.55
Episode length: 12.80 +/- 18.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 12.8     |
|    mean_reward     | 8.57     |
| time/              |          |
|    total_timesteps | 7600     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | 10.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 19       |
|    time_elapsed    | 2262     |
|    total_timesteps | 7600     |
---------------------------------
Eval num_timesteps=7800, episode_reward=5.10 +/- 2.86
Episode length: 8.00 +/- 3.85
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 8             |
|    mean_reward          | 5.1           |
| time/                   |               |
|    total_timesteps      | 7800          |
| train/                  |               |
|    approx_kl            | 0.00020827873 |
|    clip_fraction        | 0.00268       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.946        |
|    explained_variance   | 0.16          |
|    learning_rate        | 0.0001        |
|    loss                 | 1.52          |
|    n_updates            | 190           |
|    policy_gradient_loss | -0.00127      |
|    value_loss           | 3.06          |
-------------------------------------------
Eval num_timesteps=8000, episode_reward=27.81 +/- 12.83
Episode length: 40.80 +/- 18.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.8     |
|    mean_reward     | 27.8     |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | 11.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 20       |
|    time_elapsed    | 2351     |
|    total_timesteps | 8000     |
---------------------------------
Eval num_timesteps=8200, episode_reward=10.62 +/- 11.47
Episode length: 16.20 +/- 17.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.2          |
|    mean_reward          | 10.6          |
| time/                   |               |
|    total_timesteps      | 8200          |
| train/                  |               |
|    approx_kl            | 0.00095403765 |
|    clip_fraction        | 0.0123        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.931        |
|    explained_variance   | 0.202         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.86          |
|    n_updates            | 200           |
|    policy_gradient_loss | -0.00197      |
|    value_loss           | 3.51          |
-------------------------------------------
Eval num_timesteps=8400, episode_reward=18.49 +/- 14.50
Episode length: 26.40 +/- 19.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.4     |
|    mean_reward     | 18.5     |
| time/              |          |
|    total_timesteps | 8400     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | 11.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 21       |
|    time_elapsed    | 2459     |
|    total_timesteps | 8400     |
---------------------------------
Eval num_timesteps=8600, episode_reward=16.29 +/- 14.95
Episode length: 24.20 +/- 21.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 24.2        |
|    mean_reward          | 16.3        |
| time/                   |             |
|    total_timesteps      | 8600        |
| train/                  |             |
|    approx_kl            | 0.001118605 |
|    clip_fraction        | 0.033       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.873      |
|    explained_variance   | 0.199       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.22        |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00395    |
|    value_loss           | 3.82        |
-----------------------------------------
Eval num_timesteps=8800, episode_reward=7.45 +/- 4.48
Episode length: 11.60 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 11.6     |
|    mean_reward     | 7.45     |
| time/              |          |
|    total_timesteps | 8800     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.1     |
|    ep_rew_mean     | 12.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 22       |
|    time_elapsed    | 2560     |
|    total_timesteps | 8800     |
---------------------------------
Eval num_timesteps=9000, episode_reward=10.77 +/- 12.05
Episode length: 16.40 +/- 17.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | 10.8         |
| time/                   |              |
|    total_timesteps      | 9000         |
| train/                  |              |
|    approx_kl            | 0.0009641541 |
|    clip_fraction        | 0.0158       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.875       |
|    explained_variance   | 0.147        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.769        |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.00304     |
|    value_loss           | 3.11         |
------------------------------------------
Eval num_timesteps=9200, episode_reward=10.83 +/- 12.60
Episode length: 16.20 +/- 17.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | 10.8     |
| time/              |          |
|    total_timesteps | 9200     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | 11.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 23       |
|    time_elapsed    | 2669     |
|    total_timesteps | 9200     |
---------------------------------
Eval num_timesteps=9400, episode_reward=11.47 +/- 11.39
Episode length: 17.40 +/- 16.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17.4          |
|    mean_reward          | 11.5          |
| time/                   |               |
|    total_timesteps      | 9400          |
| train/                  |               |
|    approx_kl            | 0.00035820887 |
|    clip_fraction        | 0.00357       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.856        |
|    explained_variance   | 0.234         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.91          |
|    n_updates            | 230           |
|    policy_gradient_loss | -0.00196      |
|    value_loss           | 3.38          |
-------------------------------------------
Eval num_timesteps=9600, episode_reward=22.01 +/- 16.19
Episode length: 31.60 +/- 22.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.6     |
|    mean_reward     | 22       |
| time/              |          |
|    total_timesteps | 9600     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | 11.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 24       |
|    time_elapsed    | 2774     |
|    total_timesteps | 9600     |
---------------------------------
Eval num_timesteps=9800, episode_reward=16.16 +/- 15.06
Episode length: 24.00 +/- 21.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 24          |
|    mean_reward          | 16.2        |
| time/                   |             |
|    total_timesteps      | 9800        |
| train/                  |             |
|    approx_kl            | 0.000665591 |
|    clip_fraction        | 0.0109      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.842      |
|    explained_variance   | 0.153       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.992       |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00184    |
|    value_loss           | 3.47        |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=10.18 +/- 12.54
Episode length: 15.40 +/- 17.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | 10.2     |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.9     |
|    ep_rew_mean     | 12.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 25       |
|    time_elapsed    | 2884     |
|    total_timesteps | 10000    |
---------------------------------
Eval num_timesteps=10200, episode_reward=7.85 +/- 5.02
Episode length: 12.40 +/- 7.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 12.4         |
|    mean_reward          | 7.85         |
| time/                   |              |
|    total_timesteps      | 10200        |
| train/                  |              |
|    approx_kl            | 0.0014835111 |
|    clip_fraction        | 0.0616       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.78        |
|    explained_variance   | 0.22         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.653        |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00607     |
|    value_loss           | 3.1          |
------------------------------------------
Eval num_timesteps=10400, episode_reward=28.85 +/- 11.84
Episode length: 41.60 +/- 16.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 28.8     |
| time/              |          |
|    total_timesteps | 10400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.9     |
|    ep_rew_mean     | 12.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 26       |
|    time_elapsed    | 2981     |
|    total_timesteps | 10400    |
---------------------------------
Eval num_timesteps=10600, episode_reward=22.85 +/- 14.72
Episode length: 33.20 +/- 20.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.2          |
|    mean_reward          | 22.9          |
| time/                   |               |
|    total_timesteps      | 10600         |
| train/                  |               |
|    approx_kl            | 0.00043860622 |
|    clip_fraction        | 0.0103        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.775        |
|    explained_variance   | 0.203         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.556         |
|    n_updates            | 260           |
|    policy_gradient_loss | -0.00219      |
|    value_loss           | 3.7           |
-------------------------------------------
Eval num_timesteps=10800, episode_reward=22.34 +/- 15.82
Episode length: 32.20 +/- 21.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 22.3     |
| time/              |          |
|    total_timesteps | 10800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.4     |
|    ep_rew_mean     | 12.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 27       |
|    time_elapsed    | 3116     |
|    total_timesteps | 10800    |
---------------------------------
Eval num_timesteps=11000, episode_reward=15.39 +/- 16.06
Episode length: 22.60 +/- 22.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.6          |
|    mean_reward          | 15.4          |
| time/                   |               |
|    total_timesteps      | 11000         |
| train/                  |               |
|    approx_kl            | 0.00095235556 |
|    clip_fraction        | 0.0458        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.788        |
|    explained_variance   | 0.0894        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.09          |
|    n_updates            | 270           |
|    policy_gradient_loss | -0.00436      |
|    value_loss           | 3.17          |
-------------------------------------------
Eval num_timesteps=11200, episode_reward=14.84 +/- 16.50
Episode length: 21.80 +/- 23.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | 14.8     |
| time/              |          |
|    total_timesteps | 11200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.9     |
|    ep_rew_mean     | 12.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 28       |
|    time_elapsed    | 3226     |
|    total_timesteps | 11200    |
---------------------------------
Eval num_timesteps=11400, episode_reward=21.17 +/- 14.99
Episode length: 32.00 +/- 22.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32           |
|    mean_reward          | 21.2         |
| time/                   |              |
|    total_timesteps      | 11400        |
| train/                  |              |
|    approx_kl            | 0.0007055524 |
|    clip_fraction        | 0.0132       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.782       |
|    explained_variance   | 0.278        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.44         |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.0025      |
|    value_loss           | 2.7          |
------------------------------------------
Eval num_timesteps=11600, episode_reward=16.91 +/- 15.16
Episode length: 24.60 +/- 21.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 11600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.9     |
|    ep_rew_mean     | 13       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 29       |
|    time_elapsed    | 3331     |
|    total_timesteps | 11600    |
---------------------------------
Eval num_timesteps=11800, episode_reward=16.63 +/- 14.72
Episode length: 24.60 +/- 20.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.6         |
|    mean_reward          | 16.6         |
| time/                   |              |
|    total_timesteps      | 11800        |
| train/                  |              |
|    approx_kl            | 0.0011255362 |
|    clip_fraction        | 0.0297       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.729       |
|    explained_variance   | 0.176        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.48         |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.00436     |
|    value_loss           | 3.78         |
------------------------------------------
Eval num_timesteps=12000, episode_reward=24.57 +/- 13.00
Episode length: 35.60 +/- 18.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 24.6     |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.7     |
|    ep_rew_mean     | 12.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 30       |
|    time_elapsed    | 3439     |
|    total_timesteps | 12000    |
---------------------------------
Eval num_timesteps=12200, episode_reward=15.27 +/- 14.90
Episode length: 23.40 +/- 21.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.4         |
|    mean_reward          | 15.3         |
| time/                   |              |
|    total_timesteps      | 12200        |
| train/                  |              |
|    approx_kl            | 0.0011847337 |
|    clip_fraction        | 0.0357       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.686       |
|    explained_variance   | 0.0877       |
|    learning_rate        | 0.0001       |
|    loss                 | 2.43         |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.0041      |
|    value_loss           | 4.05         |
------------------------------------------
Eval num_timesteps=12400, episode_reward=9.23 +/- 12.14
Episode length: 14.60 +/- 17.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 9.23     |
| time/              |          |
|    total_timesteps | 12400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.5     |
|    ep_rew_mean     | 13.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 31       |
|    time_elapsed    | 3545     |
|    total_timesteps | 12400    |
---------------------------------
Eval num_timesteps=12600, episode_reward=22.93 +/- 14.19
Episode length: 33.60 +/- 20.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.6         |
|    mean_reward          | 22.9         |
| time/                   |              |
|    total_timesteps      | 12600        |
| train/                  |              |
|    approx_kl            | 0.0010044621 |
|    clip_fraction        | 0.0346       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.668       |
|    explained_variance   | 0.172        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.282        |
|    n_updates            | 310          |
|    policy_gradient_loss | -0.00401     |
|    value_loss           | 1.93         |
------------------------------------------
Eval num_timesteps=12800, episode_reward=12.49 +/- 13.65
Episode length: 18.00 +/- 18.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | 12.5     |
| time/              |          |
|    total_timesteps | 12800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.1     |
|    ep_rew_mean     | 13       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 32       |
|    time_elapsed    | 3655     |
|    total_timesteps | 12800    |
---------------------------------
Eval num_timesteps=13000, episode_reward=16.69 +/- 12.51
Episode length: 24.40 +/- 17.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.4         |
|    mean_reward          | 16.7         |
| time/                   |              |
|    total_timesteps      | 13000        |
| train/                  |              |
|    approx_kl            | 0.0009663344 |
|    clip_fraction        | 0.0491       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.607       |
|    explained_variance   | 0.27         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.914        |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.00355     |
|    value_loss           | 3.17         |
------------------------------------------
Eval num_timesteps=13200, episode_reward=21.15 +/- 12.00
Episode length: 30.20 +/- 16.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 21.2     |
| time/              |          |
|    total_timesteps | 13200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.5     |
|    ep_rew_mean     | 14       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 33       |
|    time_elapsed    | 3770     |
|    total_timesteps | 13200    |
---------------------------------
Eval num_timesteps=13400, episode_reward=29.98 +/- 10.70
Episode length: 42.80 +/- 14.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.8          |
|    mean_reward          | 30            |
| time/                   |               |
|    total_timesteps      | 13400         |
| train/                  |               |
|    approx_kl            | 0.00077263947 |
|    clip_fraction        | 0.0194        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.61         |
|    explained_variance   | 0.181         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.06          |
|    n_updates            | 330           |
|    policy_gradient_loss | -0.00278      |
|    value_loss           | 2.36          |
-------------------------------------------
New best mean reward!
Eval num_timesteps=13600, episode_reward=10.39 +/- 12.67
Episode length: 15.40 +/- 17.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | 10.4     |
| time/              |          |
|    total_timesteps | 13600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.4     |
|    ep_rew_mean     | 13.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 34       |
|    time_elapsed    | 3905     |
|    total_timesteps | 13600    |
---------------------------------
Eval num_timesteps=13800, episode_reward=18.33 +/- 13.83
Episode length: 26.60 +/- 19.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.6         |
|    mean_reward          | 18.3         |
| time/                   |              |
|    total_timesteps      | 13800        |
| train/                  |              |
|    approx_kl            | 0.0012386963 |
|    clip_fraction        | 0.0685       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.57        |
|    explained_variance   | 0.219        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.863        |
|    n_updates            | 340          |
|    policy_gradient_loss | -0.00478     |
|    value_loss           | 2.84         |
------------------------------------------
Eval num_timesteps=14000, episode_reward=19.23 +/- 13.04
Episode length: 28.20 +/- 17.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.2     |
|    mean_reward     | 19.2     |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | 14.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 35       |
|    time_elapsed    | 4009     |
|    total_timesteps | 14000    |
---------------------------------
Eval num_timesteps=14200, episode_reward=23.77 +/- 15.00
Episode length: 33.60 +/- 20.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.6          |
|    mean_reward          | 23.8          |
| time/                   |               |
|    total_timesteps      | 14200         |
| train/                  |               |
|    approx_kl            | 0.00053942524 |
|    clip_fraction        | 0.0266        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.591        |
|    explained_variance   | 0.227         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.85          |
|    n_updates            | 350           |
|    policy_gradient_loss | -0.00264      |
|    value_loss           | 3.16          |
-------------------------------------------
Eval num_timesteps=14400, episode_reward=18.31 +/- 13.78
Episode length: 26.80 +/- 19.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.8     |
|    mean_reward     | 18.3     |
| time/              |          |
|    total_timesteps | 14400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.3     |
|    ep_rew_mean     | 13.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 36       |
|    time_elapsed    | 4122     |
|    total_timesteps | 14400    |
---------------------------------
Eval num_timesteps=14600, episode_reward=11.32 +/- 11.64
Episode length: 17.00 +/- 16.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17          |
|    mean_reward          | 11.3        |
| time/                   |             |
|    total_timesteps      | 14600       |
| train/                  |             |
|    approx_kl            | 0.000833919 |
|    clip_fraction        | 0.015       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.606      |
|    explained_variance   | 0.348       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.91        |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.00328    |
|    value_loss           | 2.66        |
-----------------------------------------
Eval num_timesteps=14800, episode_reward=15.07 +/- 14.98
Episode length: 23.00 +/- 22.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 15.1     |
| time/              |          |
|    total_timesteps | 14800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | 14.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 37       |
|    time_elapsed    | 4237     |
|    total_timesteps | 14800    |
---------------------------------
Eval num_timesteps=15000, episode_reward=3.58 +/- 2.03
Episode length: 6.00 +/- 2.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 6            |
|    mean_reward          | 3.58         |
| time/                   |              |
|    total_timesteps      | 15000        |
| train/                  |              |
|    approx_kl            | 0.0006933256 |
|    clip_fraction        | 0.0147       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.599       |
|    explained_variance   | 0.295        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.75         |
|    n_updates            | 370          |
|    policy_gradient_loss | -0.00281     |
|    value_loss           | 2.5          |
------------------------------------------
Eval num_timesteps=15200, episode_reward=23.32 +/- 14.63
Episode length: 33.40 +/- 20.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 23.3     |
| time/              |          |
|    total_timesteps | 15200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.3     |
|    ep_rew_mean     | 13.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 38       |
|    time_elapsed    | 4327     |
|    total_timesteps | 15200    |
---------------------------------
Eval num_timesteps=15400, episode_reward=27.74 +/- 13.51
Episode length: 40.40 +/- 19.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 40.4         |
|    mean_reward          | 27.7         |
| time/                   |              |
|    total_timesteps      | 15400        |
| train/                  |              |
|    approx_kl            | 0.0004954177 |
|    clip_fraction        | 0.00625      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.623       |
|    explained_variance   | 0.257        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.89         |
|    n_updates            | 380          |
|    policy_gradient_loss | -0.00275     |
|    value_loss           | 3.2          |
------------------------------------------
Eval num_timesteps=15600, episode_reward=16.91 +/- 14.82
Episode length: 24.80 +/- 20.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 15600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.8     |
|    ep_rew_mean     | 13.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 39       |
|    time_elapsed    | 4459     |
|    total_timesteps | 15600    |
---------------------------------
Eval num_timesteps=15800, episode_reward=24.26 +/- 13.94
Episode length: 34.40 +/- 19.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.4          |
|    mean_reward          | 24.3          |
| time/                   |               |
|    total_timesteps      | 15800         |
| train/                  |               |
|    approx_kl            | 0.00032565303 |
|    clip_fraction        | 0.00848       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.657        |
|    explained_variance   | 0.266         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.374         |
|    n_updates            | 390           |
|    policy_gradient_loss | -0.00135      |
|    value_loss           | 2.14          |
-------------------------------------------
Eval num_timesteps=16000, episode_reward=24.29 +/- 12.29
Episode length: 35.80 +/- 17.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 24.3     |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.8     |
|    ep_rew_mean     | 14.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 40       |
|    time_elapsed    | 4570     |
|    total_timesteps | 16000    |
---------------------------------
Eval num_timesteps=16200, episode_reward=3.05 +/- 1.62
Episode length: 5.40 +/- 2.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 5.4          |
|    mean_reward          | 3.05         |
| time/                   |              |
|    total_timesteps      | 16200        |
| train/                  |              |
|    approx_kl            | 0.0007826863 |
|    clip_fraction        | 0.0147       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.644       |
|    explained_variance   | 0.0444       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.26         |
|    n_updates            | 400          |
|    policy_gradient_loss | -0.00318     |
|    value_loss           | 3.81         |
------------------------------------------
Eval num_timesteps=16400, episode_reward=15.69 +/- 15.40
Episode length: 23.20 +/- 21.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 15.7     |
| time/              |          |
|    total_timesteps | 16400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.6     |
|    ep_rew_mean     | 13.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 41       |
|    time_elapsed    | 4655     |
|    total_timesteps | 16400    |
---------------------------------
Eval num_timesteps=16600, episode_reward=10.78 +/- 12.31
Episode length: 16.60 +/- 17.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.6         |
|    mean_reward          | 10.8         |
| time/                   |              |
|    total_timesteps      | 16600        |
| train/                  |              |
|    approx_kl            | 0.0012122736 |
|    clip_fraction        | 0.0346       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.613       |
|    explained_variance   | 0.241        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.21         |
|    n_updates            | 410          |
|    policy_gradient_loss | -0.00344     |
|    value_loss           | 3.46         |
------------------------------------------
Eval num_timesteps=16800, episode_reward=11.20 +/- 11.26
Episode length: 17.00 +/- 16.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | 11.2     |
| time/              |          |
|    total_timesteps | 16800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | 11.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 42       |
|    time_elapsed    | 4760     |
|    total_timesteps | 16800    |
---------------------------------
Eval num_timesteps=17000, episode_reward=24.54 +/- 13.72
Episode length: 34.80 +/- 18.79
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.8          |
|    mean_reward          | 24.5          |
| time/                   |               |
|    total_timesteps      | 17000         |
| train/                  |               |
|    approx_kl            | 0.00074425683 |
|    clip_fraction        | 0.0145        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.566        |
|    explained_variance   | 0.283         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.19          |
|    n_updates            | 420           |
|    policy_gradient_loss | -0.00296      |
|    value_loss           | 3.66          |
-------------------------------------------
Eval num_timesteps=17200, episode_reward=19.19 +/- 13.73
Episode length: 27.80 +/- 19.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.8     |
|    mean_reward     | 19.2     |
| time/              |          |
|    total_timesteps | 17200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | 11.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 43       |
|    time_elapsed    | 4894     |
|    total_timesteps | 17200    |
---------------------------------
Eval num_timesteps=17400, episode_reward=8.74 +/- 5.47
Episode length: 13.20 +/- 7.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 13.2          |
|    mean_reward          | 8.74          |
| time/                   |               |
|    total_timesteps      | 17400         |
| train/                  |               |
|    approx_kl            | 0.00077708357 |
|    clip_fraction        | 0.0319        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.567        |
|    explained_variance   | 0.383         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.595         |
|    n_updates            | 430           |
|    policy_gradient_loss | -0.00394      |
|    value_loss           | 2.34          |
-------------------------------------------
Eval num_timesteps=17600, episode_reward=18.88 +/- 13.75
Episode length: 27.00 +/- 18.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27       |
|    mean_reward     | 18.9     |
| time/              |          |
|    total_timesteps | 17600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | 11       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 44       |
|    time_elapsed    | 4991     |
|    total_timesteps | 17600    |
---------------------------------
Eval num_timesteps=17800, episode_reward=17.11 +/- 15.45
Episode length: 24.60 +/- 21.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.6          |
|    mean_reward          | 17.1          |
| time/                   |               |
|    total_timesteps      | 17800         |
| train/                  |               |
|    approx_kl            | 0.00048002016 |
|    clip_fraction        | 0.00536       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.537        |
|    explained_variance   | 0.362         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.23          |
|    n_updates            | 440           |
|    policy_gradient_loss | -0.00136      |
|    value_loss           | 2.77          |
-------------------------------------------
Eval num_timesteps=18000, episode_reward=27.80 +/- 12.82
Episode length: 40.80 +/- 18.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.8     |
|    mean_reward     | 27.8     |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | 11.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 45       |
|    time_elapsed    | 5147     |
|    total_timesteps | 18000    |
---------------------------------
Eval num_timesteps=18200, episode_reward=15.73 +/- 14.89
Episode length: 23.60 +/- 21.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.6          |
|    mean_reward          | 15.7          |
| time/                   |               |
|    total_timesteps      | 18200         |
| train/                  |               |
|    approx_kl            | 0.00018045718 |
|    clip_fraction        | 0.0029        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.522        |
|    explained_variance   | 0.333         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.617         |
|    n_updates            | 450           |
|    policy_gradient_loss | -0.00132      |
|    value_loss           | 2.96          |
-------------------------------------------
Eval num_timesteps=18400, episode_reward=16.94 +/- 15.68
Episode length: 24.00 +/- 21.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 18400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.8     |
|    ep_rew_mean     | 12.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 46       |
|    time_elapsed    | 5282     |
|    total_timesteps | 18400    |
---------------------------------
Eval num_timesteps=18600, episode_reward=8.12 +/- 4.83
Episode length: 12.60 +/- 7.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 12.6         |
|    mean_reward          | 8.12         |
| time/                   |              |
|    total_timesteps      | 18600        |
| train/                  |              |
|    approx_kl            | 0.0004437421 |
|    clip_fraction        | 0.00558      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.534       |
|    explained_variance   | 0.358        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.25         |
|    n_updates            | 460          |
|    policy_gradient_loss | -0.000849    |
|    value_loss           | 2.79         |
------------------------------------------
Eval num_timesteps=18800, episode_reward=22.97 +/- 14.08
Episode length: 34.00 +/- 20.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 23       |
| time/              |          |
|    total_timesteps | 18800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.4     |
|    ep_rew_mean     | 12.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 47       |
|    time_elapsed    | 5398     |
|    total_timesteps | 18800    |
---------------------------------
Eval num_timesteps=19000, episode_reward=22.30 +/- 15.90
Episode length: 32.00 +/- 22.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32            |
|    mean_reward          | 22.3          |
| time/                   |               |
|    total_timesteps      | 19000         |
| train/                  |               |
|    approx_kl            | 0.00087412633 |
|    clip_fraction        | 0.0281        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.523        |
|    explained_variance   | 0.117         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.89          |
|    n_updates            | 470           |
|    policy_gradient_loss | -0.00363      |
|    value_loss           | 3.53          |
-------------------------------------------
Eval num_timesteps=19200, episode_reward=19.65 +/- 12.98
Episode length: 28.60 +/- 18.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.6     |
|    mean_reward     | 19.6     |
| time/              |          |
|    total_timesteps | 19200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.8     |
|    ep_rew_mean     | 13.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 48       |
|    time_elapsed    | 5518     |
|    total_timesteps | 19200    |
---------------------------------
Eval num_timesteps=19400, episode_reward=17.88 +/- 13.19
Episode length: 26.80 +/- 19.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.8          |
|    mean_reward          | 17.9          |
| time/                   |               |
|    total_timesteps      | 19400         |
| train/                  |               |
|    approx_kl            | 0.00064845145 |
|    clip_fraction        | 0.0306        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.529        |
|    explained_variance   | 0.146         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.71          |
|    n_updates            | 480           |
|    policy_gradient_loss | -0.00256      |
|    value_loss           | 2.53          |
-------------------------------------------
Eval num_timesteps=19600, episode_reward=15.58 +/- 16.38
Episode length: 22.40 +/- 22.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 15.6     |
| time/              |          |
|    total_timesteps | 19600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.5     |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 49       |
|    time_elapsed    | 5627     |
|    total_timesteps | 19600    |
---------------------------------
Eval num_timesteps=19800, episode_reward=11.73 +/- 10.89
Episode length: 18.00 +/- 16.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18           |
|    mean_reward          | 11.7         |
| time/                   |              |
|    total_timesteps      | 19800        |
| train/                  |              |
|    approx_kl            | 0.0008976354 |
|    clip_fraction        | 0.0268       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.534       |
|    explained_variance   | 0.215        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.16         |
|    n_updates            | 490          |
|    policy_gradient_loss | -0.0032      |
|    value_loss           | 2.79         |
------------------------------------------
Eval num_timesteps=20000, episode_reward=14.08 +/- 11.73
Episode length: 20.40 +/- 15.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 14.1     |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 15.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 50       |
|    time_elapsed    | 5738     |
|    total_timesteps | 20000    |
---------------------------------
Eval num_timesteps=20200, episode_reward=15.69 +/- 14.91
Episode length: 23.60 +/- 21.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.6          |
|    mean_reward          | 15.7          |
| time/                   |               |
|    total_timesteps      | 20200         |
| train/                  |               |
|    approx_kl            | 0.00093536294 |
|    clip_fraction        | 0.0391        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.491        |
|    explained_variance   | 0.12          |
|    learning_rate        | 0.0001        |
|    loss                 | 1.96          |
|    n_updates            | 500           |
|    policy_gradient_loss | -0.0043       |
|    value_loss           | 2.28          |
-------------------------------------------
Eval num_timesteps=20400, episode_reward=11.18 +/- 12.46
Episode length: 16.60 +/- 17.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | 11.2     |
| time/              |          |
|    total_timesteps | 20400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.2     |
|    ep_rew_mean     | 14.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 51       |
|    time_elapsed    | 5843     |
|    total_timesteps | 20400    |
---------------------------------
Eval num_timesteps=20600, episode_reward=16.46 +/- 15.68
Episode length: 23.80 +/- 21.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.8         |
|    mean_reward          | 16.5         |
| time/                   |              |
|    total_timesteps      | 20600        |
| train/                  |              |
|    approx_kl            | 0.0010856827 |
|    clip_fraction        | 0.0339       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.458       |
|    explained_variance   | 0.138        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.935        |
|    n_updates            | 510          |
|    policy_gradient_loss | -0.00342     |
|    value_loss           | 3.71         |
------------------------------------------
Eval num_timesteps=20800, episode_reward=12.51 +/- 12.09
Episode length: 18.40 +/- 16.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.4     |
|    mean_reward     | 12.5     |
| time/              |          |
|    total_timesteps | 20800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 15       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 52       |
|    time_elapsed    | 5955     |
|    total_timesteps | 20800    |
---------------------------------
Eval num_timesteps=21000, episode_reward=16.03 +/- 11.22
Episode length: 23.80 +/- 15.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.8          |
|    mean_reward          | 16            |
| time/                   |               |
|    total_timesteps      | 21000         |
| train/                  |               |
|    approx_kl            | 0.00046964228 |
|    clip_fraction        | 0.0234        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.426        |
|    explained_variance   | 0.269         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.654         |
|    n_updates            | 520           |
|    policy_gradient_loss | -0.00211      |
|    value_loss           | 2.95          |
-------------------------------------------
Eval num_timesteps=21200, episode_reward=20.57 +/- 15.67
Episode length: 31.20 +/- 23.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.2     |
|    mean_reward     | 20.6     |
| time/              |          |
|    total_timesteps | 21200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.5     |
|    ep_rew_mean     | 14.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 53       |
|    time_elapsed    | 6068     |
|    total_timesteps | 21200    |
---------------------------------
Eval num_timesteps=21400, episode_reward=10.41 +/- 12.10
Episode length: 15.60 +/- 17.30
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.6          |
|    mean_reward          | 10.4          |
| time/                   |               |
|    total_timesteps      | 21400         |
| train/                  |               |
|    approx_kl            | 0.00037027136 |
|    clip_fraction        | 0.00424       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.405        |
|    explained_variance   | 0.326         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.752         |
|    n_updates            | 530           |
|    policy_gradient_loss | -0.00126      |
|    value_loss           | 2.38          |
-------------------------------------------
Eval num_timesteps=21600, episode_reward=5.03 +/- 1.50
Episode length: 8.00 +/- 2.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8        |
|    mean_reward     | 5.03     |
| time/              |          |
|    total_timesteps | 21600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.2     |
|    ep_rew_mean     | 13.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 54       |
|    time_elapsed    | 6156     |
|    total_timesteps | 21600    |
---------------------------------
Eval num_timesteps=21800, episode_reward=27.56 +/- 11.61
Episode length: 41.40 +/- 17.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 41.4          |
|    mean_reward          | 27.6          |
| time/                   |               |
|    total_timesteps      | 21800         |
| train/                  |               |
|    approx_kl            | 0.00018936077 |
|    clip_fraction        | 0.00513       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.381        |
|    explained_variance   | 0.276         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.12          |
|    n_updates            | 540           |
|    policy_gradient_loss | -0.0013       |
|    value_loss           | 3.22          |
-------------------------------------------
Eval num_timesteps=22000, episode_reward=14.72 +/- 11.27
Episode length: 22.20 +/- 16.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 14.7     |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.1     |
|    ep_rew_mean     | 12.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 55       |
|    time_elapsed    | 6286     |
|    total_timesteps | 22000    |
---------------------------------
Eval num_timesteps=22200, episode_reward=16.83 +/- 15.44
Episode length: 24.40 +/- 21.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.4          |
|    mean_reward          | 16.8          |
| time/                   |               |
|    total_timesteps      | 22200         |
| train/                  |               |
|    approx_kl            | 0.00023877675 |
|    clip_fraction        | 0.00402       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.367        |
|    explained_variance   | 0.401         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.23          |
|    n_updates            | 550           |
|    policy_gradient_loss | -0.000948     |
|    value_loss           | 2.42          |
-------------------------------------------
Eval num_timesteps=22400, episode_reward=11.84 +/- 12.29
Episode length: 17.60 +/- 16.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | 11.8     |
| time/              |          |
|    total_timesteps | 22400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.8     |
|    ep_rew_mean     | 13.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 56       |
|    time_elapsed    | 6395     |
|    total_timesteps | 22400    |
---------------------------------
Eval num_timesteps=22600, episode_reward=11.14 +/- 12.33
Episode length: 16.60 +/- 16.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.6          |
|    mean_reward          | 11.1          |
| time/                   |               |
|    total_timesteps      | 22600         |
| train/                  |               |
|    approx_kl            | 0.00015416462 |
|    clip_fraction        | 0.00558       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.363        |
|    explained_variance   | 0.269         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.6           |
|    n_updates            | 560           |
|    policy_gradient_loss | -0.00168      |
|    value_loss           | 2.23          |
-------------------------------------------
Eval num_timesteps=22800, episode_reward=22.45 +/- 16.13
Episode length: 32.00 +/- 22.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32       |
|    mean_reward     | 22.5     |
| time/              |          |
|    total_timesteps | 22800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.2     |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 57       |
|    time_elapsed    | 6503     |
|    total_timesteps | 22800    |
---------------------------------
Eval num_timesteps=23000, episode_reward=16.18 +/- 14.52
Episode length: 24.40 +/- 20.96
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.4          |
|    mean_reward          | 16.2          |
| time/                   |               |
|    total_timesteps      | 23000         |
| train/                  |               |
|    approx_kl            | 0.00029641096 |
|    clip_fraction        | 0.000446      |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.36         |
|    explained_variance   | 0.212         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.405         |
|    n_updates            | 570           |
|    policy_gradient_loss | -0.000978     |
|    value_loss           | 1.74          |
-------------------------------------------
Eval num_timesteps=23200, episode_reward=16.33 +/- 15.42
Episode length: 24.00 +/- 21.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16.3     |
| time/              |          |
|    total_timesteps | 23200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23       |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 58       |
|    time_elapsed    | 6612     |
|    total_timesteps | 23200    |
---------------------------------
Eval num_timesteps=23400, episode_reward=12.96 +/- 11.98
Episode length: 19.40 +/- 17.05
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 19.4           |
|    mean_reward          | 13             |
| time/                   |                |
|    total_timesteps      | 23400          |
| train/                  |                |
|    approx_kl            | 0.000118333846 |
|    clip_fraction        | 0.00179        |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.351         |
|    explained_variance   | 0.177          |
|    learning_rate        | 0.0001         |
|    loss                 | 1.24           |
|    n_updates            | 580            |
|    policy_gradient_loss | -0.0012        |
|    value_loss           | 1.83           |
--------------------------------------------
Eval num_timesteps=23600, episode_reward=26.70 +/- 11.63
Episode length: 37.60 +/- 15.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.6     |
|    mean_reward     | 26.7     |
| time/              |          |
|    total_timesteps | 23600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24       |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 59       |
|    time_elapsed    | 6725     |
|    total_timesteps | 23600    |
---------------------------------
Eval num_timesteps=23800, episode_reward=20.60 +/- 13.11
Episode length: 30.20 +/- 18.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 30.2          |
|    mean_reward          | 20.6          |
| time/                   |               |
|    total_timesteps      | 23800         |
| train/                  |               |
|    approx_kl            | 0.00039597202 |
|    clip_fraction        | 0.01          |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.359        |
|    explained_variance   | 0.24          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.263         |
|    n_updates            | 590           |
|    policy_gradient_loss | -0.000553     |
|    value_loss           | 1.6           |
-------------------------------------------
Eval num_timesteps=24000, episode_reward=10.53 +/- 13.21
Episode length: 15.40 +/- 17.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | 10.5     |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | 17.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 60       |
|    time_elapsed    | 6845     |
|    total_timesteps | 24000    |
---------------------------------
Eval num_timesteps=24200, episode_reward=26.50 +/- 11.18
Episode length: 37.60 +/- 15.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 37.6          |
|    mean_reward          | 26.5          |
| time/                   |               |
|    total_timesteps      | 24200         |
| train/                  |               |
|    approx_kl            | 0.00012489328 |
|    clip_fraction        | 0.00112       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.366        |
|    explained_variance   | 0.315         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.32          |
|    n_updates            | 600           |
|    policy_gradient_loss | -0.000494     |
|    value_loss           | 2.04          |
-------------------------------------------
Eval num_timesteps=24400, episode_reward=16.11 +/- 13.40
Episode length: 23.00 +/- 18.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 16.1     |
| time/              |          |
|    total_timesteps | 24400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | 17.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 61       |
|    time_elapsed    | 6969     |
|    total_timesteps | 24400    |
---------------------------------
Eval num_timesteps=24600, episode_reward=17.13 +/- 14.65
Episode length: 25.20 +/- 20.25
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.2          |
|    mean_reward          | 17.1          |
| time/                   |               |
|    total_timesteps      | 24600         |
| train/                  |               |
|    approx_kl            | 0.00041819017 |
|    clip_fraction        | 0.0096        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.386        |
|    explained_variance   | 0.377         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.05          |
|    n_updates            | 610           |
|    policy_gradient_loss | -0.000984     |
|    value_loss           | 3.01          |
-------------------------------------------
Eval num_timesteps=24800, episode_reward=22.39 +/- 14.43
Episode length: 33.20 +/- 20.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | 22.4     |
| time/              |          |
|    total_timesteps | 24800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.4     |
|    ep_rew_mean     | 18.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 62       |
|    time_elapsed    | 7080     |
|    total_timesteps | 24800    |
---------------------------------
Eval num_timesteps=25000, episode_reward=12.16 +/- 11.74
Episode length: 18.40 +/- 16.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 18.4          |
|    mean_reward          | 12.2          |
| time/                   |               |
|    total_timesteps      | 25000         |
| train/                  |               |
|    approx_kl            | 0.00033925328 |
|    clip_fraction        | 0.00268       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.399        |
|    explained_variance   | 0.0926        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.26          |
|    n_updates            | 620           |
|    policy_gradient_loss | -0.00106      |
|    value_loss           | 3.12          |
-------------------------------------------
Eval num_timesteps=25200, episode_reward=19.25 +/- 14.66
Episode length: 27.20 +/- 19.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.2     |
|    mean_reward     | 19.3     |
| time/              |          |
|    total_timesteps | 25200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.6     |
|    ep_rew_mean     | 15.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 63       |
|    time_elapsed    | 7193     |
|    total_timesteps | 25200    |
---------------------------------
Eval num_timesteps=25400, episode_reward=15.63 +/- 11.67
Episode length: 22.80 +/- 15.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.8          |
|    mean_reward          | 15.6          |
| time/                   |               |
|    total_timesteps      | 25400         |
| train/                  |               |
|    approx_kl            | 2.0598727e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.404        |
|    explained_variance   | 0.33          |
|    learning_rate        | 0.0001        |
|    loss                 | 2.07          |
|    n_updates            | 630           |
|    policy_gradient_loss | -0.000179     |
|    value_loss           | 3             |
-------------------------------------------
Eval num_timesteps=25600, episode_reward=17.97 +/- 14.24
Episode length: 26.40 +/- 19.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.4     |
|    mean_reward     | 18       |
| time/              |          |
|    total_timesteps | 25600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.1     |
|    ep_rew_mean     | 15.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 64       |
|    time_elapsed    | 7309     |
|    total_timesteps | 25600    |
---------------------------------
Eval num_timesteps=25800, episode_reward=3.50 +/- 1.67
Episode length: 5.80 +/- 2.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 5.8           |
|    mean_reward          | 3.5           |
| time/                   |               |
|    total_timesteps      | 25800         |
| train/                  |               |
|    approx_kl            | 0.00035246942 |
|    clip_fraction        | 0.00357       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.435        |
|    explained_variance   | 0.202         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.671         |
|    n_updates            | 640           |
|    policy_gradient_loss | -0.000569     |
|    value_loss           | 1.59          |
-------------------------------------------
Eval num_timesteps=26000, episode_reward=5.60 +/- 2.49
Episode length: 9.20 +/- 3.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.2      |
|    mean_reward     | 5.6      |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.6     |
|    ep_rew_mean     | 15.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 65       |
|    time_elapsed    | 7378     |
|    total_timesteps | 26000    |
---------------------------------
Eval num_timesteps=26200, episode_reward=17.96 +/- 14.15
Episode length: 26.80 +/- 19.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.8          |
|    mean_reward          | 18            |
| time/                   |               |
|    total_timesteps      | 26200         |
| train/                  |               |
|    approx_kl            | 0.00051286444 |
|    clip_fraction        | 0.0422        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.455        |
|    explained_variance   | 0.174         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.53          |
|    n_updates            | 650           |
|    policy_gradient_loss | -0.00202      |
|    value_loss           | 1.67          |
-------------------------------------------
Eval num_timesteps=26400, episode_reward=30.17 +/- 11.40
Episode length: 42.20 +/- 15.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 30.2     |
| time/              |          |
|    total_timesteps | 26400    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.7     |
|    ep_rew_mean     | 16.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 66       |
|    time_elapsed    | 7509     |
|    total_timesteps | 26400    |
---------------------------------
Eval num_timesteps=26600, episode_reward=16.22 +/- 14.97
Episode length: 24.00 +/- 21.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24           |
|    mean_reward          | 16.2         |
| time/                   |              |
|    total_timesteps      | 26600        |
| train/                  |              |
|    approx_kl            | 0.0006021616 |
|    clip_fraction        | 0.0185       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.472       |
|    explained_variance   | 0.141        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.34         |
|    n_updates            | 660          |
|    policy_gradient_loss | -0.00169     |
|    value_loss           | 2.39         |
------------------------------------------
Eval num_timesteps=26800, episode_reward=9.33 +/- 12.72
Episode length: 14.20 +/- 18.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.2     |
|    mean_reward     | 9.33     |
| time/              |          |
|    total_timesteps | 26800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24       |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 67       |
|    time_elapsed    | 7640     |
|    total_timesteps | 26800    |
---------------------------------
Eval num_timesteps=27000, episode_reward=4.98 +/- 2.18
Episode length: 8.20 +/- 3.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 8.2           |
|    mean_reward          | 4.98          |
| time/                   |               |
|    total_timesteps      | 27000         |
| train/                  |               |
|    approx_kl            | 0.00013638914 |
|    clip_fraction        | 0.00201       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.48         |
|    explained_variance   | 0.136         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.398         |
|    n_updates            | 670           |
|    policy_gradient_loss | -0.00101      |
|    value_loss           | 2.93          |
-------------------------------------------
Eval num_timesteps=27200, episode_reward=11.89 +/- 11.95
Episode length: 17.60 +/- 16.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | 11.9     |
| time/              |          |
|    total_timesteps | 27200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25       |
|    ep_rew_mean     | 17.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 68       |
|    time_elapsed    | 7727     |
|    total_timesteps | 27200    |
---------------------------------
Eval num_timesteps=27400, episode_reward=22.66 +/- 15.40
Episode length: 32.60 +/- 21.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32.6          |
|    mean_reward          | 22.7          |
| time/                   |               |
|    total_timesteps      | 27400         |
| train/                  |               |
|    approx_kl            | 0.00014724942 |
|    clip_fraction        | 0.000446      |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.481        |
|    explained_variance   | 0.187         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.59          |
|    n_updates            | 680           |
|    policy_gradient_loss | -0.000749     |
|    value_loss           | 2.24          |
-------------------------------------------
Eval num_timesteps=27600, episode_reward=27.72 +/- 13.53
Episode length: 40.40 +/- 19.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.4     |
|    mean_reward     | 27.7     |
| time/              |          |
|    total_timesteps | 27600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.2     |
|    ep_rew_mean     | 17.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 69       |
|    time_elapsed    | 7832     |
|    total_timesteps | 27600    |
---------------------------------
Eval num_timesteps=27800, episode_reward=13.97 +/- 11.34
Episode length: 21.00 +/- 15.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 21            |
|    mean_reward          | 14            |
| time/                   |               |
|    total_timesteps      | 27800         |
| train/                  |               |
|    approx_kl            | 0.00017507866 |
|    clip_fraction        | 0.00067       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.472        |
|    explained_variance   | 0.246         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.478         |
|    n_updates            | 690           |
|    policy_gradient_loss | -0.000602     |
|    value_loss           | 2.26          |
-------------------------------------------
Eval num_timesteps=28000, episode_reward=10.03 +/- 12.52
Episode length: 15.20 +/- 17.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | 10       |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.3     |
|    ep_rew_mean     | 18.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 70       |
|    time_elapsed    | 7939     |
|    total_timesteps | 28000    |
---------------------------------
Eval num_timesteps=28200, episode_reward=13.02 +/- 12.37
Episode length: 19.00 +/- 17.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 19            |
|    mean_reward          | 13            |
| time/                   |               |
|    total_timesteps      | 28200         |
| train/                  |               |
|    approx_kl            | 0.00066709454 |
|    clip_fraction        | 0.0214        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.468        |
|    explained_variance   | 0.287         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.4           |
|    n_updates            | 700           |
|    policy_gradient_loss | -0.00178      |
|    value_loss           | 2.2           |
-------------------------------------------
Eval num_timesteps=28400, episode_reward=17.10 +/- 14.30
Episode length: 25.20 +/- 20.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 17.1     |
| time/              |          |
|    total_timesteps | 28400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27       |
|    ep_rew_mean     | 18.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 71       |
|    time_elapsed    | 8054     |
|    total_timesteps | 28400    |
---------------------------------
Eval num_timesteps=28600, episode_reward=18.55 +/- 15.51
Episode length: 25.80 +/- 20.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.8         |
|    mean_reward          | 18.5         |
| time/                   |              |
|    total_timesteps      | 28600        |
| train/                  |              |
|    approx_kl            | 0.0008435526 |
|    clip_fraction        | 0.046        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.486       |
|    explained_variance   | 0.283        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.02         |
|    n_updates            | 710          |
|    policy_gradient_loss | -0.00197     |
|    value_loss           | 2.17         |
------------------------------------------
Eval num_timesteps=28800, episode_reward=22.73 +/- 15.33
Episode length: 32.60 +/- 21.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.6     |
|    mean_reward     | 22.7     |
| time/              |          |
|    total_timesteps | 28800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.4     |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 72       |
|    time_elapsed    | 8162     |
|    total_timesteps | 28800    |
---------------------------------
Eval num_timesteps=29000, episode_reward=9.92 +/- 12.97
Episode length: 14.80 +/- 17.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.8         |
|    mean_reward          | 9.92         |
| time/                   |              |
|    total_timesteps      | 29000        |
| train/                  |              |
|    approx_kl            | 0.0009649775 |
|    clip_fraction        | 0.0366       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.498       |
|    explained_variance   | 0.226        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.624        |
|    n_updates            | 720          |
|    policy_gradient_loss | -0.00238     |
|    value_loss           | 2.4          |
------------------------------------------
Eval num_timesteps=29200, episode_reward=10.40 +/- 11.57
Episode length: 16.00 +/- 17.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | 10.4     |
| time/              |          |
|    total_timesteps | 29200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.6     |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 73       |
|    time_elapsed    | 8272     |
|    total_timesteps | 29200    |
---------------------------------
Eval num_timesteps=29400, episode_reward=9.03 +/- 12.47
Episode length: 14.00 +/- 18.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14           |
|    mean_reward          | 9.03         |
| time/                   |              |
|    total_timesteps      | 29400        |
| train/                  |              |
|    approx_kl            | 0.0009971295 |
|    clip_fraction        | 0.0547       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.49        |
|    explained_variance   | 0.341        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.16         |
|    n_updates            | 730          |
|    policy_gradient_loss | -0.00398     |
|    value_loss           | 2.22         |
------------------------------------------
Eval num_timesteps=29600, episode_reward=19.39 +/- 13.41
Episode length: 28.60 +/- 19.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.6     |
|    mean_reward     | 19.4     |
| time/              |          |
|    total_timesteps | 29600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 74       |
|    time_elapsed    | 8378     |
|    total_timesteps | 29600    |
---------------------------------
Eval num_timesteps=29800, episode_reward=15.18 +/- 15.81
Episode length: 22.60 +/- 22.46
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 22.6           |
|    mean_reward          | 15.2           |
| time/                   |                |
|    total_timesteps      | 29800          |
| train/                  |                |
|    approx_kl            | 0.000102537684 |
|    clip_fraction        | 0.00201        |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.486         |
|    explained_variance   | 0.267          |
|    learning_rate        | 0.0001         |
|    loss                 | 1.32           |
|    n_updates            | 740            |
|    policy_gradient_loss | -0.000398      |
|    value_loss           | 2.78           |
--------------------------------------------
Eval num_timesteps=30000, episode_reward=11.38 +/- 12.18
Episode length: 17.00 +/- 16.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | 11.4     |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 75       |
|    time_elapsed    | 8488     |
|    total_timesteps | 30000    |
---------------------------------
Eval num_timesteps=30200, episode_reward=17.12 +/- 14.10
Episode length: 25.80 +/- 20.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.8          |
|    mean_reward          | 17.1          |
| time/                   |               |
|    total_timesteps      | 30200         |
| train/                  |               |
|    approx_kl            | 0.00034343518 |
|    clip_fraction        | 0.0283        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.526        |
|    explained_variance   | 0.107         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.243         |
|    n_updates            | 750           |
|    policy_gradient_loss | -0.00202      |
|    value_loss           | 1.63          |
-------------------------------------------
Eval num_timesteps=30400, episode_reward=14.34 +/- 11.27
Episode length: 21.20 +/- 14.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.2     |
|    mean_reward     | 14.3     |
| time/              |          |
|    total_timesteps | 30400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | 17.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 76       |
|    time_elapsed    | 8607     |
|    total_timesteps | 30400    |
---------------------------------
Eval num_timesteps=30600, episode_reward=7.33 +/- 2.17
Episode length: 11.40 +/- 3.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 11.4         |
|    mean_reward          | 7.33         |
| time/                   |              |
|    total_timesteps      | 30600        |
| train/                  |              |
|    approx_kl            | 0.0006865581 |
|    clip_fraction        | 0.00937      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.499       |
|    explained_variance   | 0.264        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.884        |
|    n_updates            | 760          |
|    policy_gradient_loss | -0.00115     |
|    value_loss           | 1.69         |
------------------------------------------
Eval num_timesteps=30800, episode_reward=17.54 +/- 14.72
Episode length: 25.60 +/- 20.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 17.5     |
| time/              |          |
|    total_timesteps | 30800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.8     |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 77       |
|    time_elapsed    | 8708     |
|    total_timesteps | 30800    |
---------------------------------
Eval num_timesteps=31000, episode_reward=21.72 +/- 16.53
Episode length: 31.20 +/- 23.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 31.2          |
|    mean_reward          | 21.7          |
| time/                   |               |
|    total_timesteps      | 31000         |
| train/                  |               |
|    approx_kl            | 0.00022769028 |
|    clip_fraction        | 0.0161        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.479        |
|    explained_variance   | 0.0833        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.43          |
|    n_updates            | 770           |
|    policy_gradient_loss | -0.00268      |
|    value_loss           | 3.88          |
-------------------------------------------
Eval num_timesteps=31200, episode_reward=13.07 +/- 10.31
Episode length: 19.60 +/- 15.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 13.1     |
| time/              |          |
|    total_timesteps | 31200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | 17.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 78       |
|    time_elapsed    | 8815     |
|    total_timesteps | 31200    |
---------------------------------
Eval num_timesteps=31400, episode_reward=19.39 +/- 12.32
Episode length: 28.80 +/- 17.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 28.8          |
|    mean_reward          | 19.4          |
| time/                   |               |
|    total_timesteps      | 31400         |
| train/                  |               |
|    approx_kl            | 0.00083383237 |
|    clip_fraction        | 0.0158        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.464        |
|    explained_variance   | 0.0122        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.932         |
|    n_updates            | 780           |
|    policy_gradient_loss | -0.00414      |
|    value_loss           | 1.7           |
-------------------------------------------
Eval num_timesteps=31600, episode_reward=22.25 +/- 15.43
Episode length: 32.40 +/- 21.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.4     |
|    mean_reward     | 22.2     |
| time/              |          |
|    total_timesteps | 31600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.8     |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 79       |
|    time_elapsed    | 8924     |
|    total_timesteps | 31600    |
---------------------------------
Eval num_timesteps=31800, episode_reward=11.52 +/- 12.70
Episode length: 16.80 +/- 16.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.8          |
|    mean_reward          | 11.5          |
| time/                   |               |
|    total_timesteps      | 31800         |
| train/                  |               |
|    approx_kl            | 0.00039426543 |
|    clip_fraction        | 0.00402       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.454        |
|    explained_variance   | 0.196         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.63          |
|    n_updates            | 790           |
|    policy_gradient_loss | -0.000812     |
|    value_loss           | 2.33          |
-------------------------------------------
Eval num_timesteps=32000, episode_reward=17.49 +/- 14.78
Episode length: 25.60 +/- 21.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 17.5     |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.6     |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 80       |
|    time_elapsed    | 9042     |
|    total_timesteps | 32000    |
---------------------------------
Eval num_timesteps=32200, episode_reward=19.25 +/- 11.75
Episode length: 29.20 +/- 17.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29.2         |
|    mean_reward          | 19.2         |
| time/                   |              |
|    total_timesteps      | 32200        |
| train/                  |              |
|    approx_kl            | 0.0010061718 |
|    clip_fraction        | 0.0259       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.459       |
|    explained_variance   | 0.27         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.547        |
|    n_updates            | 800          |
|    policy_gradient_loss | -0.0019      |
|    value_loss           | 1.85         |
------------------------------------------
Eval num_timesteps=32400, episode_reward=16.69 +/- 14.98
Episode length: 24.40 +/- 20.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 32400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.4     |
|    ep_rew_mean     | 17.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 81       |
|    time_elapsed    | 9161     |
|    total_timesteps | 32400    |
---------------------------------
Eval num_timesteps=32600, episode_reward=23.81 +/- 15.39
Episode length: 33.20 +/- 20.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.2          |
|    mean_reward          | 23.8          |
| time/                   |               |
|    total_timesteps      | 32600         |
| train/                  |               |
|    approx_kl            | 0.00024289398 |
|    clip_fraction        | 0.00446       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.433        |
|    explained_variance   | 0.351         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.22          |
|    n_updates            | 810           |
|    policy_gradient_loss | -0.00123      |
|    value_loss           | 2.05          |
-------------------------------------------
Eval num_timesteps=32800, episode_reward=4.61 +/- 3.54
Episode length: 7.60 +/- 5.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7.6      |
|    mean_reward     | 4.61     |
| time/              |          |
|    total_timesteps | 32800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.4     |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 82       |
|    time_elapsed    | 9253     |
|    total_timesteps | 32800    |
---------------------------------
Eval num_timesteps=33000, episode_reward=13.70 +/- 12.10
Episode length: 20.00 +/- 16.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 20           |
|    mean_reward          | 13.7         |
| time/                   |              |
|    total_timesteps      | 33000        |
| train/                  |              |
|    approx_kl            | 0.0006069754 |
|    clip_fraction        | 0.0308       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.4         |
|    explained_variance   | 0.228        |
|    learning_rate        | 0.0001       |
|    loss                 | 2            |
|    n_updates            | 820          |
|    policy_gradient_loss | -0.00249     |
|    value_loss           | 3.62         |
------------------------------------------
Eval num_timesteps=33200, episode_reward=4.54 +/- 3.67
Episode length: 7.20 +/- 4.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7.2      |
|    mean_reward     | 4.54     |
| time/              |          |
|    total_timesteps | 33200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.3     |
|    ep_rew_mean     | 14.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 83       |
|    time_elapsed    | 9340     |
|    total_timesteps | 33200    |
---------------------------------
Eval num_timesteps=33400, episode_reward=12.35 +/- 12.65
Episode length: 17.80 +/- 16.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.8         |
|    mean_reward          | 12.4         |
| time/                   |              |
|    total_timesteps      | 33400        |
| train/                  |              |
|    approx_kl            | 5.438658e-05 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.395       |
|    explained_variance   | 0.304        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.52         |
|    n_updates            | 830          |
|    policy_gradient_loss | -0.00102     |
|    value_loss           | 3.15         |
------------------------------------------
Eval num_timesteps=33600, episode_reward=9.81 +/- 12.91
Episode length: 14.40 +/- 17.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.4     |
|    mean_reward     | 9.81     |
| time/              |          |
|    total_timesteps | 33600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.3     |
|    ep_rew_mean     | 14.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 84       |
|    time_elapsed    | 9447     |
|    total_timesteps | 33600    |
---------------------------------
Eval num_timesteps=33800, episode_reward=11.77 +/- 12.12
Episode length: 18.20 +/- 17.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.2         |
|    mean_reward          | 11.8         |
| time/                   |              |
|    total_timesteps      | 33800        |
| train/                  |              |
|    approx_kl            | 0.0005816313 |
|    clip_fraction        | 0.0174       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.391       |
|    explained_variance   | 0.265        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.599        |
|    n_updates            | 840          |
|    policy_gradient_loss | -0.0026      |
|    value_loss           | 2.19         |
------------------------------------------
Eval num_timesteps=34000, episode_reward=35.14 +/- 1.33
Episode length: 50.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 35.1     |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.6     |
|    ep_rew_mean     | 14.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 85       |
|    time_elapsed    | 9584     |
|    total_timesteps | 34000    |
---------------------------------
Eval num_timesteps=34200, episode_reward=21.35 +/- 12.88
Episode length: 30.40 +/- 17.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 30.4          |
|    mean_reward          | 21.4          |
| time/                   |               |
|    total_timesteps      | 34200         |
| train/                  |               |
|    approx_kl            | 0.00048953947 |
|    clip_fraction        | 0.0286        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.409        |
|    explained_variance   | 0.286         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.959         |
|    n_updates            | 850           |
|    policy_gradient_loss | -0.00175      |
|    value_loss           | 1.36          |
-------------------------------------------
Eval num_timesteps=34400, episode_reward=29.55 +/- 10.97
Episode length: 42.40 +/- 15.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 29.5     |
| time/              |          |
|    total_timesteps | 34400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | 14.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 86       |
|    time_elapsed    | 9706     |
|    total_timesteps | 34400    |
---------------------------------
Eval num_timesteps=34600, episode_reward=9.32 +/- 12.69
Episode length: 14.20 +/- 18.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.2          |
|    mean_reward          | 9.32          |
| time/                   |               |
|    total_timesteps      | 34600         |
| train/                  |               |
|    approx_kl            | 0.00067819376 |
|    clip_fraction        | 0.0393        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.382        |
|    explained_variance   | 0.233         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.52          |
|    n_updates            | 860           |
|    policy_gradient_loss | -0.00281      |
|    value_loss           | 2.63          |
-------------------------------------------
Eval num_timesteps=34800, episode_reward=10.59 +/- 13.27
Episode length: 15.60 +/- 17.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 10.6     |
| time/              |          |
|    total_timesteps | 34800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.1     |
|    ep_rew_mean     | 14.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 87       |
|    time_elapsed    | 9811     |
|    total_timesteps | 34800    |
---------------------------------
Eval num_timesteps=35000, episode_reward=17.42 +/- 14.18
Episode length: 25.60 +/- 20.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.6          |
|    mean_reward          | 17.4          |
| time/                   |               |
|    total_timesteps      | 35000         |
| train/                  |               |
|    approx_kl            | 0.00016504953 |
|    clip_fraction        | 0.00379       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.396        |
|    explained_variance   | 0.364         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.55          |
|    n_updates            | 870           |
|    policy_gradient_loss | -0.00115      |
|    value_loss           | 2.11          |
-------------------------------------------
Eval num_timesteps=35200, episode_reward=10.74 +/- 11.57
Episode length: 16.40 +/- 17.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | 10.7     |
| time/              |          |
|    total_timesteps | 35200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.3     |
|    ep_rew_mean     | 16       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 88       |
|    time_elapsed    | 9923     |
|    total_timesteps | 35200    |
---------------------------------
Eval num_timesteps=35400, episode_reward=11.82 +/- 12.04
Episode length: 17.40 +/- 16.49
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17.4          |
|    mean_reward          | 11.8          |
| time/                   |               |
|    total_timesteps      | 35400         |
| train/                  |               |
|    approx_kl            | 0.00042332363 |
|    clip_fraction        | 0.0214        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.409        |
|    explained_variance   | 0.277         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.124         |
|    n_updates            | 880           |
|    policy_gradient_loss | -0.00104      |
|    value_loss           | 1.45          |
-------------------------------------------
Eval num_timesteps=35600, episode_reward=16.69 +/- 15.47
Episode length: 24.00 +/- 21.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 35600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25       |
|    ep_rew_mean     | 17.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 89       |
|    time_elapsed    | 10027    |
|    total_timesteps | 35600    |
---------------------------------
Eval num_timesteps=35800, episode_reward=29.40 +/- 11.85
Episode length: 41.80 +/- 16.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 41.8          |
|    mean_reward          | 29.4          |
| time/                   |               |
|    total_timesteps      | 35800         |
| train/                  |               |
|    approx_kl            | 0.00022022666 |
|    clip_fraction        | 0.0243        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.404        |
|    explained_variance   | 0.23          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.17          |
|    n_updates            | 890           |
|    policy_gradient_loss | -0.0014       |
|    value_loss           | 1.95          |
-------------------------------------------
Eval num_timesteps=36000, episode_reward=23.71 +/- 15.05
Episode length: 33.40 +/- 20.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 23.7     |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.4     |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 90       |
|    time_elapsed    | 10183    |
|    total_timesteps | 36000    |
---------------------------------
Eval num_timesteps=36200, episode_reward=11.44 +/- 12.76
Episode length: 17.00 +/- 17.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17           |
|    mean_reward          | 11.4         |
| time/                   |              |
|    total_timesteps      | 36200        |
| train/                  |              |
|    approx_kl            | 0.0003910622 |
|    clip_fraction        | 0.0096       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.396       |
|    explained_variance   | 0.245        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.789        |
|    n_updates            | 900          |
|    policy_gradient_loss | -0.00267     |
|    value_loss           | 2.36         |
------------------------------------------
Eval num_timesteps=36400, episode_reward=12.57 +/- 13.58
Episode length: 18.00 +/- 18.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | 12.6     |
| time/              |          |
|    total_timesteps | 36400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.7     |
|    ep_rew_mean     | 18.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 91       |
|    time_elapsed    | 10289    |
|    total_timesteps | 36400    |
---------------------------------
Eval num_timesteps=36600, episode_reward=11.69 +/- 12.29
Episode length: 17.00 +/- 16.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17            |
|    mean_reward          | 11.7          |
| time/                   |               |
|    total_timesteps      | 36600         |
| train/                  |               |
|    approx_kl            | 0.00041085694 |
|    clip_fraction        | 0.00781       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.387        |
|    explained_variance   | 0.0412        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.718         |
|    n_updates            | 910           |
|    policy_gradient_loss | -0.00114      |
|    value_loss           | 2.3           |
-------------------------------------------
Eval num_timesteps=36800, episode_reward=19.36 +/- 13.92
Episode length: 27.60 +/- 18.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.6     |
|    mean_reward     | 19.4     |
| time/              |          |
|    total_timesteps | 36800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.9     |
|    ep_rew_mean     | 18.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 92       |
|    time_elapsed    | 10403    |
|    total_timesteps | 36800    |
---------------------------------
Eval num_timesteps=37000, episode_reward=4.02 +/- 2.35
Episode length: 6.80 +/- 3.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 6.8           |
|    mean_reward          | 4.02          |
| time/                   |               |
|    total_timesteps      | 37000         |
| train/                  |               |
|    approx_kl            | 0.00045713715 |
|    clip_fraction        | 0.0152        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.379        |
|    explained_variance   | 0.267         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.01          |
|    n_updates            | 920           |
|    policy_gradient_loss | -0.000685     |
|    value_loss           | 1.64          |
-------------------------------------------
Eval num_timesteps=37200, episode_reward=29.15 +/- 13.45
Episode length: 40.80 +/- 18.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.8     |
|    mean_reward     | 29.1     |
| time/              |          |
|    total_timesteps | 37200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.1     |
|    ep_rew_mean     | 18.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 93       |
|    time_elapsed    | 10490    |
|    total_timesteps | 37200    |
---------------------------------
Eval num_timesteps=37400, episode_reward=11.43 +/- 12.16
Episode length: 17.00 +/- 17.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17            |
|    mean_reward          | 11.4          |
| time/                   |               |
|    total_timesteps      | 37400         |
| train/                  |               |
|    approx_kl            | 0.00031008106 |
|    clip_fraction        | 0.00558       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.37         |
|    explained_variance   | 0.297         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.16          |
|    n_updates            | 930           |
|    policy_gradient_loss | -0.00177      |
|    value_loss           | 1.7           |
-------------------------------------------
Eval num_timesteps=37600, episode_reward=22.56 +/- 15.97
Episode length: 32.00 +/- 22.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32       |
|    mean_reward     | 22.6     |
| time/              |          |
|    total_timesteps | 37600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.6     |
|    ep_rew_mean     | 19       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 94       |
|    time_elapsed    | 10620    |
|    total_timesteps | 37600    |
---------------------------------
Eval num_timesteps=37800, episode_reward=24.34 +/- 14.02
Episode length: 34.60 +/- 19.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.6          |
|    mean_reward          | 24.3          |
| time/                   |               |
|    total_timesteps      | 37800         |
| train/                  |               |
|    approx_kl            | 0.00048434056 |
|    clip_fraction        | 0.0127        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.361        |
|    explained_variance   | 0.23          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.807         |
|    n_updates            | 940           |
|    policy_gradient_loss | -0.00224      |
|    value_loss           | 1.89          |
-------------------------------------------
Eval num_timesteps=38000, episode_reward=17.64 +/- 15.20
Episode length: 25.40 +/- 20.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.7     |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 95       |
|    time_elapsed    | 10744    |
|    total_timesteps | 38000    |
---------------------------------
Eval num_timesteps=38200, episode_reward=3.73 +/- 2.03
Episode length: 6.40 +/- 3.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 6.4           |
|    mean_reward          | 3.73          |
| time/                   |               |
|    total_timesteps      | 38200         |
| train/                  |               |
|    approx_kl            | 0.00015356511 |
|    clip_fraction        | 0.00313       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.353        |
|    explained_variance   | 0.263         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.06          |
|    n_updates            | 950           |
|    policy_gradient_loss | -0.00174      |
|    value_loss           | 3.06          |
-------------------------------------------
Eval num_timesteps=38400, episode_reward=22.93 +/- 15.08
Episode length: 32.80 +/- 21.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 22.9     |
| time/              |          |
|    total_timesteps | 38400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.5     |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 96       |
|    time_elapsed    | 10834    |
|    total_timesteps | 38400    |
---------------------------------
Eval num_timesteps=38600, episode_reward=4.63 +/- 1.22
Episode length: 7.60 +/- 1.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 7.6           |
|    mean_reward          | 4.63          |
| time/                   |               |
|    total_timesteps      | 38600         |
| train/                  |               |
|    approx_kl            | 0.00015480151 |
|    clip_fraction        | 0.0129        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.389        |
|    explained_variance   | 0.163         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.0728        |
|    n_updates            | 960           |
|    policy_gradient_loss | -0.000843     |
|    value_loss           | 1.21          |
-------------------------------------------
Eval num_timesteps=38800, episode_reward=16.60 +/- 14.62
Episode length: 24.60 +/- 20.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 38800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.5     |
|    ep_rew_mean     | 18.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 97       |
|    time_elapsed    | 10920    |
|    total_timesteps | 38800    |
---------------------------------
Eval num_timesteps=39000, episode_reward=15.29 +/- 10.74
Episode length: 22.40 +/- 14.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.4          |
|    mean_reward          | 15.3          |
| time/                   |               |
|    total_timesteps      | 39000         |
| train/                  |               |
|    approx_kl            | 0.00041444827 |
|    clip_fraction        | 0.0129        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.354        |
|    explained_variance   | 0.207         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.37          |
|    n_updates            | 970           |
|    policy_gradient_loss | -0.00214      |
|    value_loss           | 2.32          |
-------------------------------------------
Eval num_timesteps=39200, episode_reward=13.30 +/- 12.35
Episode length: 19.00 +/- 16.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 13.3     |
| time/              |          |
|    total_timesteps | 39200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.2     |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 98       |
|    time_elapsed    | 11029    |
|    total_timesteps | 39200    |
---------------------------------
Eval num_timesteps=39400, episode_reward=24.04 +/- 13.24
Episode length: 35.00 +/- 18.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35           |
|    mean_reward          | 24           |
| time/                   |              |
|    total_timesteps      | 39400        |
| train/                  |              |
|    approx_kl            | 0.0003350566 |
|    clip_fraction        | 0.00558      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.344       |
|    explained_variance   | 0.123        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.54         |
|    n_updates            | 980          |
|    policy_gradient_loss | -0.00153     |
|    value_loss           | 3.4          |
------------------------------------------
Eval num_timesteps=39600, episode_reward=12.03 +/- 12.72
Episode length: 17.40 +/- 16.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 12       |
| time/              |          |
|    total_timesteps | 39600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.8     |
|    ep_rew_mean     | 17       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 99       |
|    time_elapsed    | 11161    |
|    total_timesteps | 39600    |
---------------------------------
Eval num_timesteps=39800, episode_reward=25.69 +/- 12.45
Episode length: 36.60 +/- 16.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.6          |
|    mean_reward          | 25.7          |
| time/                   |               |
|    total_timesteps      | 39800         |
| train/                  |               |
|    approx_kl            | 0.00035155565 |
|    clip_fraction        | 0.0161        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.341        |
|    explained_variance   | 0.191         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.69          |
|    n_updates            | 990           |
|    policy_gradient_loss | -0.00127      |
|    value_loss           | 1.45          |
-------------------------------------------
Eval num_timesteps=40000, episode_reward=4.72 +/- 2.29
Episode length: 7.60 +/- 3.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7.6      |
|    mean_reward     | 4.72     |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.2     |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 100      |
|    time_elapsed    | 11253    |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=40200, episode_reward=22.89 +/- 14.79
Episode length: 33.20 +/- 20.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.2         |
|    mean_reward          | 22.9         |
| time/                   |              |
|    total_timesteps      | 40200        |
| train/                  |              |
|    approx_kl            | 0.0002084868 |
|    clip_fraction        | 0.00379      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.322       |
|    explained_variance   | 0.172        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.48         |
|    n_updates            | 1000         |
|    policy_gradient_loss | -0.00131     |
|    value_loss           | 3.5          |
------------------------------------------
Eval num_timesteps=40400, episode_reward=28.43 +/- 11.56
Episode length: 41.80 +/- 16.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 28.4     |
| time/              |          |
|    total_timesteps | 40400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | 15.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 101      |
|    time_elapsed    | 11385    |
|    total_timesteps | 40400    |
---------------------------------
Eval num_timesteps=40600, episode_reward=17.74 +/- 14.92
Episode length: 25.60 +/- 20.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.6          |
|    mean_reward          | 17.7          |
| time/                   |               |
|    total_timesteps      | 40600         |
| train/                  |               |
|    approx_kl            | 0.00018454956 |
|    clip_fraction        | 0.00446       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.31         |
|    explained_variance   | 0.348         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.62          |
|    n_updates            | 1010          |
|    policy_gradient_loss | -0.00235      |
|    value_loss           | 2.15          |
-------------------------------------------
Eval num_timesteps=40800, episode_reward=14.85 +/- 15.60
Episode length: 22.40 +/- 22.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 14.8     |
| time/              |          |
|    total_timesteps | 40800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.5     |
|    ep_rew_mean     | 13.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 102      |
|    time_elapsed    | 11492    |
|    total_timesteps | 40800    |
---------------------------------
Eval num_timesteps=41000, episode_reward=16.75 +/- 14.50
Episode length: 24.80 +/- 20.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.8          |
|    mean_reward          | 16.7          |
| time/                   |               |
|    total_timesteps      | 41000         |
| train/                  |               |
|    approx_kl            | 0.00039589632 |
|    clip_fraction        | 0.0132        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.308        |
|    explained_variance   | 0.346         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.446         |
|    n_updates            | 1020          |
|    policy_gradient_loss | -0.00278      |
|    value_loss           | 2.9           |
-------------------------------------------
Eval num_timesteps=41200, episode_reward=22.88 +/- 15.70
Episode length: 32.60 +/- 21.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.6     |
|    mean_reward     | 22.9     |
| time/              |          |
|    total_timesteps | 41200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.7     |
|    ep_rew_mean     | 14.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 103      |
|    time_elapsed    | 11625    |
|    total_timesteps | 41200    |
---------------------------------
Eval num_timesteps=41400, episode_reward=16.27 +/- 15.37
Episode length: 23.80 +/- 21.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.8          |
|    mean_reward          | 16.3          |
| time/                   |               |
|    total_timesteps      | 41400         |
| train/                  |               |
|    approx_kl            | 0.00063573197 |
|    clip_fraction        | 0.0388        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.34         |
|    explained_variance   | 0.27          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.106         |
|    n_updates            | 1030          |
|    policy_gradient_loss | -0.00283      |
|    value_loss           | 1.37          |
-------------------------------------------
Eval num_timesteps=41600, episode_reward=19.56 +/- 13.97
Episode length: 28.20 +/- 19.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.2     |
|    mean_reward     | 19.6     |
| time/              |          |
|    total_timesteps | 41600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.3     |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 104      |
|    time_elapsed    | 11761    |
|    total_timesteps | 41600    |
---------------------------------
Eval num_timesteps=41800, episode_reward=10.37 +/- 13.20
Episode length: 15.00 +/- 17.55
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15            |
|    mean_reward          | 10.4          |
| time/                   |               |
|    total_timesteps      | 41800         |
| train/                  |               |
|    approx_kl            | 0.00012443867 |
|    clip_fraction        | 0.00313       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.334        |
|    explained_variance   | 0.31          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.437         |
|    n_updates            | 1040          |
|    policy_gradient_loss | -0.000965     |
|    value_loss           | 1.58          |
-------------------------------------------
Eval num_timesteps=42000, episode_reward=8.54 +/- 13.03
Episode length: 13.00 +/- 18.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13       |
|    mean_reward     | 8.54     |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 105      |
|    time_elapsed    | 11871    |
|    total_timesteps | 42000    |
---------------------------------
Eval num_timesteps=42200, episode_reward=17.45 +/- 14.40
Episode length: 25.40 +/- 20.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.4         |
|    mean_reward          | 17.4         |
| time/                   |              |
|    total_timesteps      | 42200        |
| train/                  |              |
|    approx_kl            | 0.0004040499 |
|    clip_fraction        | 0.0096       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.351       |
|    explained_variance   | 0.306        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.637        |
|    n_updates            | 1050         |
|    policy_gradient_loss | -0.00122     |
|    value_loss           | 2.16         |
------------------------------------------
Eval num_timesteps=42400, episode_reward=23.74 +/- 14.97
Episode length: 33.40 +/- 20.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 23.7     |
| time/              |          |
|    total_timesteps | 42400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.3     |
|    ep_rew_mean     | 16       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 106      |
|    time_elapsed    | 11979    |
|    total_timesteps | 42400    |
---------------------------------
Eval num_timesteps=42600, episode_reward=23.27 +/- 14.65
Episode length: 33.60 +/- 20.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.6          |
|    mean_reward          | 23.3          |
| time/                   |               |
|    total_timesteps      | 42600         |
| train/                  |               |
|    approx_kl            | 0.00016479034 |
|    clip_fraction        | 0.00848       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.36         |
|    explained_variance   | 0.347         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.517         |
|    n_updates            | 1060          |
|    policy_gradient_loss | -0.00237      |
|    value_loss           | 2.4           |
-------------------------------------------
Eval num_timesteps=42800, episode_reward=18.61 +/- 14.02
Episode length: 27.60 +/- 20.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.6     |
|    mean_reward     | 18.6     |
| time/              |          |
|    total_timesteps | 42800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.6     |
|    ep_rew_mean     | 15.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 107      |
|    time_elapsed    | 12087    |
|    total_timesteps | 42800    |
---------------------------------
Eval num_timesteps=43000, episode_reward=9.89 +/- 11.87
Episode length: 15.40 +/- 17.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.4          |
|    mean_reward          | 9.89          |
| time/                   |               |
|    total_timesteps      | 43000         |
| train/                  |               |
|    approx_kl            | 0.00020422481 |
|    clip_fraction        | 0.00357       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.368        |
|    explained_variance   | 0.419         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.07          |
|    n_updates            | 1070          |
|    policy_gradient_loss | -0.00111      |
|    value_loss           | 2.33          |
-------------------------------------------
Eval num_timesteps=43200, episode_reward=12.37 +/- 12.70
Episode length: 18.20 +/- 16.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | 12.4     |
| time/              |          |
|    total_timesteps | 43200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.3     |
|    ep_rew_mean     | 16.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 108      |
|    time_elapsed    | 12192    |
|    total_timesteps | 43200    |
---------------------------------
Eval num_timesteps=43400, episode_reward=22.85 +/- 13.82
Episode length: 33.80 +/- 19.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.8          |
|    mean_reward          | 22.9          |
| time/                   |               |
|    total_timesteps      | 43400         |
| train/                  |               |
|    approx_kl            | 0.00041310064 |
|    clip_fraction        | 0.0154        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.385        |
|    explained_variance   | 0.382         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.664         |
|    n_updates            | 1080          |
|    policy_gradient_loss | -0.00182      |
|    value_loss           | 1.53          |
-------------------------------------------
Eval num_timesteps=43600, episode_reward=11.15 +/- 12.29
Episode length: 16.40 +/- 16.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | 11.2     |
| time/              |          |
|    total_timesteps | 43600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.4     |
|    ep_rew_mean     | 14.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 109      |
|    time_elapsed    | 12310    |
|    total_timesteps | 43600    |
---------------------------------
Eval num_timesteps=43800, episode_reward=10.25 +/- 11.74
Episode length: 16.00 +/- 17.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | 10.3         |
| time/                   |              |
|    total_timesteps      | 43800        |
| train/                  |              |
|    approx_kl            | 4.948822e-05 |
|    clip_fraction        | 0.00067      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.38        |
|    explained_variance   | 0.321        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.51         |
|    n_updates            | 1090         |
|    policy_gradient_loss | -0.000574    |
|    value_loss           | 3.48         |
------------------------------------------
Eval num_timesteps=44000, episode_reward=24.33 +/- 12.63
Episode length: 35.60 +/- 17.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 24.3     |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.8     |
|    ep_rew_mean     | 13.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 110      |
|    time_elapsed    | 12420    |
|    total_timesteps | 44000    |
---------------------------------
Eval num_timesteps=44200, episode_reward=22.04 +/- 15.23
Episode length: 32.20 +/- 21.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32.2          |
|    mean_reward          | 22            |
| time/                   |               |
|    total_timesteps      | 44200         |
| train/                  |               |
|    approx_kl            | 0.00022312945 |
|    clip_fraction        | 0.00313       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.393        |
|    explained_variance   | 0.387         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.904         |
|    n_updates            | 1100          |
|    policy_gradient_loss | -0.00127      |
|    value_loss           | 2.28          |
-------------------------------------------
Eval num_timesteps=44400, episode_reward=28.36 +/- 12.80
Episode length: 41.00 +/- 18.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41       |
|    mean_reward     | 28.4     |
| time/              |          |
|    total_timesteps | 44400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.5     |
|    ep_rew_mean     | 12.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 111      |
|    time_elapsed    | 12531    |
|    total_timesteps | 44400    |
---------------------------------
Eval num_timesteps=44600, episode_reward=10.89 +/- 12.56
Episode length: 16.20 +/- 17.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.2          |
|    mean_reward          | 10.9          |
| time/                   |               |
|    total_timesteps      | 44600         |
| train/                  |               |
|    approx_kl            | 0.00014674118 |
|    clip_fraction        | 0.00491       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.386        |
|    explained_variance   | 0.494         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.544         |
|    n_updates            | 1110          |
|    policy_gradient_loss | -0.00104      |
|    value_loss           | 2.23          |
-------------------------------------------
Eval num_timesteps=44800, episode_reward=16.75 +/- 14.65
Episode length: 24.60 +/- 20.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 44800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.1     |
|    ep_rew_mean     | 14.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 112      |
|    time_elapsed    | 12638    |
|    total_timesteps | 44800    |
---------------------------------
Eval num_timesteps=45000, episode_reward=22.63 +/- 16.37
Episode length: 31.80 +/- 22.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 31.8         |
|    mean_reward          | 22.6         |
| time/                   |              |
|    total_timesteps      | 45000        |
| train/                  |              |
|    approx_kl            | 0.0005492424 |
|    clip_fraction        | 0.0453       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.42        |
|    explained_variance   | 0.0841       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.202        |
|    n_updates            | 1120         |
|    policy_gradient_loss | -0.00175     |
|    value_loss           | 0.604        |
------------------------------------------
Eval num_timesteps=45200, episode_reward=11.13 +/- 12.80
Episode length: 16.00 +/- 17.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | 11.1     |
| time/              |          |
|    total_timesteps | 45200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | 13.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 113      |
|    time_elapsed    | 12770    |
|    total_timesteps | 45200    |
---------------------------------
Eval num_timesteps=45400, episode_reward=16.09 +/- 15.58
Episode length: 23.60 +/- 21.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.6          |
|    mean_reward          | 16.1          |
| time/                   |               |
|    total_timesteps      | 45400         |
| train/                  |               |
|    approx_kl            | 0.00052049785 |
|    clip_fraction        | 0.0152        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.426        |
|    explained_variance   | 0.17          |
|    learning_rate        | 0.0001        |
|    loss                 | 2.27          |
|    n_updates            | 1130          |
|    policy_gradient_loss | -0.00252      |
|    value_loss           | 3.1           |
-------------------------------------------
Eval num_timesteps=45600, episode_reward=10.21 +/- 11.62
Episode length: 16.00 +/- 17.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | 10.2     |
| time/              |          |
|    total_timesteps | 45600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.2     |
|    ep_rew_mean     | 14.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 114      |
|    time_elapsed    | 12880    |
|    total_timesteps | 45600    |
---------------------------------
Eval num_timesteps=45800, episode_reward=10.92 +/- 12.69
Episode length: 16.40 +/- 17.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | 10.9         |
| time/                   |              |
|    total_timesteps      | 45800        |
| train/                  |              |
|    approx_kl            | 0.0006588836 |
|    clip_fraction        | 0.0187       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.443       |
|    explained_variance   | 0.397        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.48         |
|    n_updates            | 1140         |
|    policy_gradient_loss | -0.00128     |
|    value_loss           | 2.21         |
------------------------------------------
Eval num_timesteps=46000, episode_reward=16.88 +/- 15.01
Episode length: 24.60 +/- 20.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.7     |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 115      |
|    time_elapsed    | 12984    |
|    total_timesteps | 46000    |
---------------------------------
Eval num_timesteps=46200, episode_reward=22.95 +/- 14.63
Episode length: 33.20 +/- 20.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.2         |
|    mean_reward          | 23           |
| time/                   |              |
|    total_timesteps      | 46200        |
| train/                  |              |
|    approx_kl            | 0.0004684429 |
|    clip_fraction        | 0.00826      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.475       |
|    explained_variance   | 0.314        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.963        |
|    n_updates            | 1150         |
|    policy_gradient_loss | -0.00193     |
|    value_loss           | 2.77         |
------------------------------------------
Eval num_timesteps=46400, episode_reward=21.24 +/- 16.22
Episode length: 31.20 +/- 23.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.2     |
|    mean_reward     | 21.2     |
| time/              |          |
|    total_timesteps | 46400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.2     |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 116      |
|    time_elapsed    | 13120    |
|    total_timesteps | 46400    |
---------------------------------
Eval num_timesteps=46600, episode_reward=19.13 +/- 14.71
Episode length: 27.00 +/- 19.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 27          |
|    mean_reward          | 19.1        |
| time/                   |             |
|    total_timesteps      | 46600       |
| train/                  |             |
|    approx_kl            | 0.000842414 |
|    clip_fraction        | 0.0167      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.46       |
|    explained_variance   | 0.356       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.04        |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.00243    |
|    value_loss           | 2.78        |
-----------------------------------------
Eval num_timesteps=46800, episode_reward=9.61 +/- 12.54
Episode length: 14.60 +/- 17.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 9.61     |
| time/              |          |
|    total_timesteps | 46800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.7     |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 117      |
|    time_elapsed    | 13229    |
|    total_timesteps | 46800    |
---------------------------------
Eval num_timesteps=47000, episode_reward=10.42 +/- 11.65
Episode length: 16.00 +/- 17.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16            |
|    mean_reward          | 10.4          |
| time/                   |               |
|    total_timesteps      | 47000         |
| train/                  |               |
|    approx_kl            | 0.00090422045 |
|    clip_fraction        | 0.0297        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.439        |
|    explained_variance   | 0.261         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.36          |
|    n_updates            | 1170          |
|    policy_gradient_loss | -0.00119      |
|    value_loss           | 2.27          |
-------------------------------------------
Eval num_timesteps=47200, episode_reward=9.94 +/- 11.89
Episode length: 15.40 +/- 17.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | 9.94     |
| time/              |          |
|    total_timesteps | 47200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.1     |
|    ep_rew_mean     | 13.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 118      |
|    time_elapsed    | 13340    |
|    total_timesteps | 47200    |
---------------------------------
Eval num_timesteps=47400, episode_reward=28.83 +/- 14.08
Episode length: 40.40 +/- 19.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 40.4         |
|    mean_reward          | 28.8         |
| time/                   |              |
|    total_timesteps      | 47400        |
| train/                  |              |
|    approx_kl            | 0.0007040518 |
|    clip_fraction        | 0.0228       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.439       |
|    explained_variance   | 0.309        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.02         |
|    n_updates            | 1180         |
|    policy_gradient_loss | -0.0015      |
|    value_loss           | 3.04         |
------------------------------------------
Eval num_timesteps=47600, episode_reward=16.97 +/- 15.73
Episode length: 24.20 +/- 21.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 47600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.6     |
|    ep_rew_mean     | 14.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 119      |
|    time_elapsed    | 13445    |
|    total_timesteps | 47600    |
---------------------------------
Eval num_timesteps=47800, episode_reward=17.66 +/- 14.28
Episode length: 25.80 +/- 19.85
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.8          |
|    mean_reward          | 17.7          |
| time/                   |               |
|    total_timesteps      | 47800         |
| train/                  |               |
|    approx_kl            | 0.00071876537 |
|    clip_fraction        | 0.0179        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.431        |
|    explained_variance   | 0.138         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.904         |
|    n_updates            | 1190          |
|    policy_gradient_loss | -0.00141      |
|    value_loss           | 1.44          |
-------------------------------------------
Eval num_timesteps=48000, episode_reward=19.23 +/- 13.92
Episode length: 27.20 +/- 18.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.2     |
|    mean_reward     | 19.2     |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.4     |
|    ep_rew_mean     | 14.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 120      |
|    time_elapsed    | 13553    |
|    total_timesteps | 48000    |
---------------------------------
Eval num_timesteps=48200, episode_reward=11.13 +/- 11.85
Episode length: 16.60 +/- 16.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.6          |
|    mean_reward          | 11.1          |
| time/                   |               |
|    total_timesteps      | 48200         |
| train/                  |               |
|    approx_kl            | 0.00058191933 |
|    clip_fraction        | 0.0152        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.427        |
|    explained_variance   | 0.296         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.882         |
|    n_updates            | 1200          |
|    policy_gradient_loss | -0.00141      |
|    value_loss           | 2.95          |
-------------------------------------------
Eval num_timesteps=48400, episode_reward=28.07 +/- 13.38
Episode length: 40.60 +/- 18.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.6     |
|    mean_reward     | 28.1     |
| time/              |          |
|    total_timesteps | 48400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.2     |
|    ep_rew_mean     | 15.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 121      |
|    time_elapsed    | 13663    |
|    total_timesteps | 48400    |
---------------------------------
Eval num_timesteps=48600, episode_reward=11.12 +/- 11.83
Episode length: 16.80 +/- 16.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.8          |
|    mean_reward          | 11.1          |
| time/                   |               |
|    total_timesteps      | 48600         |
| train/                  |               |
|    approx_kl            | 0.00047947798 |
|    clip_fraction        | 0.0112        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.427        |
|    explained_variance   | 0.337         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.39          |
|    n_updates            | 1210          |
|    policy_gradient_loss | -0.00183      |
|    value_loss           | 1.84          |
-------------------------------------------
Eval num_timesteps=48800, episode_reward=12.83 +/- 13.07
Episode length: 19.00 +/- 18.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 12.8     |
| time/              |          |
|    total_timesteps | 48800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.7     |
|    ep_rew_mean     | 15.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 122      |
|    time_elapsed    | 13772    |
|    total_timesteps | 48800    |
---------------------------------
Eval num_timesteps=49000, episode_reward=18.35 +/- 13.33
Episode length: 27.40 +/- 19.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 27.4          |
|    mean_reward          | 18.4          |
| time/                   |               |
|    total_timesteps      | 49000         |
| train/                  |               |
|    approx_kl            | 0.00016424859 |
|    clip_fraction        | 0.00357       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.425        |
|    explained_variance   | 0.396         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.536         |
|    n_updates            | 1220          |
|    policy_gradient_loss | -0.000935     |
|    value_loss           | 1.82          |
-------------------------------------------
Eval num_timesteps=49200, episode_reward=10.58 +/- 12.36
Episode length: 16.00 +/- 17.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | 10.6     |
| time/              |          |
|    total_timesteps | 49200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.3     |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 123      |
|    time_elapsed    | 13903    |
|    total_timesteps | 49200    |
---------------------------------
Eval num_timesteps=49400, episode_reward=5.84 +/- 3.70
Episode length: 9.00 +/- 4.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 9            |
|    mean_reward          | 5.84         |
| time/                   |              |
|    total_timesteps      | 49400        |
| train/                  |              |
|    approx_kl            | 0.0006508463 |
|    clip_fraction        | 0.0292       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.409       |
|    explained_variance   | 0.362        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.993        |
|    n_updates            | 1230         |
|    policy_gradient_loss | -0.00177     |
|    value_loss           | 2.56         |
------------------------------------------
Eval num_timesteps=49600, episode_reward=12.06 +/- 11.82
Episode length: 17.60 +/- 16.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | 12.1     |
| time/              |          |
|    total_timesteps | 49600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 124      |
|    time_elapsed    | 13992    |
|    total_timesteps | 49600    |
---------------------------------
Eval num_timesteps=49800, episode_reward=14.22 +/- 11.65
Episode length: 21.20 +/- 16.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 21.2          |
|    mean_reward          | 14.2          |
| time/                   |               |
|    total_timesteps      | 49800         |
| train/                  |               |
|    approx_kl            | 0.00017491516 |
|    clip_fraction        | 0.00469       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.405        |
|    explained_variance   | 0.203         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.94          |
|    n_updates            | 1240          |
|    policy_gradient_loss | -0.00189      |
|    value_loss           | 2.81          |
-------------------------------------------
Eval num_timesteps=50000, episode_reward=17.61 +/- 13.98
Episode length: 26.20 +/- 20.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.7     |
|    ep_rew_mean     | 14.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 125      |
|    time_elapsed    | 14110    |
|    total_timesteps | 50000    |
---------------------------------
Eval num_timesteps=50200, episode_reward=17.88 +/- 14.07
Episode length: 26.40 +/- 19.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.4          |
|    mean_reward          | 17.9          |
| time/                   |               |
|    total_timesteps      | 50200         |
| train/                  |               |
|    approx_kl            | 0.00026517228 |
|    clip_fraction        | 0.00937       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.384        |
|    explained_variance   | 0.337         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.663         |
|    n_updates            | 1250          |
|    policy_gradient_loss | -0.00182      |
|    value_loss           | 2.53          |
-------------------------------------------
Eval num_timesteps=50400, episode_reward=9.98 +/- 12.38
Episode length: 15.00 +/- 17.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | 9.98     |
| time/              |          |
|    total_timesteps | 50400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.4     |
|    ep_rew_mean     | 14.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 126      |
|    time_elapsed    | 14229    |
|    total_timesteps | 50400    |
---------------------------------
Eval num_timesteps=50600, episode_reward=17.91 +/- 15.30
Episode length: 25.40 +/- 20.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.4         |
|    mean_reward          | 17.9         |
| time/                   |              |
|    total_timesteps      | 50600        |
| train/                  |              |
|    approx_kl            | 0.0006969978 |
|    clip_fraction        | 0.0377       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.357       |
|    explained_variance   | 0.244        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.816        |
|    n_updates            | 1260         |
|    policy_gradient_loss | -0.00303     |
|    value_loss           | 2.53         |
------------------------------------------
Eval num_timesteps=50800, episode_reward=18.84 +/- 12.80
Episode length: 28.20 +/- 18.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.2     |
|    mean_reward     | 18.8     |
| time/              |          |
|    total_timesteps | 50800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.8     |
|    ep_rew_mean     | 15.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 127      |
|    time_elapsed    | 14333    |
|    total_timesteps | 50800    |
---------------------------------
Eval num_timesteps=51000, episode_reward=16.38 +/- 15.03
Episode length: 24.20 +/- 21.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.2          |
|    mean_reward          | 16.4          |
| time/                   |               |
|    total_timesteps      | 51000         |
| train/                  |               |
|    approx_kl            | 0.00029408906 |
|    clip_fraction        | 0.0127        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.343        |
|    explained_variance   | 0.305         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.719         |
|    n_updates            | 1270          |
|    policy_gradient_loss | -0.00211      |
|    value_loss           | 1.11          |
-------------------------------------------
Eval num_timesteps=51200, episode_reward=8.17 +/- 6.98
Episode length: 12.60 +/- 9.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 12.6     |
|    mean_reward     | 8.17     |
| time/              |          |
|    total_timesteps | 51200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23       |
|    ep_rew_mean     | 15.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 128      |
|    time_elapsed    | 14432    |
|    total_timesteps | 51200    |
---------------------------------
Eval num_timesteps=51400, episode_reward=12.25 +/- 12.48
Episode length: 18.00 +/- 16.43
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 18            |
|    mean_reward          | 12.2          |
| time/                   |               |
|    total_timesteps      | 51400         |
| train/                  |               |
|    approx_kl            | 0.00013631755 |
|    clip_fraction        | 0.0029        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.342        |
|    explained_variance   | 0.0778        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.446         |
|    n_updates            | 1280          |
|    policy_gradient_loss | -0.00128      |
|    value_loss           | 1.94          |
-------------------------------------------
Eval num_timesteps=51600, episode_reward=5.49 +/- 3.50
Episode length: 8.60 +/- 4.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8.6      |
|    mean_reward     | 5.49     |
| time/              |          |
|    total_timesteps | 51600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | 16.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 129      |
|    time_elapsed    | 14523    |
|    total_timesteps | 51600    |
---------------------------------
Eval num_timesteps=51800, episode_reward=16.69 +/- 15.00
Episode length: 24.40 +/- 20.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.4          |
|    mean_reward          | 16.7          |
| time/                   |               |
|    total_timesteps      | 51800         |
| train/                  |               |
|    approx_kl            | 0.00051991176 |
|    clip_fraction        | 0.0292        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.308        |
|    explained_variance   | 0.169         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.691         |
|    n_updates            | 1290          |
|    policy_gradient_loss | -0.0027       |
|    value_loss           | 2.17          |
-------------------------------------------
Eval num_timesteps=52000, episode_reward=12.68 +/- 12.56
Episode length: 18.40 +/- 16.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.4     |
|    mean_reward     | 12.7     |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.2     |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 130      |
|    time_elapsed    | 14641    |
|    total_timesteps | 52000    |
---------------------------------
Eval num_timesteps=52200, episode_reward=15.83 +/- 16.61
Episode length: 22.60 +/- 22.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.6          |
|    mean_reward          | 15.8          |
| time/                   |               |
|    total_timesteps      | 52200         |
| train/                  |               |
|    approx_kl            | 0.00029847145 |
|    clip_fraction        | 0.0161        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.293        |
|    explained_variance   | 0.312         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.194         |
|    n_updates            | 1300          |
|    policy_gradient_loss | -0.00109      |
|    value_loss           | 1.58          |
-------------------------------------------
Eval num_timesteps=52400, episode_reward=3.86 +/- 1.28
Episode length: 6.60 +/- 1.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6.6      |
|    mean_reward     | 3.86     |
| time/              |          |
|    total_timesteps | 52400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | 17.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 131      |
|    time_elapsed    | 14724    |
|    total_timesteps | 52400    |
---------------------------------
Eval num_timesteps=52600, episode_reward=17.77 +/- 14.69
Episode length: 25.40 +/- 20.24
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 25.4           |
|    mean_reward          | 17.8           |
| time/                   |                |
|    total_timesteps      | 52600          |
| train/                  |                |
|    approx_kl            | 0.000103538456 |
|    clip_fraction        | 0.00469        |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.278         |
|    explained_variance   | 0.314          |
|    learning_rate        | 0.0001         |
|    loss                 | 0.431          |
|    n_updates            | 1310           |
|    policy_gradient_loss | -0.00147       |
|    value_loss           | 1.98           |
--------------------------------------------
Eval num_timesteps=52800, episode_reward=14.47 +/- 13.08
Episode length: 21.60 +/- 18.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 14.5     |
| time/              |          |
|    total_timesteps | 52800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.7     |
|    ep_rew_mean     | 18.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 132      |
|    time_elapsed    | 14835    |
|    total_timesteps | 52800    |
---------------------------------
Eval num_timesteps=53000, episode_reward=12.39 +/- 12.47
Episode length: 19.20 +/- 18.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 19.2          |
|    mean_reward          | 12.4          |
| time/                   |               |
|    total_timesteps      | 53000         |
| train/                  |               |
|    approx_kl            | 0.00041595675 |
|    clip_fraction        | 0.0176        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.31         |
|    explained_variance   | 0.212         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.526         |
|    n_updates            | 1320          |
|    policy_gradient_loss | -0.00137      |
|    value_loss           | 1.68          |
-------------------------------------------
Eval num_timesteps=53200, episode_reward=29.44 +/- 12.85
Episode length: 41.20 +/- 17.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | 29.4     |
| time/              |          |
|    total_timesteps | 53200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.2     |
|    ep_rew_mean     | 18       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 133      |
|    time_elapsed    | 14964    |
|    total_timesteps | 53200    |
---------------------------------
Eval num_timesteps=53400, episode_reward=14.37 +/- 12.04
Episode length: 20.80 +/- 16.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 20.8         |
|    mean_reward          | 14.4         |
| time/                   |              |
|    total_timesteps      | 53400        |
| train/                  |              |
|    approx_kl            | 0.0005320977 |
|    clip_fraction        | 0.0353       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.335       |
|    explained_variance   | 0.129        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.69         |
|    n_updates            | 1330         |
|    policy_gradient_loss | -0.00154     |
|    value_loss           | 3.38         |
------------------------------------------
Eval num_timesteps=53600, episode_reward=18.09 +/- 14.37
Episode length: 26.00 +/- 19.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | 18.1     |
| time/              |          |
|    total_timesteps | 53600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.8     |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 134      |
|    time_elapsed    | 15075    |
|    total_timesteps | 53600    |
---------------------------------
Eval num_timesteps=53800, episode_reward=20.09 +/- 11.46
Episode length: 30.20 +/- 16.86
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 30.2          |
|    mean_reward          | 20.1          |
| time/                   |               |
|    total_timesteps      | 53800         |
| train/                  |               |
|    approx_kl            | 0.00011012637 |
|    clip_fraction        | 0.0029        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.346        |
|    explained_variance   | 0.346         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.07          |
|    n_updates            | 1340          |
|    policy_gradient_loss | -0.000851     |
|    value_loss           | 1.97          |
-------------------------------------------
Eval num_timesteps=54000, episode_reward=18.61 +/- 14.85
Episode length: 26.80 +/- 19.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.8     |
|    mean_reward     | 18.6     |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.8     |
|    ep_rew_mean     | 17       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 135      |
|    time_elapsed    | 15181    |
|    total_timesteps | 54000    |
---------------------------------
Eval num_timesteps=54200, episode_reward=15.72 +/- 16.69
Episode length: 22.40 +/- 22.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22.4         |
|    mean_reward          | 15.7         |
| time/                   |              |
|    total_timesteps      | 54200        |
| train/                  |              |
|    approx_kl            | 0.0005586218 |
|    clip_fraction        | 0.0364       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.347       |
|    explained_variance   | 0.271        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.715        |
|    n_updates            | 1350         |
|    policy_gradient_loss | -0.00317     |
|    value_loss           | 2.37         |
------------------------------------------
Eval num_timesteps=54400, episode_reward=16.62 +/- 15.67
Episode length: 23.80 +/- 21.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 54400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | 16       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 136      |
|    time_elapsed    | 15292    |
|    total_timesteps | 54400    |
---------------------------------
Eval num_timesteps=54600, episode_reward=13.41 +/- 11.95
Episode length: 19.60 +/- 15.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 19.6         |
|    mean_reward          | 13.4         |
| time/                   |              |
|    total_timesteps      | 54600        |
| train/                  |              |
|    approx_kl            | 0.0001180921 |
|    clip_fraction        | 0.0058       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.338       |
|    explained_variance   | 0.381        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.831        |
|    n_updates            | 1360         |
|    policy_gradient_loss | -0.001       |
|    value_loss           | 2.24         |
------------------------------------------
Eval num_timesteps=54800, episode_reward=21.38 +/- 16.06
Episode length: 31.40 +/- 22.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.4     |
|    mean_reward     | 21.4     |
| time/              |          |
|    total_timesteps | 54800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.3     |
|    ep_rew_mean     | 15.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 137      |
|    time_elapsed    | 15426    |
|    total_timesteps | 54800    |
---------------------------------
Eval num_timesteps=55000, episode_reward=22.08 +/- 15.68
Episode length: 32.00 +/- 22.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32           |
|    mean_reward          | 22.1         |
| time/                   |              |
|    total_timesteps      | 55000        |
| train/                  |              |
|    approx_kl            | 0.0005051096 |
|    clip_fraction        | 0.0129       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.353       |
|    explained_variance   | 0.272        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.167        |
|    n_updates            | 1370         |
|    policy_gradient_loss | -0.00246     |
|    value_loss           | 1.71         |
------------------------------------------
Eval num_timesteps=55200, episode_reward=27.94 +/- 12.54
Episode length: 41.00 +/- 18.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41       |
|    mean_reward     | 27.9     |
| time/              |          |
|    total_timesteps | 55200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.3     |
|    ep_rew_mean     | 16       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 138      |
|    time_elapsed    | 15581    |
|    total_timesteps | 55200    |
---------------------------------
Eval num_timesteps=55400, episode_reward=22.96 +/- 15.09
Episode length: 33.00 +/- 20.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33            |
|    mean_reward          | 23            |
| time/                   |               |
|    total_timesteps      | 55400         |
| train/                  |               |
|    approx_kl            | 0.00028359477 |
|    clip_fraction        | 0.00536       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.325        |
|    explained_variance   | 0.107         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.17          |
|    n_updates            | 1380          |
|    policy_gradient_loss | -0.000707     |
|    value_loss           | 1.91          |
-------------------------------------------
Eval num_timesteps=55600, episode_reward=12.86 +/- 10.99
Episode length: 19.40 +/- 15.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 12.9     |
| time/              |          |
|    total_timesteps | 55600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | 16.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 139      |
|    time_elapsed    | 15690    |
|    total_timesteps | 55600    |
---------------------------------
Eval num_timesteps=55800, episode_reward=19.90 +/- 13.75
Episode length: 28.20 +/- 18.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 28.2          |
|    mean_reward          | 19.9          |
| time/                   |               |
|    total_timesteps      | 55800         |
| train/                  |               |
|    approx_kl            | 0.00035207826 |
|    clip_fraction        | 0.0123        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.33         |
|    explained_variance   | 0.3           |
|    learning_rate        | 0.0001        |
|    loss                 | 1.09          |
|    n_updates            | 1390          |
|    policy_gradient_loss | -0.00137      |
|    value_loss           | 2.58          |
-------------------------------------------
Eval num_timesteps=56000, episode_reward=11.23 +/- 11.30
Episode length: 17.60 +/- 16.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | 11.2     |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.9     |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 140      |
|    time_elapsed    | 15803    |
|    total_timesteps | 56000    |
---------------------------------
Eval num_timesteps=56200, episode_reward=23.39 +/- 15.02
Episode length: 33.20 +/- 20.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.2          |
|    mean_reward          | 23.4          |
| time/                   |               |
|    total_timesteps      | 56200         |
| train/                  |               |
|    approx_kl            | 0.00017916012 |
|    clip_fraction        | 0.00357       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.329        |
|    explained_variance   | 0.346         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.38          |
|    n_updates            | 1400          |
|    policy_gradient_loss | -0.00121      |
|    value_loss           | 2.56          |
-------------------------------------------
Eval num_timesteps=56400, episode_reward=19.54 +/- 12.69
Episode length: 29.00 +/- 18.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29       |
|    mean_reward     | 19.5     |
| time/              |          |
|    total_timesteps | 56400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.2     |
|    ep_rew_mean     | 15.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 141      |
|    time_elapsed    | 15924    |
|    total_timesteps | 56400    |
---------------------------------
Eval num_timesteps=56600, episode_reward=29.51 +/- 12.16
Episode length: 41.60 +/- 16.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 41.6          |
|    mean_reward          | 29.5          |
| time/                   |               |
|    total_timesteps      | 56600         |
| train/                  |               |
|    approx_kl            | 0.00036559973 |
|    clip_fraction        | 0.00804       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.33         |
|    explained_variance   | 0.234         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.85          |
|    n_updates            | 1410          |
|    policy_gradient_loss | -0.00206      |
|    value_loss           | 2.79          |
-------------------------------------------
Eval num_timesteps=56800, episode_reward=18.08 +/- 14.86
Episode length: 25.80 +/- 19.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | 18.1     |
| time/              |          |
|    total_timesteps | 56800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.4     |
|    ep_rew_mean     | 15.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 142      |
|    time_elapsed    | 16077    |
|    total_timesteps | 56800    |
---------------------------------
Eval num_timesteps=57000, episode_reward=19.63 +/- 14.28
Episode length: 29.00 +/- 19.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 29            |
|    mean_reward          | 19.6          |
| time/                   |               |
|    total_timesteps      | 57000         |
| train/                  |               |
|    approx_kl            | 0.00048765473 |
|    clip_fraction        | 0.029         |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.355        |
|    explained_variance   | 0.401         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.982         |
|    n_updates            | 1420          |
|    policy_gradient_loss | -0.00261      |
|    value_loss           | 2.25          |
-------------------------------------------
Eval num_timesteps=57200, episode_reward=9.63 +/- 12.73
Episode length: 14.80 +/- 18.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 9.63     |
| time/              |          |
|    total_timesteps | 57200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.1     |
|    ep_rew_mean     | 15.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 143      |
|    time_elapsed    | 16206    |
|    total_timesteps | 57200    |
---------------------------------
Eval num_timesteps=57400, episode_reward=17.95 +/- 14.86
Episode length: 25.40 +/- 20.10
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.4          |
|    mean_reward          | 17.9          |
| time/                   |               |
|    total_timesteps      | 57400         |
| train/                  |               |
|    approx_kl            | 0.00010446605 |
|    clip_fraction        | 0.00223       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.348        |
|    explained_variance   | 0.463         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.29          |
|    n_updates            | 1430          |
|    policy_gradient_loss | -0.000985     |
|    value_loss           | 1.93          |
-------------------------------------------
Eval num_timesteps=57600, episode_reward=8.00 +/- 6.34
Episode length: 12.00 +/- 8.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 12       |
|    mean_reward     | 8        |
| time/              |          |
|    total_timesteps | 57600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.7     |
|    ep_rew_mean     | 14.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 144      |
|    time_elapsed    | 16296    |
|    total_timesteps | 57600    |
---------------------------------
Eval num_timesteps=57800, episode_reward=28.61 +/- 12.89
Episode length: 41.00 +/- 18.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41          |
|    mean_reward          | 28.6        |
| time/                   |             |
|    total_timesteps      | 57800       |
| train/                  |             |
|    approx_kl            | 0.000976542 |
|    clip_fraction        | 0.0312      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.376      |
|    explained_variance   | 0.287       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.913       |
|    n_updates            | 1440        |
|    policy_gradient_loss | -0.00303    |
|    value_loss           | 2.53        |
-----------------------------------------
Eval num_timesteps=58000, episode_reward=22.31 +/- 14.45
Episode length: 33.00 +/- 20.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 22.3     |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.4     |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 145      |
|    time_elapsed    | 16429    |
|    total_timesteps | 58000    |
---------------------------------
Eval num_timesteps=58200, episode_reward=4.88 +/- 1.67
Episode length: 8.00 +/- 2.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 8            |
|    mean_reward          | 4.88         |
| time/                   |              |
|    total_timesteps      | 58200        |
| train/                  |              |
|    approx_kl            | 0.0003939541 |
|    clip_fraction        | 0.0221       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.415       |
|    explained_variance   | 0.398        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.58         |
|    n_updates            | 1450         |
|    policy_gradient_loss | -0.00158     |
|    value_loss           | 2.14         |
------------------------------------------
Eval num_timesteps=58400, episode_reward=22.02 +/- 14.85
Episode length: 32.60 +/- 21.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.6     |
|    mean_reward     | 22       |
| time/              |          |
|    total_timesteps | 58400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.6     |
|    ep_rew_mean     | 14.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 146      |
|    time_elapsed    | 16519    |
|    total_timesteps | 58400    |
---------------------------------
Eval num_timesteps=58600, episode_reward=12.93 +/- 12.32
Episode length: 19.00 +/- 16.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 19            |
|    mean_reward          | 12.9          |
| time/                   |               |
|    total_timesteps      | 58600         |
| train/                  |               |
|    approx_kl            | 0.00037070684 |
|    clip_fraction        | 0.00737       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.408        |
|    explained_variance   | 0.243         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.495         |
|    n_updates            | 1460          |
|    policy_gradient_loss | -0.00277      |
|    value_loss           | 2.93          |
-------------------------------------------
Eval num_timesteps=58800, episode_reward=21.55 +/- 15.85
Episode length: 31.60 +/- 22.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.6     |
|    mean_reward     | 21.5     |
| time/              |          |
|    total_timesteps | 58800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.3     |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 147      |
|    time_elapsed    | 16624    |
|    total_timesteps | 58800    |
---------------------------------
Eval num_timesteps=59000, episode_reward=11.61 +/- 12.14
Episode length: 17.20 +/- 17.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.2         |
|    mean_reward          | 11.6         |
| time/                   |              |
|    total_timesteps      | 59000        |
| train/                  |              |
|    approx_kl            | 0.0002216162 |
|    clip_fraction        | 0.00446      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.401       |
|    explained_variance   | 0.358        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.876        |
|    n_updates            | 1470         |
|    policy_gradient_loss | -0.00151     |
|    value_loss           | 2.13         |
------------------------------------------
Eval num_timesteps=59200, episode_reward=21.94 +/- 15.37
Episode length: 32.20 +/- 21.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 21.9     |
| time/              |          |
|    total_timesteps | 59200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.8     |
|    ep_rew_mean     | 14       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 148      |
|    time_elapsed    | 16753    |
|    total_timesteps | 59200    |
---------------------------------
Eval num_timesteps=59400, episode_reward=7.78 +/- 4.39
Episode length: 12.00 +/- 6.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 12            |
|    mean_reward          | 7.78          |
| time/                   |               |
|    total_timesteps      | 59400         |
| train/                  |               |
|    approx_kl            | 0.00067933305 |
|    clip_fraction        | 0.0228        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.38         |
|    explained_variance   | 0.382         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.34          |
|    n_updates            | 1480          |
|    policy_gradient_loss | -0.00269      |
|    value_loss           | 2.84          |
-------------------------------------------
Eval num_timesteps=59600, episode_reward=22.26 +/- 15.91
Episode length: 32.00 +/- 22.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32       |
|    mean_reward     | 22.3     |
| time/              |          |
|    total_timesteps | 59600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.9     |
|    ep_rew_mean     | 13.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 149      |
|    time_elapsed    | 16850    |
|    total_timesteps | 59600    |
---------------------------------
Eval num_timesteps=59800, episode_reward=5.16 +/- 5.74
Episode length: 8.40 +/- 8.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 8.4          |
|    mean_reward          | 5.16         |
| time/                   |              |
|    total_timesteps      | 59800        |
| train/                  |              |
|    approx_kl            | 0.0005330044 |
|    clip_fraction        | 0.00982      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.372       |
|    explained_variance   | 0.337        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.02         |
|    n_updates            | 1490         |
|    policy_gradient_loss | -0.00104     |
|    value_loss           | 2.77         |
------------------------------------------
Eval num_timesteps=60000, episode_reward=19.05 +/- 13.72
Episode length: 27.80 +/- 19.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.8     |
|    mean_reward     | 19       |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.4     |
|    ep_rew_mean     | 13.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 150      |
|    time_elapsed    | 16943    |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=60200, episode_reward=10.68 +/- 11.97
Episode length: 16.20 +/- 17.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.2         |
|    mean_reward          | 10.7         |
| time/                   |              |
|    total_timesteps      | 60200        |
| train/                  |              |
|    approx_kl            | 0.0009865055 |
|    clip_fraction        | 0.0373       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.366       |
|    explained_variance   | 0.299        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.18         |
|    n_updates            | 1500         |
|    policy_gradient_loss | -0.00372     |
|    value_loss           | 2.11         |
------------------------------------------
Eval num_timesteps=60400, episode_reward=10.64 +/- 12.04
Episode length: 16.00 +/- 17.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | 10.6     |
| time/              |          |
|    total_timesteps | 60400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.6     |
|    ep_rew_mean     | 13.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 151      |
|    time_elapsed    | 17046    |
|    total_timesteps | 60400    |
---------------------------------
Eval num_timesteps=60600, episode_reward=3.84 +/- 2.26
Episode length: 6.20 +/- 3.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 6.2           |
|    mean_reward          | 3.84          |
| time/                   |               |
|    total_timesteps      | 60600         |
| train/                  |               |
|    approx_kl            | 0.00027833335 |
|    clip_fraction        | 0.00424       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.341        |
|    explained_variance   | 0.411         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.79          |
|    n_updates            | 1510          |
|    policy_gradient_loss | -0.00102      |
|    value_loss           | 2.49          |
-------------------------------------------
Eval num_timesteps=60800, episode_reward=9.12 +/- 13.28
Episode length: 13.60 +/- 18.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.6     |
|    mean_reward     | 9.12     |
| time/              |          |
|    total_timesteps | 60800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.6     |
|    ep_rew_mean     | 13.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 152      |
|    time_elapsed    | 17132    |
|    total_timesteps | 60800    |
---------------------------------
Eval num_timesteps=61000, episode_reward=16.45 +/- 15.71
Episode length: 23.80 +/- 21.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.8         |
|    mean_reward          | 16.5         |
| time/                   |              |
|    total_timesteps      | 61000        |
| train/                  |              |
|    approx_kl            | 0.0003675286 |
|    clip_fraction        | 0.00804      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.331       |
|    explained_variance   | 0.31         |
|    learning_rate        | 0.0001       |
|    loss                 | 2.42         |
|    n_updates            | 1520         |
|    policy_gradient_loss | -0.00202     |
|    value_loss           | 2.44         |
------------------------------------------
Eval num_timesteps=61200, episode_reward=23.90 +/- 14.79
Episode length: 33.80 +/- 19.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 23.9     |
| time/              |          |
|    total_timesteps | 61200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.1     |
|    ep_rew_mean     | 13       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 153      |
|    time_elapsed    | 17240    |
|    total_timesteps | 61200    |
---------------------------------
Eval num_timesteps=61400, episode_reward=9.48 +/- 13.69
Episode length: 13.80 +/- 18.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.8        |
|    mean_reward          | 9.48        |
| time/                   |             |
|    total_timesteps      | 61400       |
| train/                  |             |
|    approx_kl            | 0.000942394 |
|    clip_fraction        | 0.0429      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.295      |
|    explained_variance   | 0.295       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.36        |
|    n_updates            | 1530        |
|    policy_gradient_loss | -0.00323    |
|    value_loss           | 3.3         |
-----------------------------------------
Eval num_timesteps=61600, episode_reward=25.89 +/- 13.09
Episode length: 37.00 +/- 17.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 25.9     |
| time/              |          |
|    total_timesteps | 61600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.9     |
|    ep_rew_mean     | 13.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 154      |
|    time_elapsed    | 17357    |
|    total_timesteps | 61600    |
---------------------------------
Eval num_timesteps=61800, episode_reward=15.95 +/- 15.65
Episode length: 23.40 +/- 21.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.4          |
|    mean_reward          | 15.9          |
| time/                   |               |
|    total_timesteps      | 61800         |
| train/                  |               |
|    approx_kl            | 0.00016110178 |
|    clip_fraction        | 0.00781       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.306        |
|    explained_variance   | 0.35          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.419         |
|    n_updates            | 1540          |
|    policy_gradient_loss | -0.00188      |
|    value_loss           | 1.83          |
-------------------------------------------
Eval num_timesteps=62000, episode_reward=16.38 +/- 14.82
Episode length: 24.40 +/- 20.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 16.4     |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.4     |
|    ep_rew_mean     | 13.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 155      |
|    time_elapsed    | 17463    |
|    total_timesteps | 62000    |
---------------------------------
Eval num_timesteps=62200, episode_reward=13.02 +/- 10.37
Episode length: 19.60 +/- 15.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 19.6          |
|    mean_reward          | 13            |
| time/                   |               |
|    total_timesteps      | 62200         |
| train/                  |               |
|    approx_kl            | 0.00019148218 |
|    clip_fraction        | 0.00893       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.289        |
|    explained_variance   | 0.4           |
|    learning_rate        | 0.0001        |
|    loss                 | 1.2           |
|    n_updates            | 1550          |
|    policy_gradient_loss | -0.00122      |
|    value_loss           | 2.59          |
-------------------------------------------
Eval num_timesteps=62400, episode_reward=10.23 +/- 12.27
Episode length: 15.40 +/- 17.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | 10.2     |
| time/              |          |
|    total_timesteps | 62400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.7     |
|    ep_rew_mean     | 13.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 156      |
|    time_elapsed    | 17573    |
|    total_timesteps | 62400    |
---------------------------------
Eval num_timesteps=62600, episode_reward=22.93 +/- 14.67
Episode length: 33.20 +/- 20.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.2          |
|    mean_reward          | 22.9          |
| time/                   |               |
|    total_timesteps      | 62600         |
| train/                  |               |
|    approx_kl            | 0.00057111855 |
|    clip_fraction        | 0.0096        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.288        |
|    explained_variance   | 0.455         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.853         |
|    n_updates            | 1560          |
|    policy_gradient_loss | -0.000929     |
|    value_loss           | 2.76          |
-------------------------------------------
Eval num_timesteps=62800, episode_reward=10.53 +/- 13.14
Episode length: 15.40 +/- 17.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | 10.5     |
| time/              |          |
|    total_timesteps | 62800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.8     |
|    ep_rew_mean     | 14.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 157      |
|    time_elapsed    | 17680    |
|    total_timesteps | 62800    |
---------------------------------
Eval num_timesteps=63000, episode_reward=10.82 +/- 12.43
Episode length: 16.20 +/- 16.98
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.2          |
|    mean_reward          | 10.8          |
| time/                   |               |
|    total_timesteps      | 63000         |
| train/                  |               |
|    approx_kl            | 0.00029257472 |
|    clip_fraction        | 0.0194        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.277        |
|    explained_variance   | 0.172         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.321         |
|    n_updates            | 1570          |
|    policy_gradient_loss | -0.00282      |
|    value_loss           | 1.43          |
-------------------------------------------
Eval num_timesteps=63200, episode_reward=25.13 +/- 13.14
Episode length: 35.60 +/- 18.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 25.1     |
| time/              |          |
|    total_timesteps | 63200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.9     |
|    ep_rew_mean     | 14.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 158      |
|    time_elapsed    | 17808    |
|    total_timesteps | 63200    |
---------------------------------
Eval num_timesteps=63400, episode_reward=24.87 +/- 13.21
Episode length: 35.80 +/- 18.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.8          |
|    mean_reward          | 24.9          |
| time/                   |               |
|    total_timesteps      | 63400         |
| train/                  |               |
|    approx_kl            | 0.00017167602 |
|    clip_fraction        | 0.00781       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.281        |
|    explained_variance   | 0.324         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.612         |
|    n_updates            | 1580          |
|    policy_gradient_loss | -0.000974     |
|    value_loss           | 2.7           |
-------------------------------------------
Eval num_timesteps=63600, episode_reward=15.46 +/- 15.21
Episode length: 23.40 +/- 21.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 15.5     |
| time/              |          |
|    total_timesteps | 63600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.3     |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 159      |
|    time_elapsed    | 17922    |
|    total_timesteps | 63600    |
---------------------------------
Eval num_timesteps=63800, episode_reward=15.93 +/- 15.70
Episode length: 23.20 +/- 21.98
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.2          |
|    mean_reward          | 15.9          |
| time/                   |               |
|    total_timesteps      | 63800         |
| train/                  |               |
|    approx_kl            | 0.00036308516 |
|    clip_fraction        | 0.0118        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.276        |
|    explained_variance   | 0.348         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.385         |
|    n_updates            | 1590          |
|    policy_gradient_loss | -0.00264      |
|    value_loss           | 1.87          |
-------------------------------------------
Eval num_timesteps=64000, episode_reward=11.94 +/- 11.45
Episode length: 18.00 +/- 16.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | 11.9     |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.6     |
|    ep_rew_mean     | 15.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 160      |
|    time_elapsed    | 18029    |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=64200, episode_reward=13.98 +/- 13.02
Episode length: 21.20 +/- 18.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 21.2          |
|    mean_reward          | 14            |
| time/                   |               |
|    total_timesteps      | 64200         |
| train/                  |               |
|    approx_kl            | 0.00018189356 |
|    clip_fraction        | 0.00603       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.287        |
|    explained_variance   | 0.328         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.395         |
|    n_updates            | 1600          |
|    policy_gradient_loss | -0.00094      |
|    value_loss           | 1.56          |
-------------------------------------------
Eval num_timesteps=64400, episode_reward=28.97 +/- 12.70
Episode length: 41.20 +/- 17.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | 29       |
| time/              |          |
|    total_timesteps | 64400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.2     |
|    ep_rew_mean     | 15.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 161      |
|    time_elapsed    | 18134    |
|    total_timesteps | 64400    |
---------------------------------
Eval num_timesteps=64600, episode_reward=17.66 +/- 14.85
Episode length: 25.40 +/- 20.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.4         |
|    mean_reward          | 17.7         |
| time/                   |              |
|    total_timesteps      | 64600        |
| train/                  |              |
|    approx_kl            | 0.0002645753 |
|    clip_fraction        | 0.00893      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.277       |
|    explained_variance   | 0.46         |
|    learning_rate        | 0.0001       |
|    loss                 | 2.04         |
|    n_updates            | 1610         |
|    policy_gradient_loss | -0.00138     |
|    value_loss           | 2.27         |
------------------------------------------
Eval num_timesteps=64800, episode_reward=11.53 +/- 13.07
Episode length: 16.60 +/- 17.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | 11.5     |
| time/              |          |
|    total_timesteps | 64800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 162      |
|    time_elapsed    | 18239    |
|    total_timesteps | 64800    |
---------------------------------
Eval num_timesteps=65000, episode_reward=21.57 +/- 15.83
Episode length: 31.60 +/- 22.57
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 31.6           |
|    mean_reward          | 21.6           |
| time/                   |                |
|    total_timesteps      | 65000          |
| train/                  |                |
|    approx_kl            | 0.000114151146 |
|    clip_fraction        | 0.00737        |
|    clip_range           | 0.1            |
|    entropy_loss         | -0.265         |
|    explained_variance   | 0.344          |
|    learning_rate        | 0.0001         |
|    loss                 | 0.962          |
|    n_updates            | 1620           |
|    policy_gradient_loss | -0.00178       |
|    value_loss           | 1.73           |
--------------------------------------------
Eval num_timesteps=65200, episode_reward=23.40 +/- 14.52
Episode length: 33.60 +/- 20.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 23.4     |
| time/              |          |
|    total_timesteps | 65200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.3     |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 163      |
|    time_elapsed    | 18370    |
|    total_timesteps | 65200    |
---------------------------------
Eval num_timesteps=65400, episode_reward=22.99 +/- 14.63
Episode length: 33.40 +/- 20.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.4          |
|    mean_reward          | 23            |
| time/                   |               |
|    total_timesteps      | 65400         |
| train/                  |               |
|    approx_kl            | 0.00030379862 |
|    clip_fraction        | 0.00647       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.251        |
|    explained_variance   | 0.461         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.01          |
|    n_updates            | 1630          |
|    policy_gradient_loss | -0.00149      |
|    value_loss           | 2.64          |
-------------------------------------------
Eval num_timesteps=65600, episode_reward=24.04 +/- 13.75
Episode length: 34.40 +/- 19.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 24       |
| time/              |          |
|    total_timesteps | 65600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.9     |
|    ep_rew_mean     | 15.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 164      |
|    time_elapsed    | 18504    |
|    total_timesteps | 65600    |
---------------------------------
Eval num_timesteps=65800, episode_reward=25.61 +/- 13.33
Episode length: 36.20 +/- 17.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.2          |
|    mean_reward          | 25.6          |
| time/                   |               |
|    total_timesteps      | 65800         |
| train/                  |               |
|    approx_kl            | 0.00036371578 |
|    clip_fraction        | 0.0114        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.268        |
|    explained_variance   | -0.0123       |
|    learning_rate        | 0.0001        |
|    loss                 | 0.229         |
|    n_updates            | 1640          |
|    policy_gradient_loss | -0.000901     |
|    value_loss           | 2.09          |
-------------------------------------------
Eval num_timesteps=66000, episode_reward=12.63 +/- 11.83
Episode length: 18.80 +/- 16.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.8     |
|    mean_reward     | 12.6     |
| time/              |          |
|    total_timesteps | 66000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 165      |
|    time_elapsed    | 18632    |
|    total_timesteps | 66000    |
---------------------------------
Eval num_timesteps=66200, episode_reward=23.03 +/- 15.00
Episode length: 33.00 +/- 20.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33           |
|    mean_reward          | 23           |
| time/                   |              |
|    total_timesteps      | 66200        |
| train/                  |              |
|    approx_kl            | 0.0011451079 |
|    clip_fraction        | 0.0223       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.308       |
|    explained_variance   | 0.24         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.939        |
|    n_updates            | 1650         |
|    policy_gradient_loss | -0.00158     |
|    value_loss           | 0.944        |
------------------------------------------
Eval num_timesteps=66400, episode_reward=26.08 +/- 12.07
Episode length: 37.40 +/- 16.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.4     |
|    mean_reward     | 26.1     |
| time/              |          |
|    total_timesteps | 66400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22       |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 166      |
|    time_elapsed    | 18752    |
|    total_timesteps | 66400    |
---------------------------------
Eval num_timesteps=66600, episode_reward=23.68 +/- 13.88
Episode length: 34.40 +/- 19.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.4          |
|    mean_reward          | 23.7          |
| time/                   |               |
|    total_timesteps      | 66600         |
| train/                  |               |
|    approx_kl            | 0.00052502565 |
|    clip_fraction        | 0.0223        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.257        |
|    explained_variance   | 0.228         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.35          |
|    n_updates            | 1660          |
|    policy_gradient_loss | -0.00221      |
|    value_loss           | 3.78          |
-------------------------------------------
Eval num_timesteps=66800, episode_reward=24.59 +/- 13.55
Episode length: 35.40 +/- 18.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 24.6     |
| time/              |          |
|    total_timesteps | 66800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.6     |
|    ep_rew_mean     | 15.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 167      |
|    time_elapsed    | 18881    |
|    total_timesteps | 66800    |
---------------------------------
Eval num_timesteps=67000, episode_reward=23.25 +/- 15.22
Episode length: 33.20 +/- 20.73
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.2          |
|    mean_reward          | 23.2          |
| time/                   |               |
|    total_timesteps      | 67000         |
| train/                  |               |
|    approx_kl            | 0.00014175435 |
|    clip_fraction        | 0.00379       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.279        |
|    explained_variance   | 0.149         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.276         |
|    n_updates            | 1670          |
|    policy_gradient_loss | -0.000679     |
|    value_loss           | 2.31          |
-------------------------------------------
Eval num_timesteps=67200, episode_reward=22.38 +/- 14.86
Episode length: 32.80 +/- 21.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 22.4     |
| time/              |          |
|    total_timesteps | 67200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.8     |
|    ep_rew_mean     | 15.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 168      |
|    time_elapsed    | 19015    |
|    total_timesteps | 67200    |
---------------------------------
Eval num_timesteps=67400, episode_reward=5.52 +/- 2.03
Episode length: 8.80 +/- 2.79
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 8.8           |
|    mean_reward          | 5.52          |
| time/                   |               |
|    total_timesteps      | 67400         |
| train/                  |               |
|    approx_kl            | 0.00035475794 |
|    clip_fraction        | 0.0154        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.282        |
|    explained_variance   | 0.419         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.607         |
|    n_updates            | 1680          |
|    policy_gradient_loss | -0.002        |
|    value_loss           | 1.38          |
-------------------------------------------
Eval num_timesteps=67600, episode_reward=22.85 +/- 14.76
Episode length: 33.20 +/- 20.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | 22.8     |
| time/              |          |
|    total_timesteps | 67600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.2     |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 169      |
|    time_elapsed    | 19107    |
|    total_timesteps | 67600    |
---------------------------------
Eval num_timesteps=67800, episode_reward=18.21 +/- 14.68
Episode length: 25.80 +/- 19.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.8          |
|    mean_reward          | 18.2          |
| time/                   |               |
|    total_timesteps      | 67800         |
| train/                  |               |
|    approx_kl            | 0.00065682695 |
|    clip_fraction        | 0.0375        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.269        |
|    explained_variance   | 0.222         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.81          |
|    n_updates            | 1690          |
|    policy_gradient_loss | -0.00295      |
|    value_loss           | 2.01          |
-------------------------------------------
Eval num_timesteps=68000, episode_reward=22.13 +/- 15.13
Episode length: 32.40 +/- 21.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.4     |
|    mean_reward     | 22.1     |
| time/              |          |
|    total_timesteps | 68000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.3     |
|    ep_rew_mean     | 16.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 170      |
|    time_elapsed    | 19212    |
|    total_timesteps | 68000    |
---------------------------------
Eval num_timesteps=68200, episode_reward=10.24 +/- 12.26
Episode length: 15.60 +/- 17.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.6          |
|    mean_reward          | 10.2          |
| time/                   |               |
|    total_timesteps      | 68200         |
| train/                  |               |
|    approx_kl            | 0.00024361761 |
|    clip_fraction        | 0.0103        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.259        |
|    explained_variance   | 0.328         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.977         |
|    n_updates            | 1700          |
|    policy_gradient_loss | -0.00194      |
|    value_loss           | 1.65          |
-------------------------------------------
Eval num_timesteps=68400, episode_reward=14.99 +/- 16.37
Episode length: 22.00 +/- 22.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22       |
|    mean_reward     | 15       |
| time/              |          |
|    total_timesteps | 68400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.1     |
|    ep_rew_mean     | 15.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 171      |
|    time_elapsed    | 19342    |
|    total_timesteps | 68400    |
---------------------------------
Eval num_timesteps=68600, episode_reward=28.38 +/- 13.91
Episode length: 40.40 +/- 19.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 40.4         |
|    mean_reward          | 28.4         |
| time/                   |              |
|    total_timesteps      | 68600        |
| train/                  |              |
|    approx_kl            | 0.0010345301 |
|    clip_fraction        | 0.0172       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.276       |
|    explained_variance   | 0.428        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.762        |
|    n_updates            | 1710         |
|    policy_gradient_loss | -0.0012      |
|    value_loss           | 2.42         |
------------------------------------------
Eval num_timesteps=68800, episode_reward=16.64 +/- 16.01
Episode length: 23.60 +/- 21.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 68800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.5     |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 172      |
|    time_elapsed    | 19474    |
|    total_timesteps | 68800    |
---------------------------------
Eval num_timesteps=69000, episode_reward=10.95 +/- 12.46
Episode length: 16.20 +/- 17.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.2         |
|    mean_reward          | 11           |
| time/                   |              |
|    total_timesteps      | 69000        |
| train/                  |              |
|    approx_kl            | 0.0007397675 |
|    clip_fraction        | 0.0147       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.318       |
|    explained_variance   | -0.0126      |
|    learning_rate        | 0.0001       |
|    loss                 | 0.639        |
|    n_updates            | 1720         |
|    policy_gradient_loss | -0.00238     |
|    value_loss           | 1.33         |
------------------------------------------
Eval num_timesteps=69200, episode_reward=15.88 +/- 16.10
Episode length: 22.80 +/- 22.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 15.9     |
| time/              |          |
|    total_timesteps | 69200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26       |
|    ep_rew_mean     | 18       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 173      |
|    time_elapsed    | 19582    |
|    total_timesteps | 69200    |
---------------------------------
Eval num_timesteps=69400, episode_reward=16.04 +/- 15.11
Episode length: 23.80 +/- 21.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.8         |
|    mean_reward          | 16           |
| time/                   |              |
|    total_timesteps      | 69400        |
| train/                  |              |
|    approx_kl            | 0.0018606997 |
|    clip_fraction        | 0.0326       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.313       |
|    explained_variance   | 0.29         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.503        |
|    n_updates            | 1730         |
|    policy_gradient_loss | -0.0038      |
|    value_loss           | 2.1          |
------------------------------------------
Eval num_timesteps=69600, episode_reward=17.45 +/- 14.00
Episode length: 25.80 +/- 19.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 69600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.5     |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 174      |
|    time_elapsed    | 19692    |
|    total_timesteps | 69600    |
---------------------------------
Eval num_timesteps=69800, episode_reward=10.78 +/- 11.33
Episode length: 16.60 +/- 16.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.6          |
|    mean_reward          | 10.8          |
| time/                   |               |
|    total_timesteps      | 69800         |
| train/                  |               |
|    approx_kl            | 0.00034980808 |
|    clip_fraction        | 0.00759       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.281        |
|    explained_variance   | 0.372         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.834         |
|    n_updates            | 1740          |
|    policy_gradient_loss | -0.00132      |
|    value_loss           | 2.71          |
-------------------------------------------
Eval num_timesteps=70000, episode_reward=12.41 +/- 10.78
Episode length: 19.00 +/- 15.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 12.4     |
| time/              |          |
|    total_timesteps | 70000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.2     |
|    ep_rew_mean     | 17.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 175      |
|    time_elapsed    | 19794    |
|    total_timesteps | 70000    |
---------------------------------
Eval num_timesteps=70200, episode_reward=18.41 +/- 13.58
Episode length: 27.00 +/- 19.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 27           |
|    mean_reward          | 18.4         |
| time/                   |              |
|    total_timesteps      | 70200        |
| train/                  |              |
|    approx_kl            | 0.0007828855 |
|    clip_fraction        | 0.031        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.378       |
|    explained_variance   | -0.0456      |
|    learning_rate        | 0.0001       |
|    loss                 | 0.91         |
|    n_updates            | 1750         |
|    policy_gradient_loss | -0.00115     |
|    value_loss           | 1.76         |
------------------------------------------
Eval num_timesteps=70400, episode_reward=10.34 +/- 11.57
Episode length: 16.20 +/- 16.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | 10.3     |
| time/              |          |
|    total_timesteps | 70400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | 17.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 176      |
|    time_elapsed    | 19906    |
|    total_timesteps | 70400    |
---------------------------------
Eval num_timesteps=70600, episode_reward=8.81 +/- 12.86
Episode length: 13.40 +/- 18.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 13.4         |
|    mean_reward          | 8.81         |
| time/                   |              |
|    total_timesteps      | 70600        |
| train/                  |              |
|    approx_kl            | 0.0015560071 |
|    clip_fraction        | 0.0391       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.316       |
|    explained_variance   | 0.318        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.647        |
|    n_updates            | 1760         |
|    policy_gradient_loss | -0.00417     |
|    value_loss           | 2.12         |
------------------------------------------
Eval num_timesteps=70800, episode_reward=23.55 +/- 15.35
Episode length: 33.20 +/- 20.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | 23.5     |
| time/              |          |
|    total_timesteps | 70800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.6     |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 177      |
|    time_elapsed    | 20015    |
|    total_timesteps | 70800    |
---------------------------------
Eval num_timesteps=71000, episode_reward=22.51 +/- 15.14
Episode length: 32.60 +/- 21.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.6         |
|    mean_reward          | 22.5         |
| time/                   |              |
|    total_timesteps      | 71000        |
| train/                  |              |
|    approx_kl            | 0.0013088081 |
|    clip_fraction        | 0.0429       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.361       |
|    explained_variance   | 0.116        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.56         |
|    n_updates            | 1770         |
|    policy_gradient_loss | -0.00137     |
|    value_loss           | 1.94         |
------------------------------------------
Eval num_timesteps=71200, episode_reward=17.85 +/- 14.07
Episode length: 26.00 +/- 19.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | 17.8     |
| time/              |          |
|    total_timesteps | 71200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 178      |
|    time_elapsed    | 20148    |
|    total_timesteps | 71200    |
---------------------------------
Eval num_timesteps=71400, episode_reward=17.11 +/- 14.05
Episode length: 25.60 +/- 20.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.6         |
|    mean_reward          | 17.1         |
| time/                   |              |
|    total_timesteps      | 71400        |
| train/                  |              |
|    approx_kl            | 0.0006884172 |
|    clip_fraction        | 0.0324       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.339       |
|    explained_variance   | 0.269        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.934        |
|    n_updates            | 1780         |
|    policy_gradient_loss | -0.00333     |
|    value_loss           | 2.75         |
------------------------------------------
Eval num_timesteps=71600, episode_reward=23.59 +/- 15.15
Episode length: 33.20 +/- 20.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | 23.6     |
| time/              |          |
|    total_timesteps | 71600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25       |
|    ep_rew_mean     | 17.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 179      |
|    time_elapsed    | 20254    |
|    total_timesteps | 71600    |
---------------------------------
Eval num_timesteps=71800, episode_reward=11.50 +/- 11.26
Episode length: 17.60 +/- 16.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17.6          |
|    mean_reward          | 11.5          |
| time/                   |               |
|    total_timesteps      | 71800         |
| train/                  |               |
|    approx_kl            | 0.00032169465 |
|    clip_fraction        | 0.0136        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.346        |
|    explained_variance   | 0.309         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.34          |
|    n_updates            | 1790          |
|    policy_gradient_loss | -0.00176      |
|    value_loss           | 1.8           |
-------------------------------------------
Eval num_timesteps=72000, episode_reward=11.18 +/- 13.17
Episode length: 16.60 +/- 18.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | 11.2     |
| time/              |          |
|    total_timesteps | 72000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.5     |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 180      |
|    time_elapsed    | 20372    |
|    total_timesteps | 72000    |
---------------------------------
Eval num_timesteps=72200, episode_reward=12.41 +/- 12.74
Episode length: 17.80 +/- 17.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.8         |
|    mean_reward          | 12.4         |
| time/                   |              |
|    total_timesteps      | 72200        |
| train/                  |              |
|    approx_kl            | 0.0010511244 |
|    clip_fraction        | 0.0379       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.364       |
|    explained_variance   | 0.354        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.33         |
|    n_updates            | 1800         |
|    policy_gradient_loss | -0.00317     |
|    value_loss           | 2.39         |
------------------------------------------
Eval num_timesteps=72400, episode_reward=15.57 +/- 15.92
Episode length: 22.80 +/- 22.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 15.6     |
| time/              |          |
|    total_timesteps | 72400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25       |
|    ep_rew_mean     | 17.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 181      |
|    time_elapsed    | 20476    |
|    total_timesteps | 72400    |
---------------------------------
Eval num_timesteps=72600, episode_reward=9.61 +/- 12.48
Episode length: 14.40 +/- 17.86
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.4          |
|    mean_reward          | 9.61          |
| time/                   |               |
|    total_timesteps      | 72600         |
| train/                  |               |
|    approx_kl            | 0.00024412891 |
|    clip_fraction        | 0.00647       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.364        |
|    explained_variance   | 0.342         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.277         |
|    n_updates            | 1810          |
|    policy_gradient_loss | -0.00217      |
|    value_loss           | 1.86          |
-------------------------------------------
Eval num_timesteps=72800, episode_reward=9.32 +/- 12.31
Episode length: 14.60 +/- 18.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 9.32     |
| time/              |          |
|    total_timesteps | 72800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | 17.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 182      |
|    time_elapsed    | 20579    |
|    total_timesteps | 72800    |
---------------------------------
Eval num_timesteps=73000, episode_reward=9.64 +/- 12.01
Episode length: 15.00 +/- 17.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15           |
|    mean_reward          | 9.64         |
| time/                   |              |
|    total_timesteps      | 73000        |
| train/                  |              |
|    approx_kl            | 0.0006948091 |
|    clip_fraction        | 0.0429       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.347       |
|    explained_variance   | -0.00594     |
|    learning_rate        | 0.0001       |
|    loss                 | 1.32         |
|    n_updates            | 1820         |
|    policy_gradient_loss | -0.00369     |
|    value_loss           | 2.49         |
------------------------------------------
Eval num_timesteps=73200, episode_reward=6.82 +/- 4.72
Episode length: 10.60 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10.6     |
|    mean_reward     | 6.82     |
| time/              |          |
|    total_timesteps | 73200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.3     |
|    ep_rew_mean     | 15.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 183      |
|    time_elapsed    | 20666    |
|    total_timesteps | 73200    |
---------------------------------
Eval num_timesteps=73400, episode_reward=10.52 +/- 12.24
Episode length: 15.80 +/- 17.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | 10.5         |
| time/                   |              |
|    total_timesteps      | 73400        |
| train/                  |              |
|    approx_kl            | 0.0002132899 |
|    clip_fraction        | 0.00759      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.308       |
|    explained_variance   | 0.239        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.34         |
|    n_updates            | 1830         |
|    policy_gradient_loss | -0.00191     |
|    value_loss           | 3.97         |
------------------------------------------
Eval num_timesteps=73600, episode_reward=23.89 +/- 13.98
Episode length: 34.20 +/- 19.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 23.9     |
| time/              |          |
|    total_timesteps | 73600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.1     |
|    ep_rew_mean     | 14.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 184      |
|    time_elapsed    | 20774    |
|    total_timesteps | 73600    |
---------------------------------
Eval num_timesteps=73800, episode_reward=16.80 +/- 15.38
Episode length: 24.20 +/- 21.10
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.2          |
|    mean_reward          | 16.8          |
| time/                   |               |
|    total_timesteps      | 73800         |
| train/                  |               |
|    approx_kl            | 0.00037539232 |
|    clip_fraction        | 0.0141        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.327        |
|    explained_variance   | 0.233         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.552         |
|    n_updates            | 1840          |
|    policy_gradient_loss | -0.00209      |
|    value_loss           | 2.44          |
-------------------------------------------
Eval num_timesteps=74000, episode_reward=17.61 +/- 14.82
Episode length: 25.40 +/- 20.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 74000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.7     |
|    ep_rew_mean     | 15.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 185      |
|    time_elapsed    | 20881    |
|    total_timesteps | 74000    |
---------------------------------
Eval num_timesteps=74200, episode_reward=15.87 +/- 15.44
Episode length: 23.60 +/- 21.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.6         |
|    mean_reward          | 15.9         |
| time/                   |              |
|    total_timesteps      | 74200        |
| train/                  |              |
|    approx_kl            | 0.0006733566 |
|    clip_fraction        | 0.0129       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.329       |
|    explained_variance   | 0.0646       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.145        |
|    n_updates            | 1850         |
|    policy_gradient_loss | -0.00217     |
|    value_loss           | 1.77         |
------------------------------------------
Eval num_timesteps=74400, episode_reward=22.37 +/- 15.30
Episode length: 32.40 +/- 21.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.4     |
|    mean_reward     | 22.4     |
| time/              |          |
|    total_timesteps | 74400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.2     |
|    ep_rew_mean     | 16.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 186      |
|    time_elapsed    | 21010    |
|    total_timesteps | 74400    |
---------------------------------
Eval num_timesteps=74600, episode_reward=12.50 +/- 11.75
Episode length: 18.60 +/- 16.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 18.6          |
|    mean_reward          | 12.5          |
| time/                   |               |
|    total_timesteps      | 74600         |
| train/                  |               |
|    approx_kl            | 0.00036712814 |
|    clip_fraction        | 0.0154        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.29         |
|    explained_variance   | 0.218         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.53          |
|    n_updates            | 1860          |
|    policy_gradient_loss | -0.00254      |
|    value_loss           | 2.36          |
-------------------------------------------
Eval num_timesteps=74800, episode_reward=14.07 +/- 12.56
Episode length: 20.60 +/- 16.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 14.1     |
| time/              |          |
|    total_timesteps | 74800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 15       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 187      |
|    time_elapsed    | 21130    |
|    total_timesteps | 74800    |
---------------------------------
Eval num_timesteps=75000, episode_reward=22.56 +/- 15.17
Episode length: 32.60 +/- 21.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.6         |
|    mean_reward          | 22.6         |
| time/                   |              |
|    total_timesteps      | 75000        |
| train/                  |              |
|    approx_kl            | 0.0006374552 |
|    clip_fraction        | 0.0152       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.309       |
|    explained_variance   | 0.13         |
|    learning_rate        | 0.0001       |
|    loss                 | 1.97         |
|    n_updates            | 1870         |
|    policy_gradient_loss | -0.00292     |
|    value_loss           | 2.76         |
------------------------------------------
Eval num_timesteps=75200, episode_reward=16.44 +/- 14.30
Episode length: 24.60 +/- 20.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 16.4     |
| time/              |          |
|    total_timesteps | 75200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.7     |
|    ep_rew_mean     | 15.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 188      |
|    time_elapsed    | 21242    |
|    total_timesteps | 75200    |
---------------------------------
Eval num_timesteps=75400, episode_reward=3.69 +/- 1.78
Episode length: 6.20 +/- 2.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 6.2          |
|    mean_reward          | 3.69         |
| time/                   |              |
|    total_timesteps      | 75400        |
| train/                  |              |
|    approx_kl            | 0.0006173727 |
|    clip_fraction        | 0.0295       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.342       |
|    explained_variance   | 0.364        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.11         |
|    n_updates            | 1880         |
|    policy_gradient_loss | -0.0021      |
|    value_loss           | 2.03         |
------------------------------------------
Eval num_timesteps=75600, episode_reward=20.10 +/- 14.67
Episode length: 29.40 +/- 20.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.4     |
|    mean_reward     | 20.1     |
| time/              |          |
|    total_timesteps | 75600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.4     |
|    ep_rew_mean     | 16.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 189      |
|    time_elapsed    | 21327    |
|    total_timesteps | 75600    |
---------------------------------
Eval num_timesteps=75800, episode_reward=15.59 +/- 15.53
Episode length: 23.20 +/- 22.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.2         |
|    mean_reward          | 15.6         |
| time/                   |              |
|    total_timesteps      | 75800        |
| train/                  |              |
|    approx_kl            | 0.0003297788 |
|    clip_fraction        | 0.0462       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.36        |
|    explained_variance   | 0.265        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.33         |
|    n_updates            | 1890         |
|    policy_gradient_loss | -0.00233     |
|    value_loss           | 2.26         |
------------------------------------------
Eval num_timesteps=76000, episode_reward=12.51 +/- 12.49
Episode length: 18.40 +/- 16.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.4     |
|    mean_reward     | 12.5     |
| time/              |          |
|    total_timesteps | 76000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.1     |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 190      |
|    time_elapsed    | 21434    |
|    total_timesteps | 76000    |
---------------------------------
Eval num_timesteps=76200, episode_reward=15.97 +/- 15.17
Episode length: 23.80 +/- 21.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.8          |
|    mean_reward          | 16            |
| time/                   |               |
|    total_timesteps      | 76200         |
| train/                  |               |
|    approx_kl            | 0.00022643965 |
|    clip_fraction        | 0.01          |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.335        |
|    explained_variance   | 0.172         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.907         |
|    n_updates            | 1900          |
|    policy_gradient_loss | -0.00219      |
|    value_loss           | 3.62          |
-------------------------------------------
Eval num_timesteps=76400, episode_reward=22.81 +/- 9.71
Episode length: 33.60 +/- 13.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 22.8     |
| time/              |          |
|    total_timesteps | 76400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.5     |
|    ep_rew_mean     | 13.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 191      |
|    time_elapsed    | 21547    |
|    total_timesteps | 76400    |
---------------------------------
Eval num_timesteps=76600, episode_reward=17.34 +/- 14.07
Episode length: 26.00 +/- 20.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26            |
|    mean_reward          | 17.3          |
| time/                   |               |
|    total_timesteps      | 76600         |
| train/                  |               |
|    approx_kl            | 0.00046741564 |
|    clip_fraction        | 0.0241        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.309        |
|    explained_variance   | 0.358         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.12          |
|    n_updates            | 1910          |
|    policy_gradient_loss | -0.00341      |
|    value_loss           | 2.46          |
-------------------------------------------
Eval num_timesteps=76800, episode_reward=23.54 +/- 14.81
Episode length: 33.40 +/- 20.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 23.5     |
| time/              |          |
|    total_timesteps | 76800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.1     |
|    ep_rew_mean     | 14.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 192      |
|    time_elapsed    | 21652    |
|    total_timesteps | 76800    |
---------------------------------
Eval num_timesteps=77000, episode_reward=11.15 +/- 12.82
Episode length: 16.60 +/- 17.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.6          |
|    mean_reward          | 11.1          |
| time/                   |               |
|    total_timesteps      | 77000         |
| train/                  |               |
|    approx_kl            | 0.00069543056 |
|    clip_fraction        | 0.021         |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.338        |
|    explained_variance   | 0.201         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.813         |
|    n_updates            | 1920          |
|    policy_gradient_loss | -0.00227      |
|    value_loss           | 2.22          |
-------------------------------------------
Eval num_timesteps=77200, episode_reward=18.49 +/- 13.52
Episode length: 27.00 +/- 18.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27       |
|    mean_reward     | 18.5     |
| time/              |          |
|    total_timesteps | 77200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | 14       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 193      |
|    time_elapsed    | 21755    |
|    total_timesteps | 77200    |
---------------------------------
Eval num_timesteps=77400, episode_reward=18.23 +/- 14.56
Episode length: 26.60 +/- 20.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.6         |
|    mean_reward          | 18.2         |
| time/                   |              |
|    total_timesteps      | 77400        |
| train/                  |              |
|    approx_kl            | 0.0004142362 |
|    clip_fraction        | 0.0208       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.313       |
|    explained_variance   | 0.365        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.373        |
|    n_updates            | 1930         |
|    policy_gradient_loss | -0.00185     |
|    value_loss           | 2.35         |
------------------------------------------
Eval num_timesteps=77600, episode_reward=16.61 +/- 15.19
Episode length: 24.40 +/- 21.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 77600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.3     |
|    ep_rew_mean     | 13.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 194      |
|    time_elapsed    | 21864    |
|    total_timesteps | 77600    |
---------------------------------
Eval num_timesteps=77800, episode_reward=21.32 +/- 12.52
Episode length: 30.80 +/- 17.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 30.8          |
|    mean_reward          | 21.3          |
| time/                   |               |
|    total_timesteps      | 77800         |
| train/                  |               |
|    approx_kl            | 0.00040647932 |
|    clip_fraction        | 0.0158        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.314        |
|    explained_variance   | 0.412         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.14          |
|    n_updates            | 1940          |
|    policy_gradient_loss | -0.00196      |
|    value_loss           | 2.56          |
-------------------------------------------
Eval num_timesteps=78000, episode_reward=16.99 +/- 15.64
Episode length: 24.20 +/- 21.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 78000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.1     |
|    ep_rew_mean     | 13.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 195      |
|    time_elapsed    | 21986    |
|    total_timesteps | 78000    |
---------------------------------
Eval num_timesteps=78200, episode_reward=14.61 +/- 11.35
Episode length: 21.60 +/- 15.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 21.6          |
|    mean_reward          | 14.6          |
| time/                   |               |
|    total_timesteps      | 78200         |
| train/                  |               |
|    approx_kl            | 0.00025935657 |
|    clip_fraction        | 0.0103        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.331        |
|    explained_variance   | 0.309         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.32          |
|    n_updates            | 1950          |
|    policy_gradient_loss | -0.00171      |
|    value_loss           | 2.54          |
-------------------------------------------
Eval num_timesteps=78400, episode_reward=17.86 +/- 13.57
Episode length: 26.20 +/- 19.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 78400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.7     |
|    ep_rew_mean     | 14.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 196      |
|    time_elapsed    | 22093    |
|    total_timesteps | 78400    |
---------------------------------
Eval num_timesteps=78600, episode_reward=25.49 +/- 11.03
Episode length: 37.00 +/- 15.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 37           |
|    mean_reward          | 25.5         |
| time/                   |              |
|    total_timesteps      | 78600        |
| train/                  |              |
|    approx_kl            | 0.0005039288 |
|    clip_fraction        | 0.00893      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.346       |
|    explained_variance   | 0.0609       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.57         |
|    n_updates            | 1960         |
|    policy_gradient_loss | -0.00171     |
|    value_loss           | 2.52         |
------------------------------------------
Eval num_timesteps=78800, episode_reward=23.34 +/- 14.14
Episode length: 33.80 +/- 19.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 23.3     |
| time/              |          |
|    total_timesteps | 78800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.9     |
|    ep_rew_mean     | 15.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 197      |
|    time_elapsed    | 22208    |
|    total_timesteps | 78800    |
---------------------------------
Eval num_timesteps=79000, episode_reward=15.79 +/- 16.20
Episode length: 22.80 +/- 22.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22.8         |
|    mean_reward          | 15.8         |
| time/                   |              |
|    total_timesteps      | 79000        |
| train/                  |              |
|    approx_kl            | 0.0006898209 |
|    clip_fraction        | 0.0371       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.342       |
|    explained_variance   | 0.215        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.434        |
|    n_updates            | 1970         |
|    policy_gradient_loss | -0.00311     |
|    value_loss           | 1.98         |
------------------------------------------
Eval num_timesteps=79200, episode_reward=22.21 +/- 15.94
Episode length: 32.00 +/- 22.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32       |
|    mean_reward     | 22.2     |
| time/              |          |
|    total_timesteps | 79200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.2     |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 198      |
|    time_elapsed    | 22335    |
|    total_timesteps | 79200    |
---------------------------------
Eval num_timesteps=79400, episode_reward=12.50 +/- 11.71
Episode length: 19.40 +/- 17.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 19.4         |
|    mean_reward          | 12.5         |
| time/                   |              |
|    total_timesteps      | 79400        |
| train/                  |              |
|    approx_kl            | 0.0011816478 |
|    clip_fraction        | 0.0364       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.316       |
|    explained_variance   | 0.169        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.18         |
|    n_updates            | 1980         |
|    policy_gradient_loss | -0.00236     |
|    value_loss           | 2.73         |
------------------------------------------
Eval num_timesteps=79600, episode_reward=10.49 +/- 12.65
Episode length: 15.60 +/- 17.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 10.5     |
| time/              |          |
|    total_timesteps | 79600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.8     |
|    ep_rew_mean     | 15.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 199      |
|    time_elapsed    | 22439    |
|    total_timesteps | 79600    |
---------------------------------
Eval num_timesteps=79800, episode_reward=10.68 +/- 12.68
Episode length: 15.80 +/- 17.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.8          |
|    mean_reward          | 10.7          |
| time/                   |               |
|    total_timesteps      | 79800         |
| train/                  |               |
|    approx_kl            | 0.00023058696 |
|    clip_fraction        | 0.00982       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.306        |
|    explained_variance   | 0.35          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.997         |
|    n_updates            | 1990          |
|    policy_gradient_loss | -0.00122      |
|    value_loss           | 2.01          |
-------------------------------------------
Eval num_timesteps=80000, episode_reward=16.20 +/- 15.50
Episode length: 23.60 +/- 21.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 16.2     |
| time/              |          |
|    total_timesteps | 80000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 200      |
|    time_elapsed    | 22549    |
|    total_timesteps | 80000    |
---------------------------------
Eval num_timesteps=80200, episode_reward=25.74 +/- 12.61
Episode length: 36.40 +/- 16.85
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.4          |
|    mean_reward          | 25.7          |
| time/                   |               |
|    total_timesteps      | 80200         |
| train/                  |               |
|    approx_kl            | 0.00095411204 |
|    clip_fraction        | 0.00937       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.305        |
|    explained_variance   | 0.201         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.53          |
|    n_updates            | 2000          |
|    policy_gradient_loss | -0.0018       |
|    value_loss           | 1.65          |
-------------------------------------------
Eval num_timesteps=80400, episode_reward=22.96 +/- 15.57
Episode length: 32.80 +/- 21.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 23       |
| time/              |          |
|    total_timesteps | 80400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | 17.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 201      |
|    time_elapsed    | 22663    |
|    total_timesteps | 80400    |
---------------------------------
Eval num_timesteps=80600, episode_reward=10.87 +/- 11.84
Episode length: 16.40 +/- 16.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | 10.9         |
| time/                   |              |
|    total_timesteps      | 80600        |
| train/                  |              |
|    approx_kl            | 0.0010177323 |
|    clip_fraction        | 0.0154       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.315       |
|    explained_variance   | 0.46         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.589        |
|    n_updates            | 2010         |
|    policy_gradient_loss | -0.00184     |
|    value_loss           | 1.72         |
------------------------------------------
Eval num_timesteps=80800, episode_reward=16.17 +/- 15.03
Episode length: 23.80 +/- 21.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | 16.2     |
| time/              |          |
|    total_timesteps | 80800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.3     |
|    ep_rew_mean     | 17.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 202      |
|    time_elapsed    | 22769    |
|    total_timesteps | 80800    |
---------------------------------
Eval num_timesteps=81000, episode_reward=22.61 +/- 15.13
Episode length: 32.80 +/- 21.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32.8          |
|    mean_reward          | 22.6          |
| time/                   |               |
|    total_timesteps      | 81000         |
| train/                  |               |
|    approx_kl            | 0.00022179594 |
|    clip_fraction        | 0.0112        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.328        |
|    explained_variance   | 0.253         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.59          |
|    n_updates            | 2020          |
|    policy_gradient_loss | -0.00152      |
|    value_loss           | 1.82          |
-------------------------------------------
Eval num_timesteps=81200, episode_reward=17.01 +/- 15.33
Episode length: 24.40 +/- 21.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 81200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.7     |
|    ep_rew_mean     | 14.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 203      |
|    time_elapsed    | 22880    |
|    total_timesteps | 81200    |
---------------------------------
Eval num_timesteps=81400, episode_reward=17.07 +/- 14.29
Episode length: 25.20 +/- 20.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.2         |
|    mean_reward          | 17.1         |
| time/                   |              |
|    total_timesteps      | 81400        |
| train/                  |              |
|    approx_kl            | 0.0002690136 |
|    clip_fraction        | 0.0141       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.274       |
|    explained_variance   | 0.225        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.36         |
|    n_updates            | 2030         |
|    policy_gradient_loss | -0.00261     |
|    value_loss           | 4.23         |
------------------------------------------
Eval num_timesteps=81600, episode_reward=10.93 +/- 12.98
Episode length: 15.80 +/- 17.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | 10.9     |
| time/              |          |
|    total_timesteps | 81600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.3     |
|    ep_rew_mean     | 14.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 204      |
|    time_elapsed    | 22988    |
|    total_timesteps | 81600    |
---------------------------------
Eval num_timesteps=81800, episode_reward=18.16 +/- 14.43
Episode length: 26.00 +/- 19.85
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26            |
|    mean_reward          | 18.2          |
| time/                   |               |
|    total_timesteps      | 81800         |
| train/                  |               |
|    approx_kl            | 9.3060946e-05 |
|    clip_fraction        | 0.00246       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.293        |
|    explained_variance   | 0.386         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.183         |
|    n_updates            | 2040          |
|    policy_gradient_loss | -0.000925     |
|    value_loss           | 2.36          |
-------------------------------------------
Eval num_timesteps=82000, episode_reward=17.87 +/- 13.31
Episode length: 26.60 +/- 19.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.6     |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 82000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.3     |
|    ep_rew_mean     | 15.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 205      |
|    time_elapsed    | 23096    |
|    total_timesteps | 82000    |
---------------------------------
Eval num_timesteps=82200, episode_reward=28.77 +/- 11.98
Episode length: 41.60 +/- 16.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 41.6          |
|    mean_reward          | 28.8          |
| time/                   |               |
|    total_timesteps      | 82200         |
| train/                  |               |
|    approx_kl            | 0.00047158782 |
|    clip_fraction        | 0.0165        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.289        |
|    explained_variance   | 0.106         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.84          |
|    n_updates            | 2050          |
|    policy_gradient_loss | -0.00146      |
|    value_loss           | 1.5           |
-------------------------------------------
Eval num_timesteps=82400, episode_reward=15.39 +/- 15.16
Episode length: 23.20 +/- 21.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 15.4     |
| time/              |          |
|    total_timesteps | 82400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.2     |
|    ep_rew_mean     | 14.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 206      |
|    time_elapsed    | 23201    |
|    total_timesteps | 82400    |
---------------------------------
Eval num_timesteps=82600, episode_reward=9.89 +/- 13.53
Episode length: 14.40 +/- 18.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.4          |
|    mean_reward          | 9.89          |
| time/                   |               |
|    total_timesteps      | 82600         |
| train/                  |               |
|    approx_kl            | 0.00021914007 |
|    clip_fraction        | 0.0167        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.261        |
|    explained_variance   | 0.2           |
|    learning_rate        | 0.0001        |
|    loss                 | 2.26          |
|    n_updates            | 2060          |
|    policy_gradient_loss | -0.00215      |
|    value_loss           | 2.21          |
-------------------------------------------
Eval num_timesteps=82800, episode_reward=10.92 +/- 13.25
Episode length: 15.80 +/- 17.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | 10.9     |
| time/              |          |
|    total_timesteps | 82800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.9     |
|    ep_rew_mean     | 14.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 207      |
|    time_elapsed    | 23311    |
|    total_timesteps | 82800    |
---------------------------------
Eval num_timesteps=83000, episode_reward=21.48 +/- 11.27
Episode length: 31.60 +/- 16.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 31.6         |
|    mean_reward          | 21.5         |
| time/                   |              |
|    total_timesteps      | 83000        |
| train/                  |              |
|    approx_kl            | 0.0003792291 |
|    clip_fraction        | 0.0116       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.262       |
|    explained_variance   | 0.336        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.22         |
|    n_updates            | 2070         |
|    policy_gradient_loss | -0.000948    |
|    value_loss           | 2.05         |
------------------------------------------
Eval num_timesteps=83200, episode_reward=22.78 +/- 15.70
Episode length: 32.40 +/- 21.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.4     |
|    mean_reward     | 22.8     |
| time/              |          |
|    total_timesteps | 83200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.2     |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 208      |
|    time_elapsed    | 23419    |
|    total_timesteps | 83200    |
---------------------------------
Eval num_timesteps=83400, episode_reward=21.46 +/- 16.85
Episode length: 30.80 +/- 23.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30.8         |
|    mean_reward          | 21.5         |
| time/                   |              |
|    total_timesteps      | 83400        |
| train/                  |              |
|    approx_kl            | 0.0011120337 |
|    clip_fraction        | 0.0212       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.301       |
|    explained_variance   | 0.273        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.435        |
|    n_updates            | 2080         |
|    policy_gradient_loss | -0.00146     |
|    value_loss           | 2.17         |
------------------------------------------
Eval num_timesteps=83600, episode_reward=19.17 +/- 13.87
Episode length: 27.40 +/- 18.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.4     |
|    mean_reward     | 19.2     |
| time/              |          |
|    total_timesteps | 83600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 209      |
|    time_elapsed    | 23528    |
|    total_timesteps | 83600    |
---------------------------------
Eval num_timesteps=83800, episode_reward=15.70 +/- 16.29
Episode length: 22.60 +/- 22.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22.6         |
|    mean_reward          | 15.7         |
| time/                   |              |
|    total_timesteps      | 83800        |
| train/                  |              |
|    approx_kl            | 0.0004896483 |
|    clip_fraction        | 0.0333       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.278       |
|    explained_variance   | 0.251        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.76         |
|    n_updates            | 2090         |
|    policy_gradient_loss | -0.0025      |
|    value_loss           | 2.97         |
------------------------------------------
Eval num_timesteps=84000, episode_reward=16.19 +/- 15.86
Episode length: 23.40 +/- 21.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 16.2     |
| time/              |          |
|    total_timesteps | 84000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.5     |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 210      |
|    time_elapsed    | 23632    |
|    total_timesteps | 84000    |
---------------------------------
Eval num_timesteps=84200, episode_reward=21.97 +/- 16.27
Episode length: 31.60 +/- 22.57
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 31.6          |
|    mean_reward          | 22            |
| time/                   |               |
|    total_timesteps      | 84200         |
| train/                  |               |
|    approx_kl            | 0.00023096187 |
|    clip_fraction        | 0.0143        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.277        |
|    explained_variance   | 0.429         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.443         |
|    n_updates            | 2100          |
|    policy_gradient_loss | -0.00123      |
|    value_loss           | 1.33          |
-------------------------------------------
Eval num_timesteps=84400, episode_reward=16.35 +/- 15.85
Episode length: 23.60 +/- 21.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 16.3     |
| time/              |          |
|    total_timesteps | 84400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.8     |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 211      |
|    time_elapsed    | 23739    |
|    total_timesteps | 84400    |
---------------------------------
Eval num_timesteps=84600, episode_reward=19.67 +/- 13.90
Episode length: 27.80 +/- 18.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 27.8          |
|    mean_reward          | 19.7          |
| time/                   |               |
|    total_timesteps      | 84600         |
| train/                  |               |
|    approx_kl            | 0.00035766806 |
|    clip_fraction        | 0.0134        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.277        |
|    explained_variance   | 0.353         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.0566        |
|    n_updates            | 2110          |
|    policy_gradient_loss | -0.00235      |
|    value_loss           | 1.13          |
-------------------------------------------
Eval num_timesteps=84800, episode_reward=16.81 +/- 15.84
Episode length: 24.00 +/- 21.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 84800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.5     |
|    ep_rew_mean     | 16.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 212      |
|    time_elapsed    | 23849    |
|    total_timesteps | 84800    |
---------------------------------
Eval num_timesteps=85000, episode_reward=16.55 +/- 14.20
Episode length: 24.80 +/- 20.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.8         |
|    mean_reward          | 16.6         |
| time/                   |              |
|    total_timesteps      | 85000        |
| train/                  |              |
|    approx_kl            | 0.0002976701 |
|    clip_fraction        | 0.01         |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.285       |
|    explained_variance   | 0.276        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.71         |
|    n_updates            | 2120         |
|    policy_gradient_loss | -0.00134     |
|    value_loss           | 2.88         |
------------------------------------------
Eval num_timesteps=85200, episode_reward=22.69 +/- 14.16
Episode length: 33.60 +/- 20.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 22.7     |
| time/              |          |
|    total_timesteps | 85200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 213      |
|    time_elapsed    | 23977    |
|    total_timesteps | 85200    |
---------------------------------
Eval num_timesteps=85400, episode_reward=17.56 +/- 14.76
Episode length: 25.20 +/- 20.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.2          |
|    mean_reward          | 17.6          |
| time/                   |               |
|    total_timesteps      | 85400         |
| train/                  |               |
|    approx_kl            | 0.00025023255 |
|    clip_fraction        | 0.00558       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.303        |
|    explained_variance   | 0.429         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.631         |
|    n_updates            | 2130          |
|    policy_gradient_loss | -0.00224      |
|    value_loss           | 1.43          |
-------------------------------------------
Eval num_timesteps=85600, episode_reward=10.29 +/- 13.30
Episode length: 15.00 +/- 17.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | 10.3     |
| time/              |          |
|    total_timesteps | 85600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.2     |
|    ep_rew_mean     | 17.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 214      |
|    time_elapsed    | 24084    |
|    total_timesteps | 85600    |
---------------------------------
Eval num_timesteps=85800, episode_reward=17.58 +/- 13.59
Episode length: 26.60 +/- 19.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.6         |
|    mean_reward          | 17.6         |
| time/                   |              |
|    total_timesteps      | 85800        |
| train/                  |              |
|    approx_kl            | 0.0002928122 |
|    clip_fraction        | 0.00313      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.284       |
|    explained_variance   | 0.047        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.723        |
|    n_updates            | 2140         |
|    policy_gradient_loss | -0.00133     |
|    value_loss           | 1.8          |
------------------------------------------
Eval num_timesteps=86000, episode_reward=12.49 +/- 11.22
Episode length: 19.00 +/- 15.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 12.5     |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.9     |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 215      |
|    time_elapsed    | 24195    |
|    total_timesteps | 86000    |
---------------------------------
Eval num_timesteps=86200, episode_reward=9.43 +/- 13.09
Episode length: 14.00 +/- 18.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14           |
|    mean_reward          | 9.43         |
| time/                   |              |
|    total_timesteps      | 86200        |
| train/                  |              |
|    approx_kl            | 0.0005328075 |
|    clip_fraction        | 0.0237       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.294       |
|    explained_variance   | 0.356        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.984        |
|    n_updates            | 2150         |
|    policy_gradient_loss | -0.00183     |
|    value_loss           | 1.56         |
------------------------------------------
Eval num_timesteps=86400, episode_reward=17.89 +/- 14.54
Episode length: 25.80 +/- 19.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 86400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.3     |
|    ep_rew_mean     | 18.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 216      |
|    time_elapsed    | 24301    |
|    total_timesteps | 86400    |
---------------------------------
Eval num_timesteps=86600, episode_reward=10.22 +/- 12.09
Episode length: 15.80 +/- 17.86
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.8          |
|    mean_reward          | 10.2          |
| time/                   |               |
|    total_timesteps      | 86600         |
| train/                  |               |
|    approx_kl            | 0.00040790119 |
|    clip_fraction        | 0.0143        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.309        |
|    explained_variance   | 0.246         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.18          |
|    n_updates            | 2160          |
|    policy_gradient_loss | -0.0012       |
|    value_loss           | 2.23          |
-------------------------------------------
Eval num_timesteps=86800, episode_reward=23.81 +/- 14.49
Episode length: 33.80 +/- 19.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 23.8     |
| time/              |          |
|    total_timesteps | 86800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.2     |
|    ep_rew_mean     | 18.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 217      |
|    time_elapsed    | 24407    |
|    total_timesteps | 86800    |
---------------------------------
Eval num_timesteps=87000, episode_reward=12.83 +/- 12.11
Episode length: 19.00 +/- 16.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 19            |
|    mean_reward          | 12.8          |
| time/                   |               |
|    total_timesteps      | 87000         |
| train/                  |               |
|    approx_kl            | 0.00010967547 |
|    clip_fraction        | 0.00737       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.317        |
|    explained_variance   | 0.31          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.437         |
|    n_updates            | 2170          |
|    policy_gradient_loss | -0.00196      |
|    value_loss           | 2.17          |
-------------------------------------------
Eval num_timesteps=87200, episode_reward=17.12 +/- 14.73
Episode length: 25.00 +/- 20.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | 17.1     |
| time/              |          |
|    total_timesteps | 87200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.8     |
|    ep_rew_mean     | 18.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 218      |
|    time_elapsed    | 24534    |
|    total_timesteps | 87200    |
---------------------------------
Eval num_timesteps=87400, episode_reward=12.87 +/- 12.34
Episode length: 19.20 +/- 17.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 19.2         |
|    mean_reward          | 12.9         |
| time/                   |              |
|    total_timesteps      | 87400        |
| train/                  |              |
|    approx_kl            | 0.0024570937 |
|    clip_fraction        | 0.0643       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.363       |
|    explained_variance   | 0.243        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.4          |
|    n_updates            | 2180         |
|    policy_gradient_loss | -0.00251     |
|    value_loss           | 1.18         |
------------------------------------------
Eval num_timesteps=87600, episode_reward=17.66 +/- 14.43
Episode length: 26.20 +/- 20.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | 17.7     |
| time/              |          |
|    total_timesteps | 87600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.3     |
|    ep_rew_mean     | 18.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 219      |
|    time_elapsed    | 24638    |
|    total_timesteps | 87600    |
---------------------------------
Eval num_timesteps=87800, episode_reward=28.83 +/- 14.09
Episode length: 40.40 +/- 19.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 40.4          |
|    mean_reward          | 28.8          |
| time/                   |               |
|    total_timesteps      | 87800         |
| train/                  |               |
|    approx_kl            | 0.00043019798 |
|    clip_fraction        | 0.0239        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.344        |
|    explained_variance   | 0.287         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.76          |
|    n_updates            | 2190          |
|    policy_gradient_loss | -0.00329      |
|    value_loss           | 2.43          |
-------------------------------------------
Eval num_timesteps=88000, episode_reward=14.87 +/- 10.84
Episode length: 22.40 +/- 15.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 14.9     |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.4     |
|    ep_rew_mean     | 18.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 220      |
|    time_elapsed    | 24748    |
|    total_timesteps | 88000    |
---------------------------------
Eval num_timesteps=88200, episode_reward=23.53 +/- 14.83
Episode length: 33.40 +/- 20.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.4        |
|    mean_reward          | 23.5        |
| time/                   |             |
|    total_timesteps      | 88200       |
| train/                  |             |
|    approx_kl            | 9.31582e-05 |
|    clip_fraction        | 0.00826     |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.345      |
|    explained_variance   | 0.319       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.83        |
|    n_updates            | 2200        |
|    policy_gradient_loss | -0.0017     |
|    value_loss           | 1.69        |
-----------------------------------------
Eval num_timesteps=88400, episode_reward=21.00 +/- 16.50
Episode length: 30.80 +/- 23.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.8     |
|    mean_reward     | 21       |
| time/              |          |
|    total_timesteps | 88400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.6     |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 221      |
|    time_elapsed    | 24881    |
|    total_timesteps | 88400    |
---------------------------------
Eval num_timesteps=88600, episode_reward=16.43 +/- 15.33
Episode length: 24.00 +/- 21.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24            |
|    mean_reward          | 16.4          |
| time/                   |               |
|    total_timesteps      | 88600         |
| train/                  |               |
|    approx_kl            | 0.00026348486 |
|    clip_fraction        | 0.0107        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.33         |
|    explained_variance   | 0.24          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.929         |
|    n_updates            | 2210          |
|    policy_gradient_loss | -0.00184      |
|    value_loss           | 2.82          |
-------------------------------------------
Eval num_timesteps=88800, episode_reward=23.03 +/- 15.01
Episode length: 33.00 +/- 20.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 23       |
| time/              |          |
|    total_timesteps | 88800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.9     |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 222      |
|    time_elapsed    | 25009    |
|    total_timesteps | 88800    |
---------------------------------
Eval num_timesteps=89000, episode_reward=17.97 +/- 15.44
Episode length: 25.20 +/- 20.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.2          |
|    mean_reward          | 18            |
| time/                   |               |
|    total_timesteps      | 89000         |
| train/                  |               |
|    approx_kl            | 0.00032584634 |
|    clip_fraction        | 0.0123        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.32         |
|    explained_variance   | 0.42          |
|    learning_rate        | 0.0001        |
|    loss                 | 1.23          |
|    n_updates            | 2220          |
|    policy_gradient_loss | -0.00157      |
|    value_loss           | 2.46          |
-------------------------------------------
Eval num_timesteps=89200, episode_reward=23.48 +/- 13.96
Episode length: 34.00 +/- 19.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 23.5     |
| time/              |          |
|    total_timesteps | 89200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.1     |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 223      |
|    time_elapsed    | 25144    |
|    total_timesteps | 89200    |
---------------------------------
Eval num_timesteps=89400, episode_reward=10.66 +/- 13.11
Episode length: 15.60 +/- 17.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.6          |
|    mean_reward          | 10.7          |
| time/                   |               |
|    total_timesteps      | 89400         |
| train/                  |               |
|    approx_kl            | 0.00052486535 |
|    clip_fraction        | 0.015         |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.322        |
|    explained_variance   | 0.314         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.81          |
|    n_updates            | 2230          |
|    policy_gradient_loss | -0.00232      |
|    value_loss           | 2.4           |
-------------------------------------------
Eval num_timesteps=89600, episode_reward=9.77 +/- 13.47
Episode length: 14.20 +/- 17.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.2     |
|    mean_reward     | 9.77     |
| time/              |          |
|    total_timesteps | 89600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.4     |
|    ep_rew_mean     | 14.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 224      |
|    time_elapsed    | 25248    |
|    total_timesteps | 89600    |
---------------------------------
Eval num_timesteps=89800, episode_reward=5.24 +/- 5.30
Episode length: 8.60 +/- 8.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 8.6          |
|    mean_reward          | 5.24         |
| time/                   |              |
|    total_timesteps      | 89800        |
| train/                  |              |
|    approx_kl            | 0.0011478569 |
|    clip_fraction        | 0.0176       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.327       |
|    explained_variance   | 0.318        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.64         |
|    n_updates            | 2240         |
|    policy_gradient_loss | -0.00206     |
|    value_loss           | 2.03         |
------------------------------------------
Eval num_timesteps=90000, episode_reward=35.14 +/- 0.54
Episode length: 50.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 35.1     |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.1     |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 225      |
|    time_elapsed    | 25363    |
|    total_timesteps | 90000    |
---------------------------------
Eval num_timesteps=90200, episode_reward=10.89 +/- 12.97
Episode length: 16.00 +/- 17.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16            |
|    mean_reward          | 10.9          |
| time/                   |               |
|    total_timesteps      | 90200         |
| train/                  |               |
|    approx_kl            | 0.00024937134 |
|    clip_fraction        | 0.0138        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.319        |
|    explained_variance   | 0.333         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.82          |
|    n_updates            | 2250          |
|    policy_gradient_loss | -0.00267      |
|    value_loss           | 2.67          |
-------------------------------------------
Eval num_timesteps=90400, episode_reward=16.09 +/- 15.03
Episode length: 23.80 +/- 21.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | 16.1     |
| time/              |          |
|    total_timesteps | 90400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.6     |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 226      |
|    time_elapsed    | 25465    |
|    total_timesteps | 90400    |
---------------------------------
Eval num_timesteps=90600, episode_reward=21.90 +/- 16.34
Episode length: 31.40 +/- 22.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 31.4          |
|    mean_reward          | 21.9          |
| time/                   |               |
|    total_timesteps      | 90600         |
| train/                  |               |
|    approx_kl            | 0.00036287613 |
|    clip_fraction        | 0.0109        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.357        |
|    explained_variance   | 0.418         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.39          |
|    n_updates            | 2260          |
|    policy_gradient_loss | -0.00278      |
|    value_loss           | 1.54          |
-------------------------------------------
Eval num_timesteps=90800, episode_reward=11.20 +/- 5.69
Episode length: 17.00 +/- 8.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | 11.2     |
| time/              |          |
|    total_timesteps | 90800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.5     |
|    ep_rew_mean     | 16.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 227      |
|    time_elapsed    | 25582    |
|    total_timesteps | 90800    |
---------------------------------
Eval num_timesteps=91000, episode_reward=15.24 +/- 12.53
Episode length: 22.20 +/- 17.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22.2         |
|    mean_reward          | 15.2         |
| time/                   |              |
|    total_timesteps      | 91000        |
| train/                  |              |
|    approx_kl            | 0.0007907803 |
|    clip_fraction        | 0.0174       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.366       |
|    explained_variance   | 0.24         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.15         |
|    n_updates            | 2270         |
|    policy_gradient_loss | -0.00173     |
|    value_loss           | 1.17         |
------------------------------------------
Eval num_timesteps=91200, episode_reward=16.22 +/- 16.40
Episode length: 23.00 +/- 22.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 16.2     |
| time/              |          |
|    total_timesteps | 91200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.4     |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 228      |
|    time_elapsed    | 25686    |
|    total_timesteps | 91200    |
---------------------------------
Eval num_timesteps=91400, episode_reward=21.89 +/- 14.05
Episode length: 33.00 +/- 20.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33            |
|    mean_reward          | 21.9          |
| time/                   |               |
|    total_timesteps      | 91400         |
| train/                  |               |
|    approx_kl            | 0.00052024826 |
|    clip_fraction        | 0.0241        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.374        |
|    explained_variance   | 0.311         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.931         |
|    n_updates            | 2280          |
|    policy_gradient_loss | -0.00363      |
|    value_loss           | 2.34          |
-------------------------------------------
Eval num_timesteps=91600, episode_reward=22.56 +/- 16.47
Episode length: 31.80 +/- 22.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.8     |
|    mean_reward     | 22.6     |
| time/              |          |
|    total_timesteps | 91600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.5     |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 229      |
|    time_elapsed    | 25814    |
|    total_timesteps | 91600    |
---------------------------------
Eval num_timesteps=91800, episode_reward=23.29 +/- 15.07
Episode length: 33.00 +/- 20.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33           |
|    mean_reward          | 23.3         |
| time/                   |              |
|    total_timesteps      | 91800        |
| train/                  |              |
|    approx_kl            | 0.0006931134 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.361       |
|    explained_variance   | 0.273        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.32         |
|    n_updates            | 2290         |
|    policy_gradient_loss | -0.00225     |
|    value_loss           | 1.58         |
------------------------------------------
Eval num_timesteps=92000, episode_reward=23.80 +/- 13.15
Episode length: 35.00 +/- 18.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 23.8     |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.4     |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 230      |
|    time_elapsed    | 25922    |
|    total_timesteps | 92000    |
---------------------------------
Eval num_timesteps=92200, episode_reward=17.50 +/- 16.15
Episode length: 24.60 +/- 21.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.6         |
|    mean_reward          | 17.5         |
| time/                   |              |
|    total_timesteps      | 92200        |
| train/                  |              |
|    approx_kl            | 0.0010587339 |
|    clip_fraction        | 0.0348       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.354       |
|    explained_variance   | 0.366        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.299        |
|    n_updates            | 2300         |
|    policy_gradient_loss | -0.0024      |
|    value_loss           | 2.44         |
------------------------------------------
Eval num_timesteps=92400, episode_reward=16.29 +/- 14.89
Episode length: 24.20 +/- 21.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 16.3     |
| time/              |          |
|    total_timesteps | 92400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.1     |
|    ep_rew_mean     | 16       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 231      |
|    time_elapsed    | 26030    |
|    total_timesteps | 92400    |
---------------------------------
Eval num_timesteps=92600, episode_reward=18.82 +/- 12.97
Episode length: 27.80 +/- 18.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 27.8         |
|    mean_reward          | 18.8         |
| time/                   |              |
|    total_timesteps      | 92600        |
| train/                  |              |
|    approx_kl            | 0.0011813614 |
|    clip_fraction        | 0.025        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.381       |
|    explained_variance   | 0.436        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.06         |
|    n_updates            | 2310         |
|    policy_gradient_loss | -0.00296     |
|    value_loss           | 2.55         |
------------------------------------------
Eval num_timesteps=92800, episode_reward=12.85 +/- 10.69
Episode length: 19.60 +/- 15.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 12.8     |
| time/              |          |
|    total_timesteps | 92800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.9     |
|    ep_rew_mean     | 14.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 232      |
|    time_elapsed    | 26149    |
|    total_timesteps | 92800    |
---------------------------------
Eval num_timesteps=93000, episode_reward=22.01 +/- 14.83
Episode length: 32.60 +/- 21.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32.6          |
|    mean_reward          | 22            |
| time/                   |               |
|    total_timesteps      | 93000         |
| train/                  |               |
|    approx_kl            | 0.00053381204 |
|    clip_fraction        | 0.0154        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.396        |
|    explained_variance   | 0.0819        |
|    learning_rate        | 0.0001        |
|    loss                 | 1.05          |
|    n_updates            | 2320          |
|    policy_gradient_loss | -0.00354      |
|    value_loss           | 3.49          |
-------------------------------------------
Eval num_timesteps=93200, episode_reward=21.81 +/- 16.43
Episode length: 31.40 +/- 22.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.4     |
|    mean_reward     | 21.8     |
| time/              |          |
|    total_timesteps | 93200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.5     |
|    ep_rew_mean     | 14       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 233      |
|    time_elapsed    | 26256    |
|    total_timesteps | 93200    |
---------------------------------
Eval num_timesteps=93400, episode_reward=17.62 +/- 14.68
Episode length: 25.60 +/- 19.93
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.6          |
|    mean_reward          | 17.6          |
| time/                   |               |
|    total_timesteps      | 93400         |
| train/                  |               |
|    approx_kl            | 0.00027250565 |
|    clip_fraction        | 0.0123        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.405        |
|    explained_variance   | 0.133         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.05          |
|    n_updates            | 2330          |
|    policy_gradient_loss | -0.00223      |
|    value_loss           | 2.83          |
-------------------------------------------
Eval num_timesteps=93600, episode_reward=17.37 +/- 14.06
Episode length: 25.60 +/- 20.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 93600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20       |
|    ep_rew_mean     | 13.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 234      |
|    time_elapsed    | 26363    |
|    total_timesteps | 93600    |
---------------------------------
Eval num_timesteps=93800, episode_reward=11.82 +/- 13.07
Episode length: 17.20 +/- 17.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17.2          |
|    mean_reward          | 11.8          |
| time/                   |               |
|    total_timesteps      | 93800         |
| train/                  |               |
|    approx_kl            | 0.00069462985 |
|    clip_fraction        | 0.0156        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.426        |
|    explained_variance   | 0.434         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.953         |
|    n_updates            | 2340          |
|    policy_gradient_loss | -0.00213      |
|    value_loss           | 1.44          |
-------------------------------------------
Eval num_timesteps=94000, episode_reward=16.97 +/- 15.31
Episode length: 24.60 +/- 20.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.6     |
|    ep_rew_mean     | 13.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 235      |
|    time_elapsed    | 26467    |
|    total_timesteps | 94000    |
---------------------------------
Eval num_timesteps=94200, episode_reward=18.61 +/- 13.33
Episode length: 27.20 +/- 19.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 27.2          |
|    mean_reward          | 18.6          |
| time/                   |               |
|    total_timesteps      | 94200         |
| train/                  |               |
|    approx_kl            | 0.00037714187 |
|    clip_fraction        | 0.0123        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.441        |
|    explained_variance   | 0.307         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.56          |
|    n_updates            | 2350          |
|    policy_gradient_loss | -0.00276      |
|    value_loss           | 2.9           |
-------------------------------------------
Eval num_timesteps=94400, episode_reward=22.12 +/- 12.39
Episode length: 31.60 +/- 16.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.6     |
|    mean_reward     | 22.1     |
| time/              |          |
|    total_timesteps | 94400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | 14       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 236      |
|    time_elapsed    | 26580    |
|    total_timesteps | 94400    |
---------------------------------
Eval num_timesteps=94600, episode_reward=30.69 +/- 10.41
Episode length: 43.20 +/- 13.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 43.2         |
|    mean_reward          | 30.7         |
| time/                   |              |
|    total_timesteps      | 94600        |
| train/                  |              |
|    approx_kl            | 0.0001744258 |
|    clip_fraction        | 0.000446     |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.457       |
|    explained_variance   | 0.12         |
|    learning_rate        | 0.0001       |
|    loss                 | 1.27         |
|    n_updates            | 2360         |
|    policy_gradient_loss | -0.000632    |
|    value_loss           | 3.01         |
------------------------------------------
Eval num_timesteps=94800, episode_reward=28.61 +/- 12.36
Episode length: 41.40 +/- 17.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | 28.6     |
| time/              |          |
|    total_timesteps | 94800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.8     |
|    ep_rew_mean     | 15.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 237      |
|    time_elapsed    | 26731    |
|    total_timesteps | 94800    |
---------------------------------
Eval num_timesteps=95000, episode_reward=16.14 +/- 15.50
Episode length: 23.60 +/- 21.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.6         |
|    mean_reward          | 16.1         |
| time/                   |              |
|    total_timesteps      | 95000        |
| train/                  |              |
|    approx_kl            | 0.0012044861 |
|    clip_fraction        | 0.0475       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.426       |
|    explained_variance   | 0.286        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.242        |
|    n_updates            | 2370         |
|    policy_gradient_loss | -0.00421     |
|    value_loss           | 1.32         |
------------------------------------------
Eval num_timesteps=95200, episode_reward=18.73 +/- 13.94
Episode length: 27.20 +/- 18.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.2     |
|    mean_reward     | 18.7     |
| time/              |          |
|    total_timesteps | 95200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.4     |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 238      |
|    time_elapsed    | 26838    |
|    total_timesteps | 95200    |
---------------------------------
Eval num_timesteps=95400, episode_reward=13.91 +/- 12.15
Episode length: 20.40 +/- 17.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 20.4         |
|    mean_reward          | 13.9         |
| time/                   |              |
|    total_timesteps      | 95400        |
| train/                  |              |
|    approx_kl            | 0.0006282146 |
|    clip_fraction        | 0.00982      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.4         |
|    explained_variance   | 0.352        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.42         |
|    n_updates            | 2380         |
|    policy_gradient_loss | -0.00196     |
|    value_loss           | 2.03         |
------------------------------------------
Eval num_timesteps=95600, episode_reward=2.79 +/- 1.71
Episode length: 5.00 +/- 2.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 2.79     |
| time/              |          |
|    total_timesteps | 95600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.8     |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 239      |
|    time_elapsed    | 26919    |
|    total_timesteps | 95600    |
---------------------------------
Eval num_timesteps=95800, episode_reward=15.39 +/- 15.63
Episode length: 22.80 +/- 22.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 22.8        |
|    mean_reward          | 15.4        |
| time/                   |             |
|    total_timesteps      | 95800       |
| train/                  |             |
|    approx_kl            | 0.000617783 |
|    clip_fraction        | 0.0355      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.348      |
|    explained_variance   | 0.229       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.27        |
|    n_updates            | 2390        |
|    policy_gradient_loss | -0.00391    |
|    value_loss           | 3.37        |
-----------------------------------------
Eval num_timesteps=96000, episode_reward=11.76 +/- 13.09
Episode length: 17.60 +/- 18.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | 11.8     |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.8     |
|    ep_rew_mean     | 14.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 240      |
|    time_elapsed    | 27039    |
|    total_timesteps | 96000    |
---------------------------------
Eval num_timesteps=96200, episode_reward=12.89 +/- 12.12
Episode length: 19.20 +/- 17.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 19.2         |
|    mean_reward          | 12.9         |
| time/                   |              |
|    total_timesteps      | 96200        |
| train/                  |              |
|    approx_kl            | 0.0008375368 |
|    clip_fraction        | 0.0154       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.357       |
|    explained_variance   | 0.327        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.44         |
|    n_updates            | 2400         |
|    policy_gradient_loss | -0.00372     |
|    value_loss           | 2.84         |
------------------------------------------
Eval num_timesteps=96400, episode_reward=20.99 +/- 16.52
Episode length: 30.80 +/- 23.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.8     |
|    mean_reward     | 21       |
| time/              |          |
|    total_timesteps | 96400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.8     |
|    ep_rew_mean     | 14.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 241      |
|    time_elapsed    | 27143    |
|    total_timesteps | 96400    |
---------------------------------
Eval num_timesteps=96600, episode_reward=9.43 +/- 13.67
Episode length: 13.80 +/- 18.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 13.8          |
|    mean_reward          | 9.43          |
| time/                   |               |
|    total_timesteps      | 96600         |
| train/                  |               |
|    approx_kl            | 0.00046487726 |
|    clip_fraction        | 0.0299        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.374        |
|    explained_variance   | 0.424         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.838         |
|    n_updates            | 2410          |
|    policy_gradient_loss | -0.00224      |
|    value_loss           | 1.92          |
-------------------------------------------
Eval num_timesteps=96800, episode_reward=9.11 +/- 12.27
Episode length: 14.20 +/- 18.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.2     |
|    mean_reward     | 9.11     |
| time/              |          |
|    total_timesteps | 96800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | 14.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 242      |
|    time_elapsed    | 27250    |
|    total_timesteps | 96800    |
---------------------------------
Eval num_timesteps=97000, episode_reward=2.48 +/- 1.26
Episode length: 4.40 +/- 1.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 4.4          |
|    mean_reward          | 2.48         |
| time/                   |              |
|    total_timesteps      | 97000        |
| train/                  |              |
|    approx_kl            | 0.0001963055 |
|    clip_fraction        | 0.0127       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.414       |
|    explained_variance   | 0.491        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.481        |
|    n_updates            | 2420         |
|    policy_gradient_loss | -0.00148     |
|    value_loss           | 1.66         |
------------------------------------------
Eval num_timesteps=97200, episode_reward=9.80 +/- 13.11
Episode length: 14.40 +/- 18.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.4     |
|    mean_reward     | 9.8      |
| time/              |          |
|    total_timesteps | 97200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.3     |
|    ep_rew_mean     | 13.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 243      |
|    time_elapsed    | 27333    |
|    total_timesteps | 97200    |
---------------------------------
Eval num_timesteps=97400, episode_reward=9.81 +/- 12.36
Episode length: 15.00 +/- 17.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15           |
|    mean_reward          | 9.81         |
| time/                   |              |
|    total_timesteps      | 97400        |
| train/                  |              |
|    approx_kl            | 0.0004823877 |
|    clip_fraction        | 0.0196       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.386       |
|    explained_variance   | 0.334        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.988        |
|    n_updates            | 2430         |
|    policy_gradient_loss | -0.00194     |
|    value_loss           | 1.62         |
------------------------------------------
Eval num_timesteps=97600, episode_reward=18.30 +/- 14.98
Episode length: 26.00 +/- 20.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | 18.3     |
| time/              |          |
|    total_timesteps | 97600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22       |
|    ep_rew_mean     | 15.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 244      |
|    time_elapsed    | 27435    |
|    total_timesteps | 97600    |
---------------------------------
Eval num_timesteps=97800, episode_reward=16.27 +/- 14.91
Episode length: 24.00 +/- 21.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24            |
|    mean_reward          | 16.3          |
| time/                   |               |
|    total_timesteps      | 97800         |
| train/                  |               |
|    approx_kl            | 0.00020846844 |
|    clip_fraction        | 0.00848       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.386        |
|    explained_variance   | -0.18         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.145         |
|    n_updates            | 2440          |
|    policy_gradient_loss | -0.00156      |
|    value_loss           | 1.92          |
-------------------------------------------
Eval num_timesteps=98000, episode_reward=22.69 +/- 15.45
Episode length: 32.60 +/- 21.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.6     |
|    mean_reward     | 22.7     |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | 16.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 245      |
|    time_elapsed    | 27587    |
|    total_timesteps | 98000    |
---------------------------------
Eval num_timesteps=98200, episode_reward=19.01 +/- 13.95
Episode length: 27.20 +/- 19.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 27.2         |
|    mean_reward          | 19           |
| time/                   |              |
|    total_timesteps      | 98200        |
| train/                  |              |
|    approx_kl            | 0.0010562843 |
|    clip_fraction        | 0.0496       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.41        |
|    explained_variance   | 0.167        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.06         |
|    n_updates            | 2450         |
|    policy_gradient_loss | -0.00272     |
|    value_loss           | 1.8          |
------------------------------------------
Eval num_timesteps=98400, episode_reward=7.36 +/- 3.89
Episode length: 11.40 +/- 5.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 11.4     |
|    mean_reward     | 7.36     |
| time/              |          |
|    total_timesteps | 98400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.3     |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 246      |
|    time_elapsed    | 27686    |
|    total_timesteps | 98400    |
---------------------------------
Eval num_timesteps=98600, episode_reward=15.48 +/- 15.57
Episode length: 23.00 +/- 22.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 23          |
|    mean_reward          | 15.5        |
| time/                   |             |
|    total_timesteps      | 98600       |
| train/                  |             |
|    approx_kl            | 0.001682551 |
|    clip_fraction        | 0.0607      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.455      |
|    explained_variance   | 0.38        |
|    learning_rate        | 0.0001      |
|    loss                 | 1.17        |
|    n_updates            | 2460        |
|    policy_gradient_loss | -0.00351    |
|    value_loss           | 1.94        |
-----------------------------------------
Eval num_timesteps=98800, episode_reward=23.94 +/- 13.89
Episode length: 34.60 +/- 18.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 23.9     |
| time/              |          |
|    total_timesteps | 98800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.8     |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 247      |
|    time_elapsed    | 27792    |
|    total_timesteps | 98800    |
---------------------------------
Eval num_timesteps=99000, episode_reward=12.50 +/- 11.60
Episode length: 18.60 +/- 16.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.6         |
|    mean_reward          | 12.5         |
| time/                   |              |
|    total_timesteps      | 99000        |
| train/                  |              |
|    approx_kl            | 0.0006417929 |
|    clip_fraction        | 0.0167       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.489       |
|    explained_variance   | 0.29         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.394        |
|    n_updates            | 2470         |
|    policy_gradient_loss | -0.00217     |
|    value_loss           | 2.14         |
------------------------------------------
Eval num_timesteps=99200, episode_reward=23.39 +/- 15.40
Episode length: 33.00 +/- 20.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 23.4     |
| time/              |          |
|    total_timesteps | 99200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.4     |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 248      |
|    time_elapsed    | 27895    |
|    total_timesteps | 99200    |
---------------------------------
Eval num_timesteps=99400, episode_reward=14.37 +/- 16.43
Episode length: 21.40 +/- 23.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 21.4         |
|    mean_reward          | 14.4         |
| time/                   |              |
|    total_timesteps      | 99400        |
| train/                  |              |
|    approx_kl            | 0.0018350221 |
|    clip_fraction        | 0.0556       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.517       |
|    explained_variance   | 0.219        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.83         |
|    n_updates            | 2480         |
|    policy_gradient_loss | -0.0058      |
|    value_loss           | 2.63         |
------------------------------------------
Eval num_timesteps=99600, episode_reward=19.25 +/- 13.51
Episode length: 28.20 +/- 18.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.2     |
|    mean_reward     | 19.2     |
| time/              |          |
|    total_timesteps | 99600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.9     |
|    ep_rew_mean     | 18.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 249      |
|    time_elapsed    | 28003    |
|    total_timesteps | 99600    |
---------------------------------
Eval num_timesteps=99800, episode_reward=17.50 +/- 14.37
Episode length: 25.40 +/- 20.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.4          |
|    mean_reward          | 17.5          |
| time/                   |               |
|    total_timesteps      | 99800         |
| train/                  |               |
|    approx_kl            | 0.00038577404 |
|    clip_fraction        | 0.0141        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.53         |
|    explained_variance   | 0.315         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.417         |
|    n_updates            | 2490          |
|    policy_gradient_loss | -0.00243      |
|    value_loss           | 1.6           |
-------------------------------------------
Eval num_timesteps=100000, episode_reward=12.01 +/- 12.46
Episode length: 17.20 +/- 16.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | 12       |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.6     |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 250      |
|    time_elapsed    | 28109    |
|    total_timesteps | 100000   |
---------------------------------
Eval num_timesteps=100200, episode_reward=23.67 +/- 12.91
Episode length: 35.20 +/- 18.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.2         |
|    mean_reward          | 23.7         |
| time/                   |              |
|    total_timesteps      | 100200       |
| train/                  |              |
|    approx_kl            | 0.0008514215 |
|    clip_fraction        | 0.0205       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.499       |
|    explained_variance   | 0.354        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.459        |
|    n_updates            | 2500         |
|    policy_gradient_loss | -0.00287     |
|    value_loss           | 2.21         |
------------------------------------------
Eval num_timesteps=100400, episode_reward=12.46 +/- 11.36
Episode length: 18.80 +/- 16.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.8     |
|    mean_reward     | 12.5     |
| time/              |          |
|    total_timesteps | 100400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 251      |
|    time_elapsed    | 28215    |
|    total_timesteps | 100400   |
---------------------------------
Eval num_timesteps=100600, episode_reward=3.70 +/- 1.41
Episode length: 6.20 +/- 1.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 6.2           |
|    mean_reward          | 3.7           |
| time/                   |               |
|    total_timesteps      | 100600        |
| train/                  |               |
|    approx_kl            | 0.00039419517 |
|    clip_fraction        | 0.0138        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.501        |
|    explained_variance   | 0.29          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.279         |
|    n_updates            | 2510          |
|    policy_gradient_loss | -0.0026       |
|    value_loss           | 2.34          |
-------------------------------------------
Eval num_timesteps=100800, episode_reward=9.52 +/- 12.08
Episode length: 14.80 +/- 17.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 9.52     |
| time/              |          |
|    total_timesteps | 100800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24       |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 252      |
|    time_elapsed    | 28298    |
|    total_timesteps | 100800   |
---------------------------------
Eval num_timesteps=101000, episode_reward=21.60 +/- 16.22
Episode length: 31.40 +/- 22.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 31.4          |
|    mean_reward          | 21.6          |
| time/                   |               |
|    total_timesteps      | 101000        |
| train/                  |               |
|    approx_kl            | 0.00027006332 |
|    clip_fraction        | 0.0029        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.5          |
|    explained_variance   | 0.171         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.69          |
|    n_updates            | 2520          |
|    policy_gradient_loss | -0.00124      |
|    value_loss           | 2.18          |
-------------------------------------------
Eval num_timesteps=101200, episode_reward=12.35 +/- 12.44
Episode length: 17.80 +/- 16.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | 12.4     |
| time/              |          |
|    total_timesteps | 101200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.3     |
|    ep_rew_mean     | 15.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 253      |
|    time_elapsed    | 28406    |
|    total_timesteps | 101200   |
---------------------------------
Eval num_timesteps=101400, episode_reward=29.19 +/- 11.15
Episode length: 42.20 +/- 15.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.2          |
|    mean_reward          | 29.2          |
| time/                   |               |
|    total_timesteps      | 101400        |
| train/                  |               |
|    approx_kl            | 0.00065282744 |
|    clip_fraction        | 0.0252        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.476        |
|    explained_variance   | 0.383         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.687         |
|    n_updates            | 2530          |
|    policy_gradient_loss | -0.00465      |
|    value_loss           | 2.12          |
-------------------------------------------
Eval num_timesteps=101600, episode_reward=23.18 +/- 15.68
Episode length: 32.60 +/- 21.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.6     |
|    mean_reward     | 23.2     |
| time/              |          |
|    total_timesteps | 101600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.3     |
|    ep_rew_mean     | 16       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 254      |
|    time_elapsed    | 28538    |
|    total_timesteps | 101600   |
---------------------------------
Eval num_timesteps=101800, episode_reward=30.03 +/- 10.01
Episode length: 43.20 +/- 13.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 43.2         |
|    mean_reward          | 30           |
| time/                   |              |
|    total_timesteps      | 101800       |
| train/                  |              |
|    approx_kl            | 0.0009132637 |
|    clip_fraction        | 0.0192       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.466       |
|    explained_variance   | 0.302        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.564        |
|    n_updates            | 2540         |
|    policy_gradient_loss | -0.00334     |
|    value_loss           | 2.11         |
------------------------------------------
Eval num_timesteps=102000, episode_reward=8.75 +/- 13.49
Episode length: 13.20 +/- 18.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.2     |
|    mean_reward     | 8.75     |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.2     |
|    ep_rew_mean     | 16.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 255      |
|    time_elapsed    | 28666    |
|    total_timesteps | 102000   |
---------------------------------
Eval num_timesteps=102200, episode_reward=10.98 +/- 12.26
Episode length: 16.60 +/- 17.39
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.6       |
|    mean_reward          | 11         |
| time/                   |            |
|    total_timesteps      | 102200     |
| train/                  |            |
|    approx_kl            | 0.00047681 |
|    clip_fraction        | 0.0129     |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.489     |
|    explained_variance   | 0.264      |
|    learning_rate        | 0.0001     |
|    loss                 | 3.07       |
|    n_updates            | 2550       |
|    policy_gradient_loss | -0.00256   |
|    value_loss           | 1.57       |
----------------------------------------
Eval num_timesteps=102400, episode_reward=11.37 +/- 12.01
Episode length: 17.00 +/- 17.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | 11.4     |
| time/              |          |
|    total_timesteps | 102400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.6     |
|    ep_rew_mean     | 17       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 256      |
|    time_elapsed    | 28767    |
|    total_timesteps | 102400   |
---------------------------------
Eval num_timesteps=102600, episode_reward=18.70 +/- 13.86
Episode length: 27.00 +/- 18.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 27           |
|    mean_reward          | 18.7         |
| time/                   |              |
|    total_timesteps      | 102600       |
| train/                  |              |
|    approx_kl            | 0.0012681687 |
|    clip_fraction        | 0.104        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.514       |
|    explained_variance   | 0.232        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.372        |
|    n_updates            | 2560         |
|    policy_gradient_loss | -0.00511     |
|    value_loss           | 2.35         |
------------------------------------------
Eval num_timesteps=102800, episode_reward=10.29 +/- 13.29
Episode length: 15.00 +/- 17.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | 10.3     |
| time/              |          |
|    total_timesteps | 102800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.1     |
|    ep_rew_mean     | 18.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 257      |
|    time_elapsed    | 28875    |
|    total_timesteps | 102800   |
---------------------------------
Eval num_timesteps=103000, episode_reward=9.86 +/- 12.43
Episode length: 15.00 +/- 17.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15          |
|    mean_reward          | 9.86        |
| time/                   |             |
|    total_timesteps      | 103000      |
| train/                  |             |
|    approx_kl            | 0.001162674 |
|    clip_fraction        | 0.0783      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.569      |
|    explained_variance   | 0.115       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.691       |
|    n_updates            | 2570        |
|    policy_gradient_loss | -0.00337    |
|    value_loss           | 1.59        |
-----------------------------------------
Eval num_timesteps=103200, episode_reward=16.21 +/- 14.12
Episode length: 24.80 +/- 20.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 16.2     |
| time/              |          |
|    total_timesteps | 103200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.1     |
|    ep_rew_mean     | 18.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 258      |
|    time_elapsed    | 28982    |
|    total_timesteps | 103200   |
---------------------------------
Eval num_timesteps=103400, episode_reward=15.79 +/- 14.81
Episode length: 23.80 +/- 21.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.8         |
|    mean_reward          | 15.8         |
| time/                   |              |
|    total_timesteps      | 103400       |
| train/                  |              |
|    approx_kl            | 0.0010861894 |
|    clip_fraction        | 0.0527       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.532       |
|    explained_variance   | 0.352        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.384        |
|    n_updates            | 2580         |
|    policy_gradient_loss | -0.00539     |
|    value_loss           | 1.54         |
------------------------------------------
Eval num_timesteps=103600, episode_reward=22.67 +/- 14.09
Episode length: 33.60 +/- 20.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 22.7     |
| time/              |          |
|    total_timesteps | 103600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.6     |
|    ep_rew_mean     | 19.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 259      |
|    time_elapsed    | 29094    |
|    total_timesteps | 103600   |
---------------------------------
Eval num_timesteps=103800, episode_reward=14.01 +/- 11.30
Episode length: 20.60 +/- 15.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 20.6          |
|    mean_reward          | 14            |
| time/                   |               |
|    total_timesteps      | 103800        |
| train/                  |               |
|    approx_kl            | 0.00071150874 |
|    clip_fraction        | 0.0234        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.543        |
|    explained_variance   | 0.238         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.563         |
|    n_updates            | 2590          |
|    policy_gradient_loss | -0.00326      |
|    value_loss           | 1.45          |
-------------------------------------------
Eval num_timesteps=104000, episode_reward=19.41 +/- 13.44
Episode length: 28.00 +/- 18.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28       |
|    mean_reward     | 19.4     |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29       |
|    ep_rew_mean     | 20.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 260      |
|    time_elapsed    | 29213    |
|    total_timesteps | 104000   |
---------------------------------
Eval num_timesteps=104200, episode_reward=21.21 +/- 15.36
Episode length: 31.80 +/- 22.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 31.8         |
|    mean_reward          | 21.2         |
| time/                   |              |
|    total_timesteps      | 104200       |
| train/                  |              |
|    approx_kl            | 0.0009845236 |
|    clip_fraction        | 0.0449       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.582       |
|    explained_variance   | 0.254        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.12         |
|    n_updates            | 2600         |
|    policy_gradient_loss | -0.00368     |
|    value_loss           | 1.45         |
------------------------------------------
Eval num_timesteps=104400, episode_reward=13.25 +/- 11.79
Episode length: 19.60 +/- 16.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 13.3     |
| time/              |          |
|    total_timesteps | 104400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.5     |
|    ep_rew_mean     | 19.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 261      |
|    time_elapsed    | 29316    |
|    total_timesteps | 104400   |
---------------------------------
Eval num_timesteps=104600, episode_reward=16.98 +/- 15.17
Episode length: 25.00 +/- 21.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25            |
|    mean_reward          | 17            |
| time/                   |               |
|    total_timesteps      | 104600        |
| train/                  |               |
|    approx_kl            | 0.00060345075 |
|    clip_fraction        | 0.0239        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.553        |
|    explained_variance   | 0.185         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.952         |
|    n_updates            | 2610          |
|    policy_gradient_loss | -0.00286      |
|    value_loss           | 3.83          |
-------------------------------------------
Eval num_timesteps=104800, episode_reward=15.86 +/- 14.30
Episode length: 24.20 +/- 21.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 15.9     |
| time/              |          |
|    total_timesteps | 104800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.8     |
|    ep_rew_mean     | 17.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 262      |
|    time_elapsed    | 29422    |
|    total_timesteps | 104800   |
---------------------------------
Eval num_timesteps=105000, episode_reward=4.82 +/- 1.63
Episode length: 8.00 +/- 2.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 8            |
|    mean_reward          | 4.82         |
| time/                   |              |
|    total_timesteps      | 105000       |
| train/                  |              |
|    approx_kl            | 0.0012274848 |
|    clip_fraction        | 0.0708       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.523       |
|    explained_variance   | 0.277        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.35         |
|    n_updates            | 2620         |
|    policy_gradient_loss | -0.00722     |
|    value_loss           | 2.13         |
------------------------------------------
Eval num_timesteps=105200, episode_reward=5.48 +/- 3.32
Episode length: 8.80 +/- 4.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8.8      |
|    mean_reward     | 5.48     |
| time/              |          |
|    total_timesteps | 105200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.5     |
|    ep_rew_mean     | 14.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 263      |
|    time_elapsed    | 29487    |
|    total_timesteps | 105200   |
---------------------------------
Eval num_timesteps=105400, episode_reward=21.72 +/- 14.72
Episode length: 32.40 +/- 21.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.4         |
|    mean_reward          | 21.7         |
| time/                   |              |
|    total_timesteps      | 105400       |
| train/                  |              |
|    approx_kl            | 0.0003427754 |
|    clip_fraction        | 0.00781      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.517       |
|    explained_variance   | 0.323        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.84         |
|    n_updates            | 2630         |
|    policy_gradient_loss | -0.00205     |
|    value_loss           | 3.33         |
------------------------------------------
Eval num_timesteps=105600, episode_reward=9.83 +/- 13.00
Episode length: 14.60 +/- 17.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 9.83     |
| time/              |          |
|    total_timesteps | 105600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.9     |
|    ep_rew_mean     | 14.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 264      |
|    time_elapsed    | 29592    |
|    total_timesteps | 105600   |
---------------------------------
Eval num_timesteps=105800, episode_reward=2.85 +/- 2.11
Episode length: 5.00 +/- 2.83
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 5             |
|    mean_reward          | 2.85          |
| time/                   |               |
|    total_timesteps      | 105800        |
| train/                  |               |
|    approx_kl            | 0.00078760873 |
|    clip_fraction        | 0.0181        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.522        |
|    explained_variance   | 0.204         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.67          |
|    n_updates            | 2640          |
|    policy_gradient_loss | -0.00345      |
|    value_loss           | 2.85          |
-------------------------------------------
Eval num_timesteps=106000, episode_reward=16.94 +/- 13.66
Episode length: 25.80 +/- 20.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.2     |
|    ep_rew_mean     | 13.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 265      |
|    time_elapsed    | 29678    |
|    total_timesteps | 106000   |
---------------------------------
Eval num_timesteps=106200, episode_reward=13.12 +/- 11.78
Episode length: 19.40 +/- 16.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 19.4         |
|    mean_reward          | 13.1         |
| time/                   |              |
|    total_timesteps      | 106200       |
| train/                  |              |
|    approx_kl            | 0.0008338614 |
|    clip_fraction        | 0.0161       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.57        |
|    explained_variance   | 0.133        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.893        |
|    n_updates            | 2650         |
|    policy_gradient_loss | -0.00241     |
|    value_loss           | 2.03         |
------------------------------------------
Eval num_timesteps=106400, episode_reward=13.63 +/- 10.68
Episode length: 20.40 +/- 15.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 13.6     |
| time/              |          |
|    total_timesteps | 106400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.8     |
|    ep_rew_mean     | 13.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 266      |
|    time_elapsed    | 29780    |
|    total_timesteps | 106400   |
---------------------------------
Eval num_timesteps=106600, episode_reward=10.78 +/- 12.53
Episode length: 16.00 +/- 17.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | 10.8         |
| time/                   |              |
|    total_timesteps      | 106600       |
| train/                  |              |
|    approx_kl            | 0.0009334459 |
|    clip_fraction        | 0.0205       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.552       |
|    explained_variance   | 0.372        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.41         |
|    n_updates            | 2660         |
|    policy_gradient_loss | -0.00616     |
|    value_loss           | 2.71         |
------------------------------------------
Eval num_timesteps=106800, episode_reward=23.43 +/- 13.10
Episode length: 34.60 +/- 18.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 23.4     |
| time/              |          |
|    total_timesteps | 106800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.4     |
|    ep_rew_mean     | 13.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 267      |
|    time_elapsed    | 29908    |
|    total_timesteps | 106800   |
---------------------------------
Eval num_timesteps=107000, episode_reward=11.42 +/- 12.12
Episode length: 17.00 +/- 16.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17           |
|    mean_reward          | 11.4         |
| time/                   |              |
|    total_timesteps      | 107000       |
| train/                  |              |
|    approx_kl            | 0.0010668902 |
|    clip_fraction        | 0.0478       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.551       |
|    explained_variance   | 0.302        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.572        |
|    n_updates            | 2670         |
|    policy_gradient_loss | -0.00333     |
|    value_loss           | 1.93         |
------------------------------------------
Eval num_timesteps=107200, episode_reward=19.10 +/- 13.63
Episode length: 27.80 +/- 19.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.8     |
|    mean_reward     | 19.1     |
| time/              |          |
|    total_timesteps | 107200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.4     |
|    ep_rew_mean     | 14.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 268      |
|    time_elapsed    | 30011    |
|    total_timesteps | 107200   |
---------------------------------
Eval num_timesteps=107400, episode_reward=5.67 +/- 2.48
Episode length: 8.80 +/- 3.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 8.8          |
|    mean_reward          | 5.67         |
| time/                   |              |
|    total_timesteps      | 107400       |
| train/                  |              |
|    approx_kl            | 0.0014585705 |
|    clip_fraction        | 0.0554       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.517       |
|    explained_variance   | 0.34         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.799        |
|    n_updates            | 2680         |
|    policy_gradient_loss | -0.00363     |
|    value_loss           | 2.43         |
------------------------------------------
Eval num_timesteps=107600, episode_reward=10.35 +/- 12.88
Episode length: 15.40 +/- 17.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | 10.4     |
| time/              |          |
|    total_timesteps | 107600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | 16.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 269      |
|    time_elapsed    | 30099    |
|    total_timesteps | 107600   |
---------------------------------
Eval num_timesteps=107800, episode_reward=12.15 +/- 12.36
Episode length: 17.60 +/- 17.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.6         |
|    mean_reward          | 12.2         |
| time/                   |              |
|    total_timesteps      | 107800       |
| train/                  |              |
|    approx_kl            | 0.0014531768 |
|    clip_fraction        | 0.0377       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.545       |
|    explained_variance   | -0.0144      |
|    learning_rate        | 0.0001       |
|    loss                 | 0.721        |
|    n_updates            | 2690         |
|    policy_gradient_loss | -0.00288     |
|    value_loss           | 2.22         |
------------------------------------------
Eval num_timesteps=108000, episode_reward=18.12 +/- 14.29
Episode length: 26.20 +/- 19.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | 18.1     |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.4     |
|    ep_rew_mean     | 16.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 270      |
|    time_elapsed    | 30206    |
|    total_timesteps | 108000   |
---------------------------------
Eval num_timesteps=108200, episode_reward=28.26 +/- 13.01
Episode length: 40.80 +/- 18.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 40.8          |
|    mean_reward          | 28.3          |
| time/                   |               |
|    total_timesteps      | 108200        |
| train/                  |               |
|    approx_kl            | 0.00086203415 |
|    clip_fraction        | 0.056         |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.495        |
|    explained_variance   | 0.229         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.25          |
|    n_updates            | 2700          |
|    policy_gradient_loss | -0.00585      |
|    value_loss           | 2.5           |
-------------------------------------------
Eval num_timesteps=108400, episode_reward=17.81 +/- 14.79
Episode length: 25.60 +/- 20.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 17.8     |
| time/              |          |
|    total_timesteps | 108400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.5     |
|    ep_rew_mean     | 16.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 271      |
|    time_elapsed    | 30333    |
|    total_timesteps | 108400   |
---------------------------------
Eval num_timesteps=108600, episode_reward=13.29 +/- 12.05
Episode length: 19.40 +/- 15.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 19.4         |
|    mean_reward          | 13.3         |
| time/                   |              |
|    total_timesteps      | 108600       |
| train/                  |              |
|    approx_kl            | 0.0005219794 |
|    clip_fraction        | 0.0192       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.496       |
|    explained_variance   | 0.179        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.771        |
|    n_updates            | 2710         |
|    policy_gradient_loss | -0.00293     |
|    value_loss           | 2.04         |
------------------------------------------
Eval num_timesteps=108800, episode_reward=22.67 +/- 14.48
Episode length: 33.20 +/- 20.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | 22.7     |
| time/              |          |
|    total_timesteps | 108800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | 17.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 272      |
|    time_elapsed    | 30462    |
|    total_timesteps | 108800   |
---------------------------------
Eval num_timesteps=109000, episode_reward=5.29 +/- 1.66
Episode length: 8.40 +/- 2.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 8.4          |
|    mean_reward          | 5.29         |
| time/                   |              |
|    total_timesteps      | 109000       |
| train/                  |              |
|    approx_kl            | 0.0007262697 |
|    clip_fraction        | 0.0143       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.488       |
|    explained_variance   | 0.295        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.979        |
|    n_updates            | 2720         |
|    policy_gradient_loss | -0.00409     |
|    value_loss           | 2.49         |
------------------------------------------
Eval num_timesteps=109200, episode_reward=19.55 +/- 13.00
Episode length: 28.20 +/- 18.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.2     |
|    mean_reward     | 19.5     |
| time/              |          |
|    total_timesteps | 109200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.5     |
|    ep_rew_mean     | 18.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 273      |
|    time_elapsed    | 30555    |
|    total_timesteps | 109200   |
---------------------------------
Eval num_timesteps=109400, episode_reward=9.02 +/- 13.86
Episode length: 13.20 +/- 18.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 13.2          |
|    mean_reward          | 9.02          |
| time/                   |               |
|    total_timesteps      | 109400        |
| train/                  |               |
|    approx_kl            | 0.00042313058 |
|    clip_fraction        | 0.0138        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.477        |
|    explained_variance   | 0.283         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.11          |
|    n_updates            | 2730          |
|    policy_gradient_loss | -0.0015       |
|    value_loss           | 1.4           |
-------------------------------------------
Eval num_timesteps=109600, episode_reward=20.67 +/- 12.27
Episode length: 29.80 +/- 16.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.8     |
|    mean_reward     | 20.7     |
| time/              |          |
|    total_timesteps | 109600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.2     |
|    ep_rew_mean     | 18       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 274      |
|    time_elapsed    | 30682    |
|    total_timesteps | 109600   |
---------------------------------
Eval num_timesteps=109800, episode_reward=16.53 +/- 14.71
Episode length: 24.60 +/- 20.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.6          |
|    mean_reward          | 16.5          |
| time/                   |               |
|    total_timesteps      | 109800        |
| train/                  |               |
|    approx_kl            | 0.00036113962 |
|    clip_fraction        | 0.0058        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.458        |
|    explained_variance   | 0.293         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.03          |
|    n_updates            | 2740          |
|    policy_gradient_loss | -0.00295      |
|    value_loss           | 2.69          |
-------------------------------------------
Eval num_timesteps=110000, episode_reward=10.85 +/- 13.01
Episode length: 15.60 +/- 17.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 10.9     |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.4     |
|    ep_rew_mean     | 18.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 275      |
|    time_elapsed    | 30790    |
|    total_timesteps | 110000   |
---------------------------------
Eval num_timesteps=110200, episode_reward=14.17 +/- 11.89
Episode length: 21.40 +/- 17.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 21.4          |
|    mean_reward          | 14.2          |
| time/                   |               |
|    total_timesteps      | 110200        |
| train/                  |               |
|    approx_kl            | 0.00093637325 |
|    clip_fraction        | 0.0136        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.456        |
|    explained_variance   | 0.245         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.242         |
|    n_updates            | 2750          |
|    policy_gradient_loss | -0.00141      |
|    value_loss           | 1.53          |
-------------------------------------------
Eval num_timesteps=110400, episode_reward=24.70 +/- 12.52
Episode length: 36.00 +/- 17.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 24.7     |
| time/              |          |
|    total_timesteps | 110400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | 17.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 276      |
|    time_elapsed    | 30898    |
|    total_timesteps | 110400   |
---------------------------------
Eval num_timesteps=110600, episode_reward=18.11 +/- 14.74
Episode length: 25.80 +/- 19.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.8         |
|    mean_reward          | 18.1         |
| time/                   |              |
|    total_timesteps      | 110600       |
| train/                  |              |
|    approx_kl            | 0.0007323218 |
|    clip_fraction        | 0.0121       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.449       |
|    explained_variance   | 0.351        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.417        |
|    n_updates            | 2760         |
|    policy_gradient_loss | -0.00201     |
|    value_loss           | 2.15         |
------------------------------------------
Eval num_timesteps=110800, episode_reward=16.15 +/- 15.07
Episode length: 23.80 +/- 21.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | 16.2     |
| time/              |          |
|    total_timesteps | 110800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.7     |
|    ep_rew_mean     | 17.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 277      |
|    time_elapsed    | 31010    |
|    total_timesteps | 110800   |
---------------------------------
Eval num_timesteps=111000, episode_reward=16.73 +/- 15.00
Episode length: 24.40 +/- 21.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.4         |
|    mean_reward          | 16.7         |
| time/                   |              |
|    total_timesteps      | 111000       |
| train/                  |              |
|    approx_kl            | 0.0008487857 |
|    clip_fraction        | 0.0549       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.43        |
|    explained_variance   | 0.444        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.93         |
|    n_updates            | 2770         |
|    policy_gradient_loss | -0.00327     |
|    value_loss           | 1.9          |
------------------------------------------
Eval num_timesteps=111200, episode_reward=16.52 +/- 14.23
Episode length: 24.80 +/- 20.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 111200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.8     |
|    ep_rew_mean     | 18.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 278      |
|    time_elapsed    | 31121    |
|    total_timesteps | 111200   |
---------------------------------
Eval num_timesteps=111400, episode_reward=19.20 +/- 12.87
Episode length: 28.20 +/- 18.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 28.2        |
|    mean_reward          | 19.2        |
| time/                   |             |
|    total_timesteps      | 111400      |
| train/                  |             |
|    approx_kl            | 0.000572068 |
|    clip_fraction        | 0.0185      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.456      |
|    explained_variance   | -0.0212     |
|    learning_rate        | 0.0001      |
|    loss                 | 0.702       |
|    n_updates            | 2780        |
|    policy_gradient_loss | -0.0037     |
|    value_loss           | 1.34        |
-----------------------------------------
Eval num_timesteps=111600, episode_reward=18.54 +/- 13.81
Episode length: 26.80 +/- 19.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.8     |
|    mean_reward     | 18.5     |
| time/              |          |
|    total_timesteps | 111600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.1     |
|    ep_rew_mean     | 18       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 279      |
|    time_elapsed    | 31231    |
|    total_timesteps | 111600   |
---------------------------------
Eval num_timesteps=111800, episode_reward=18.12 +/- 14.35
Episode length: 26.60 +/- 19.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.6          |
|    mean_reward          | 18.1          |
| time/                   |               |
|    total_timesteps      | 111800        |
| train/                  |               |
|    approx_kl            | 0.00091959786 |
|    clip_fraction        | 0.023         |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.441        |
|    explained_variance   | 0.315         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.36          |
|    n_updates            | 2790          |
|    policy_gradient_loss | -0.00401      |
|    value_loss           | 2.63          |
-------------------------------------------
Eval num_timesteps=112000, episode_reward=16.50 +/- 14.87
Episode length: 24.60 +/- 21.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.7     |
|    ep_rew_mean     | 17       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 280      |
|    time_elapsed    | 31342    |
|    total_timesteps | 112000   |
---------------------------------
Eval num_timesteps=112200, episode_reward=10.62 +/- 13.09
Episode length: 15.20 +/- 17.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.2        |
|    mean_reward          | 10.6        |
| time/                   |             |
|    total_timesteps      | 112200      |
| train/                  |             |
|    approx_kl            | 0.000723222 |
|    clip_fraction        | 0.0232      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.411      |
|    explained_variance   | 0.369       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.46        |
|    n_updates            | 2800        |
|    policy_gradient_loss | -0.0023     |
|    value_loss           | 2.25        |
-----------------------------------------
Eval num_timesteps=112400, episode_reward=6.23 +/- 6.08
Episode length: 9.60 +/- 8.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.6      |
|    mean_reward     | 6.23     |
| time/              |          |
|    total_timesteps | 112400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.9     |
|    ep_rew_mean     | 15.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 281      |
|    time_elapsed    | 31436    |
|    total_timesteps | 112400   |
---------------------------------
Eval num_timesteps=112600, episode_reward=10.63 +/- 13.07
Episode length: 15.40 +/- 17.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.4          |
|    mean_reward          | 10.6          |
| time/                   |               |
|    total_timesteps      | 112600        |
| train/                  |               |
|    approx_kl            | 0.00037316527 |
|    clip_fraction        | 0.00625       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.41         |
|    explained_variance   | 0.377         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.799         |
|    n_updates            | 2810          |
|    policy_gradient_loss | -0.00186      |
|    value_loss           | 2.02          |
-------------------------------------------
Eval num_timesteps=112800, episode_reward=16.91 +/- 14.95
Episode length: 24.80 +/- 20.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 112800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.3     |
|    ep_rew_mean     | 14.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 282      |
|    time_elapsed    | 31545    |
|    total_timesteps | 112800   |
---------------------------------
Eval num_timesteps=113000, episode_reward=16.24 +/- 15.83
Episode length: 23.40 +/- 21.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.4          |
|    mean_reward          | 16.2          |
| time/                   |               |
|    total_timesteps      | 113000        |
| train/                  |               |
|    approx_kl            | 0.00051588577 |
|    clip_fraction        | 0.0067        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.393        |
|    explained_variance   | 0.368         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.476         |
|    n_updates            | 2820          |
|    policy_gradient_loss | -0.00167      |
|    value_loss           | 3.1           |
-------------------------------------------
Eval num_timesteps=113200, episode_reward=17.70 +/- 13.76
Episode length: 26.00 +/- 19.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | 17.7     |
| time/              |          |
|    total_timesteps | 113200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.7     |
|    ep_rew_mean     | 14       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 283      |
|    time_elapsed    | 31648    |
|    total_timesteps | 113200   |
---------------------------------
Eval num_timesteps=113400, episode_reward=29.58 +/- 13.12
Episode length: 41.20 +/- 17.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 41.2          |
|    mean_reward          | 29.6          |
| time/                   |               |
|    total_timesteps      | 113400        |
| train/                  |               |
|    approx_kl            | 0.00033454783 |
|    clip_fraction        | 0.0123        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.375        |
|    explained_variance   | 0.324         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.3           |
|    n_updates            | 2830          |
|    policy_gradient_loss | -0.00276      |
|    value_loss           | 2.47          |
-------------------------------------------
Eval num_timesteps=113600, episode_reward=22.48 +/- 14.90
Episode length: 33.00 +/- 21.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 22.5     |
| time/              |          |
|    total_timesteps | 113600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.1     |
|    ep_rew_mean     | 13.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 284      |
|    time_elapsed    | 31761    |
|    total_timesteps | 113600   |
---------------------------------
Eval num_timesteps=113800, episode_reward=24.82 +/- 13.65
Episode length: 35.00 +/- 18.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35            |
|    mean_reward          | 24.8          |
| time/                   |               |
|    total_timesteps      | 113800        |
| train/                  |               |
|    approx_kl            | 0.00028474163 |
|    clip_fraction        | 0.0203        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.383        |
|    explained_variance   | 0.534         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.523         |
|    n_updates            | 2840          |
|    policy_gradient_loss | -0.000765     |
|    value_loss           | 1.63          |
-------------------------------------------
Eval num_timesteps=114000, episode_reward=11.69 +/- 12.04
Episode length: 17.40 +/- 16.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 11.7     |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.8     |
|    ep_rew_mean     | 12.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 285      |
|    time_elapsed    | 31874    |
|    total_timesteps | 114000   |
---------------------------------
Eval num_timesteps=114200, episode_reward=17.01 +/- 14.30
Episode length: 25.20 +/- 20.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.2          |
|    mean_reward          | 17            |
| time/                   |               |
|    total_timesteps      | 114200        |
| train/                  |               |
|    approx_kl            | 0.00051643315 |
|    clip_fraction        | 0.0272        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.383        |
|    explained_variance   | 0.249         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.24          |
|    n_updates            | 2850          |
|    policy_gradient_loss | -0.0037       |
|    value_loss           | 3.2           |
-------------------------------------------
Eval num_timesteps=114400, episode_reward=16.05 +/- 14.62
Episode length: 24.00 +/- 21.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 114400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.5     |
|    ep_rew_mean     | 13.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 286      |
|    time_elapsed    | 31979    |
|    total_timesteps | 114400   |
---------------------------------
Eval num_timesteps=114600, episode_reward=10.21 +/- 11.64
Episode length: 15.80 +/- 17.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | 10.2         |
| time/                   |              |
|    total_timesteps      | 114600       |
| train/                  |              |
|    approx_kl            | 0.0007162883 |
|    clip_fraction        | 0.0132       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.408       |
|    explained_variance   | 0.423        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.351        |
|    n_updates            | 2860         |
|    policy_gradient_loss | -0.00201     |
|    value_loss           | 1.7          |
------------------------------------------
Eval num_timesteps=114800, episode_reward=19.94 +/- 12.12
Episode length: 29.60 +/- 17.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.6     |
|    mean_reward     | 19.9     |
| time/              |          |
|    total_timesteps | 114800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.6     |
|    ep_rew_mean     | 14.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 287      |
|    time_elapsed    | 32090    |
|    total_timesteps | 114800   |
---------------------------------
Eval num_timesteps=115000, episode_reward=30.43 +/- 8.71
Episode length: 44.00 +/- 12.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 44           |
|    mean_reward          | 30.4         |
| time/                   |              |
|    total_timesteps      | 115000       |
| train/                  |              |
|    approx_kl            | 0.0008660684 |
|    clip_fraction        | 0.0355       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.398       |
|    explained_variance   | 0.5          |
|    learning_rate        | 0.0001       |
|    loss                 | 1.73         |
|    n_updates            | 2870         |
|    policy_gradient_loss | -0.00198     |
|    value_loss           | 1.38         |
------------------------------------------
Eval num_timesteps=115200, episode_reward=6.36 +/- 4.12
Episode length: 10.20 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10.2     |
|    mean_reward     | 6.36     |
| time/              |          |
|    total_timesteps | 115200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.8     |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 288      |
|    time_elapsed    | 32189    |
|    total_timesteps | 115200   |
---------------------------------
Eval num_timesteps=115400, episode_reward=17.89 +/- 14.35
Episode length: 26.00 +/- 19.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26            |
|    mean_reward          | 17.9          |
| time/                   |               |
|    total_timesteps      | 115400        |
| train/                  |               |
|    approx_kl            | 0.00010865674 |
|    clip_fraction        | 0.00357       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.371        |
|    explained_variance   | 0.411         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.852         |
|    n_updates            | 2880          |
|    policy_gradient_loss | -0.00118      |
|    value_loss           | 2.23          |
-------------------------------------------
Eval num_timesteps=115600, episode_reward=30.54 +/- 11.22
Episode length: 42.40 +/- 15.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 30.5     |
| time/              |          |
|    total_timesteps | 115600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.5     |
|    ep_rew_mean     | 15.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 289      |
|    time_elapsed    | 32343    |
|    total_timesteps | 115600   |
---------------------------------
Eval num_timesteps=115800, episode_reward=24.40 +/- 13.27
Episode length: 35.20 +/- 18.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.2          |
|    mean_reward          | 24.4          |
| time/                   |               |
|    total_timesteps      | 115800        |
| train/                  |               |
|    approx_kl            | 0.00016276857 |
|    clip_fraction        | 0.00201       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.369        |
|    explained_variance   | 0.255         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.2           |
|    n_updates            | 2890          |
|    policy_gradient_loss | -0.00134      |
|    value_loss           | 2.32          |
-------------------------------------------
Eval num_timesteps=116000, episode_reward=8.88 +/- 2.54
Episode length: 13.60 +/- 3.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.6     |
|    mean_reward     | 8.88     |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.6     |
|    ep_rew_mean     | 15.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 290      |
|    time_elapsed    | 32441    |
|    total_timesteps | 116000   |
---------------------------------
Eval num_timesteps=116200, episode_reward=11.75 +/- 11.50
Episode length: 18.00 +/- 16.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 18            |
|    mean_reward          | 11.8          |
| time/                   |               |
|    total_timesteps      | 116200        |
| train/                  |               |
|    approx_kl            | 0.00025840368 |
|    clip_fraction        | 0.00737       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.348        |
|    explained_variance   | 0.364         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.02          |
|    n_updates            | 2900          |
|    policy_gradient_loss | -0.00198      |
|    value_loss           | 2.78          |
-------------------------------------------
Eval num_timesteps=116400, episode_reward=22.84 +/- 14.88
Episode length: 33.00 +/- 21.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 22.8     |
| time/              |          |
|    total_timesteps | 116400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.2     |
|    ep_rew_mean     | 14.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 291      |
|    time_elapsed    | 32544    |
|    total_timesteps | 116400   |
---------------------------------
Eval num_timesteps=116600, episode_reward=2.69 +/- 1.26
Episode length: 4.80 +/- 1.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 4.8          |
|    mean_reward          | 2.69         |
| time/                   |              |
|    total_timesteps      | 116600       |
| train/                  |              |
|    approx_kl            | 0.0004340108 |
|    clip_fraction        | 0.0105       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.346       |
|    explained_variance   | 0.282        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.26         |
|    n_updates            | 2910         |
|    policy_gradient_loss | -0.0017      |
|    value_loss           | 3            |
------------------------------------------
Eval num_timesteps=116800, episode_reward=17.49 +/- 14.88
Episode length: 25.20 +/- 20.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 17.5     |
| time/              |          |
|    total_timesteps | 116800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.6     |
|    ep_rew_mean     | 14.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 292      |
|    time_elapsed    | 32625    |
|    total_timesteps | 116800   |
---------------------------------
Eval num_timesteps=117000, episode_reward=35.14 +/- 1.51
Episode length: 50.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 50            |
|    mean_reward          | 35.1          |
| time/                   |               |
|    total_timesteps      | 117000        |
| train/                  |               |
|    approx_kl            | 0.00035870288 |
|    clip_fraction        | 0.0203        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.329        |
|    explained_variance   | -0.189        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.232         |
|    n_updates            | 2920          |
|    policy_gradient_loss | -0.00164      |
|    value_loss           | 2.04          |
-------------------------------------------
Eval num_timesteps=117200, episode_reward=17.12 +/- 15.07
Episode length: 24.80 +/- 20.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 17.1     |
| time/              |          |
|    total_timesteps | 117200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22       |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 293      |
|    time_elapsed    | 32752    |
|    total_timesteps | 117200   |
---------------------------------
Eval num_timesteps=117400, episode_reward=10.96 +/- 12.39
Episode length: 16.20 +/- 17.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.2          |
|    mean_reward          | 11            |
| time/                   |               |
|    total_timesteps      | 117400        |
| train/                  |               |
|    approx_kl            | 0.00047140688 |
|    clip_fraction        | 0.0201        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.339        |
|    explained_variance   | 0.345         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.5           |
|    n_updates            | 2930          |
|    policy_gradient_loss | -0.00195      |
|    value_loss           | 2.06          |
-------------------------------------------
Eval num_timesteps=117600, episode_reward=8.88 +/- 12.34
Episode length: 13.80 +/- 18.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.8     |
|    mean_reward     | 8.88     |
| time/              |          |
|    total_timesteps | 117600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.1     |
|    ep_rew_mean     | 14.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 294      |
|    time_elapsed    | 32858    |
|    total_timesteps | 117600   |
---------------------------------
Eval num_timesteps=117800, episode_reward=10.58 +/- 11.74
Episode length: 16.20 +/- 17.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.2          |
|    mean_reward          | 10.6          |
| time/                   |               |
|    total_timesteps      | 117800        |
| train/                  |               |
|    approx_kl            | 0.00042776804 |
|    clip_fraction        | 0.00982       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.357        |
|    explained_variance   | 0.261         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.24          |
|    n_updates            | 2940          |
|    policy_gradient_loss | -0.00118      |
|    value_loss           | 2.3           |
-------------------------------------------
Eval num_timesteps=118000, episode_reward=12.95 +/- 11.30
Episode length: 19.20 +/- 16.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.2     |
|    mean_reward     | 13       |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.1     |
|    ep_rew_mean     | 14.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 295      |
|    time_elapsed    | 32971    |
|    total_timesteps | 118000   |
---------------------------------
Eval num_timesteps=118200, episode_reward=9.20 +/- 12.17
Episode length: 14.20 +/- 18.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.2          |
|    mean_reward          | 9.2           |
| time/                   |               |
|    total_timesteps      | 118200        |
| train/                  |               |
|    approx_kl            | 0.00055974536 |
|    clip_fraction        | 0.0257        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.377        |
|    explained_variance   | 0.234         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.06          |
|    n_updates            | 2950          |
|    policy_gradient_loss | -0.00266      |
|    value_loss           | 3.95          |
-------------------------------------------
Eval num_timesteps=118400, episode_reward=22.81 +/- 14.30
Episode length: 33.40 +/- 20.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 22.8     |
| time/              |          |
|    total_timesteps | 118400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.4     |
|    ep_rew_mean     | 14.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 296      |
|    time_elapsed    | 33078    |
|    total_timesteps | 118400   |
---------------------------------
Eval num_timesteps=118600, episode_reward=10.53 +/- 11.56
Episode length: 16.40 +/- 17.02
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.4          |
|    mean_reward          | 10.5          |
| time/                   |               |
|    total_timesteps      | 118600        |
| train/                  |               |
|    approx_kl            | 0.00040205396 |
|    clip_fraction        | 0.0129        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.403        |
|    explained_variance   | 0.366         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.596         |
|    n_updates            | 2960          |
|    policy_gradient_loss | -0.00185      |
|    value_loss           | 1.95          |
-------------------------------------------
Eval num_timesteps=118800, episode_reward=16.82 +/- 14.94
Episode length: 24.40 +/- 20.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 118800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.2     |
|    ep_rew_mean     | 15.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 297      |
|    time_elapsed    | 33186    |
|    total_timesteps | 118800   |
---------------------------------
Eval num_timesteps=119000, episode_reward=10.78 +/- 12.49
Episode length: 15.80 +/- 17.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | 10.8         |
| time/                   |              |
|    total_timesteps      | 119000       |
| train/                  |              |
|    approx_kl            | 0.0003933176 |
|    clip_fraction        | 0.0121       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.392       |
|    explained_variance   | 0.312        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.03         |
|    n_updates            | 2970         |
|    policy_gradient_loss | -0.00206     |
|    value_loss           | 2.06         |
------------------------------------------
Eval num_timesteps=119200, episode_reward=19.52 +/- 12.36
Episode length: 28.80 +/- 17.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.8     |
|    mean_reward     | 19.5     |
| time/              |          |
|    total_timesteps | 119200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.1     |
|    ep_rew_mean     | 15.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 298      |
|    time_elapsed    | 33298    |
|    total_timesteps | 119200   |
---------------------------------
Eval num_timesteps=119400, episode_reward=23.45 +/- 14.44
Episode length: 33.80 +/- 19.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.8          |
|    mean_reward          | 23.4          |
| time/                   |               |
|    total_timesteps      | 119400        |
| train/                  |               |
|    approx_kl            | 0.00040642757 |
|    clip_fraction        | 0.0295        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.388        |
|    explained_variance   | 0.315         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.01          |
|    n_updates            | 2980          |
|    policy_gradient_loss | -0.00409      |
|    value_loss           | 1.62          |
-------------------------------------------
Eval num_timesteps=119600, episode_reward=15.96 +/- 16.03
Episode length: 23.00 +/- 22.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 119600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.4     |
|    ep_rew_mean     | 15.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 299      |
|    time_elapsed    | 33406    |
|    total_timesteps | 119600   |
---------------------------------
Eval num_timesteps=119800, episode_reward=23.05 +/- 15.34
Episode length: 32.80 +/- 21.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.8         |
|    mean_reward          | 23.1         |
| time/                   |              |
|    total_timesteps      | 119800       |
| train/                  |              |
|    approx_kl            | 0.0006342888 |
|    clip_fraction        | 0.0431       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.408       |
|    explained_variance   | 0.241        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.17         |
|    n_updates            | 2990         |
|    policy_gradient_loss | -0.00233     |
|    value_loss           | 2.95         |
------------------------------------------
Eval num_timesteps=120000, episode_reward=13.88 +/- 13.45
Episode length: 20.40 +/- 18.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 13.9     |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.1     |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 300      |
|    time_elapsed    | 33511    |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=120200, episode_reward=15.28 +/- 15.74
Episode length: 22.60 +/- 22.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.6          |
|    mean_reward          | 15.3          |
| time/                   |               |
|    total_timesteps      | 120200        |
| train/                  |               |
|    approx_kl            | 0.00038586996 |
|    clip_fraction        | 0.00647       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.432        |
|    explained_variance   | 0.146         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.07          |
|    n_updates            | 3000          |
|    policy_gradient_loss | -0.00191      |
|    value_loss           | 3             |
-------------------------------------------
Eval num_timesteps=120400, episode_reward=9.12 +/- 13.31
Episode length: 13.60 +/- 18.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.6     |
|    mean_reward     | 9.12     |
| time/              |          |
|    total_timesteps | 120400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25       |
|    ep_rew_mean     | 17.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 301      |
|    time_elapsed    | 33618    |
|    total_timesteps | 120400   |
---------------------------------
Eval num_timesteps=120600, episode_reward=22.55 +/- 11.58
Episode length: 31.60 +/- 15.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 31.6          |
|    mean_reward          | 22.6          |
| time/                   |               |
|    total_timesteps      | 120600        |
| train/                  |               |
|    approx_kl            | 0.00066872843 |
|    clip_fraction        | 0.0263        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.418        |
|    explained_variance   | 0.262         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.13          |
|    n_updates            | 3010          |
|    policy_gradient_loss | -0.00179      |
|    value_loss           | 2.15          |
-------------------------------------------
Eval num_timesteps=120800, episode_reward=17.45 +/- 14.00
Episode length: 25.80 +/- 19.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 120800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.5     |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 302      |
|    time_elapsed    | 33735    |
|    total_timesteps | 120800   |
---------------------------------
Eval num_timesteps=121000, episode_reward=17.17 +/- 16.08
Episode length: 24.20 +/- 21.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.2         |
|    mean_reward          | 17.2         |
| time/                   |              |
|    total_timesteps      | 121000       |
| train/                  |              |
|    approx_kl            | 0.0011217062 |
|    clip_fraction        | 0.0522       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.445       |
|    explained_variance   | 0.367        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.282        |
|    n_updates            | 3020         |
|    policy_gradient_loss | -0.00275     |
|    value_loss           | 1.51         |
------------------------------------------
Eval num_timesteps=121200, episode_reward=6.70 +/- 3.05
Episode length: 10.60 +/- 4.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10.6     |
|    mean_reward     | 6.7      |
| time/              |          |
|    total_timesteps | 121200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 303      |
|    time_elapsed    | 33825    |
|    total_timesteps | 121200   |
---------------------------------
Eval num_timesteps=121400, episode_reward=12.91 +/- 13.30
Episode length: 19.20 +/- 18.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 19.2          |
|    mean_reward          | 12.9          |
| time/                   |               |
|    total_timesteps      | 121400        |
| train/                  |               |
|    approx_kl            | 0.00018246898 |
|    clip_fraction        | 0.00871       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.433        |
|    explained_variance   | 0.274         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.49          |
|    n_updates            | 3030          |
|    policy_gradient_loss | -0.00132      |
|    value_loss           | 3.22          |
-------------------------------------------
Eval num_timesteps=121600, episode_reward=17.30 +/- 15.43
Episode length: 24.60 +/- 20.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 17.3     |
| time/              |          |
|    total_timesteps | 121600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | 16.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 304      |
|    time_elapsed    | 33968    |
|    total_timesteps | 121600   |
---------------------------------
Eval num_timesteps=121800, episode_reward=21.52 +/- 16.33
Episode length: 31.20 +/- 23.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 31.2         |
|    mean_reward          | 21.5         |
| time/                   |              |
|    total_timesteps      | 121800       |
| train/                  |              |
|    approx_kl            | 0.0006716688 |
|    clip_fraction        | 0.0232       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.451       |
|    explained_variance   | 0.359        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.388        |
|    n_updates            | 3040         |
|    policy_gradient_loss | -0.0028      |
|    value_loss           | 1.85         |
------------------------------------------
Eval num_timesteps=122000, episode_reward=24.05 +/- 15.04
Episode length: 33.60 +/- 20.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 24.1     |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | 16.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 305      |
|    time_elapsed    | 34101    |
|    total_timesteps | 122000   |
---------------------------------
Eval num_timesteps=122200, episode_reward=18.25 +/- 14.78
Episode length: 26.00 +/- 19.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26           |
|    mean_reward          | 18.2         |
| time/                   |              |
|    total_timesteps      | 122200       |
| train/                  |              |
|    approx_kl            | 0.0005630533 |
|    clip_fraction        | 0.0123       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.448       |
|    explained_variance   | 0.0841       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.17         |
|    n_updates            | 3050         |
|    policy_gradient_loss | -0.00258     |
|    value_loss           | 3.3          |
------------------------------------------
Eval num_timesteps=122400, episode_reward=16.34 +/- 15.75
Episode length: 23.60 +/- 21.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 16.3     |
| time/              |          |
|    total_timesteps | 122400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.4     |
|    ep_rew_mean     | 16.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 306      |
|    time_elapsed    | 34212    |
|    total_timesteps | 122400   |
---------------------------------
Eval num_timesteps=122600, episode_reward=19.66 +/- 13.23
Episode length: 28.40 +/- 18.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 28.4          |
|    mean_reward          | 19.7          |
| time/                   |               |
|    total_timesteps      | 122600        |
| train/                  |               |
|    approx_kl            | 0.00088782713 |
|    clip_fraction        | 0.083         |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.486        |
|    explained_variance   | 0.281         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.372         |
|    n_updates            | 3060          |
|    policy_gradient_loss | -0.00503      |
|    value_loss           | 1.28          |
-------------------------------------------
Eval num_timesteps=122800, episode_reward=16.15 +/- 15.89
Episode length: 23.20 +/- 21.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 16.1     |
| time/              |          |
|    total_timesteps | 122800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.3     |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 307      |
|    time_elapsed    | 34324    |
|    total_timesteps | 122800   |
---------------------------------
Eval num_timesteps=123000, episode_reward=17.54 +/- 13.85
Episode length: 26.00 +/- 19.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26            |
|    mean_reward          | 17.5          |
| time/                   |               |
|    total_timesteps      | 123000        |
| train/                  |               |
|    approx_kl            | 0.00045612708 |
|    clip_fraction        | 0.0165        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.488        |
|    explained_variance   | 0.269         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.28          |
|    n_updates            | 3070          |
|    policy_gradient_loss | -0.00377      |
|    value_loss           | 2.87          |
-------------------------------------------
Eval num_timesteps=123200, episode_reward=16.41 +/- 15.23
Episode length: 24.00 +/- 21.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16.4     |
| time/              |          |
|    total_timesteps | 123200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.1     |
|    ep_rew_mean     | 15.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 308      |
|    time_elapsed    | 34429    |
|    total_timesteps | 123200   |
---------------------------------
Eval num_timesteps=123400, episode_reward=7.15 +/- 2.39
Episode length: 11.20 +/- 3.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 11.2         |
|    mean_reward          | 7.15         |
| time/                   |              |
|    total_timesteps      | 123400       |
| train/                  |              |
|    approx_kl            | 0.0005873713 |
|    clip_fraction        | 0.0194       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.496       |
|    explained_variance   | 0.619        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.676        |
|    n_updates            | 3080         |
|    policy_gradient_loss | -0.00276     |
|    value_loss           | 1.43         |
------------------------------------------
Eval num_timesteps=123600, episode_reward=22.29 +/- 15.86
Episode length: 32.00 +/- 22.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32       |
|    mean_reward     | 22.3     |
| time/              |          |
|    total_timesteps | 123600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.3     |
|    ep_rew_mean     | 15.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 309      |
|    time_elapsed    | 34525    |
|    total_timesteps | 123600   |
---------------------------------
Eval num_timesteps=123800, episode_reward=25.53 +/- 12.82
Episode length: 36.00 +/- 17.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36           |
|    mean_reward          | 25.5         |
| time/                   |              |
|    total_timesteps      | 123800       |
| train/                  |              |
|    approx_kl            | 0.0007290176 |
|    clip_fraction        | 0.0154       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.485       |
|    explained_variance   | 0.412        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.67         |
|    n_updates            | 3090         |
|    policy_gradient_loss | -0.00366     |
|    value_loss           | 2.09         |
------------------------------------------
Eval num_timesteps=124000, episode_reward=4.92 +/- 3.03
Episode length: 7.80 +/- 4.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7.8      |
|    mean_reward     | 4.92     |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22       |
|    ep_rew_mean     | 15       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 310      |
|    time_elapsed    | 34615    |
|    total_timesteps | 124000   |
---------------------------------
Eval num_timesteps=124200, episode_reward=23.27 +/- 14.31
Episode length: 33.60 +/- 20.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.6         |
|    mean_reward          | 23.3         |
| time/                   |              |
|    total_timesteps      | 124200       |
| train/                  |              |
|    approx_kl            | 0.0005767898 |
|    clip_fraction        | 0.0174       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.487       |
|    explained_variance   | 0.52         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.985        |
|    n_updates            | 3100         |
|    policy_gradient_loss | -0.00334     |
|    value_loss           | 2            |
------------------------------------------
Eval num_timesteps=124400, episode_reward=29.19 +/- 11.20
Episode length: 42.20 +/- 15.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 29.2     |
| time/              |          |
|    total_timesteps | 124400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.4     |
|    ep_rew_mean     | 15.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 311      |
|    time_elapsed    | 34745    |
|    total_timesteps | 124400   |
---------------------------------
Eval num_timesteps=124600, episode_reward=13.56 +/- 13.02
Episode length: 19.80 +/- 17.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 19.8         |
|    mean_reward          | 13.6         |
| time/                   |              |
|    total_timesteps      | 124600       |
| train/                  |              |
|    approx_kl            | 0.0012245259 |
|    clip_fraction        | 0.0346       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.516       |
|    explained_variance   | 0.236        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.192        |
|    n_updates            | 3110         |
|    policy_gradient_loss | -0.00339     |
|    value_loss           | 1.48         |
------------------------------------------
Eval num_timesteps=124800, episode_reward=17.90 +/- 14.14
Episode length: 26.20 +/- 19.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 124800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.6     |
|    ep_rew_mean     | 14.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 312      |
|    time_elapsed    | 34852    |
|    total_timesteps | 124800   |
---------------------------------
Eval num_timesteps=125000, episode_reward=25.11 +/- 12.47
Episode length: 36.00 +/- 17.25
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36            |
|    mean_reward          | 25.1          |
| time/                   |               |
|    total_timesteps      | 125000        |
| train/                  |               |
|    approx_kl            | 0.00065979856 |
|    clip_fraction        | 0.0152        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.499        |
|    explained_variance   | 0.384         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.91          |
|    n_updates            | 3120          |
|    policy_gradient_loss | -0.00305      |
|    value_loss           | 2.92          |
-------------------------------------------
Eval num_timesteps=125200, episode_reward=16.27 +/- 14.44
Episode length: 24.40 +/- 20.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 16.3     |
| time/              |          |
|    total_timesteps | 125200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.8     |
|    ep_rew_mean     | 15.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 313      |
|    time_elapsed    | 34983    |
|    total_timesteps | 125200   |
---------------------------------
Eval num_timesteps=125400, episode_reward=12.06 +/- 12.97
Episode length: 17.40 +/- 17.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.4         |
|    mean_reward          | 12.1         |
| time/                   |              |
|    total_timesteps      | 125400       |
| train/                  |              |
|    approx_kl            | 0.0008471605 |
|    clip_fraction        | 0.0587       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.467       |
|    explained_variance   | 0.383        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.441        |
|    n_updates            | 3130         |
|    policy_gradient_loss | -0.00594     |
|    value_loss           | 1.64         |
------------------------------------------
Eval num_timesteps=125600, episode_reward=19.89 +/- 12.86
Episode length: 28.80 +/- 17.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.8     |
|    mean_reward     | 19.9     |
| time/              |          |
|    total_timesteps | 125600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | 16.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 314      |
|    time_elapsed    | 35093    |
|    total_timesteps | 125600   |
---------------------------------
Eval num_timesteps=125800, episode_reward=20.97 +/- 12.02
Episode length: 30.80 +/- 17.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30.8         |
|    mean_reward          | 21           |
| time/                   |              |
|    total_timesteps      | 125800       |
| train/                  |              |
|    approx_kl            | 0.0010405156 |
|    clip_fraction        | 0.0451       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.505       |
|    explained_variance   | 0.342        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.956        |
|    n_updates            | 3140         |
|    policy_gradient_loss | -0.00239     |
|    value_loss           | 1.86         |
------------------------------------------
Eval num_timesteps=126000, episode_reward=17.00 +/- 15.18
Episode length: 24.60 +/- 20.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.2     |
|    ep_rew_mean     | 16.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 315      |
|    time_elapsed    | 35199    |
|    total_timesteps | 126000   |
---------------------------------
Eval num_timesteps=126200, episode_reward=22.66 +/- 14.99
Episode length: 32.80 +/- 21.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 32.8        |
|    mean_reward          | 22.7        |
| time/                   |             |
|    total_timesteps      | 126200      |
| train/                  |             |
|    approx_kl            | 0.002184949 |
|    clip_fraction        | 0.0498      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.525      |
|    explained_variance   | 0.4         |
|    learning_rate        | 0.0001      |
|    loss                 | 0.519       |
|    n_updates            | 3150        |
|    policy_gradient_loss | -0.0052     |
|    value_loss           | 1.56        |
-----------------------------------------
Eval num_timesteps=126400, episode_reward=28.78 +/- 11.40
Episode length: 42.00 +/- 16.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 28.8     |
| time/              |          |
|    total_timesteps | 126400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.8     |
|    ep_rew_mean     | 17.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 316      |
|    time_elapsed    | 35332    |
|    total_timesteps | 126400   |
---------------------------------
Eval num_timesteps=126600, episode_reward=35.81 +/- 0.83
Episode length: 50.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | 35.8         |
| time/                   |              |
|    total_timesteps      | 126600       |
| train/                  |              |
|    approx_kl            | 0.0018220013 |
|    clip_fraction        | 0.035        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.521       |
|    explained_variance   | 0.0841       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.165        |
|    n_updates            | 3160         |
|    policy_gradient_loss | -0.00359     |
|    value_loss           | 1.71         |
------------------------------------------
New best mean reward!
Eval num_timesteps=126800, episode_reward=23.98 +/- 13.46
Episode length: 35.00 +/- 18.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 24       |
| time/              |          |
|    total_timesteps | 126800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25       |
|    ep_rew_mean     | 17.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 317      |
|    time_elapsed    | 35471    |
|    total_timesteps | 126800   |
---------------------------------
Eval num_timesteps=127000, episode_reward=9.51 +/- 13.23
Episode length: 14.20 +/- 18.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.2          |
|    mean_reward          | 9.51          |
| time/                   |               |
|    total_timesteps      | 127000        |
| train/                  |               |
|    approx_kl            | 0.00047798108 |
|    clip_fraction        | 0.00781       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.507        |
|    explained_variance   | 0.272         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.08          |
|    n_updates            | 3170          |
|    policy_gradient_loss | -0.0025       |
|    value_loss           | 2.37          |
-------------------------------------------
Eval num_timesteps=127200, episode_reward=11.51 +/- 13.05
Episode length: 16.80 +/- 17.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | 11.5     |
| time/              |          |
|    total_timesteps | 127200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.7     |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 318      |
|    time_elapsed    | 35574    |
|    total_timesteps | 127200   |
---------------------------------
Eval num_timesteps=127400, episode_reward=11.68 +/- 12.34
Episode length: 17.40 +/- 17.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.4         |
|    mean_reward          | 11.7         |
| time/                   |              |
|    total_timesteps      | 127400       |
| train/                  |              |
|    approx_kl            | 0.0010280329 |
|    clip_fraction        | 0.0277       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.505       |
|    explained_variance   | 0.26         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.897        |
|    n_updates            | 3180         |
|    policy_gradient_loss | -0.00374     |
|    value_loss           | 1.78         |
------------------------------------------
Eval num_timesteps=127600, episode_reward=9.93 +/- 12.37
Episode length: 15.00 +/- 17.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | 9.93     |
| time/              |          |
|    total_timesteps | 127600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.2     |
|    ep_rew_mean     | 18.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 319      |
|    time_elapsed    | 35676    |
|    total_timesteps | 127600   |
---------------------------------
Eval num_timesteps=127800, episode_reward=23.33 +/- 14.16
Episode length: 33.80 +/- 19.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.8         |
|    mean_reward          | 23.3         |
| time/                   |              |
|    total_timesteps      | 127800       |
| train/                  |              |
|    approx_kl            | 0.0014144693 |
|    clip_fraction        | 0.0605       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.491       |
|    explained_variance   | 0.205        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.379        |
|    n_updates            | 3190         |
|    policy_gradient_loss | -0.00575     |
|    value_loss           | 1.68         |
------------------------------------------
Eval num_timesteps=128000, episode_reward=21.72 +/- 15.25
Episode length: 32.20 +/- 21.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 21.7     |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27       |
|    ep_rew_mean     | 18.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 320      |
|    time_elapsed    | 35807    |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=128200, episode_reward=15.41 +/- 14.67
Episode length: 23.60 +/- 21.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.6          |
|    mean_reward          | 15.4          |
| time/                   |               |
|    total_timesteps      | 128200        |
| train/                  |               |
|    approx_kl            | 0.00075720146 |
|    clip_fraction        | 0.0192        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.493        |
|    explained_variance   | 0.38          |
|    learning_rate        | 0.0001        |
|    loss                 | 1.72          |
|    n_updates            | 3200          |
|    policy_gradient_loss | -0.00532      |
|    value_loss           | 1.92          |
-------------------------------------------
Eval num_timesteps=128400, episode_reward=4.84 +/- 2.27
Episode length: 7.80 +/- 3.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7.8      |
|    mean_reward     | 4.84     |
| time/              |          |
|    total_timesteps | 128400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.6     |
|    ep_rew_mean     | 18.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 321      |
|    time_elapsed    | 35896    |
|    total_timesteps | 128400   |
---------------------------------
Eval num_timesteps=128600, episode_reward=11.09 +/- 12.89
Episode length: 16.20 +/- 17.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.2         |
|    mean_reward          | 11.1         |
| time/                   |              |
|    total_timesteps      | 128600       |
| train/                  |              |
|    approx_kl            | 0.0013488948 |
|    clip_fraction        | 0.0179       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.484       |
|    explained_variance   | 0.318        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.147        |
|    n_updates            | 3210         |
|    policy_gradient_loss | -0.00402     |
|    value_loss           | 1.36         |
------------------------------------------
Eval num_timesteps=128800, episode_reward=17.40 +/- 15.47
Episode length: 24.80 +/- 20.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 128800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26       |
|    ep_rew_mean     | 17.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 322      |
|    time_elapsed    | 35999    |
|    total_timesteps | 128800   |
---------------------------------
Eval num_timesteps=129000, episode_reward=22.54 +/- 14.66
Episode length: 33.00 +/- 20.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33            |
|    mean_reward          | 22.5          |
| time/                   |               |
|    total_timesteps      | 129000        |
| train/                  |               |
|    approx_kl            | 0.00048676055 |
|    clip_fraction        | 0.0145        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.488        |
|    explained_variance   | -0.0391       |
|    learning_rate        | 0.0001        |
|    loss                 | 0.291         |
|    n_updates            | 3220          |
|    policy_gradient_loss | -0.00342      |
|    value_loss           | 3.67          |
-------------------------------------------
Eval num_timesteps=129200, episode_reward=20.43 +/- 14.22
Episode length: 29.80 +/- 20.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.8     |
|    mean_reward     | 20.4     |
| time/              |          |
|    total_timesteps | 129200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.3     |
|    ep_rew_mean     | 17.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 323      |
|    time_elapsed    | 36146    |
|    total_timesteps | 129200   |
---------------------------------
Eval num_timesteps=129400, episode_reward=13.01 +/- 10.29
Episode length: 19.80 +/- 15.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 19.8         |
|    mean_reward          | 13           |
| time/                   |              |
|    total_timesteps      | 129400       |
| train/                  |              |
|    approx_kl            | 0.0011523913 |
|    clip_fraction        | 0.0424       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.506       |
|    explained_variance   | 0.277        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.397        |
|    n_updates            | 3230         |
|    policy_gradient_loss | -0.00434     |
|    value_loss           | 1.7          |
------------------------------------------
Eval num_timesteps=129600, episode_reward=23.33 +/- 13.24
Episode length: 34.60 +/- 18.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 23.3     |
| time/              |          |
|    total_timesteps | 129600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | 17.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 324      |
|    time_elapsed    | 36256    |
|    total_timesteps | 129600   |
---------------------------------
Eval num_timesteps=129800, episode_reward=16.09 +/- 15.12
Episode length: 23.80 +/- 21.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.8         |
|    mean_reward          | 16.1         |
| time/                   |              |
|    total_timesteps      | 129800       |
| train/                  |              |
|    approx_kl            | 0.0011690313 |
|    clip_fraction        | 0.0386       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.519       |
|    explained_variance   | 0.158        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.616        |
|    n_updates            | 3240         |
|    policy_gradient_loss | -0.00517     |
|    value_loss           | 2.3          |
------------------------------------------
Eval num_timesteps=130000, episode_reward=5.18 +/- 3.13
Episode length: 8.40 +/- 4.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8.4      |
|    mean_reward     | 5.18     |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.6     |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 325      |
|    time_elapsed    | 36341    |
|    total_timesteps | 130000   |
---------------------------------
Eval num_timesteps=130200, episode_reward=16.57 +/- 14.21
Episode length: 24.80 +/- 20.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.8         |
|    mean_reward          | 16.6         |
| time/                   |              |
|    total_timesteps      | 130200       |
| train/                  |              |
|    approx_kl            | 0.0008724573 |
|    clip_fraction        | 0.0353       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.484       |
|    explained_variance   | 0.332        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.858        |
|    n_updates            | 3250         |
|    policy_gradient_loss | -0.00388     |
|    value_loss           | 1.77         |
------------------------------------------
Eval num_timesteps=130400, episode_reward=11.31 +/- 11.20
Episode length: 17.40 +/- 16.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 11.3     |
| time/              |          |
|    total_timesteps | 130400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24       |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 326      |
|    time_elapsed    | 36452    |
|    total_timesteps | 130400   |
---------------------------------
Eval num_timesteps=130600, episode_reward=10.27 +/- 12.95
Episode length: 15.20 +/- 17.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.2         |
|    mean_reward          | 10.3         |
| time/                   |              |
|    total_timesteps      | 130600       |
| train/                  |              |
|    approx_kl            | 0.0004996189 |
|    clip_fraction        | 0.0179       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.463       |
|    explained_variance   | 0.262        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.01         |
|    n_updates            | 3260         |
|    policy_gradient_loss | -0.00313     |
|    value_loss           | 3.21         |
------------------------------------------
Eval num_timesteps=130800, episode_reward=27.05 +/- 12.14
Episode length: 39.00 +/- 17.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 27       |
| time/              |          |
|    total_timesteps | 130800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | 17.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 327      |
|    time_elapsed    | 36558    |
|    total_timesteps | 130800   |
---------------------------------
Eval num_timesteps=131000, episode_reward=15.73 +/- 15.82
Episode length: 23.00 +/- 22.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23           |
|    mean_reward          | 15.7         |
| time/                   |              |
|    total_timesteps      | 131000       |
| train/                  |              |
|    approx_kl            | 0.0016581852 |
|    clip_fraction        | 0.0484       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.477       |
|    explained_variance   | -0.332       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0209       |
|    n_updates            | 3270         |
|    policy_gradient_loss | -0.00462     |
|    value_loss           | 0.712        |
------------------------------------------
Eval num_timesteps=131200, episode_reward=14.88 +/- 16.46
Episode length: 21.80 +/- 23.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | 14.9     |
| time/              |          |
|    total_timesteps | 131200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | 17.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 328      |
|    time_elapsed    | 36665    |
|    total_timesteps | 131200   |
---------------------------------
Eval num_timesteps=131400, episode_reward=34.25 +/- 0.83
Episode length: 50.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 50            |
|    mean_reward          | 34.2          |
| time/                   |               |
|    total_timesteps      | 131400        |
| train/                  |               |
|    approx_kl            | 0.00089189474 |
|    clip_fraction        | 0.0337        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.428        |
|    explained_variance   | 0.278         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.24          |
|    n_updates            | 3280          |
|    policy_gradient_loss | -0.00298      |
|    value_loss           | 2.32          |
-------------------------------------------
Eval num_timesteps=131600, episode_reward=10.48 +/- 13.21
Episode length: 15.40 +/- 17.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | 10.5     |
| time/              |          |
|    total_timesteps | 131600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.6     |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 329      |
|    time_elapsed    | 36798    |
|    total_timesteps | 131600   |
---------------------------------
Eval num_timesteps=131800, episode_reward=29.16 +/- 9.83
Episode length: 43.00 +/- 14.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 43           |
|    mean_reward          | 29.2         |
| time/                   |              |
|    total_timesteps      | 131800       |
| train/                  |              |
|    approx_kl            | 0.0012270551 |
|    clip_fraction        | 0.0221       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.465       |
|    explained_variance   | 0.214        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.452        |
|    n_updates            | 3290         |
|    policy_gradient_loss | -0.00193     |
|    value_loss           | 1.61         |
------------------------------------------
Eval num_timesteps=132000, episode_reward=18.38 +/- 14.44
Episode length: 26.40 +/- 19.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.4     |
|    mean_reward     | 18.4     |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | 17.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 330      |
|    time_elapsed    | 36927    |
|    total_timesteps | 132000   |
---------------------------------
Eval num_timesteps=132200, episode_reward=20.96 +/- 15.67
Episode length: 31.40 +/- 22.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 31.4          |
|    mean_reward          | 21            |
| time/                   |               |
|    total_timesteps      | 132200        |
| train/                  |               |
|    approx_kl            | 0.00055937376 |
|    clip_fraction        | 0.00558       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.478        |
|    explained_variance   | 0.375         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.19          |
|    n_updates            | 3300          |
|    policy_gradient_loss | -0.00174      |
|    value_loss           | 2.42          |
-------------------------------------------
Eval num_timesteps=132400, episode_reward=23.37 +/- 14.60
Episode length: 33.40 +/- 20.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 23.4     |
| time/              |          |
|    total_timesteps | 132400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.9     |
|    ep_rew_mean     | 17.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 331      |
|    time_elapsed    | 37061    |
|    total_timesteps | 132400   |
---------------------------------
Eval num_timesteps=132600, episode_reward=8.67 +/- 5.39
Episode length: 13.00 +/- 7.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 13           |
|    mean_reward          | 8.67         |
| time/                   |              |
|    total_timesteps      | 132600       |
| train/                  |              |
|    approx_kl            | 0.0011777533 |
|    clip_fraction        | 0.0232       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.499       |
|    explained_variance   | 0.241        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.79         |
|    n_updates            | 3310         |
|    policy_gradient_loss | -0.00173     |
|    value_loss           | 0.68         |
------------------------------------------
Eval num_timesteps=132800, episode_reward=22.16 +/- 15.14
Episode length: 32.40 +/- 21.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.4     |
|    mean_reward     | 22.2     |
| time/              |          |
|    total_timesteps | 132800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.9     |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 332      |
|    time_elapsed    | 37157    |
|    total_timesteps | 132800   |
---------------------------------
Eval num_timesteps=133000, episode_reward=22.76 +/- 15.33
Episode length: 32.80 +/- 21.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.8         |
|    mean_reward          | 22.8         |
| time/                   |              |
|    total_timesteps      | 133000       |
| train/                  |              |
|    approx_kl            | 0.0013449242 |
|    clip_fraction        | 0.0953       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.52        |
|    explained_variance   | 0.241        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.879        |
|    n_updates            | 3320         |
|    policy_gradient_loss | -0.00661     |
|    value_loss           | 2.48         |
------------------------------------------
Eval num_timesteps=133200, episode_reward=24.52 +/- 13.59
Episode length: 34.80 +/- 18.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 24.5     |
| time/              |          |
|    total_timesteps | 133200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.7     |
|    ep_rew_mean     | 18.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 333      |
|    time_elapsed    | 37292    |
|    total_timesteps | 133200   |
---------------------------------
Eval num_timesteps=133400, episode_reward=17.02 +/- 13.52
Episode length: 25.60 +/- 20.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.6         |
|    mean_reward          | 17           |
| time/                   |              |
|    total_timesteps      | 133400       |
| train/                  |              |
|    approx_kl            | 0.0016545564 |
|    clip_fraction        | 0.0397       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.494       |
|    explained_variance   | 0.382        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.04         |
|    n_updates            | 3330         |
|    policy_gradient_loss | -0.00473     |
|    value_loss           | 1.81         |
------------------------------------------
Eval num_timesteps=133600, episode_reward=12.41 +/- 11.28
Episode length: 18.80 +/- 16.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.8     |
|    mean_reward     | 12.4     |
| time/              |          |
|    total_timesteps | 133600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25       |
|    ep_rew_mean     | 17.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 334      |
|    time_elapsed    | 37401    |
|    total_timesteps | 133600   |
---------------------------------
Eval num_timesteps=133800, episode_reward=18.21 +/- 14.46
Episode length: 26.00 +/- 19.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26           |
|    mean_reward          | 18.2         |
| time/                   |              |
|    total_timesteps      | 133800       |
| train/                  |              |
|    approx_kl            | 0.0009253235 |
|    clip_fraction        | 0.0141       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.488       |
|    explained_variance   | 0.148        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.967        |
|    n_updates            | 3340         |
|    policy_gradient_loss | -0.00268     |
|    value_loss           | 2.6          |
------------------------------------------
Eval num_timesteps=134000, episode_reward=9.29 +/- 12.16
Episode length: 14.60 +/- 17.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 9.29     |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.6     |
|    ep_rew_mean     | 18.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 335      |
|    time_elapsed    | 37510    |
|    total_timesteps | 134000   |
---------------------------------
Eval num_timesteps=134200, episode_reward=8.44 +/- 8.01
Episode length: 12.80 +/- 10.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 12.8         |
|    mean_reward          | 8.44         |
| time/                   |              |
|    total_timesteps      | 134200       |
| train/                  |              |
|    approx_kl            | 0.0012436913 |
|    clip_fraction        | 0.0337       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.526       |
|    explained_variance   | 0.155        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.56         |
|    n_updates            | 3350         |
|    policy_gradient_loss | -0.00409     |
|    value_loss           | 1.6          |
------------------------------------------
Eval num_timesteps=134400, episode_reward=18.33 +/- 13.67
Episode length: 26.80 +/- 18.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.8     |
|    mean_reward     | 18.3     |
| time/              |          |
|    total_timesteps | 134400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.1     |
|    ep_rew_mean     | 18       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 336      |
|    time_elapsed    | 37604    |
|    total_timesteps | 134400   |
---------------------------------
Eval num_timesteps=134600, episode_reward=16.00 +/- 15.68
Episode length: 23.40 +/- 21.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.4         |
|    mean_reward          | 16           |
| time/                   |              |
|    total_timesteps      | 134600       |
| train/                  |              |
|    approx_kl            | 0.0010783572 |
|    clip_fraction        | 0.0272       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.523       |
|    explained_variance   | 0.237        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.84         |
|    n_updates            | 3360         |
|    policy_gradient_loss | -0.00333     |
|    value_loss           | 2.19         |
------------------------------------------
Eval num_timesteps=134800, episode_reward=6.38 +/- 4.99
Episode length: 9.80 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.8      |
|    mean_reward     | 6.38     |
| time/              |          |
|    total_timesteps | 134800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.6     |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 337      |
|    time_elapsed    | 37698    |
|    total_timesteps | 134800   |
---------------------------------
Eval num_timesteps=135000, episode_reward=14.30 +/- 11.20
Episode length: 20.60 +/- 15.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 20.6          |
|    mean_reward          | 14.3          |
| time/                   |               |
|    total_timesteps      | 135000        |
| train/                  |               |
|    approx_kl            | 0.00045664716 |
|    clip_fraction        | 0.0107        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.502        |
|    explained_variance   | 0.41          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.633         |
|    n_updates            | 3370          |
|    policy_gradient_loss | -0.00207      |
|    value_loss           | 1.86          |
-------------------------------------------
Eval num_timesteps=135200, episode_reward=15.53 +/- 15.21
Episode length: 23.40 +/- 22.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 15.5     |
| time/              |          |
|    total_timesteps | 135200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.1     |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 338      |
|    time_elapsed    | 37800    |
|    total_timesteps | 135200   |
---------------------------------
Eval num_timesteps=135400, episode_reward=17.32 +/- 15.04
Episode length: 24.80 +/- 20.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.8         |
|    mean_reward          | 17.3         |
| time/                   |              |
|    total_timesteps      | 135400       |
| train/                  |              |
|    approx_kl            | 0.0009097429 |
|    clip_fraction        | 0.0183       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.529       |
|    explained_variance   | 0.354        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.28         |
|    n_updates            | 3380         |
|    policy_gradient_loss | -0.00401     |
|    value_loss           | 2.11         |
------------------------------------------
Eval num_timesteps=135600, episode_reward=11.56 +/- 11.52
Episode length: 17.80 +/- 16.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | 11.6     |
| time/              |          |
|    total_timesteps | 135600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.4     |
|    ep_rew_mean     | 18.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 339      |
|    time_elapsed    | 37911    |
|    total_timesteps | 135600   |
---------------------------------
Eval num_timesteps=135800, episode_reward=16.54 +/- 16.13
Episode length: 23.60 +/- 21.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.6         |
|    mean_reward          | 16.5         |
| time/                   |              |
|    total_timesteps      | 135800       |
| train/                  |              |
|    approx_kl            | 0.0008572844 |
|    clip_fraction        | 0.0225       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.553       |
|    explained_variance   | 0.299        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.12         |
|    n_updates            | 3390         |
|    policy_gradient_loss | -0.00536     |
|    value_loss           | 1.48         |
------------------------------------------
Eval num_timesteps=136000, episode_reward=8.87 +/- 12.35
Episode length: 14.00 +/- 18.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14       |
|    mean_reward     | 8.87     |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.9     |
|    ep_rew_mean     | 18.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 340      |
|    time_elapsed    | 38040    |
|    total_timesteps | 136000   |
---------------------------------
Eval num_timesteps=136200, episode_reward=21.85 +/- 14.60
Episode length: 32.60 +/- 21.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 32.6        |
|    mean_reward          | 21.8        |
| time/                   |             |
|    total_timesteps      | 136200      |
| train/                  |             |
|    approx_kl            | 0.001478881 |
|    clip_fraction        | 0.0828      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.574      |
|    explained_variance   | 0.161       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.1         |
|    n_updates            | 3400        |
|    policy_gradient_loss | -0.00431    |
|    value_loss           | 1.06        |
-----------------------------------------
Eval num_timesteps=136400, episode_reward=16.40 +/- 14.42
Episode length: 24.60 +/- 20.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 16.4     |
| time/              |          |
|    total_timesteps | 136400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.2     |
|    ep_rew_mean     | 18.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 341      |
|    time_elapsed    | 38147    |
|    total_timesteps | 136400   |
---------------------------------
Eval num_timesteps=136600, episode_reward=12.65 +/- 11.62
Episode length: 18.40 +/- 15.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.4         |
|    mean_reward          | 12.6         |
| time/                   |              |
|    total_timesteps      | 136600       |
| train/                  |              |
|    approx_kl            | 0.0022116774 |
|    clip_fraction        | 0.0772       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.529       |
|    explained_variance   | 0.322        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.1          |
|    n_updates            | 3410         |
|    policy_gradient_loss | -0.00457     |
|    value_loss           | 1.48         |
------------------------------------------
Eval num_timesteps=136800, episode_reward=17.57 +/- 13.40
Episode length: 26.40 +/- 19.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.4     |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 136800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27       |
|    ep_rew_mean     | 18.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 342      |
|    time_elapsed    | 38259    |
|    total_timesteps | 136800   |
---------------------------------
Eval num_timesteps=137000, episode_reward=11.09 +/- 13.09
Episode length: 16.20 +/- 17.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.2         |
|    mean_reward          | 11.1         |
| time/                   |              |
|    total_timesteps      | 137000       |
| train/                  |              |
|    approx_kl            | 0.0008120534 |
|    clip_fraction        | 0.0152       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.528       |
|    explained_variance   | 0.199        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.337        |
|    n_updates            | 3420         |
|    policy_gradient_loss | -0.00388     |
|    value_loss           | 2.18         |
------------------------------------------
Eval num_timesteps=137200, episode_reward=19.81 +/- 13.53
Episode length: 29.00 +/- 18.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29       |
|    mean_reward     | 19.8     |
| time/              |          |
|    total_timesteps | 137200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.5     |
|    ep_rew_mean     | 19.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 343      |
|    time_elapsed    | 38368    |
|    total_timesteps | 137200   |
---------------------------------
Eval num_timesteps=137400, episode_reward=28.74 +/- 12.07
Episode length: 41.60 +/- 16.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 41.6          |
|    mean_reward          | 28.7          |
| time/                   |               |
|    total_timesteps      | 137400        |
| train/                  |               |
|    approx_kl            | 0.00088910986 |
|    clip_fraction        | 0.0612        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.558        |
|    explained_variance   | 0.121         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.0953        |
|    n_updates            | 3430          |
|    policy_gradient_loss | -0.00419      |
|    value_loss           | 0.645         |
-------------------------------------------
Eval num_timesteps=137600, episode_reward=14.63 +/- 15.33
Episode length: 22.40 +/- 22.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 14.6     |
| time/              |          |
|    total_timesteps | 137600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28       |
|    ep_rew_mean     | 19.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 344      |
|    time_elapsed    | 38476    |
|    total_timesteps | 137600   |
---------------------------------
Eval num_timesteps=137800, episode_reward=18.86 +/- 12.13
Episode length: 28.60 +/- 17.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 28.6          |
|    mean_reward          | 18.9          |
| time/                   |               |
|    total_timesteps      | 137800        |
| train/                  |               |
|    approx_kl            | 0.00094470446 |
|    clip_fraction        | 0.0241        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.507        |
|    explained_variance   | 0.381         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.632         |
|    n_updates            | 3440          |
|    policy_gradient_loss | -0.00336      |
|    value_loss           | 1.53          |
-------------------------------------------
Eval num_timesteps=138000, episode_reward=17.91 +/- 14.16
Episode length: 26.20 +/- 19.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.5     |
|    ep_rew_mean     | 19.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 345      |
|    time_elapsed    | 38595    |
|    total_timesteps | 138000   |
---------------------------------
Eval num_timesteps=138200, episode_reward=17.32 +/- 15.45
Episode length: 24.80 +/- 20.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.8          |
|    mean_reward          | 17.3          |
| time/                   |               |
|    total_timesteps      | 138200        |
| train/                  |               |
|    approx_kl            | 0.00055607967 |
|    clip_fraction        | 0.00759       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.504        |
|    explained_variance   | 0.195         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.59          |
|    n_updates            | 3450          |
|    policy_gradient_loss | -0.0014       |
|    value_loss           | 2.48          |
-------------------------------------------
Eval num_timesteps=138400, episode_reward=29.72 +/- 9.59
Episode length: 43.40 +/- 13.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.4     |
|    mean_reward     | 29.7     |
| time/              |          |
|    total_timesteps | 138400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.2     |
|    ep_rew_mean     | 19.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 346      |
|    time_elapsed    | 38711    |
|    total_timesteps | 138400   |
---------------------------------
Eval num_timesteps=138600, episode_reward=29.53 +/- 12.70
Episode length: 41.40 +/- 17.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.4         |
|    mean_reward          | 29.5         |
| time/                   |              |
|    total_timesteps      | 138600       |
| train/                  |              |
|    approx_kl            | 0.0017562037 |
|    clip_fraction        | 0.0866       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.449       |
|    explained_variance   | 0.421        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.562        |
|    n_updates            | 3460         |
|    policy_gradient_loss | -0.00477     |
|    value_loss           | 1.56         |
------------------------------------------
Eval num_timesteps=138800, episode_reward=12.09 +/- 12.02
Episode length: 17.80 +/- 16.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | 12.1     |
| time/              |          |
|    total_timesteps | 138800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.6     |
|    ep_rew_mean     | 19.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 347      |
|    time_elapsed    | 38823    |
|    total_timesteps | 138800   |
---------------------------------
Eval num_timesteps=139000, episode_reward=10.02 +/- 13.37
Episode length: 14.60 +/- 17.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.6         |
|    mean_reward          | 10           |
| time/                   |              |
|    total_timesteps      | 139000       |
| train/                  |              |
|    approx_kl            | 0.0006492569 |
|    clip_fraction        | 0.0152       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.456       |
|    explained_variance   | 0.356        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.926        |
|    n_updates            | 3470         |
|    policy_gradient_loss | -0.00231     |
|    value_loss           | 1.82         |
------------------------------------------
Eval num_timesteps=139200, episode_reward=29.09 +/- 11.92
Episode length: 41.80 +/- 16.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 29.1     |
| time/              |          |
|    total_timesteps | 139200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.2     |
|    ep_rew_mean     | 18       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 348      |
|    time_elapsed    | 38931    |
|    total_timesteps | 139200   |
---------------------------------
Eval num_timesteps=139400, episode_reward=11.50 +/- 12.30
Episode length: 17.20 +/- 16.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.2         |
|    mean_reward          | 11.5         |
| time/                   |              |
|    total_timesteps      | 139400       |
| train/                  |              |
|    approx_kl            | 0.0009337174 |
|    clip_fraction        | 0.0145       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.459       |
|    explained_variance   | 0.303        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.444        |
|    n_updates            | 3480         |
|    policy_gradient_loss | -0.00238     |
|    value_loss           | 2.06         |
------------------------------------------
Eval num_timesteps=139600, episode_reward=10.23 +/- 12.82
Episode length: 15.40 +/- 17.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | 10.2     |
| time/              |          |
|    total_timesteps | 139600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 349      |
|    time_elapsed    | 39036    |
|    total_timesteps | 139600   |
---------------------------------
Eval num_timesteps=139800, episode_reward=22.01 +/- 15.76
Episode length: 32.00 +/- 22.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32            |
|    mean_reward          | 22            |
| time/                   |               |
|    total_timesteps      | 139800        |
| train/                  |               |
|    approx_kl            | 0.00045325555 |
|    clip_fraction        | 0.00424       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.436        |
|    explained_variance   | 0.402         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.949         |
|    n_updates            | 3490          |
|    policy_gradient_loss | -0.00223      |
|    value_loss           | 2.64          |
-------------------------------------------
Eval num_timesteps=140000, episode_reward=7.03 +/- 4.93
Episode length: 10.80 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10.8     |
|    mean_reward     | 7.03     |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.7     |
|    ep_rew_mean     | 17.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 350      |
|    time_elapsed    | 39127    |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=140200, episode_reward=7.54 +/- 5.83
Episode length: 11.40 +/- 8.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 7.54          |
| time/                   |               |
|    total_timesteps      | 140200        |
| train/                  |               |
|    approx_kl            | 0.00032261253 |
|    clip_fraction        | 0.0121        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.441        |
|    explained_variance   | 0.254         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.0341        |
|    n_updates            | 3500          |
|    policy_gradient_loss | -0.00232      |
|    value_loss           | 1.06          |
-------------------------------------------
Eval num_timesteps=140400, episode_reward=11.23 +/- 13.23
Episode length: 16.20 +/- 17.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | 11.2     |
| time/              |          |
|    total_timesteps | 140400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.5     |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 351      |
|    time_elapsed    | 39215    |
|    total_timesteps | 140400   |
---------------------------------
Eval num_timesteps=140600, episode_reward=15.40 +/- 14.68
Episode length: 23.40 +/- 21.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.4          |
|    mean_reward          | 15.4          |
| time/                   |               |
|    total_timesteps      | 140600        |
| train/                  |               |
|    approx_kl            | 0.00030054603 |
|    clip_fraction        | 0.0259        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.435        |
|    explained_variance   | 0.457         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.906         |
|    n_updates            | 3510          |
|    policy_gradient_loss | -0.00115      |
|    value_loss           | 1.5           |
-------------------------------------------
Eval num_timesteps=140800, episode_reward=16.13 +/- 15.51
Episode length: 23.60 +/- 21.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 16.1     |
| time/              |          |
|    total_timesteps | 140800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.9     |
|    ep_rew_mean     | 15.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 352      |
|    time_elapsed    | 39324    |
|    total_timesteps | 140800   |
---------------------------------
Eval num_timesteps=141000, episode_reward=19.45 +/- 14.89
Episode length: 27.40 +/- 20.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 27.4          |
|    mean_reward          | 19.5          |
| time/                   |               |
|    total_timesteps      | 141000        |
| train/                  |               |
|    approx_kl            | 0.00041222665 |
|    clip_fraction        | 0.00714       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.423        |
|    explained_variance   | 0.371         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.49          |
|    n_updates            | 3520          |
|    policy_gradient_loss | -0.00224      |
|    value_loss           | 3.07          |
-------------------------------------------
Eval num_timesteps=141200, episode_reward=10.38 +/- 12.62
Episode length: 15.80 +/- 17.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | 10.4     |
| time/              |          |
|    total_timesteps | 141200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.1     |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 353      |
|    time_elapsed    | 39451    |
|    total_timesteps | 141200   |
---------------------------------
Eval num_timesteps=141400, episode_reward=27.98 +/- 13.00
Episode length: 40.80 +/- 18.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 40.8          |
|    mean_reward          | 28            |
| time/                   |               |
|    total_timesteps      | 141400        |
| train/                  |               |
|    approx_kl            | 0.00092566136 |
|    clip_fraction        | 0.0319        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.462        |
|    explained_variance   | 0.42          |
|    learning_rate        | 0.0001        |
|    loss                 | 0.68          |
|    n_updates            | 3530          |
|    policy_gradient_loss | -0.0024       |
|    value_loss           | 1.34          |
-------------------------------------------
Eval num_timesteps=141600, episode_reward=18.27 +/- 12.96
Episode length: 27.20 +/- 18.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.2     |
|    mean_reward     | 18.3     |
| time/              |          |
|    total_timesteps | 141600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.5     |
|    ep_rew_mean     | 14.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 354      |
|    time_elapsed    | 39579    |
|    total_timesteps | 141600   |
---------------------------------
Eval num_timesteps=141800, episode_reward=27.17 +/- 9.63
Episode length: 39.00 +/- 13.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 39          |
|    mean_reward          | 27.2        |
| time/                   |             |
|    total_timesteps      | 141800      |
| train/                  |             |
|    approx_kl            | 0.001022826 |
|    clip_fraction        | 0.0339      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.478      |
|    explained_variance   | 0.361       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.725       |
|    n_updates            | 3540        |
|    policy_gradient_loss | -0.00265    |
|    value_loss           | 2.23        |
-----------------------------------------
Eval num_timesteps=142000, episode_reward=22.94 +/- 15.97
Episode length: 32.40 +/- 21.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.4     |
|    mean_reward     | 22.9     |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.8     |
|    ep_rew_mean     | 15.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 355      |
|    time_elapsed    | 39720    |
|    total_timesteps | 142000   |
---------------------------------
Eval num_timesteps=142200, episode_reward=14.89 +/- 10.77
Episode length: 22.00 +/- 15.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22            |
|    mean_reward          | 14.9          |
| time/                   |               |
|    total_timesteps      | 142200        |
| train/                  |               |
|    approx_kl            | 0.00042082515 |
|    clip_fraction        | 0.0268        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.509        |
|    explained_variance   | 0.437         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.681         |
|    n_updates            | 3550          |
|    policy_gradient_loss | -0.00242      |
|    value_loss           | 1.74          |
-------------------------------------------
Eval num_timesteps=142400, episode_reward=14.99 +/- 15.07
Episode length: 22.80 +/- 22.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 15       |
| time/              |          |
|    total_timesteps | 142400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.5     |
|    ep_rew_mean     | 15.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 356      |
|    time_elapsed    | 39835    |
|    total_timesteps | 142400   |
---------------------------------
Eval num_timesteps=142600, episode_reward=16.39 +/- 15.40
Episode length: 23.80 +/- 21.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.8         |
|    mean_reward          | 16.4         |
| time/                   |              |
|    total_timesteps      | 142600       |
| train/                  |              |
|    approx_kl            | 0.0012600581 |
|    clip_fraction        | 0.0304       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.51        |
|    explained_variance   | 0.208        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.238        |
|    n_updates            | 3560         |
|    policy_gradient_loss | -0.00284     |
|    value_loss           | 2.16         |
------------------------------------------
Eval num_timesteps=142800, episode_reward=11.76 +/- 12.55
Episode length: 18.00 +/- 18.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | 11.8     |
| time/              |          |
|    total_timesteps | 142800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23       |
|    ep_rew_mean     | 15.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 357      |
|    time_elapsed    | 39941    |
|    total_timesteps | 142800   |
---------------------------------
Eval num_timesteps=143000, episode_reward=17.61 +/- 15.26
Episode length: 25.00 +/- 20.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 25          |
|    mean_reward          | 17.6        |
| time/                   |             |
|    total_timesteps      | 143000      |
| train/                  |             |
|    approx_kl            | 0.001022353 |
|    clip_fraction        | 0.025       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.53       |
|    explained_variance   | 0.418       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.49        |
|    n_updates            | 3570        |
|    policy_gradient_loss | -0.00258    |
|    value_loss           | 2.36        |
-----------------------------------------
Eval num_timesteps=143200, episode_reward=29.44 +/- 11.79
Episode length: 41.80 +/- 16.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 29.4     |
| time/              |          |
|    total_timesteps | 143200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.8     |
|    ep_rew_mean     | 15.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 358      |
|    time_elapsed    | 40070    |
|    total_timesteps | 143200   |
---------------------------------
Eval num_timesteps=143400, episode_reward=9.89 +/- 12.39
Episode length: 15.40 +/- 18.25
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.4          |
|    mean_reward          | 9.89          |
| time/                   |               |
|    total_timesteps      | 143400        |
| train/                  |               |
|    approx_kl            | 0.00078074646 |
|    clip_fraction        | 0.0315        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.552        |
|    explained_variance   | 0.536         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.354         |
|    n_updates            | 3580          |
|    policy_gradient_loss | -0.00438      |
|    value_loss           | 1.17          |
-------------------------------------------
Eval num_timesteps=143600, episode_reward=11.49 +/- 12.71
Episode length: 16.60 +/- 16.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | 11.5     |
| time/              |          |
|    total_timesteps | 143600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.9     |
|    ep_rew_mean     | 15.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 359      |
|    time_elapsed    | 40174    |
|    total_timesteps | 143600   |
---------------------------------
Eval num_timesteps=143800, episode_reward=19.37 +/- 13.61
Episode length: 27.80 +/- 18.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 27.8         |
|    mean_reward          | 19.4         |
| time/                   |              |
|    total_timesteps      | 143800       |
| train/                  |              |
|    approx_kl            | 0.0013146832 |
|    clip_fraction        | 0.0391       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.531       |
|    explained_variance   | 0.425        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.869        |
|    n_updates            | 3590         |
|    policy_gradient_loss | -0.00348     |
|    value_loss           | 1.62         |
------------------------------------------
Eval num_timesteps=144000, episode_reward=23.42 +/- 14.52
Episode length: 33.60 +/- 20.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 23.4     |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.9     |
|    ep_rew_mean     | 15.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 360      |
|    time_elapsed    | 40280    |
|    total_timesteps | 144000   |
---------------------------------
Eval num_timesteps=144200, episode_reward=12.90 +/- 10.82
Episode length: 19.60 +/- 16.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 19.6          |
|    mean_reward          | 12.9          |
| time/                   |               |
|    total_timesteps      | 144200        |
| train/                  |               |
|    approx_kl            | 0.00046923477 |
|    clip_fraction        | 0.00915       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.516        |
|    explained_variance   | 0.317         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.27          |
|    n_updates            | 3600          |
|    policy_gradient_loss | -0.00272      |
|    value_loss           | 2.42          |
-------------------------------------------
Eval num_timesteps=144400, episode_reward=15.25 +/- 15.76
Episode length: 22.60 +/- 22.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 15.2     |
| time/              |          |
|    total_timesteps | 144400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | 16.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 361      |
|    time_elapsed    | 40393    |
|    total_timesteps | 144400   |
---------------------------------
Eval num_timesteps=144600, episode_reward=16.27 +/- 12.25
Episode length: 23.40 +/- 16.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.4          |
|    mean_reward          | 16.3          |
| time/                   |               |
|    total_timesteps      | 144600        |
| train/                  |               |
|    approx_kl            | 0.00043522674 |
|    clip_fraction        | 0.0125        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.539        |
|    explained_variance   | -0.146        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.479         |
|    n_updates            | 3610          |
|    policy_gradient_loss | -0.00362      |
|    value_loss           | 1.5           |
-------------------------------------------
Eval num_timesteps=144800, episode_reward=6.47 +/- 3.68
Episode length: 10.00 +/- 5.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 6.47     |
| time/              |          |
|    total_timesteps | 144800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.5     |
|    ep_rew_mean     | 16.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 362      |
|    time_elapsed    | 40479    |
|    total_timesteps | 144800   |
---------------------------------
Eval num_timesteps=145000, episode_reward=10.63 +/- 11.51
Episode length: 16.20 +/- 17.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.2          |
|    mean_reward          | 10.6          |
| time/                   |               |
|    total_timesteps      | 145000        |
| train/                  |               |
|    approx_kl            | 0.00067932624 |
|    clip_fraction        | 0.0163        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.525        |
|    explained_variance   | 0.249         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.01          |
|    n_updates            | 3620          |
|    policy_gradient_loss | -0.00232      |
|    value_loss           | 3.07          |
-------------------------------------------
Eval num_timesteps=145200, episode_reward=15.98 +/- 15.57
Episode length: 23.40 +/- 21.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 145200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.7     |
|    ep_rew_mean     | 16.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 363      |
|    time_elapsed    | 40583    |
|    total_timesteps | 145200   |
---------------------------------
Eval num_timesteps=145400, episode_reward=18.32 +/- 13.45
Episode length: 27.00 +/- 19.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 27            |
|    mean_reward          | 18.3          |
| time/                   |               |
|    total_timesteps      | 145400        |
| train/                  |               |
|    approx_kl            | 0.00066026486 |
|    clip_fraction        | 0.00692       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.539        |
|    explained_variance   | 0.251         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.0647        |
|    n_updates            | 3630          |
|    policy_gradient_loss | -0.00151      |
|    value_loss           | 2.3           |
-------------------------------------------
Eval num_timesteps=145600, episode_reward=9.52 +/- 12.01
Episode length: 14.80 +/- 17.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 9.52     |
| time/              |          |
|    total_timesteps | 145600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | 16       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 364      |
|    time_elapsed    | 40685    |
|    total_timesteps | 145600   |
---------------------------------
Eval num_timesteps=145800, episode_reward=23.95 +/- 14.02
Episode length: 34.40 +/- 19.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.4        |
|    mean_reward          | 23.9        |
| time/                   |             |
|    total_timesteps      | 145800      |
| train/                  |             |
|    approx_kl            | 0.001338003 |
|    clip_fraction        | 0.0183      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.524      |
|    explained_variance   | 0.273       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.491       |
|    n_updates            | 3640        |
|    policy_gradient_loss | -0.0024     |
|    value_loss           | 2.39        |
-----------------------------------------
Eval num_timesteps=146000, episode_reward=17.02 +/- 14.77
Episode length: 25.00 +/- 20.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.7     |
|    ep_rew_mean     | 16.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 365      |
|    time_elapsed    | 40816    |
|    total_timesteps | 146000   |
---------------------------------
Eval num_timesteps=146200, episode_reward=17.62 +/- 14.93
Episode length: 25.60 +/- 20.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 25.6        |
|    mean_reward          | 17.6        |
| time/                   |             |
|    total_timesteps      | 146200      |
| train/                  |             |
|    approx_kl            | 0.001180904 |
|    clip_fraction        | 0.0219      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.481      |
|    explained_variance   | 0.281       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.02        |
|    n_updates            | 3650        |
|    policy_gradient_loss | -0.00311    |
|    value_loss           | 2.04        |
-----------------------------------------
Eval num_timesteps=146400, episode_reward=16.45 +/- 15.24
Episode length: 24.20 +/- 21.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 16.4     |
| time/              |          |
|    total_timesteps | 146400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.5     |
|    ep_rew_mean     | 16.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 366      |
|    time_elapsed    | 40927    |
|    total_timesteps | 146400   |
---------------------------------
Eval num_timesteps=146600, episode_reward=24.43 +/- 12.98
Episode length: 35.20 +/- 18.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.2         |
|    mean_reward          | 24.4         |
| time/                   |              |
|    total_timesteps      | 146600       |
| train/                  |              |
|    approx_kl            | 0.0006857304 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.485       |
|    explained_variance   | 0.158        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.119        |
|    n_updates            | 3660         |
|    policy_gradient_loss | -0.00244     |
|    value_loss           | 1.82         |
------------------------------------------
Eval num_timesteps=146800, episode_reward=12.55 +/- 12.05
Episode length: 18.60 +/- 16.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 12.6     |
| time/              |          |
|    total_timesteps | 146800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | 17       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 367      |
|    time_elapsed    | 41037    |
|    total_timesteps | 146800   |
---------------------------------
Eval num_timesteps=147000, episode_reward=4.24 +/- 1.96
Episode length: 7.20 +/- 2.86
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 7.2           |
|    mean_reward          | 4.24          |
| time/                   |               |
|    total_timesteps      | 147000        |
| train/                  |               |
|    approx_kl            | 0.00071922236 |
|    clip_fraction        | 0.0167        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.484        |
|    explained_variance   | 0.211         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.551         |
|    n_updates            | 3670          |
|    policy_gradient_loss | -0.0031       |
|    value_loss           | 1.33          |
-------------------------------------------
Eval num_timesteps=147200, episode_reward=17.24 +/- 15.45
Episode length: 24.40 +/- 20.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 17.2     |
| time/              |          |
|    total_timesteps | 147200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.7     |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 368      |
|    time_elapsed    | 41127    |
|    total_timesteps | 147200   |
---------------------------------
Eval num_timesteps=147400, episode_reward=22.93 +/- 14.19
Episode length: 33.60 +/- 20.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.6         |
|    mean_reward          | 22.9         |
| time/                   |              |
|    total_timesteps      | 147400       |
| train/                  |              |
|    approx_kl            | 0.0007652809 |
|    clip_fraction        | 0.0263       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.506       |
|    explained_variance   | 0.269        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.99         |
|    n_updates            | 3680         |
|    policy_gradient_loss | -0.00347     |
|    value_loss           | 2.69         |
------------------------------------------
Eval num_timesteps=147600, episode_reward=24.43 +/- 13.27
Episode length: 35.00 +/- 18.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 24.4     |
| time/              |          |
|    total_timesteps | 147600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.3     |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 369      |
|    time_elapsed    | 41239    |
|    total_timesteps | 147600   |
---------------------------------
Eval num_timesteps=147800, episode_reward=10.97 +/- 12.01
Episode length: 16.60 +/- 17.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.6          |
|    mean_reward          | 11            |
| time/                   |               |
|    total_timesteps      | 147800        |
| train/                  |               |
|    approx_kl            | 0.00074506074 |
|    clip_fraction        | 0.0176        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.507        |
|    explained_variance   | 0.279         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.87          |
|    n_updates            | 3690          |
|    policy_gradient_loss | -0.00449      |
|    value_loss           | 1.92          |
-------------------------------------------
Eval num_timesteps=148000, episode_reward=23.30 +/- 15.95
Episode length: 32.60 +/- 21.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.6     |
|    mean_reward     | 23.3     |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.2     |
|    ep_rew_mean     | 17.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 370      |
|    time_elapsed    | 41369    |
|    total_timesteps | 148000   |
---------------------------------
Eval num_timesteps=148200, episode_reward=28.91 +/- 11.68
Episode length: 41.80 +/- 16.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.8         |
|    mean_reward          | 28.9         |
| time/                   |              |
|    total_timesteps      | 148200       |
| train/                  |              |
|    approx_kl            | 0.0021320537 |
|    clip_fraction        | 0.0571       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.471       |
|    explained_variance   | 0.211        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.05         |
|    n_updates            | 3700         |
|    policy_gradient_loss | -0.00473     |
|    value_loss           | 3.42         |
------------------------------------------
Eval num_timesteps=148400, episode_reward=15.29 +/- 15.76
Episode length: 22.80 +/- 22.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 15.3     |
| time/              |          |
|    total_timesteps | 148400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25       |
|    ep_rew_mean     | 17.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 371      |
|    time_elapsed    | 41522    |
|    total_timesteps | 148400   |
---------------------------------
Eval num_timesteps=148600, episode_reward=16.64 +/- 15.56
Episode length: 24.00 +/- 21.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24           |
|    mean_reward          | 16.6         |
| time/                   |              |
|    total_timesteps      | 148600       |
| train/                  |              |
|    approx_kl            | 0.0009577334 |
|    clip_fraction        | 0.0339       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.45        |
|    explained_variance   | 0.255        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.946        |
|    n_updates            | 3710         |
|    policy_gradient_loss | -0.00368     |
|    value_loss           | 1.69         |
------------------------------------------
Eval num_timesteps=148800, episode_reward=34.69 +/- 1.09
Episode length: 50.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 34.7     |
| time/              |          |
|    total_timesteps | 148800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | 16       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 372      |
|    time_elapsed    | 41649    |
|    total_timesteps | 148800   |
---------------------------------
Eval num_timesteps=149000, episode_reward=18.64 +/- 13.55
Episode length: 27.00 +/- 18.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 27           |
|    mean_reward          | 18.6         |
| time/                   |              |
|    total_timesteps      | 149000       |
| train/                  |              |
|    approx_kl            | 0.0005121679 |
|    clip_fraction        | 0.0192       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.426       |
|    explained_variance   | 0.338        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.46         |
|    n_updates            | 3720         |
|    policy_gradient_loss | -0.00306     |
|    value_loss           | 2.21         |
------------------------------------------
Eval num_timesteps=149200, episode_reward=3.10 +/- 2.58
Episode length: 5.40 +/- 3.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5.4      |
|    mean_reward     | 3.1      |
| time/              |          |
|    total_timesteps | 149200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.7     |
|    ep_rew_mean     | 15.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 373      |
|    time_elapsed    | 41732    |
|    total_timesteps | 149200   |
---------------------------------
Eval num_timesteps=149400, episode_reward=20.75 +/- 16.35
Episode length: 30.80 +/- 23.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.8        |
|    mean_reward          | 20.8        |
| time/                   |             |
|    total_timesteps      | 149400      |
| train/                  |             |
|    approx_kl            | 0.002036598 |
|    clip_fraction        | 0.0723      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.464      |
|    explained_variance   | 0.217       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.07        |
|    n_updates            | 3730        |
|    policy_gradient_loss | -0.00418    |
|    value_loss           | 2.69        |
-----------------------------------------
Eval num_timesteps=149600, episode_reward=11.26 +/- 11.73
Episode length: 16.80 +/- 16.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | 11.3     |
| time/              |          |
|    total_timesteps | 149600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.5     |
|    ep_rew_mean     | 15.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 374      |
|    time_elapsed    | 41837    |
|    total_timesteps | 149600   |
---------------------------------
Eval num_timesteps=149800, episode_reward=10.00 +/- 11.84
Episode length: 15.40 +/- 17.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.4          |
|    mean_reward          | 10            |
| time/                   |               |
|    total_timesteps      | 149800        |
| train/                  |               |
|    approx_kl            | 0.00063494314 |
|    clip_fraction        | 0.0165        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.449        |
|    explained_variance   | 0.321         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.65          |
|    n_updates            | 3740          |
|    policy_gradient_loss | -0.00335      |
|    value_loss           | 2.48          |
-------------------------------------------
Eval num_timesteps=150000, episode_reward=9.33 +/- 7.11
Episode length: 14.40 +/- 10.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.4     |
|    mean_reward     | 9.33     |
| time/              |          |
|    total_timesteps | 150000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.1     |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 375      |
|    time_elapsed    | 41930    |
|    total_timesteps | 150000   |
---------------------------------
Eval num_timesteps=150200, episode_reward=10.36 +/- 11.83
Episode length: 15.80 +/- 17.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.8          |
|    mean_reward          | 10.4          |
| time/                   |               |
|    total_timesteps      | 150200        |
| train/                  |               |
|    approx_kl            | 0.00071051315 |
|    clip_fraction        | 0.0118        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.476        |
|    explained_variance   | 0.269         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.49          |
|    n_updates            | 3750          |
|    policy_gradient_loss | -0.00302      |
|    value_loss           | 2.27          |
-------------------------------------------
Eval num_timesteps=150400, episode_reward=16.16 +/- 15.07
Episode length: 24.00 +/- 21.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16.2     |
| time/              |          |
|    total_timesteps | 150400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.8     |
|    ep_rew_mean     | 13.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 376      |
|    time_elapsed    | 42034    |
|    total_timesteps | 150400   |
---------------------------------
Eval num_timesteps=150600, episode_reward=4.37 +/- 3.00
Episode length: 7.20 +/- 4.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 7.2           |
|    mean_reward          | 4.37          |
| time/                   |               |
|    total_timesteps      | 150600        |
| train/                  |               |
|    approx_kl            | 0.00082289893 |
|    clip_fraction        | 0.0415        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.451        |
|    explained_variance   | 0.357         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.95          |
|    n_updates            | 3760          |
|    policy_gradient_loss | -0.00411      |
|    value_loss           | 3.47          |
-------------------------------------------
Eval num_timesteps=150800, episode_reward=17.00 +/- 14.69
Episode length: 25.00 +/- 21.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 150800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | 14.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 377      |
|    time_elapsed    | 42122    |
|    total_timesteps | 150800   |
---------------------------------
Eval num_timesteps=151000, episode_reward=29.99 +/- 11.25
Episode length: 42.40 +/- 15.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.4        |
|    mean_reward          | 30          |
| time/                   |             |
|    total_timesteps      | 151000      |
| train/                  |             |
|    approx_kl            | 0.001104866 |
|    clip_fraction        | 0.0219      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.47       |
|    explained_variance   | 0.216       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0686      |
|    n_updates            | 3770        |
|    policy_gradient_loss | -0.00407    |
|    value_loss           | 1.08        |
-----------------------------------------
Eval num_timesteps=151200, episode_reward=14.90 +/- 12.31
Episode length: 22.20 +/- 17.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 14.9     |
| time/              |          |
|    total_timesteps | 151200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.3     |
|    ep_rew_mean     | 15.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 378      |
|    time_elapsed    | 42235    |
|    total_timesteps | 151200   |
---------------------------------
Eval num_timesteps=151400, episode_reward=30.51 +/- 9.10
Episode length: 43.80 +/- 12.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 43.8         |
|    mean_reward          | 30.5         |
| time/                   |              |
|    total_timesteps      | 151400       |
| train/                  |              |
|    approx_kl            | 0.0007440158 |
|    clip_fraction        | 0.0257       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.467       |
|    explained_variance   | 0.288        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.108        |
|    n_updates            | 3780         |
|    policy_gradient_loss | -0.00263     |
|    value_loss           | 1.15         |
------------------------------------------
Eval num_timesteps=151600, episode_reward=5.39 +/- 2.75
Episode length: 8.60 +/- 3.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8.6      |
|    mean_reward     | 5.39     |
| time/              |          |
|    total_timesteps | 151600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.3     |
|    ep_rew_mean     | 15.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 379      |
|    time_elapsed    | 42328    |
|    total_timesteps | 151600   |
---------------------------------
Eval num_timesteps=151800, episode_reward=11.04 +/- 12.13
Episode length: 16.60 +/- 17.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.6         |
|    mean_reward          | 11           |
| time/                   |              |
|    total_timesteps      | 151800       |
| train/                  |              |
|    approx_kl            | 0.0016359736 |
|    clip_fraction        | 0.0324       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.491       |
|    explained_variance   | 0.228        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.52         |
|    n_updates            | 3790         |
|    policy_gradient_loss | -0.00368     |
|    value_loss           | 2.41         |
------------------------------------------
Eval num_timesteps=152000, episode_reward=22.29 +/- 15.48
Episode length: 32.40 +/- 21.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.4     |
|    mean_reward     | 22.3     |
| time/              |          |
|    total_timesteps | 152000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | 16.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 380      |
|    time_elapsed    | 42457    |
|    total_timesteps | 152000   |
---------------------------------
Eval num_timesteps=152200, episode_reward=4.11 +/- 3.31
Episode length: 6.80 +/- 4.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 6.8          |
|    mean_reward          | 4.11         |
| time/                   |              |
|    total_timesteps      | 152200       |
| train/                  |              |
|    approx_kl            | 0.0010210202 |
|    clip_fraction        | 0.0143       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.505       |
|    explained_variance   | 0.377        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.571        |
|    n_updates            | 3800         |
|    policy_gradient_loss | -0.00349     |
|    value_loss           | 1.71         |
------------------------------------------
Eval num_timesteps=152400, episode_reward=12.95 +/- 11.67
Episode length: 19.20 +/- 16.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.2     |
|    mean_reward     | 12.9     |
| time/              |          |
|    total_timesteps | 152400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.4     |
|    ep_rew_mean     | 16.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 381      |
|    time_elapsed    | 42541    |
|    total_timesteps | 152400   |
---------------------------------
Eval num_timesteps=152600, episode_reward=22.27 +/- 14.96
Episode length: 32.60 +/- 21.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 32.6        |
|    mean_reward          | 22.3        |
| time/                   |             |
|    total_timesteps      | 152600      |
| train/                  |             |
|    approx_kl            | 0.002166804 |
|    clip_fraction        | 0.0732      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.518      |
|    explained_variance   | 0.109       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.15        |
|    n_updates            | 3810        |
|    policy_gradient_loss | -0.0078     |
|    value_loss           | 1.48        |
-----------------------------------------
Eval num_timesteps=152800, episode_reward=16.49 +/- 13.83
Episode length: 25.00 +/- 20.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 152800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | 17.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 382      |
|    time_elapsed    | 42669    |
|    total_timesteps | 152800   |
---------------------------------
Eval num_timesteps=153000, episode_reward=9.94 +/- 12.47
Episode length: 15.00 +/- 17.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15           |
|    mean_reward          | 9.94         |
| time/                   |              |
|    total_timesteps      | 153000       |
| train/                  |              |
|    approx_kl            | 0.0011577241 |
|    clip_fraction        | 0.0326       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.489       |
|    explained_variance   | 0.281        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.01         |
|    n_updates            | 3820         |
|    policy_gradient_loss | -0.00381     |
|    value_loss           | 3            |
------------------------------------------
Eval num_timesteps=153200, episode_reward=11.90 +/- 12.99
Episode length: 17.40 +/- 17.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 11.9     |
| time/              |          |
|    total_timesteps | 153200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27       |
|    ep_rew_mean     | 18.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 383      |
|    time_elapsed    | 42780    |
|    total_timesteps | 153200   |
---------------------------------
Eval num_timesteps=153400, episode_reward=14.33 +/- 11.07
Episode length: 21.80 +/- 15.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 21.8         |
|    mean_reward          | 14.3         |
| time/                   |              |
|    total_timesteps      | 153400       |
| train/                  |              |
|    approx_kl            | 0.0011521785 |
|    clip_fraction        | 0.021        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.47        |
|    explained_variance   | 0.242        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0516       |
|    n_updates            | 3830         |
|    policy_gradient_loss | -0.00535     |
|    value_loss           | 0.769        |
------------------------------------------
Eval num_timesteps=153600, episode_reward=21.45 +/- 16.43
Episode length: 31.20 +/- 23.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.2     |
|    mean_reward     | 21.4     |
| time/              |          |
|    total_timesteps | 153600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.7     |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 384      |
|    time_elapsed    | 42909    |
|    total_timesteps | 153600   |
---------------------------------
Eval num_timesteps=153800, episode_reward=23.33 +/- 14.22
Episode length: 33.80 +/- 19.96
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.8          |
|    mean_reward          | 23.3          |
| time/                   |               |
|    total_timesteps      | 153800        |
| train/                  |               |
|    approx_kl            | 0.00097332377 |
|    clip_fraction        | 0.0319        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.448        |
|    explained_variance   | 0.327         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.84          |
|    n_updates            | 3840          |
|    policy_gradient_loss | -0.0028       |
|    value_loss           | 3.03          |
-------------------------------------------
Eval num_timesteps=154000, episode_reward=23.05 +/- 14.69
Episode length: 33.40 +/- 20.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 23.1     |
| time/              |          |
|    total_timesteps | 154000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.8     |
|    ep_rew_mean     | 17       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 385      |
|    time_elapsed    | 43044    |
|    total_timesteps | 154000   |
---------------------------------
Eval num_timesteps=154200, episode_reward=15.61 +/- 15.87
Episode length: 22.80 +/- 22.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22.8         |
|    mean_reward          | 15.6         |
| time/                   |              |
|    total_timesteps      | 154200       |
| train/                  |              |
|    approx_kl            | 0.0005354062 |
|    clip_fraction        | 0.0161       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.463       |
|    explained_variance   | 0.343        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.118        |
|    n_updates            | 3850         |
|    policy_gradient_loss | -0.00234     |
|    value_loss           | 1.35         |
------------------------------------------
Eval num_timesteps=154400, episode_reward=17.39 +/- 10.88
Episode length: 25.00 +/- 14.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 154400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.3     |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 386      |
|    time_elapsed    | 43148    |
|    total_timesteps | 154400   |
---------------------------------
Eval num_timesteps=154600, episode_reward=10.53 +/- 4.93
Episode length: 15.80 +/- 6.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.8          |
|    mean_reward          | 10.5          |
| time/                   |               |
|    total_timesteps      | 154600        |
| train/                  |               |
|    approx_kl            | 0.00082672015 |
|    clip_fraction        | 0.0138        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.448        |
|    explained_variance   | 0.361         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.05          |
|    n_updates            | 3860          |
|    policy_gradient_loss | -0.0026       |
|    value_loss           | 2.25          |
-------------------------------------------
Eval num_timesteps=154800, episode_reward=17.75 +/- 12.85
Episode length: 27.00 +/- 18.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27       |
|    mean_reward     | 17.7     |
| time/              |          |
|    total_timesteps | 154800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.4     |
|    ep_rew_mean     | 15.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 387      |
|    time_elapsed    | 43254    |
|    total_timesteps | 154800   |
---------------------------------
Eval num_timesteps=155000, episode_reward=9.66 +/- 12.70
Episode length: 14.80 +/- 18.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.8         |
|    mean_reward          | 9.66         |
| time/                   |              |
|    total_timesteps      | 155000       |
| train/                  |              |
|    approx_kl            | 0.0010992418 |
|    clip_fraction        | 0.0906       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.464       |
|    explained_variance   | 0.39         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.959        |
|    n_updates            | 3870         |
|    policy_gradient_loss | -0.0054      |
|    value_loss           | 1.74         |
------------------------------------------
Eval num_timesteps=155200, episode_reward=17.35 +/- 13.55
Episode length: 26.00 +/- 19.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 155200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 388      |
|    time_elapsed    | 43360    |
|    total_timesteps | 155200   |
---------------------------------
Eval num_timesteps=155400, episode_reward=16.37 +/- 16.61
Episode length: 23.00 +/- 22.07
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 23         |
|    mean_reward          | 16.4       |
| time/                   |            |
|    total_timesteps      | 155400     |
| train/                  |            |
|    approx_kl            | 0.00278297 |
|    clip_fraction        | 0.0908     |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.5       |
|    explained_variance   | 0.323      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.594      |
|    n_updates            | 3880       |
|    policy_gradient_loss | -0.00684   |
|    value_loss           | 2.08       |
----------------------------------------
Eval num_timesteps=155600, episode_reward=21.30 +/- 14.80
Episode length: 32.20 +/- 21.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 21.3     |
| time/              |          |
|    total_timesteps | 155600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.7     |
|    ep_rew_mean     | 16.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 389      |
|    time_elapsed    | 43490    |
|    total_timesteps | 155600   |
---------------------------------
Eval num_timesteps=155800, episode_reward=29.41 +/- 12.40
Episode length: 41.60 +/- 16.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.6        |
|    mean_reward          | 29.4        |
| time/                   |             |
|    total_timesteps      | 155800      |
| train/                  |             |
|    approx_kl            | 0.001141158 |
|    clip_fraction        | 0.0406      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.533      |
|    explained_variance   | 0.264       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.581       |
|    n_updates            | 3890        |
|    policy_gradient_loss | -0.00283    |
|    value_loss           | 1.6         |
-----------------------------------------
Eval num_timesteps=156000, episode_reward=9.34 +/- 12.67
Episode length: 14.20 +/- 18.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.2     |
|    mean_reward     | 9.34     |
| time/              |          |
|    total_timesteps | 156000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | 17.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 390      |
|    time_elapsed    | 43596    |
|    total_timesteps | 156000   |
---------------------------------
Eval num_timesteps=156200, episode_reward=3.63 +/- 2.77
Episode length: 6.00 +/- 3.85
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 6             |
|    mean_reward          | 3.63          |
| time/                   |               |
|    total_timesteps      | 156200        |
| train/                  |               |
|    approx_kl            | 0.00086173054 |
|    clip_fraction        | 0.0663        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.529        |
|    explained_variance   | 0.423         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.308         |
|    n_updates            | 3900          |
|    policy_gradient_loss | -0.00547      |
|    value_loss           | 1.38          |
-------------------------------------------
Eval num_timesteps=156400, episode_reward=10.45 +/- 12.44
Episode length: 15.60 +/- 17.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 10.4     |
| time/              |          |
|    total_timesteps | 156400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | 15.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 391      |
|    time_elapsed    | 43687    |
|    total_timesteps | 156400   |
---------------------------------
Eval num_timesteps=156600, episode_reward=4.46 +/- 2.21
Episode length: 7.40 +/- 3.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 7.4          |
|    mean_reward          | 4.46         |
| time/                   |              |
|    total_timesteps      | 156600       |
| train/                  |              |
|    approx_kl            | 0.0011122298 |
|    clip_fraction        | 0.0281       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.497       |
|    explained_variance   | 0.276        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.71         |
|    n_updates            | 3910         |
|    policy_gradient_loss | -0.00372     |
|    value_loss           | 3.04         |
------------------------------------------
Eval num_timesteps=156800, episode_reward=13.77 +/- 12.17
Episode length: 19.60 +/- 16.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 13.8     |
| time/              |          |
|    total_timesteps | 156800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.4     |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 392      |
|    time_elapsed    | 43779    |
|    total_timesteps | 156800   |
---------------------------------
Eval num_timesteps=157000, episode_reward=28.16 +/- 13.78
Episode length: 40.40 +/- 19.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 40.4        |
|    mean_reward          | 28.2        |
| time/                   |             |
|    total_timesteps      | 157000      |
| train/                  |             |
|    approx_kl            | 0.001906379 |
|    clip_fraction        | 0.0728      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.536      |
|    explained_variance   | -0.0791     |
|    learning_rate        | 0.0001      |
|    loss                 | 2.14        |
|    n_updates            | 3920        |
|    policy_gradient_loss | -0.00488    |
|    value_loss           | 2.33        |
-----------------------------------------
Eval num_timesteps=157200, episode_reward=15.85 +/- 15.37
Episode length: 23.40 +/- 21.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 15.8     |
| time/              |          |
|    total_timesteps | 157200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 393      |
|    time_elapsed    | 43882    |
|    total_timesteps | 157200   |
---------------------------------
Eval num_timesteps=157400, episode_reward=26.51 +/- 12.55
Episode length: 37.40 +/- 16.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 37.4         |
|    mean_reward          | 26.5         |
| time/                   |              |
|    total_timesteps      | 157400       |
| train/                  |              |
|    approx_kl            | 0.0015092809 |
|    clip_fraction        | 0.0531       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.521       |
|    explained_variance   | 0.352        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.734        |
|    n_updates            | 3930         |
|    policy_gradient_loss | -0.00613     |
|    value_loss           | 2.19         |
------------------------------------------
Eval num_timesteps=157600, episode_reward=11.83 +/- 11.17
Episode length: 18.20 +/- 16.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | 11.8     |
| time/              |          |
|    total_timesteps | 157600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 14.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 394      |
|    time_elapsed    | 44011    |
|    total_timesteps | 157600   |
---------------------------------
Eval num_timesteps=157800, episode_reward=12.11 +/- 12.81
Episode length: 17.40 +/- 17.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.4         |
|    mean_reward          | 12.1         |
| time/                   |              |
|    total_timesteps      | 157800       |
| train/                  |              |
|    approx_kl            | 0.0011751567 |
|    clip_fraction        | 0.0248       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.516       |
|    explained_variance   | 0.382        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.89         |
|    n_updates            | 3940         |
|    policy_gradient_loss | -0.00354     |
|    value_loss           | 2.7          |
------------------------------------------
Eval num_timesteps=158000, episode_reward=15.90 +/- 14.76
Episode length: 24.00 +/- 21.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 15.9     |
| time/              |          |
|    total_timesteps | 158000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.4     |
|    ep_rew_mean     | 13.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 395      |
|    time_elapsed    | 44116    |
|    total_timesteps | 158000   |
---------------------------------
Eval num_timesteps=158200, episode_reward=16.81 +/- 14.44
Episode length: 25.00 +/- 20.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25           |
|    mean_reward          | 16.8         |
| time/                   |              |
|    total_timesteps      | 158200       |
| train/                  |              |
|    approx_kl            | 0.0010956199 |
|    clip_fraction        | 0.0614       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.556       |
|    explained_variance   | 0.267        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.61         |
|    n_updates            | 3950         |
|    policy_gradient_loss | -0.00576     |
|    value_loss           | 2.74         |
------------------------------------------
Eval num_timesteps=158400, episode_reward=11.96 +/- 12.22
Episode length: 17.80 +/- 17.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | 12       |
| time/              |          |
|    total_timesteps | 158400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21       |
|    ep_rew_mean     | 14.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 396      |
|    time_elapsed    | 44221    |
|    total_timesteps | 158400   |
---------------------------------
Eval num_timesteps=158600, episode_reward=16.53 +/- 14.81
Episode length: 24.60 +/- 20.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.6         |
|    mean_reward          | 16.5         |
| time/                   |              |
|    total_timesteps      | 158600       |
| train/                  |              |
|    approx_kl            | 0.0017312596 |
|    clip_fraction        | 0.0487       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.555       |
|    explained_variance   | 0.288        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.57         |
|    n_updates            | 3960         |
|    policy_gradient_loss | -0.00218     |
|    value_loss           | 2.39         |
------------------------------------------
Eval num_timesteps=158800, episode_reward=11.09 +/- 12.85
Episode length: 16.00 +/- 17.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | 11.1     |
| time/              |          |
|    total_timesteps | 158800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20       |
|    ep_rew_mean     | 13.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 397      |
|    time_elapsed    | 44324    |
|    total_timesteps | 158800   |
---------------------------------
Eval num_timesteps=159000, episode_reward=19.34 +/- 13.85
Episode length: 27.60 +/- 18.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 27.6         |
|    mean_reward          | 19.3         |
| time/                   |              |
|    total_timesteps      | 159000       |
| train/                  |              |
|    approx_kl            | 0.0017116053 |
|    clip_fraction        | 0.0538       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.556       |
|    explained_variance   | 0.358        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.483        |
|    n_updates            | 3970         |
|    policy_gradient_loss | -0.00422     |
|    value_loss           | 1.74         |
------------------------------------------
Eval num_timesteps=159200, episode_reward=22.72 +/- 15.34
Episode length: 32.60 +/- 21.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.6     |
|    mean_reward     | 22.7     |
| time/              |          |
|    total_timesteps | 159200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.2     |
|    ep_rew_mean     | 14.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 398      |
|    time_elapsed    | 44439    |
|    total_timesteps | 159200   |
---------------------------------
Eval num_timesteps=159400, episode_reward=27.52 +/- 13.37
Episode length: 40.40 +/- 19.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 40.4         |
|    mean_reward          | 27.5         |
| time/                   |              |
|    total_timesteps      | 159400       |
| train/                  |              |
|    approx_kl            | 0.0008004168 |
|    clip_fraction        | 0.0362       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.534       |
|    explained_variance   | 0.173        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.624        |
|    n_updates            | 3980         |
|    policy_gradient_loss | -0.00299     |
|    value_loss           | 2.16         |
------------------------------------------
Eval num_timesteps=159600, episode_reward=17.69 +/- 15.10
Episode length: 25.20 +/- 20.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 17.7     |
| time/              |          |
|    total_timesteps | 159600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.1     |
|    ep_rew_mean     | 14.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 399      |
|    time_elapsed    | 44570    |
|    total_timesteps | 159600   |
---------------------------------
Eval num_timesteps=159800, episode_reward=14.77 +/- 16.12
Episode length: 22.00 +/- 22.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22           |
|    mean_reward          | 14.8         |
| time/                   |              |
|    total_timesteps      | 159800       |
| train/                  |              |
|    approx_kl            | 0.0011568016 |
|    clip_fraction        | 0.0404       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.554       |
|    explained_variance   | 0.283        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.605        |
|    n_updates            | 3990         |
|    policy_gradient_loss | -0.00263     |
|    value_loss           | 2.28         |
------------------------------------------
Eval num_timesteps=160000, episode_reward=11.45 +/- 11.89
Episode length: 17.20 +/- 16.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | 11.4     |
| time/              |          |
|    total_timesteps | 160000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.7     |
|    ep_rew_mean     | 16.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 400      |
|    time_elapsed    | 44699    |
|    total_timesteps | 160000   |
---------------------------------
Eval num_timesteps=160200, episode_reward=16.55 +/- 15.15
Episode length: 24.20 +/- 21.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 24.2        |
|    mean_reward          | 16.6        |
| time/                   |             |
|    total_timesteps      | 160200      |
| train/                  |             |
|    approx_kl            | 0.001609047 |
|    clip_fraction        | 0.0467      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.525      |
|    explained_variance   | 0.0721      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.575       |
|    n_updates            | 4000        |
|    policy_gradient_loss | -0.00384    |
|    value_loss           | 1.56        |
-----------------------------------------
Eval num_timesteps=160400, episode_reward=17.18 +/- 15.07
Episode length: 24.60 +/- 20.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 17.2     |
| time/              |          |
|    total_timesteps | 160400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.6     |
|    ep_rew_mean     | 16.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 401      |
|    time_elapsed    | 44804    |
|    total_timesteps | 160400   |
---------------------------------
Eval num_timesteps=160600, episode_reward=12.53 +/- 12.29
Episode length: 18.40 +/- 16.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.4        |
|    mean_reward          | 12.5        |
| time/                   |             |
|    total_timesteps      | 160600      |
| train/                  |             |
|    approx_kl            | 0.001084916 |
|    clip_fraction        | 0.0565      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.509      |
|    explained_variance   | 0.192       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.156       |
|    n_updates            | 4010        |
|    policy_gradient_loss | -0.00382    |
|    value_loss           | 1.13        |
-----------------------------------------
Eval num_timesteps=160800, episode_reward=28.52 +/- 11.93
Episode length: 41.60 +/- 16.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 28.5     |
| time/              |          |
|    total_timesteps | 160800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.4     |
|    ep_rew_mean     | 18.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 402      |
|    time_elapsed    | 44915    |
|    total_timesteps | 160800   |
---------------------------------
Eval num_timesteps=161000, episode_reward=18.14 +/- 15.00
Episode length: 25.80 +/- 20.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.8         |
|    mean_reward          | 18.1         |
| time/                   |              |
|    total_timesteps      | 161000       |
| train/                  |              |
|    approx_kl            | 0.0010960103 |
|    clip_fraction        | 0.0257       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.537       |
|    explained_variance   | 0.111        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.663        |
|    n_updates            | 4020         |
|    policy_gradient_loss | -0.00401     |
|    value_loss           | 2.14         |
------------------------------------------
Eval num_timesteps=161200, episode_reward=17.73 +/- 14.60
Episode length: 25.40 +/- 20.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | 17.7     |
| time/              |          |
|    total_timesteps | 161200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.6     |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 403      |
|    time_elapsed    | 45029    |
|    total_timesteps | 161200   |
---------------------------------
Eval num_timesteps=161400, episode_reward=23.78 +/- 15.37
Episode length: 33.20 +/- 20.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.2         |
|    mean_reward          | 23.8         |
| time/                   |              |
|    total_timesteps      | 161400       |
| train/                  |              |
|    approx_kl            | 0.0010635142 |
|    clip_fraction        | 0.0344       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.486       |
|    explained_variance   | 0.384        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.49         |
|    n_updates            | 4030         |
|    policy_gradient_loss | -0.00423     |
|    value_loss           | 2.75         |
------------------------------------------
Eval num_timesteps=161600, episode_reward=28.41 +/- 12.72
Episode length: 41.00 +/- 18.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41       |
|    mean_reward     | 28.4     |
| time/              |          |
|    total_timesteps | 161600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 404      |
|    time_elapsed    | 45138    |
|    total_timesteps | 161600   |
---------------------------------
Eval num_timesteps=161800, episode_reward=11.63 +/- 12.62
Episode length: 17.00 +/- 17.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17            |
|    mean_reward          | 11.6          |
| time/                   |               |
|    total_timesteps      | 161800        |
| train/                  |               |
|    approx_kl            | 0.00069142104 |
|    clip_fraction        | 0.0134        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.528        |
|    explained_variance   | 0.335         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.06          |
|    n_updates            | 4040          |
|    policy_gradient_loss | -0.00304      |
|    value_loss           | 3.13          |
-------------------------------------------
Eval num_timesteps=162000, episode_reward=29.27 +/- 12.64
Episode length: 41.40 +/- 17.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | 29.3     |
| time/              |          |
|    total_timesteps | 162000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.8     |
|    ep_rew_mean     | 17       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 405      |
|    time_elapsed    | 45243    |
|    total_timesteps | 162000   |
---------------------------------
Eval num_timesteps=162200, episode_reward=24.56 +/- 14.02
Episode length: 34.80 +/- 18.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.8         |
|    mean_reward          | 24.6         |
| time/                   |              |
|    total_timesteps      | 162200       |
| train/                  |              |
|    approx_kl            | 0.0018008229 |
|    clip_fraction        | 0.0321       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.536       |
|    explained_variance   | 0.246        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.237        |
|    n_updates            | 4050         |
|    policy_gradient_loss | -0.00357     |
|    value_loss           | 1.55         |
------------------------------------------
Eval num_timesteps=162400, episode_reward=15.76 +/- 14.92
Episode length: 23.60 +/- 21.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 15.8     |
| time/              |          |
|    total_timesteps | 162400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25       |
|    ep_rew_mean     | 17.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 406      |
|    time_elapsed    | 45371    |
|    total_timesteps | 162400   |
---------------------------------
Eval num_timesteps=162600, episode_reward=11.88 +/- 11.93
Episode length: 17.40 +/- 16.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.4        |
|    mean_reward          | 11.9        |
| time/                   |             |
|    total_timesteps      | 162600      |
| train/                  |             |
|    approx_kl            | 0.000857953 |
|    clip_fraction        | 0.0109      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.522      |
|    explained_variance   | 0.475       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.159       |
|    n_updates            | 4060        |
|    policy_gradient_loss | -0.00274    |
|    value_loss           | 1.02        |
-----------------------------------------
Eval num_timesteps=162800, episode_reward=27.37 +/- 12.55
Episode length: 40.80 +/- 18.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.8     |
|    mean_reward     | 27.4     |
| time/              |          |
|    total_timesteps | 162800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.3     |
|    ep_rew_mean     | 15.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 407      |
|    time_elapsed    | 45499    |
|    total_timesteps | 162800   |
---------------------------------
Eval num_timesteps=163000, episode_reward=9.23 +/- 12.85
Episode length: 14.20 +/- 18.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.2         |
|    mean_reward          | 9.23         |
| time/                   |              |
|    total_timesteps      | 163000       |
| train/                  |              |
|    approx_kl            | 0.0035976707 |
|    clip_fraction        | 0.185        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.583       |
|    explained_variance   | 0.253        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.14         |
|    n_updates            | 4070         |
|    policy_gradient_loss | -0.00645     |
|    value_loss           | 2.29         |
------------------------------------------
Eval num_timesteps=163200, episode_reward=19.71 +/- 12.65
Episode length: 29.00 +/- 18.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29       |
|    mean_reward     | 19.7     |
| time/              |          |
|    total_timesteps | 163200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.6     |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 408      |
|    time_elapsed    | 45613    |
|    total_timesteps | 163200   |
---------------------------------
Eval num_timesteps=163400, episode_reward=18.94 +/- 13.99
Episode length: 27.20 +/- 19.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 27.2         |
|    mean_reward          | 18.9         |
| time/                   |              |
|    total_timesteps      | 163400       |
| train/                  |              |
|    approx_kl            | 0.0039717075 |
|    clip_fraction        | 0.0538       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.575       |
|    explained_variance   | -0.154       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.264        |
|    n_updates            | 4080         |
|    policy_gradient_loss | -0.00163     |
|    value_loss           | 1.27         |
------------------------------------------
Eval num_timesteps=163600, episode_reward=24.57 +/- 13.32
Episode length: 35.80 +/- 18.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 24.6     |
| time/              |          |
|    total_timesteps | 163600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.8     |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 409      |
|    time_elapsed    | 45724    |
|    total_timesteps | 163600   |
---------------------------------
Eval num_timesteps=163800, episode_reward=17.77 +/- 14.94
Episode length: 25.60 +/- 20.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 25.6        |
|    mean_reward          | 17.8        |
| time/                   |             |
|    total_timesteps      | 163800      |
| train/                  |             |
|    approx_kl            | 0.004063426 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.618      |
|    explained_variance   | 0.183       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.431       |
|    n_updates            | 4090        |
|    policy_gradient_loss | -0.00853    |
|    value_loss           | 1.64        |
-----------------------------------------
Eval num_timesteps=164000, episode_reward=9.32 +/- 13.16
Episode length: 13.80 +/- 18.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.8     |
|    mean_reward     | 9.32     |
| time/              |          |
|    total_timesteps | 164000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.4     |
|    ep_rew_mean     | 18.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 410      |
|    time_elapsed    | 45832    |
|    total_timesteps | 164000   |
---------------------------------
Eval num_timesteps=164200, episode_reward=10.63 +/- 12.59
Episode length: 15.80 +/- 17.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | 10.6         |
| time/                   |              |
|    total_timesteps      | 164200       |
| train/                  |              |
|    approx_kl            | 0.0012395779 |
|    clip_fraction        | 0.0295       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.63        |
|    explained_variance   | 0.249        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.721        |
|    n_updates            | 4100         |
|    policy_gradient_loss | -0.00315     |
|    value_loss           | 2.06         |
------------------------------------------
Eval num_timesteps=164400, episode_reward=28.12 +/- 12.73
Episode length: 41.00 +/- 18.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41       |
|    mean_reward     | 28.1     |
| time/              |          |
|    total_timesteps | 164400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.3     |
|    ep_rew_mean     | 18.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 411      |
|    time_elapsed    | 45936    |
|    total_timesteps | 164400   |
---------------------------------
Eval num_timesteps=164600, episode_reward=9.17 +/- 13.36
Episode length: 13.60 +/- 18.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 13.6         |
|    mean_reward          | 9.17         |
| time/                   |              |
|    total_timesteps      | 164600       |
| train/                  |              |
|    approx_kl            | 0.0023029016 |
|    clip_fraction        | 0.0556       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.611       |
|    explained_variance   | 0.35         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.651        |
|    n_updates            | 4110         |
|    policy_gradient_loss | -0.00549     |
|    value_loss           | 2.38         |
------------------------------------------
Eval num_timesteps=164800, episode_reward=15.65 +/- 15.97
Episode length: 23.00 +/- 22.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 15.7     |
| time/              |          |
|    total_timesteps | 164800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.5     |
|    ep_rew_mean     | 18.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 412      |
|    time_elapsed    | 46042    |
|    total_timesteps | 164800   |
---------------------------------
Eval num_timesteps=165000, episode_reward=18.54 +/- 13.06
Episode length: 27.20 +/- 18.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 27.2          |
|    mean_reward          | 18.5          |
| time/                   |               |
|    total_timesteps      | 165000        |
| train/                  |               |
|    approx_kl            | 0.00086982443 |
|    clip_fraction        | 0.0375        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.574        |
|    explained_variance   | 0.236         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.03          |
|    n_updates            | 4120          |
|    policy_gradient_loss | -0.00511      |
|    value_loss           | 2.45          |
-------------------------------------------
Eval num_timesteps=165200, episode_reward=2.28 +/- 0.81
Episode length: 4.20 +/- 1.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.2      |
|    mean_reward     | 2.28     |
| time/              |          |
|    total_timesteps | 165200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.3     |
|    ep_rew_mean     | 18.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 413      |
|    time_elapsed    | 46128    |
|    total_timesteps | 165200   |
---------------------------------
Eval num_timesteps=165400, episode_reward=5.85 +/- 2.45
Episode length: 9.40 +/- 3.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 9.4          |
|    mean_reward          | 5.85         |
| time/                   |              |
|    total_timesteps      | 165400       |
| train/                  |              |
|    approx_kl            | 0.0022931185 |
|    clip_fraction        | 0.079        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.602       |
|    explained_variance   | 0.308        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.486        |
|    n_updates            | 4130         |
|    policy_gradient_loss | -0.0071      |
|    value_loss           | 1.06         |
------------------------------------------
Eval num_timesteps=165600, episode_reward=17.66 +/- 14.82
Episode length: 25.20 +/- 20.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 17.7     |
| time/              |          |
|    total_timesteps | 165600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.7     |
|    ep_rew_mean     | 17       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 414      |
|    time_elapsed    | 46220    |
|    total_timesteps | 165600   |
---------------------------------
Eval num_timesteps=165800, episode_reward=5.48 +/- 5.60
Episode length: 8.60 +/- 7.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 8.6          |
|    mean_reward          | 5.48         |
| time/                   |              |
|    total_timesteps      | 165800       |
| train/                  |              |
|    approx_kl            | 0.0014548766 |
|    clip_fraction        | 0.0429       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.63        |
|    explained_variance   | 0.323        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.407        |
|    n_updates            | 4140         |
|    policy_gradient_loss | -0.00421     |
|    value_loss           | 2.94         |
------------------------------------------
Eval num_timesteps=166000, episode_reward=29.25 +/- 13.29
Episode length: 41.00 +/- 18.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41       |
|    mean_reward     | 29.2     |
| time/              |          |
|    total_timesteps | 166000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.4     |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 415      |
|    time_elapsed    | 46313    |
|    total_timesteps | 166000   |
---------------------------------
Eval num_timesteps=166200, episode_reward=21.20 +/- 14.96
Episode length: 32.00 +/- 22.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32           |
|    mean_reward          | 21.2         |
| time/                   |              |
|    total_timesteps      | 166200       |
| train/                  |              |
|    approx_kl            | 0.0017660893 |
|    clip_fraction        | 0.0578       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.614       |
|    explained_variance   | 0.397        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.945        |
|    n_updates            | 4150         |
|    policy_gradient_loss | -0.0069      |
|    value_loss           | 2.32         |
------------------------------------------
Eval num_timesteps=166400, episode_reward=35.14 +/- 0.89
Episode length: 50.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 35.1     |
| time/              |          |
|    total_timesteps | 166400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.5     |
|    ep_rew_mean     | 14.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 416      |
|    time_elapsed    | 46442    |
|    total_timesteps | 166400   |
---------------------------------
Eval num_timesteps=166600, episode_reward=19.02 +/- 15.19
Episode length: 26.80 +/- 20.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.8         |
|    mean_reward          | 19           |
| time/                   |              |
|    total_timesteps      | 166600       |
| train/                  |              |
|    approx_kl            | 0.0026144765 |
|    clip_fraction        | 0.0426       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.617       |
|    explained_variance   | 0.377        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.741        |
|    n_updates            | 4160         |
|    policy_gradient_loss | -0.00481     |
|    value_loss           | 1.73         |
------------------------------------------
Eval num_timesteps=166800, episode_reward=15.25 +/- 11.97
Episode length: 22.40 +/- 17.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 15.3     |
| time/              |          |
|    total_timesteps | 166800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.3     |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 417      |
|    time_elapsed    | 46546    |
|    total_timesteps | 166800   |
---------------------------------
Eval num_timesteps=167000, episode_reward=18.09 +/- 14.98
Episode length: 26.20 +/- 20.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.2         |
|    mean_reward          | 18.1         |
| time/                   |              |
|    total_timesteps      | 167000       |
| train/                  |              |
|    approx_kl            | 0.0021185041 |
|    clip_fraction        | 0.0545       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.615       |
|    explained_variance   | 0.498        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.953        |
|    n_updates            | 4170         |
|    policy_gradient_loss | -0.00274     |
|    value_loss           | 1.89         |
------------------------------------------
Eval num_timesteps=167200, episode_reward=3.95 +/- 0.40
Episode length: 6.60 +/- 0.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6.6      |
|    mean_reward     | 3.95     |
| time/              |          |
|    total_timesteps | 167200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.9     |
|    ep_rew_mean     | 14.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 418      |
|    time_elapsed    | 46641    |
|    total_timesteps | 167200   |
---------------------------------
Eval num_timesteps=167400, episode_reward=2.40 +/- 1.55
Episode length: 4.40 +/- 2.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 4.4           |
|    mean_reward          | 2.4           |
| time/                   |               |
|    total_timesteps      | 167400        |
| train/                  |               |
|    approx_kl            | 0.00088080805 |
|    clip_fraction        | 0.0344        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.643        |
|    explained_variance   | 0.518         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.825         |
|    n_updates            | 4180          |
|    policy_gradient_loss | -0.00546      |
|    value_loss           | 1.85          |
-------------------------------------------
Eval num_timesteps=167600, episode_reward=11.29 +/- 11.21
Episode length: 17.40 +/- 16.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 11.3     |
| time/              |          |
|    total_timesteps | 167600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.4     |
|    ep_rew_mean     | 14.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 419      |
|    time_elapsed    | 46725    |
|    total_timesteps | 167600   |
---------------------------------
Eval num_timesteps=167800, episode_reward=15.05 +/- 11.70
Episode length: 21.60 +/- 15.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 21.6         |
|    mean_reward          | 15.1         |
| time/                   |              |
|    total_timesteps      | 167800       |
| train/                  |              |
|    approx_kl            | 0.0014005381 |
|    clip_fraction        | 0.0616       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.64        |
|    explained_variance   | 0.454        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.585        |
|    n_updates            | 4190         |
|    policy_gradient_loss | -0.00735     |
|    value_loss           | 1.8          |
------------------------------------------
Eval num_timesteps=168000, episode_reward=6.76 +/- 2.52
Episode length: 10.60 +/- 3.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10.6     |
|    mean_reward     | 6.76     |
| time/              |          |
|    total_timesteps | 168000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.6     |
|    ep_rew_mean     | 14.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 420      |
|    time_elapsed    | 46827    |
|    total_timesteps | 168000   |
---------------------------------
Eval num_timesteps=168200, episode_reward=11.38 +/- 12.86
Episode length: 16.60 +/- 17.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.6         |
|    mean_reward          | 11.4         |
| time/                   |              |
|    total_timesteps      | 168200       |
| train/                  |              |
|    approx_kl            | 0.0016585466 |
|    clip_fraction        | 0.0563       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.64        |
|    explained_variance   | 0.433        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.823        |
|    n_updates            | 4200         |
|    policy_gradient_loss | -0.00621     |
|    value_loss           | 1.99         |
------------------------------------------
Eval num_timesteps=168400, episode_reward=17.64 +/- 14.61
Episode length: 26.00 +/- 20.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 168400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.7     |
|    ep_rew_mean     | 15.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 421      |
|    time_elapsed    | 46933    |
|    total_timesteps | 168400   |
---------------------------------
Eval num_timesteps=168600, episode_reward=25.05 +/- 12.58
Episode length: 36.00 +/- 17.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36           |
|    mean_reward          | 25           |
| time/                   |              |
|    total_timesteps      | 168600       |
| train/                  |              |
|    approx_kl            | 0.0015312675 |
|    clip_fraction        | 0.0261       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.668       |
|    explained_variance   | 0.225        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.473        |
|    n_updates            | 4210         |
|    policy_gradient_loss | -0.0026      |
|    value_loss           | 1.68         |
------------------------------------------
Eval num_timesteps=168800, episode_reward=9.51 +/- 12.07
Episode length: 14.60 +/- 17.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 9.51     |
| time/              |          |
|    total_timesteps | 168800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.9     |
|    ep_rew_mean     | 15.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 422      |
|    time_elapsed    | 47047    |
|    total_timesteps | 168800   |
---------------------------------
Eval num_timesteps=169000, episode_reward=8.10 +/- 5.90
Episode length: 12.60 +/- 8.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 12.6         |
|    mean_reward          | 8.1          |
| time/                   |              |
|    total_timesteps      | 169000       |
| train/                  |              |
|    approx_kl            | 0.0031025861 |
|    clip_fraction        | 0.133        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.674       |
|    explained_variance   | 0.369        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.386        |
|    n_updates            | 4220         |
|    policy_gradient_loss | -0.0079      |
|    value_loss           | 1.96         |
------------------------------------------
Eval num_timesteps=169200, episode_reward=22.64 +/- 15.95
Episode length: 32.20 +/- 21.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 22.6     |
| time/              |          |
|    total_timesteps | 169200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.1     |
|    ep_rew_mean     | 15.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 423      |
|    time_elapsed    | 47137    |
|    total_timesteps | 169200   |
---------------------------------
Eval num_timesteps=169400, episode_reward=28.84 +/- 14.04
Episode length: 40.40 +/- 19.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 40.4         |
|    mean_reward          | 28.8         |
| time/                   |              |
|    total_timesteps      | 169400       |
| train/                  |              |
|    approx_kl            | 0.0024841768 |
|    clip_fraction        | 0.0873       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.663       |
|    explained_variance   | 0.228        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.51         |
|    n_updates            | 4230         |
|    policy_gradient_loss | -0.00556     |
|    value_loss           | 2.29         |
------------------------------------------
Eval num_timesteps=169600, episode_reward=11.77 +/- 13.12
Episode length: 17.20 +/- 17.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | 11.8     |
| time/              |          |
|    total_timesteps | 169600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23       |
|    ep_rew_mean     | 15.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 424      |
|    time_elapsed    | 47265    |
|    total_timesteps | 169600   |
---------------------------------
Eval num_timesteps=169800, episode_reward=18.98 +/- 13.69
Episode length: 27.40 +/- 19.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 27.4         |
|    mean_reward          | 19           |
| time/                   |              |
|    total_timesteps      | 169800       |
| train/                  |              |
|    approx_kl            | 0.0021183193 |
|    clip_fraction        | 0.0658       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.658       |
|    explained_variance   | 0.379        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.86         |
|    n_updates            | 4240         |
|    policy_gradient_loss | -0.00417     |
|    value_loss           | 2.62         |
------------------------------------------
Eval num_timesteps=170000, episode_reward=23.41 +/- 16.37
Episode length: 32.40 +/- 21.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.4     |
|    mean_reward     | 23.4     |
| time/              |          |
|    total_timesteps | 170000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 425      |
|    time_elapsed    | 47377    |
|    total_timesteps | 170000   |
---------------------------------
Eval num_timesteps=170200, episode_reward=6.96 +/- 5.02
Episode length: 11.00 +/- 6.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 11           |
|    mean_reward          | 6.96         |
| time/                   |              |
|    total_timesteps      | 170200       |
| train/                  |              |
|    approx_kl            | 0.0013238607 |
|    clip_fraction        | 0.0408       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.679       |
|    explained_variance   | 0.402        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.15         |
|    n_updates            | 4250         |
|    policy_gradient_loss | -0.00539     |
|    value_loss           | 1.94         |
------------------------------------------
Eval num_timesteps=170400, episode_reward=14.79 +/- 16.10
Episode length: 22.00 +/- 22.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22       |
|    mean_reward     | 14.8     |
| time/              |          |
|    total_timesteps | 170400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 426      |
|    time_elapsed    | 47466    |
|    total_timesteps | 170400   |
---------------------------------
Eval num_timesteps=170600, episode_reward=23.31 +/- 13.29
Episode length: 34.40 +/- 19.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.4        |
|    mean_reward          | 23.3        |
| time/                   |             |
|    total_timesteps      | 170600      |
| train/                  |             |
|    approx_kl            | 0.002412992 |
|    clip_fraction        | 0.06        |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.693      |
|    explained_variance   | 0.299       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.66        |
|    n_updates            | 4260        |
|    policy_gradient_loss | -0.00658    |
|    value_loss           | 2.64        |
-----------------------------------------
Eval num_timesteps=170800, episode_reward=15.69 +/- 15.90
Episode length: 23.00 +/- 22.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 15.7     |
| time/              |          |
|    total_timesteps | 170800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.5     |
|    ep_rew_mean     | 14.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 427      |
|    time_elapsed    | 47576    |
|    total_timesteps | 170800   |
---------------------------------
Eval num_timesteps=171000, episode_reward=7.97 +/- 7.00
Episode length: 12.80 +/- 10.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 12.8         |
|    mean_reward          | 7.97         |
| time/                   |              |
|    total_timesteps      | 171000       |
| train/                  |              |
|    approx_kl            | 0.0018374914 |
|    clip_fraction        | 0.0502       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.691       |
|    explained_variance   | 0.186        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.923        |
|    n_updates            | 4270         |
|    policy_gradient_loss | -0.00549     |
|    value_loss           | 2.39         |
------------------------------------------
Eval num_timesteps=171200, episode_reward=27.29 +/- 13.28
Episode length: 40.40 +/- 19.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.4     |
|    mean_reward     | 27.3     |
| time/              |          |
|    total_timesteps | 171200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.5     |
|    ep_rew_mean     | 14       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 428      |
|    time_elapsed    | 47670    |
|    total_timesteps | 171200   |
---------------------------------
Eval num_timesteps=171400, episode_reward=8.29 +/- 5.34
Episode length: 12.80 +/- 7.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 12.8         |
|    mean_reward          | 8.29         |
| time/                   |              |
|    total_timesteps      | 171400       |
| train/                  |              |
|    approx_kl            | 0.0030254335 |
|    clip_fraction        | 0.0645       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.674       |
|    explained_variance   | 0.308        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.68         |
|    n_updates            | 4280         |
|    policy_gradient_loss | -0.00699     |
|    value_loss           | 2.57         |
------------------------------------------
Eval num_timesteps=171600, episode_reward=9.91 +/- 12.30
Episode length: 15.00 +/- 17.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | 9.91     |
| time/              |          |
|    total_timesteps | 171600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.3     |
|    ep_rew_mean     | 15.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 429      |
|    time_elapsed    | 47763    |
|    total_timesteps | 171600   |
---------------------------------
Eval num_timesteps=171800, episode_reward=6.36 +/- 3.42
Episode length: 10.00 +/- 5.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 10           |
|    mean_reward          | 6.36         |
| time/                   |              |
|    total_timesteps      | 171800       |
| train/                  |              |
|    approx_kl            | 0.0017960124 |
|    clip_fraction        | 0.0549       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.693       |
|    explained_variance   | 0.00572      |
|    learning_rate        | 0.0001       |
|    loss                 | 1.66         |
|    n_updates            | 4290         |
|    policy_gradient_loss | -0.00314     |
|    value_loss           | 1.87         |
------------------------------------------
Eval num_timesteps=172000, episode_reward=18.54 +/- 14.11
Episode length: 26.60 +/- 19.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.6     |
|    mean_reward     | 18.5     |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.4     |
|    ep_rew_mean     | 16.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 430      |
|    time_elapsed    | 47857    |
|    total_timesteps | 172000   |
---------------------------------
Eval num_timesteps=172200, episode_reward=5.85 +/- 2.49
Episode length: 9.40 +/- 3.93
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 9.4        |
|    mean_reward          | 5.85       |
| time/                   |            |
|    total_timesteps      | 172200     |
| train/                  |            |
|    approx_kl            | 0.00458115 |
|    clip_fraction        | 0.126      |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.685     |
|    explained_variance   | 0.274      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.456      |
|    n_updates            | 4300       |
|    policy_gradient_loss | -0.00631   |
|    value_loss           | 1.38       |
----------------------------------------
Eval num_timesteps=172400, episode_reward=5.66 +/- 3.77
Episode length: 9.00 +/- 5.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 5.66     |
| time/              |          |
|    total_timesteps | 172400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | 17.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 431      |
|    time_elapsed    | 47932    |
|    total_timesteps | 172400   |
---------------------------------
Eval num_timesteps=172600, episode_reward=15.81 +/- 16.18
Episode length: 22.80 +/- 22.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 22.8        |
|    mean_reward          | 15.8        |
| time/                   |             |
|    total_timesteps      | 172600      |
| train/                  |             |
|    approx_kl            | 0.003019206 |
|    clip_fraction        | 0.0786      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.678      |
|    explained_variance   | 0.357       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.275       |
|    n_updates            | 4310        |
|    policy_gradient_loss | -0.00513    |
|    value_loss           | 1.47        |
-----------------------------------------
Eval num_timesteps=172800, episode_reward=19.63 +/- 12.83
Episode length: 29.20 +/- 18.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.2     |
|    mean_reward     | 19.6     |
| time/              |          |
|    total_timesteps | 172800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.8     |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 432      |
|    time_elapsed    | 48052    |
|    total_timesteps | 172800   |
---------------------------------
Eval num_timesteps=173000, episode_reward=12.31 +/- 11.77
Episode length: 18.00 +/- 16.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18           |
|    mean_reward          | 12.3         |
| time/                   |              |
|    total_timesteps      | 173000       |
| train/                  |              |
|    approx_kl            | 0.0037672275 |
|    clip_fraction        | 0.142        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.697       |
|    explained_variance   | 0.417        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.246        |
|    n_updates            | 4320         |
|    policy_gradient_loss | -0.00993     |
|    value_loss           | 1.59         |
------------------------------------------
Eval num_timesteps=173200, episode_reward=24.17 +/- 12.85
Episode length: 35.20 +/- 18.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 24.2     |
| time/              |          |
|    total_timesteps | 173200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.9     |
|    ep_rew_mean     | 17.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 433      |
|    time_elapsed    | 48171    |
|    total_timesteps | 173200   |
---------------------------------
Eval num_timesteps=173400, episode_reward=12.53 +/- 12.17
Episode length: 18.20 +/- 16.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.2         |
|    mean_reward          | 12.5         |
| time/                   |              |
|    total_timesteps      | 173400       |
| train/                  |              |
|    approx_kl            | 0.0046362653 |
|    clip_fraction        | 0.204        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.677       |
|    explained_variance   | 0.0953       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.585        |
|    n_updates            | 4330         |
|    policy_gradient_loss | -0.00721     |
|    value_loss           | 3.1          |
------------------------------------------
Eval num_timesteps=173600, episode_reward=11.12 +/- 13.13
Episode length: 16.20 +/- 17.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | 11.1     |
| time/              |          |
|    total_timesteps | 173600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.8     |
|    ep_rew_mean     | 17       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 434      |
|    time_elapsed    | 48284    |
|    total_timesteps | 173600   |
---------------------------------
Eval num_timesteps=173800, episode_reward=10.49 +/- 13.24
Episode length: 15.20 +/- 17.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.2         |
|    mean_reward          | 10.5         |
| time/                   |              |
|    total_timesteps      | 173800       |
| train/                  |              |
|    approx_kl            | 0.0016939853 |
|    clip_fraction        | 0.0612       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.668       |
|    explained_variance   | 0.397        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.3          |
|    n_updates            | 4340         |
|    policy_gradient_loss | -0.00475     |
|    value_loss           | 2.63         |
------------------------------------------
Eval num_timesteps=174000, episode_reward=23.05 +/- 15.82
Episode length: 32.40 +/- 21.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.4     |
|    mean_reward     | 23       |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.6     |
|    ep_rew_mean     | 15.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 435      |
|    time_elapsed    | 48394    |
|    total_timesteps | 174000   |
---------------------------------
Eval num_timesteps=174200, episode_reward=11.34 +/- 11.15
Episode length: 17.20 +/- 16.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.2         |
|    mean_reward          | 11.3         |
| time/                   |              |
|    total_timesteps      | 174200       |
| train/                  |              |
|    approx_kl            | 0.0020706353 |
|    clip_fraction        | 0.0435       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.658       |
|    explained_variance   | 0.255        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.03         |
|    n_updates            | 4350         |
|    policy_gradient_loss | -0.00507     |
|    value_loss           | 2.77         |
------------------------------------------
Eval num_timesteps=174400, episode_reward=28.77 +/- 11.98
Episode length: 41.60 +/- 16.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 28.8     |
| time/              |          |
|    total_timesteps | 174400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.1     |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 436      |
|    time_elapsed    | 48500    |
|    total_timesteps | 174400   |
---------------------------------
Eval num_timesteps=174600, episode_reward=8.78 +/- 4.96
Episode length: 13.80 +/- 7.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 13.8       |
|    mean_reward          | 8.78       |
| time/                   |            |
|    total_timesteps      | 174600     |
| train/                  |            |
|    approx_kl            | 0.00302977 |
|    clip_fraction        | 0.0658     |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.667     |
|    explained_variance   | 0.121      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.536      |
|    n_updates            | 4360       |
|    policy_gradient_loss | -0.00528   |
|    value_loss           | 1.67       |
----------------------------------------
Eval num_timesteps=174800, episode_reward=23.22 +/- 14.50
Episode length: 33.60 +/- 20.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 23.2     |
| time/              |          |
|    total_timesteps | 174800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.4     |
|    ep_rew_mean     | 16.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 437      |
|    time_elapsed    | 48615    |
|    total_timesteps | 174800   |
---------------------------------
Eval num_timesteps=175000, episode_reward=18.98 +/- 13.27
Episode length: 28.40 +/- 19.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 28.4        |
|    mean_reward          | 19          |
| time/                   |             |
|    total_timesteps      | 175000      |
| train/                  |             |
|    approx_kl            | 0.002488503 |
|    clip_fraction        | 0.0777      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.64       |
|    explained_variance   | 0.028       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.192       |
|    n_updates            | 4370        |
|    policy_gradient_loss | -0.00733    |
|    value_loss           | 0.591       |
-----------------------------------------
Eval num_timesteps=175200, episode_reward=13.09 +/- 12.10
Episode length: 19.00 +/- 16.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 13.1     |
| time/              |          |
|    total_timesteps | 175200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.3     |
|    ep_rew_mean     | 16       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 438      |
|    time_elapsed    | 48717    |
|    total_timesteps | 175200   |
---------------------------------
Eval num_timesteps=175400, episode_reward=15.91 +/- 14.30
Episode length: 24.20 +/- 21.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.2         |
|    mean_reward          | 15.9         |
| time/                   |              |
|    total_timesteps      | 175400       |
| train/                  |              |
|    approx_kl            | 0.0018134901 |
|    clip_fraction        | 0.0547       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.64        |
|    explained_variance   | 0.333        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.442        |
|    n_updates            | 4380         |
|    policy_gradient_loss | -0.00534     |
|    value_loss           | 2.46         |
------------------------------------------
Eval num_timesteps=175600, episode_reward=4.08 +/- 2.34
Episode length: 6.80 +/- 3.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6.8      |
|    mean_reward     | 4.08     |
| time/              |          |
|    total_timesteps | 175600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.9     |
|    ep_rew_mean     | 15.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 439      |
|    time_elapsed    | 48804    |
|    total_timesteps | 175600   |
---------------------------------
Eval num_timesteps=175800, episode_reward=13.54 +/- 11.85
Episode length: 19.80 +/- 16.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 19.8         |
|    mean_reward          | 13.5         |
| time/                   |              |
|    total_timesteps      | 175800       |
| train/                  |              |
|    approx_kl            | 0.0026284943 |
|    clip_fraction        | 0.0433       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.632       |
|    explained_variance   | 0.33         |
|    learning_rate        | 0.0001       |
|    loss                 | 1.36         |
|    n_updates            | 4390         |
|    policy_gradient_loss | -0.00773     |
|    value_loss           | 2.76         |
------------------------------------------
Eval num_timesteps=176000, episode_reward=11.67 +/- 12.09
Episode length: 17.20 +/- 16.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | 11.7     |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.2     |
|    ep_rew_mean     | 16.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 440      |
|    time_elapsed    | 48910    |
|    total_timesteps | 176000   |
---------------------------------
Eval num_timesteps=176200, episode_reward=3.94 +/- 3.69
Episode length: 6.60 +/- 5.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 6.6          |
|    mean_reward          | 3.94         |
| time/                   |              |
|    total_timesteps      | 176200       |
| train/                  |              |
|    approx_kl            | 0.0022352585 |
|    clip_fraction        | 0.0621       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.633       |
|    explained_variance   | 0.295        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.803        |
|    n_updates            | 4400         |
|    policy_gradient_loss | -0.00496     |
|    value_loss           | 1.58         |
------------------------------------------
Eval num_timesteps=176400, episode_reward=22.19 +/- 16.89
Episode length: 31.20 +/- 23.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.2     |
|    mean_reward     | 22.2     |
| time/              |          |
|    total_timesteps | 176400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.4     |
|    ep_rew_mean     | 18.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 441      |
|    time_elapsed    | 48997    |
|    total_timesteps | 176400   |
---------------------------------
Eval num_timesteps=176600, episode_reward=9.78 +/- 11.89
Episode length: 15.20 +/- 17.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.2         |
|    mean_reward          | 9.78         |
| time/                   |              |
|    total_timesteps      | 176600       |
| train/                  |              |
|    approx_kl            | 0.0018786717 |
|    clip_fraction        | 0.0435       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.665       |
|    explained_variance   | 0.138        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.537        |
|    n_updates            | 4410         |
|    policy_gradient_loss | -0.00365     |
|    value_loss           | 0.948        |
------------------------------------------
Eval num_timesteps=176800, episode_reward=16.09 +/- 15.10
Episode length: 24.00 +/- 21.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16.1     |
| time/              |          |
|    total_timesteps | 176800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.7     |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 442      |
|    time_elapsed    | 49105    |
|    total_timesteps | 176800   |
---------------------------------
Eval num_timesteps=177000, episode_reward=16.80 +/- 13.41
Episode length: 24.20 +/- 17.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.2         |
|    mean_reward          | 16.8         |
| time/                   |              |
|    total_timesteps      | 177000       |
| train/                  |              |
|    approx_kl            | 0.0011998886 |
|    clip_fraction        | 0.0464       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.619       |
|    explained_variance   | 0.238        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.67         |
|    n_updates            | 4420         |
|    policy_gradient_loss | -0.00356     |
|    value_loss           | 2.8          |
------------------------------------------
Eval num_timesteps=177200, episode_reward=10.88 +/- 12.08
Episode length: 16.40 +/- 17.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | 10.9     |
| time/              |          |
|    total_timesteps | 177200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | 16       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 443      |
|    time_elapsed    | 49219    |
|    total_timesteps | 177200   |
---------------------------------
Eval num_timesteps=177400, episode_reward=18.33 +/- 12.78
Episode length: 27.40 +/- 18.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 27.4         |
|    mean_reward          | 18.3         |
| time/                   |              |
|    total_timesteps      | 177400       |
| train/                  |              |
|    approx_kl            | 0.0010714364 |
|    clip_fraction        | 0.0185       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.588       |
|    explained_variance   | 0.119        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.34         |
|    n_updates            | 4430         |
|    policy_gradient_loss | -0.00324     |
|    value_loss           | 2.82         |
------------------------------------------
Eval num_timesteps=177600, episode_reward=35.36 +/- 1.63
Episode length: 50.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 35.4     |
| time/              |          |
|    total_timesteps | 177600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.2     |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 444      |
|    time_elapsed    | 49353    |
|    total_timesteps | 177600   |
---------------------------------
Eval num_timesteps=177800, episode_reward=32.12 +/- 7.53
Episode length: 45.00 +/- 10.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 45           |
|    mean_reward          | 32.1         |
| time/                   |              |
|    total_timesteps      | 177800       |
| train/                  |              |
|    approx_kl            | 0.0033450958 |
|    clip_fraction        | 0.149        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.628       |
|    explained_variance   | 0.343        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.23         |
|    n_updates            | 4440         |
|    policy_gradient_loss | -0.0106      |
|    value_loss           | 2.17         |
------------------------------------------
Eval num_timesteps=178000, episode_reward=23.66 +/- 15.08
Episode length: 33.40 +/- 20.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 23.7     |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 445      |
|    time_elapsed    | 49484    |
|    total_timesteps | 178000   |
---------------------------------
Eval num_timesteps=178200, episode_reward=10.25 +/- 12.19
Episode length: 15.60 +/- 17.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.6         |
|    mean_reward          | 10.2         |
| time/                   |              |
|    total_timesteps      | 178200       |
| train/                  |              |
|    approx_kl            | 0.0012474323 |
|    clip_fraction        | 0.0478       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.617       |
|    explained_variance   | 0.498        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.08         |
|    n_updates            | 4450         |
|    policy_gradient_loss | -0.00515     |
|    value_loss           | 1.92         |
------------------------------------------
Eval num_timesteps=178400, episode_reward=17.71 +/- 14.66
Episode length: 25.80 +/- 20.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | 17.7     |
| time/              |          |
|    total_timesteps | 178400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.4     |
|    ep_rew_mean     | 14.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 446      |
|    time_elapsed    | 49590    |
|    total_timesteps | 178400   |
---------------------------------
Eval num_timesteps=178600, episode_reward=16.62 +/- 15.55
Episode length: 24.00 +/- 21.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24           |
|    mean_reward          | 16.6         |
| time/                   |              |
|    total_timesteps      | 178600       |
| train/                  |              |
|    approx_kl            | 0.0040625446 |
|    clip_fraction        | 0.0996       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.64        |
|    explained_variance   | 0.402        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.64         |
|    n_updates            | 4460         |
|    policy_gradient_loss | -0.00823     |
|    value_loss           | 2.44         |
------------------------------------------
Eval num_timesteps=178800, episode_reward=13.59 +/- 11.67
Episode length: 20.00 +/- 15.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20       |
|    mean_reward     | 13.6     |
| time/              |          |
|    total_timesteps | 178800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 447      |
|    time_elapsed    | 49696    |
|    total_timesteps | 178800   |
---------------------------------
Eval num_timesteps=179000, episode_reward=17.51 +/- 15.97
Episode length: 24.40 +/- 21.32
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 24.4       |
|    mean_reward          | 17.5       |
| time/                   |            |
|    total_timesteps      | 179000     |
| train/                  |            |
|    approx_kl            | 0.00348407 |
|    clip_fraction        | 0.137      |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.652     |
|    explained_variance   | 0.283      |
|    learning_rate        | 0.0001     |
|    loss                 | 1.98       |
|    n_updates            | 4470       |
|    policy_gradient_loss | -0.00849   |
|    value_loss           | 2.3        |
----------------------------------------
Eval num_timesteps=179200, episode_reward=11.62 +/- 11.27
Episode length: 17.80 +/- 16.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | 11.6     |
| time/              |          |
|    total_timesteps | 179200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.5     |
|    ep_rew_mean     | 15.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 448      |
|    time_elapsed    | 49806    |
|    total_timesteps | 179200   |
---------------------------------
Eval num_timesteps=179400, episode_reward=5.13 +/- 1.43
Episode length: 8.20 +/- 1.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 8.2          |
|    mean_reward          | 5.13         |
| time/                   |              |
|    total_timesteps      | 179400       |
| train/                  |              |
|    approx_kl            | 0.0024371785 |
|    clip_fraction        | 0.0908       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.671       |
|    explained_variance   | 0.345        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.187        |
|    n_updates            | 4480         |
|    policy_gradient_loss | -0.00517     |
|    value_loss           | 1.32         |
------------------------------------------
Eval num_timesteps=179600, episode_reward=11.67 +/- 11.08
Episode length: 18.00 +/- 16.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | 11.7     |
| time/              |          |
|    total_timesteps | 179600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.2     |
|    ep_rew_mean     | 15.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 449      |
|    time_elapsed    | 49892    |
|    total_timesteps | 179600   |
---------------------------------
Eval num_timesteps=179800, episode_reward=8.97 +/- 12.38
Episode length: 14.00 +/- 18.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14           |
|    mean_reward          | 8.97         |
| time/                   |              |
|    total_timesteps      | 179800       |
| train/                  |              |
|    approx_kl            | 0.0011736917 |
|    clip_fraction        | 0.023        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.629       |
|    explained_variance   | 0.25         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.756        |
|    n_updates            | 4490         |
|    policy_gradient_loss | -0.00514     |
|    value_loss           | 2.88         |
------------------------------------------
Eval num_timesteps=180000, episode_reward=16.88 +/- 14.73
Episode length: 25.20 +/- 20.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.3     |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 450      |
|    time_elapsed    | 49998    |
|    total_timesteps | 180000   |
---------------------------------
Eval num_timesteps=180200, episode_reward=16.89 +/- 15.31
Episode length: 24.40 +/- 20.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.4         |
|    mean_reward          | 16.9         |
| time/                   |              |
|    total_timesteps      | 180200       |
| train/                  |              |
|    approx_kl            | 0.0017408595 |
|    clip_fraction        | 0.052        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.644       |
|    explained_variance   | 0.408        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.823        |
|    n_updates            | 4500         |
|    policy_gradient_loss | -0.00797     |
|    value_loss           | 2.24         |
------------------------------------------
Eval num_timesteps=180400, episode_reward=15.47 +/- 15.63
Episode length: 23.00 +/- 22.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 15.5     |
| time/              |          |
|    total_timesteps | 180400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.1     |
|    ep_rew_mean     | 15.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 451      |
|    time_elapsed    | 50105    |
|    total_timesteps | 180400   |
---------------------------------
Eval num_timesteps=180600, episode_reward=28.61 +/- 12.85
Episode length: 41.00 +/- 18.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41           |
|    mean_reward          | 28.6         |
| time/                   |              |
|    total_timesteps      | 180600       |
| train/                  |              |
|    approx_kl            | 0.0019062806 |
|    clip_fraction        | 0.0429       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.662       |
|    explained_variance   | 0.342        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.219        |
|    n_updates            | 4510         |
|    policy_gradient_loss | -0.00509     |
|    value_loss           | 1.38         |
------------------------------------------
Eval num_timesteps=180800, episode_reward=27.73 +/- 13.48
Episode length: 40.40 +/- 19.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.4     |
|    mean_reward     | 27.7     |
| time/              |          |
|    total_timesteps | 180800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23       |
|    ep_rew_mean     | 15.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 452      |
|    time_elapsed    | 50235    |
|    total_timesteps | 180800   |
---------------------------------
Eval num_timesteps=181000, episode_reward=14.42 +/- 16.83
Episode length: 21.20 +/- 23.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 21.2         |
|    mean_reward          | 14.4         |
| time/                   |              |
|    total_timesteps      | 181000       |
| train/                  |              |
|    approx_kl            | 0.0030957018 |
|    clip_fraction        | 0.185        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.66        |
|    explained_variance   | 0.302        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.2          |
|    n_updates            | 4520         |
|    policy_gradient_loss | -0.0089      |
|    value_loss           | 2.91         |
------------------------------------------
Eval num_timesteps=181200, episode_reward=22.96 +/- 14.18
Episode length: 33.60 +/- 20.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 23       |
| time/              |          |
|    total_timesteps | 181200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23       |
|    ep_rew_mean     | 15.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 453      |
|    time_elapsed    | 50338    |
|    total_timesteps | 181200   |
---------------------------------
Eval num_timesteps=181400, episode_reward=23.30 +/- 15.06
Episode length: 33.20 +/- 20.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.2         |
|    mean_reward          | 23.3         |
| time/                   |              |
|    total_timesteps      | 181400       |
| train/                  |              |
|    approx_kl            | 0.0014417485 |
|    clip_fraction        | 0.0545       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.639       |
|    explained_variance   | 0.293        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.483        |
|    n_updates            | 4530         |
|    policy_gradient_loss | -0.00349     |
|    value_loss           | 2.05         |
------------------------------------------
Eval num_timesteps=181600, episode_reward=17.56 +/- 14.33
Episode length: 25.60 +/- 19.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 181600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.2     |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 454      |
|    time_elapsed    | 50450    |
|    total_timesteps | 181600   |
---------------------------------
Eval num_timesteps=181800, episode_reward=17.45 +/- 14.83
Episode length: 25.00 +/- 20.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25           |
|    mean_reward          | 17.4         |
| time/                   |              |
|    total_timesteps      | 181800       |
| train/                  |              |
|    approx_kl            | 0.0036656712 |
|    clip_fraction        | 0.217        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.676       |
|    explained_variance   | 0.108        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.246        |
|    n_updates            | 4540         |
|    policy_gradient_loss | -0.0116      |
|    value_loss           | 0.722        |
------------------------------------------
Eval num_timesteps=182000, episode_reward=23.06 +/- 13.32
Episode length: 34.40 +/- 19.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 23.1     |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.7     |
|    ep_rew_mean     | 17       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 455      |
|    time_elapsed    | 50582    |
|    total_timesteps | 182000   |
---------------------------------
Eval num_timesteps=182200, episode_reward=16.30 +/- 15.76
Episode length: 23.60 +/- 21.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 23.6        |
|    mean_reward          | 16.3        |
| time/                   |             |
|    total_timesteps      | 182200      |
| train/                  |             |
|    approx_kl            | 0.002895417 |
|    clip_fraction        | 0.0893      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.645      |
|    explained_variance   | 0.285       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.27        |
|    n_updates            | 4550        |
|    policy_gradient_loss | -0.00685    |
|    value_loss           | 1.8         |
-----------------------------------------
Eval num_timesteps=182400, episode_reward=11.55 +/- 13.45
Episode length: 16.80 +/- 18.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | 11.5     |
| time/              |          |
|    total_timesteps | 182400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.5     |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 456      |
|    time_elapsed    | 50686    |
|    total_timesteps | 182400   |
---------------------------------
Eval num_timesteps=182600, episode_reward=19.92 +/- 13.66
Episode length: 28.60 +/- 18.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 28.6         |
|    mean_reward          | 19.9         |
| time/                   |              |
|    total_timesteps      | 182600       |
| train/                  |              |
|    approx_kl            | 0.0024122752 |
|    clip_fraction        | 0.0882       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.627       |
|    explained_variance   | 0.252        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.368        |
|    n_updates            | 4560         |
|    policy_gradient_loss | -0.0058      |
|    value_loss           | 2.51         |
------------------------------------------
Eval num_timesteps=182800, episode_reward=16.81 +/- 13.63
Episode length: 25.60 +/- 20.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 182800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.6     |
|    ep_rew_mean     | 18.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 457      |
|    time_elapsed    | 50795    |
|    total_timesteps | 182800   |
---------------------------------
Eval num_timesteps=183000, episode_reward=24.44 +/- 14.35
Episode length: 35.00 +/- 19.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35           |
|    mean_reward          | 24.4         |
| time/                   |              |
|    total_timesteps      | 183000       |
| train/                  |              |
|    approx_kl            | 0.0017088706 |
|    clip_fraction        | 0.0498       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.647       |
|    explained_variance   | 0.301        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.54         |
|    n_updates            | 4570         |
|    policy_gradient_loss | -0.00382     |
|    value_loss           | 2.1          |
------------------------------------------
Eval num_timesteps=183200, episode_reward=22.38 +/- 15.80
Episode length: 32.20 +/- 21.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 22.4     |
| time/              |          |
|    total_timesteps | 183200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.6     |
|    ep_rew_mean     | 18.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 458      |
|    time_elapsed    | 50934    |
|    total_timesteps | 183200   |
---------------------------------
Eval num_timesteps=183400, episode_reward=11.01 +/- 11.87
Episode length: 16.60 +/- 16.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.6         |
|    mean_reward          | 11           |
| time/                   |              |
|    total_timesteps      | 183400       |
| train/                  |              |
|    approx_kl            | 0.0017972917 |
|    clip_fraction        | 0.0373       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.651       |
|    explained_variance   | 0.139        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.661        |
|    n_updates            | 4580         |
|    policy_gradient_loss | -0.00515     |
|    value_loss           | 2.42         |
------------------------------------------
Eval num_timesteps=183600, episode_reward=15.67 +/- 15.00
Episode length: 23.40 +/- 21.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 15.7     |
| time/              |          |
|    total_timesteps | 183600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.6     |
|    ep_rew_mean     | 19.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 459      |
|    time_elapsed    | 51039    |
|    total_timesteps | 183600   |
---------------------------------
Eval num_timesteps=183800, episode_reward=24.07 +/- 14.58
Episode length: 34.00 +/- 19.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34           |
|    mean_reward          | 24.1         |
| time/                   |              |
|    total_timesteps      | 183800       |
| train/                  |              |
|    approx_kl            | 0.0015559626 |
|    clip_fraction        | 0.0866       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.629       |
|    explained_variance   | 0.278        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.807        |
|    n_updates            | 4590         |
|    policy_gradient_loss | -0.00658     |
|    value_loss           | 1.46         |
------------------------------------------
Eval num_timesteps=184000, episode_reward=13.31 +/- 11.28
Episode length: 19.40 +/- 15.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 13.3     |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.7     |
|    ep_rew_mean     | 18.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 460      |
|    time_elapsed    | 51152    |
|    total_timesteps | 184000   |
---------------------------------
Eval num_timesteps=184200, episode_reward=17.06 +/- 15.29
Episode length: 24.60 +/- 20.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.6         |
|    mean_reward          | 17.1         |
| time/                   |              |
|    total_timesteps      | 184200       |
| train/                  |              |
|    approx_kl            | 0.0016963911 |
|    clip_fraction        | 0.0415       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.624       |
|    explained_variance   | 0.283        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.48         |
|    n_updates            | 4600         |
|    policy_gradient_loss | -0.00414     |
|    value_loss           | 2.33         |
------------------------------------------
Eval num_timesteps=184400, episode_reward=18.70 +/- 13.52
Episode length: 27.00 +/- 19.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27       |
|    mean_reward     | 18.7     |
| time/              |          |
|    total_timesteps | 184400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.5     |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 461      |
|    time_elapsed    | 51264    |
|    total_timesteps | 184400   |
---------------------------------
Eval num_timesteps=184600, episode_reward=22.22 +/- 14.69
Episode length: 32.80 +/- 21.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.8         |
|    mean_reward          | 22.2         |
| time/                   |              |
|    total_timesteps      | 184600       |
| train/                  |              |
|    approx_kl            | 0.0027274159 |
|    clip_fraction        | 0.0951       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.628       |
|    explained_variance   | 0.369        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.512        |
|    n_updates            | 4610         |
|    policy_gradient_loss | -0.00576     |
|    value_loss           | 1.84         |
------------------------------------------
Eval num_timesteps=184800, episode_reward=4.78 +/- 2.31
Episode length: 7.80 +/- 3.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7.8      |
|    mean_reward     | 4.78     |
| time/              |          |
|    total_timesteps | 184800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25       |
|    ep_rew_mean     | 17.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 462      |
|    time_elapsed    | 51356    |
|    total_timesteps | 184800   |
---------------------------------
Eval num_timesteps=185000, episode_reward=27.76 +/- 12.89
Episode length: 40.80 +/- 18.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 40.8         |
|    mean_reward          | 27.8         |
| time/                   |              |
|    total_timesteps      | 185000       |
| train/                  |              |
|    approx_kl            | 0.0018406427 |
|    clip_fraction        | 0.0681       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.652       |
|    explained_variance   | 0.602        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.11         |
|    n_updates            | 4620         |
|    policy_gradient_loss | -0.00486     |
|    value_loss           | 0.923        |
------------------------------------------
Eval num_timesteps=185200, episode_reward=22.54 +/- 15.58
Episode length: 32.40 +/- 21.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.4     |
|    mean_reward     | 22.5     |
| time/              |          |
|    total_timesteps | 185200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.2     |
|    ep_rew_mean     | 17.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 463      |
|    time_elapsed    | 51465    |
|    total_timesteps | 185200   |
---------------------------------
Eval num_timesteps=185400, episode_reward=11.68 +/- 12.29
Episode length: 17.00 +/- 16.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17           |
|    mean_reward          | 11.7         |
| time/                   |              |
|    total_timesteps      | 185400       |
| train/                  |              |
|    approx_kl            | 0.0026323001 |
|    clip_fraction        | 0.0772       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.662       |
|    explained_variance   | 0.0929       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.22         |
|    n_updates            | 4630         |
|    policy_gradient_loss | -0.00735     |
|    value_loss           | 2.26         |
------------------------------------------
Eval num_timesteps=185600, episode_reward=22.75 +/- 13.91
Episode length: 32.80 +/- 19.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 22.7     |
| time/              |          |
|    total_timesteps | 185600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.7     |
|    ep_rew_mean     | 17.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 464      |
|    time_elapsed    | 51574    |
|    total_timesteps | 185600   |
---------------------------------
Eval num_timesteps=185800, episode_reward=9.05 +/- 12.29
Episode length: 14.20 +/- 18.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.2         |
|    mean_reward          | 9.05         |
| time/                   |              |
|    total_timesteps      | 185800       |
| train/                  |              |
|    approx_kl            | 0.0022783254 |
|    clip_fraction        | 0.0792       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.628       |
|    explained_variance   | 0.403        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.183        |
|    n_updates            | 4640         |
|    policy_gradient_loss | -0.00422     |
|    value_loss           | 2.08         |
------------------------------------------
Eval num_timesteps=186000, episode_reward=24.01 +/- 11.18
Episode length: 35.20 +/- 15.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 24       |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 465      |
|    time_elapsed    | 51705    |
|    total_timesteps | 186000   |
---------------------------------
Eval num_timesteps=186200, episode_reward=22.74 +/- 14.86
Episode length: 33.00 +/- 20.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33          |
|    mean_reward          | 22.7        |
| time/                   |             |
|    total_timesteps      | 186200      |
| train/                  |             |
|    approx_kl            | 0.002222782 |
|    clip_fraction        | 0.0842      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.655      |
|    explained_variance   | 0.356       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.437       |
|    n_updates            | 4650        |
|    policy_gradient_loss | -0.00459    |
|    value_loss           | 1.96        |
-----------------------------------------
Eval num_timesteps=186400, episode_reward=12.15 +/- 12.01
Episode length: 17.80 +/- 16.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | 12.1     |
| time/              |          |
|    total_timesteps | 186400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 466      |
|    time_elapsed    | 51812    |
|    total_timesteps | 186400   |
---------------------------------
Eval num_timesteps=186600, episode_reward=23.80 +/- 14.92
Episode length: 33.60 +/- 20.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.6         |
|    mean_reward          | 23.8         |
| time/                   |              |
|    total_timesteps      | 186600       |
| train/                  |              |
|    approx_kl            | 0.0014072146 |
|    clip_fraction        | 0.0533       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.634       |
|    explained_variance   | 0.487        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.709        |
|    n_updates            | 4660         |
|    policy_gradient_loss | -0.00285     |
|    value_loss           | 1.65         |
------------------------------------------
Eval num_timesteps=186800, episode_reward=16.75 +/- 15.04
Episode length: 24.60 +/- 20.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 186800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.1     |
|    ep_rew_mean     | 15.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 467      |
|    time_elapsed    | 51940    |
|    total_timesteps | 186800   |
---------------------------------
Eval num_timesteps=187000, episode_reward=17.89 +/- 14.89
Episode length: 25.60 +/- 20.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.6         |
|    mean_reward          | 17.9         |
| time/                   |              |
|    total_timesteps      | 187000       |
| train/                  |              |
|    approx_kl            | 0.0013633755 |
|    clip_fraction        | 0.0187       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.651       |
|    explained_variance   | 0.463        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.78         |
|    n_updates            | 4670         |
|    policy_gradient_loss | -0.00309     |
|    value_loss           | 2.08         |
------------------------------------------
Eval num_timesteps=187200, episode_reward=9.86 +/- 13.04
Episode length: 14.60 +/- 17.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 9.86     |
| time/              |          |
|    total_timesteps | 187200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.7     |
|    ep_rew_mean     | 15.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 468      |
|    time_elapsed    | 52050    |
|    total_timesteps | 187200   |
---------------------------------
Eval num_timesteps=187400, episode_reward=14.71 +/- 11.56
Episode length: 21.40 +/- 15.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 21.4         |
|    mean_reward          | 14.7         |
| time/                   |              |
|    total_timesteps      | 187400       |
| train/                  |              |
|    approx_kl            | 0.0010930054 |
|    clip_fraction        | 0.0471       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.623       |
|    explained_variance   | 0.397        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.69         |
|    n_updates            | 4680         |
|    policy_gradient_loss | -0.004       |
|    value_loss           | 2.11         |
------------------------------------------
Eval num_timesteps=187600, episode_reward=14.92 +/- 15.55
Episode length: 22.40 +/- 22.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 14.9     |
| time/              |          |
|    total_timesteps | 187600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.7     |
|    ep_rew_mean     | 15.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 469      |
|    time_elapsed    | 52154    |
|    total_timesteps | 187600   |
---------------------------------
Eval num_timesteps=187800, episode_reward=16.21 +/- 15.92
Episode length: 23.40 +/- 21.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.4         |
|    mean_reward          | 16.2         |
| time/                   |              |
|    total_timesteps      | 187800       |
| train/                  |              |
|    approx_kl            | 0.0038057317 |
|    clip_fraction        | 0.106        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.661       |
|    explained_variance   | 0.343        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.14         |
|    n_updates            | 4690         |
|    policy_gradient_loss | -0.00642     |
|    value_loss           | 1.4          |
------------------------------------------
Eval num_timesteps=188000, episode_reward=29.13 +/- 10.15
Episode length: 42.60 +/- 14.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.6     |
|    mean_reward     | 29.1     |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 15       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 470      |
|    time_elapsed    | 52265    |
|    total_timesteps | 188000   |
---------------------------------
Eval num_timesteps=188200, episode_reward=9.72 +/- 13.03
Episode length: 14.40 +/- 17.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.4         |
|    mean_reward          | 9.72         |
| time/                   |              |
|    total_timesteps      | 188200       |
| train/                  |              |
|    approx_kl            | 0.0012356897 |
|    clip_fraction        | 0.0259       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.621       |
|    explained_variance   | 0.294        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.73         |
|    n_updates            | 4700         |
|    policy_gradient_loss | -0.00259     |
|    value_loss           | 3.11         |
------------------------------------------
Eval num_timesteps=188400, episode_reward=17.81 +/- 13.70
Episode length: 26.00 +/- 19.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | 17.8     |
| time/              |          |
|    total_timesteps | 188400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.4     |
|    ep_rew_mean     | 13.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 471      |
|    time_elapsed    | 52367    |
|    total_timesteps | 188400   |
---------------------------------
Eval num_timesteps=188600, episode_reward=16.04 +/- 16.01
Episode length: 23.20 +/- 21.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.2         |
|    mean_reward          | 16           |
| time/                   |              |
|    total_timesteps      | 188600       |
| train/                  |              |
|    approx_kl            | 0.0015870838 |
|    clip_fraction        | 0.0527       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.614       |
|    explained_variance   | 0.311        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.11         |
|    n_updates            | 4710         |
|    policy_gradient_loss | -0.00486     |
|    value_loss           | 3.05         |
------------------------------------------
Eval num_timesteps=188800, episode_reward=15.89 +/- 15.19
Episode length: 23.60 +/- 21.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 15.9     |
| time/              |          |
|    total_timesteps | 188800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.7     |
|    ep_rew_mean     | 13.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 472      |
|    time_elapsed    | 52471    |
|    total_timesteps | 188800   |
---------------------------------
Eval num_timesteps=189000, episode_reward=17.30 +/- 14.60
Episode length: 25.20 +/- 20.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.2         |
|    mean_reward          | 17.3         |
| time/                   |              |
|    total_timesteps      | 189000       |
| train/                  |              |
|    approx_kl            | 0.0004563635 |
|    clip_fraction        | 0.0165       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.653       |
|    explained_variance   | 0.259        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.654        |
|    n_updates            | 4720         |
|    policy_gradient_loss | -0.00248     |
|    value_loss           | 2.46         |
------------------------------------------
Eval num_timesteps=189200, episode_reward=23.09 +/- 15.41
Episode length: 32.80 +/- 21.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 23.1     |
| time/              |          |
|    total_timesteps | 189200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.9     |
|    ep_rew_mean     | 13.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 473      |
|    time_elapsed    | 52627    |
|    total_timesteps | 189200   |
---------------------------------
Eval num_timesteps=189400, episode_reward=29.21 +/- 11.64
Episode length: 41.80 +/- 16.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.8         |
|    mean_reward          | 29.2         |
| time/                   |              |
|    total_timesteps      | 189400       |
| train/                  |              |
|    approx_kl            | 0.0028017221 |
|    clip_fraction        | 0.0734       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.634       |
|    explained_variance   | 0.488        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.445        |
|    n_updates            | 4730         |
|    policy_gradient_loss | -0.00726     |
|    value_loss           | 1.8          |
------------------------------------------
Eval num_timesteps=189600, episode_reward=22.69 +/- 14.95
Episode length: 33.00 +/- 20.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 22.7     |
| time/              |          |
|    total_timesteps | 189600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.8     |
|    ep_rew_mean     | 13.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 474      |
|    time_elapsed    | 52760    |
|    total_timesteps | 189600   |
---------------------------------
Eval num_timesteps=189800, episode_reward=16.84 +/- 14.59
Episode length: 24.80 +/- 20.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.8         |
|    mean_reward          | 16.8         |
| time/                   |              |
|    total_timesteps      | 189800       |
| train/                  |              |
|    approx_kl            | 0.0040039667 |
|    clip_fraction        | 0.109        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.658       |
|    explained_variance   | 0.415        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.261        |
|    n_updates            | 4740         |
|    policy_gradient_loss | -0.0062      |
|    value_loss           | 1.56         |
------------------------------------------
Eval num_timesteps=190000, episode_reward=17.57 +/- 14.86
Episode length: 25.60 +/- 20.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.1     |
|    ep_rew_mean     | 14.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 475      |
|    time_elapsed    | 52874    |
|    total_timesteps | 190000   |
---------------------------------
Eval num_timesteps=190200, episode_reward=24.21 +/- 14.07
Episode length: 34.60 +/- 19.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.6         |
|    mean_reward          | 24.2         |
| time/                   |              |
|    total_timesteps      | 190200       |
| train/                  |              |
|    approx_kl            | 0.0033980566 |
|    clip_fraction        | 0.0813       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.657       |
|    explained_variance   | 0.281        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.185        |
|    n_updates            | 4750         |
|    policy_gradient_loss | -0.00634     |
|    value_loss           | 1.92         |
------------------------------------------
Eval num_timesteps=190400, episode_reward=5.05 +/- 0.78
Episode length: 8.00 +/- 1.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8        |
|    mean_reward     | 5.05     |
| time/              |          |
|    total_timesteps | 190400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.8     |
|    ep_rew_mean     | 14.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 476      |
|    time_elapsed    | 52962    |
|    total_timesteps | 190400   |
---------------------------------
Eval num_timesteps=190600, episode_reward=21.65 +/- 15.73
Episode length: 31.80 +/- 22.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 31.8         |
|    mean_reward          | 21.7         |
| time/                   |              |
|    total_timesteps      | 190600       |
| train/                  |              |
|    approx_kl            | 0.0017253312 |
|    clip_fraction        | 0.0348       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.63        |
|    explained_variance   | 0.213        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.07         |
|    n_updates            | 4760         |
|    policy_gradient_loss | -0.00498     |
|    value_loss           | 3.37         |
------------------------------------------
Eval num_timesteps=190800, episode_reward=16.18 +/- 15.52
Episode length: 23.60 +/- 21.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 16.2     |
| time/              |          |
|    total_timesteps | 190800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | 16.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 477      |
|    time_elapsed    | 53071    |
|    total_timesteps | 190800   |
---------------------------------
Eval num_timesteps=191000, episode_reward=16.18 +/- 15.99
Episode length: 23.40 +/- 21.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.4         |
|    mean_reward          | 16.2         |
| time/                   |              |
|    total_timesteps      | 191000       |
| train/                  |              |
|    approx_kl            | 0.0029927164 |
|    clip_fraction        | 0.0804       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.653       |
|    explained_variance   | 0.296        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.07         |
|    n_updates            | 4770         |
|    policy_gradient_loss | -0.00518     |
|    value_loss           | 1.34         |
------------------------------------------
Eval num_timesteps=191200, episode_reward=5.33 +/- 1.60
Episode length: 8.40 +/- 2.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8.4      |
|    mean_reward     | 5.33     |
| time/              |          |
|    total_timesteps | 191200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 478      |
|    time_elapsed    | 53163    |
|    total_timesteps | 191200   |
---------------------------------
Eval num_timesteps=191400, episode_reward=14.35 +/- 11.57
Episode length: 20.80 +/- 15.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 20.8         |
|    mean_reward          | 14.4         |
| time/                   |              |
|    total_timesteps      | 191400       |
| train/                  |              |
|    approx_kl            | 0.0014425798 |
|    clip_fraction        | 0.05         |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.635       |
|    explained_variance   | 0.295        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.18         |
|    n_updates            | 4780         |
|    policy_gradient_loss | -0.00416     |
|    value_loss           | 2.29         |
------------------------------------------
Eval num_timesteps=191600, episode_reward=13.05 +/- 10.68
Episode length: 19.80 +/- 15.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.8     |
|    mean_reward     | 13.1     |
| time/              |          |
|    total_timesteps | 191600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.3     |
|    ep_rew_mean     | 17.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 479      |
|    time_elapsed    | 53282    |
|    total_timesteps | 191600   |
---------------------------------
Eval num_timesteps=191800, episode_reward=34.25 +/- 0.83
Episode length: 50.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 34.2        |
| time/                   |             |
|    total_timesteps      | 191800      |
| train/                  |             |
|    approx_kl            | 0.002772712 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.615      |
|    explained_variance   | 0.322       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.519       |
|    n_updates            | 4790        |
|    policy_gradient_loss | -0.0067     |
|    value_loss           | 1.4         |
-----------------------------------------
Eval num_timesteps=192000, episode_reward=20.97 +/- 11.58
Episode length: 30.20 +/- 16.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 21       |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | 17.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 480      |
|    time_elapsed    | 53418    |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=192200, episode_reward=8.96 +/- 12.26
Episode length: 14.00 +/- 18.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14           |
|    mean_reward          | 8.96         |
| time/                   |              |
|    total_timesteps      | 192200       |
| train/                  |              |
|    approx_kl            | 0.0023142786 |
|    clip_fraction        | 0.176        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.598       |
|    explained_variance   | 0.154        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.588        |
|    n_updates            | 4800         |
|    policy_gradient_loss | -0.0105      |
|    value_loss           | 2.41         |
------------------------------------------
Eval num_timesteps=192400, episode_reward=18.36 +/- 14.71
Episode length: 26.60 +/- 20.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.6     |
|    mean_reward     | 18.4     |
| time/              |          |
|    total_timesteps | 192400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.7     |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 481      |
|    time_elapsed    | 53536    |
|    total_timesteps | 192400   |
---------------------------------
Eval num_timesteps=192600, episode_reward=18.08 +/- 14.54
Episode length: 26.00 +/- 19.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26           |
|    mean_reward          | 18.1         |
| time/                   |              |
|    total_timesteps      | 192600       |
| train/                  |              |
|    approx_kl            | 0.0021188937 |
|    clip_fraction        | 0.0929       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.647       |
|    explained_variance   | 0.181        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.12         |
|    n_updates            | 4810         |
|    policy_gradient_loss | -0.00807     |
|    value_loss           | 1.67         |
------------------------------------------
Eval num_timesteps=192800, episode_reward=16.20 +/- 14.34
Episode length: 24.80 +/- 21.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 16.2     |
| time/              |          |
|    total_timesteps | 192800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.2     |
|    ep_rew_mean     | 17.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 482      |
|    time_elapsed    | 53638    |
|    total_timesteps | 192800   |
---------------------------------
Eval num_timesteps=193000, episode_reward=9.38 +/- 12.61
Episode length: 14.40 +/- 17.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.4        |
|    mean_reward          | 9.38        |
| time/                   |             |
|    total_timesteps      | 193000      |
| train/                  |             |
|    approx_kl            | 0.002770416 |
|    clip_fraction        | 0.0915      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.609      |
|    explained_variance   | 0.246       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.52        |
|    n_updates            | 4820        |
|    policy_gradient_loss | -0.00659    |
|    value_loss           | 3.09        |
-----------------------------------------
Eval num_timesteps=193200, episode_reward=13.68 +/- 12.70
Episode length: 19.60 +/- 16.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 13.7     |
| time/              |          |
|    total_timesteps | 193200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.7     |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 483      |
|    time_elapsed    | 53743    |
|    total_timesteps | 193200   |
---------------------------------
Eval num_timesteps=193400, episode_reward=12.92 +/- 11.44
Episode length: 19.20 +/- 16.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 19.2         |
|    mean_reward          | 12.9         |
| time/                   |              |
|    total_timesteps      | 193400       |
| train/                  |              |
|    approx_kl            | 0.0028135127 |
|    clip_fraction        | 0.0871       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.658       |
|    explained_variance   | 0.175        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.33         |
|    n_updates            | 4830         |
|    policy_gradient_loss | -0.00861     |
|    value_loss           | 1.37         |
------------------------------------------
Eval num_timesteps=193600, episode_reward=22.81 +/- 15.25
Episode length: 32.80 +/- 21.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 22.8     |
| time/              |          |
|    total_timesteps | 193600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.4     |
|    ep_rew_mean     | 18.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 484      |
|    time_elapsed    | 53848    |
|    total_timesteps | 193600   |
---------------------------------
Eval num_timesteps=193800, episode_reward=15.09 +/- 11.94
Episode length: 21.80 +/- 15.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 21.8        |
|    mean_reward          | 15.1        |
| time/                   |             |
|    total_timesteps      | 193800      |
| train/                  |             |
|    approx_kl            | 0.002467135 |
|    clip_fraction        | 0.0768      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.663      |
|    explained_variance   | 0.314       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.171       |
|    n_updates            | 4840        |
|    policy_gradient_loss | -0.00487    |
|    value_loss           | 0.952       |
-----------------------------------------
Eval num_timesteps=194000, episode_reward=21.49 +/- 16.36
Episode length: 31.20 +/- 23.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.2     |
|    mean_reward     | 21.5     |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.8     |
|    ep_rew_mean     | 18.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 485      |
|    time_elapsed    | 53967    |
|    total_timesteps | 194000   |
---------------------------------
Eval num_timesteps=194200, episode_reward=5.38 +/- 2.29
Episode length: 8.40 +/- 3.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 8.4          |
|    mean_reward          | 5.38         |
| time/                   |              |
|    total_timesteps      | 194200       |
| train/                  |              |
|    approx_kl            | 0.0020841747 |
|    clip_fraction        | 0.0828       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.642       |
|    explained_variance   | -0.0212      |
|    learning_rate        | 0.0001       |
|    loss                 | 0.183        |
|    n_updates            | 4850         |
|    policy_gradient_loss | -0.00798     |
|    value_loss           | 2.78         |
------------------------------------------
Eval num_timesteps=194400, episode_reward=16.29 +/- 14.91
Episode length: 24.20 +/- 21.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 16.3     |
| time/              |          |
|    total_timesteps | 194400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.3     |
|    ep_rew_mean     | 17.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 486      |
|    time_elapsed    | 54052    |
|    total_timesteps | 194400   |
---------------------------------
Eval num_timesteps=194600, episode_reward=14.98 +/- 15.94
Episode length: 22.20 +/- 22.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 22.2        |
|    mean_reward          | 15          |
| time/                   |             |
|    total_timesteps      | 194600      |
| train/                  |             |
|    approx_kl            | 0.002049117 |
|    clip_fraction        | 0.0741      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.622      |
|    explained_variance   | 0.31        |
|    learning_rate        | 0.0001      |
|    loss                 | 2.07        |
|    n_updates            | 4860        |
|    policy_gradient_loss | -0.00519    |
|    value_loss           | 2.76        |
-----------------------------------------
Eval num_timesteps=194800, episode_reward=16.66 +/- 15.57
Episode length: 24.00 +/- 21.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 194800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.1     |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 487      |
|    time_elapsed    | 54180    |
|    total_timesteps | 194800   |
---------------------------------
Eval num_timesteps=195000, episode_reward=25.01 +/- 13.79
Episode length: 35.20 +/- 18.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.2         |
|    mean_reward          | 25           |
| time/                   |              |
|    total_timesteps      | 195000       |
| train/                  |              |
|    approx_kl            | 0.0036736599 |
|    clip_fraction        | 0.134        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.613       |
|    explained_variance   | 0.0748       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.305        |
|    n_updates            | 4870         |
|    policy_gradient_loss | -0.00631     |
|    value_loss           | 1.55         |
------------------------------------------
Eval num_timesteps=195200, episode_reward=22.54 +/- 15.11
Episode length: 32.80 +/- 21.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 22.5     |
| time/              |          |
|    total_timesteps | 195200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.7     |
|    ep_rew_mean     | 18.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 488      |
|    time_elapsed    | 54312    |
|    total_timesteps | 195200   |
---------------------------------
Eval num_timesteps=195400, episode_reward=25.49 +/- 12.77
Episode length: 36.80 +/- 17.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.8         |
|    mean_reward          | 25.5         |
| time/                   |              |
|    total_timesteps      | 195400       |
| train/                  |              |
|    approx_kl            | 0.0030056888 |
|    clip_fraction        | 0.114        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.63        |
|    explained_variance   | 0.101        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.21         |
|    n_updates            | 4880         |
|    policy_gradient_loss | -0.00768     |
|    value_loss           | 1.8          |
------------------------------------------
Eval num_timesteps=195600, episode_reward=15.87 +/- 15.28
Episode length: 23.40 +/- 21.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 15.9     |
| time/              |          |
|    total_timesteps | 195600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.9     |
|    ep_rew_mean     | 17.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 489      |
|    time_elapsed    | 54455    |
|    total_timesteps | 195600   |
---------------------------------
Eval num_timesteps=195800, episode_reward=28.11 +/- 8.28
Episode length: 40.60 +/- 12.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 40.6         |
|    mean_reward          | 28.1         |
| time/                   |              |
|    total_timesteps      | 195800       |
| train/                  |              |
|    approx_kl            | 0.0010972228 |
|    clip_fraction        | 0.0453       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.619       |
|    explained_variance   | 0.337        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.965        |
|    n_updates            | 4890         |
|    policy_gradient_loss | -0.00582     |
|    value_loss           | 2.35         |
------------------------------------------
Eval num_timesteps=196000, episode_reward=6.20 +/- 3.99
Episode length: 9.80 +/- 5.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.8      |
|    mean_reward     | 6.2      |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.2     |
|    ep_rew_mean     | 15.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 490      |
|    time_elapsed    | 54552    |
|    total_timesteps | 196000   |
---------------------------------
Eval num_timesteps=196200, episode_reward=21.69 +/- 15.66
Episode length: 31.80 +/- 22.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 31.8         |
|    mean_reward          | 21.7         |
| time/                   |              |
|    total_timesteps      | 196200       |
| train/                  |              |
|    approx_kl            | 0.0028442726 |
|    clip_fraction        | 0.0629       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.627       |
|    explained_variance   | 0.443        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.291        |
|    n_updates            | 4900         |
|    policy_gradient_loss | -0.00495     |
|    value_loss           | 2.11         |
------------------------------------------
Eval num_timesteps=196400, episode_reward=29.08 +/- 14.15
Episode length: 40.40 +/- 19.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.4     |
|    mean_reward     | 29.1     |
| time/              |          |
|    total_timesteps | 196400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23       |
|    ep_rew_mean     | 15.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 491      |
|    time_elapsed    | 54683    |
|    total_timesteps | 196400   |
---------------------------------
Eval num_timesteps=196600, episode_reward=16.85 +/- 14.90
Episode length: 24.60 +/- 20.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.6         |
|    mean_reward          | 16.8         |
| time/                   |              |
|    total_timesteps      | 196600       |
| train/                  |              |
|    approx_kl            | 0.0024591635 |
|    clip_fraction        | 0.098        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.637       |
|    explained_variance   | 0.228        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.657        |
|    n_updates            | 4910         |
|    policy_gradient_loss | -0.00719     |
|    value_loss           | 2.1          |
------------------------------------------
Eval num_timesteps=196800, episode_reward=7.06 +/- 4.11
Episode length: 11.00 +/- 6.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 11       |
|    mean_reward     | 7.06     |
| time/              |          |
|    total_timesteps | 196800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 492      |
|    time_elapsed    | 54771    |
|    total_timesteps | 196800   |
---------------------------------
Eval num_timesteps=197000, episode_reward=6.35 +/- 3.23
Episode length: 9.80 +/- 4.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 9.8          |
|    mean_reward          | 6.35         |
| time/                   |              |
|    total_timesteps      | 197000       |
| train/                  |              |
|    approx_kl            | 0.0025780287 |
|    clip_fraction        | 0.106        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.615       |
|    explained_variance   | 0.226        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.86         |
|    n_updates            | 4920         |
|    policy_gradient_loss | -0.00746     |
|    value_loss           | 2.08         |
------------------------------------------
Eval num_timesteps=197200, episode_reward=17.26 +/- 14.21
Episode length: 25.40 +/- 20.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | 17.3     |
| time/              |          |
|    total_timesteps | 197200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 15       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 493      |
|    time_elapsed    | 54865    |
|    total_timesteps | 197200   |
---------------------------------
Eval num_timesteps=197400, episode_reward=21.90 +/- 12.65
Episode length: 31.60 +/- 17.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 31.6         |
|    mean_reward          | 21.9         |
| time/                   |              |
|    total_timesteps      | 197400       |
| train/                  |              |
|    approx_kl            | 0.0019647584 |
|    clip_fraction        | 0.106        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.624       |
|    explained_variance   | 0.374        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.597        |
|    n_updates            | 4930         |
|    policy_gradient_loss | -0.00688     |
|    value_loss           | 2.3          |
------------------------------------------
Eval num_timesteps=197600, episode_reward=21.94 +/- 15.37
Episode length: 32.20 +/- 21.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 21.9     |
| time/              |          |
|    total_timesteps | 197600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.5     |
|    ep_rew_mean     | 15.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 494      |
|    time_elapsed    | 54973    |
|    total_timesteps | 197600   |
---------------------------------
Eval num_timesteps=197800, episode_reward=21.28 +/- 17.09
Episode length: 30.60 +/- 23.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30.6         |
|    mean_reward          | 21.3         |
| time/                   |              |
|    total_timesteps      | 197800       |
| train/                  |              |
|    approx_kl            | 0.0028100947 |
|    clip_fraction        | 0.103        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.632       |
|    explained_variance   | 0.447        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.43         |
|    n_updates            | 4940         |
|    policy_gradient_loss | -0.00725     |
|    value_loss           | 1.55         |
------------------------------------------
Eval num_timesteps=198000, episode_reward=9.21 +/- 6.23
Episode length: 14.00 +/- 9.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14       |
|    mean_reward     | 9.21     |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.2     |
|    ep_rew_mean     | 15.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 495      |
|    time_elapsed    | 55066    |
|    total_timesteps | 198000   |
---------------------------------
Eval num_timesteps=198200, episode_reward=29.29 +/- 13.16
Episode length: 41.00 +/- 18.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41           |
|    mean_reward          | 29.3         |
| time/                   |              |
|    total_timesteps      | 198200       |
| train/                  |              |
|    approx_kl            | 0.0016874576 |
|    clip_fraction        | 0.0411       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.603       |
|    explained_variance   | 0.241        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.704        |
|    n_updates            | 4950         |
|    policy_gradient_loss | -0.00472     |
|    value_loss           | 3.1          |
------------------------------------------
Eval num_timesteps=198400, episode_reward=6.50 +/- 4.75
Episode length: 10.20 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10.2     |
|    mean_reward     | 6.5      |
| time/              |          |
|    total_timesteps | 198400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.4     |
|    ep_rew_mean     | 15.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 496      |
|    time_elapsed    | 55158    |
|    total_timesteps | 198400   |
---------------------------------
Eval num_timesteps=198600, episode_reward=24.25 +/- 13.38
Episode length: 35.40 +/- 18.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.4        |
|    mean_reward          | 24.2        |
| time/                   |             |
|    total_timesteps      | 198600      |
| train/                  |             |
|    approx_kl            | 0.002091625 |
|    clip_fraction        | 0.0658      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.614      |
|    explained_variance   | 0.17        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.673       |
|    n_updates            | 4960        |
|    policy_gradient_loss | -0.00786    |
|    value_loss           | 2.3         |
-----------------------------------------
Eval num_timesteps=198800, episode_reward=11.46 +/- 13.45
Episode length: 16.80 +/- 18.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | 11.5     |
| time/              |          |
|    total_timesteps | 198800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.1     |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 497      |
|    time_elapsed    | 55272    |
|    total_timesteps | 198800   |
---------------------------------
Eval num_timesteps=199000, episode_reward=10.20 +/- 11.70
Episode length: 15.80 +/- 17.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | 10.2         |
| time/                   |              |
|    total_timesteps      | 199000       |
| train/                  |              |
|    approx_kl            | 0.0028701532 |
|    clip_fraction        | 0.0513       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.607       |
|    explained_variance   | 0.411        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.993        |
|    n_updates            | 4970         |
|    policy_gradient_loss | -0.00374     |
|    value_loss           | 2.28         |
------------------------------------------
Eval num_timesteps=199200, episode_reward=17.55 +/- 14.60
Episode length: 25.80 +/- 20.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | 17.5     |
| time/              |          |
|    total_timesteps | 199200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.1     |
|    ep_rew_mean     | 15.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 498      |
|    time_elapsed    | 55375    |
|    total_timesteps | 199200   |
---------------------------------
Eval num_timesteps=199400, episode_reward=15.82 +/- 11.60
Episode length: 23.20 +/- 15.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.2         |
|    mean_reward          | 15.8         |
| time/                   |              |
|    total_timesteps      | 199400       |
| train/                  |              |
|    approx_kl            | 0.0034667067 |
|    clip_fraction        | 0.191        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.626       |
|    explained_variance   | 0.156        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.859        |
|    n_updates            | 4980         |
|    policy_gradient_loss | -0.00875     |
|    value_loss           | 2.41         |
------------------------------------------
Eval num_timesteps=199600, episode_reward=10.57 +/- 11.57
Episode length: 16.20 +/- 17.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | 10.6     |
| time/              |          |
|    total_timesteps | 199600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.5     |
|    ep_rew_mean     | 16.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 499      |
|    time_elapsed    | 55477    |
|    total_timesteps | 199600   |
---------------------------------
Eval num_timesteps=199800, episode_reward=16.93 +/- 15.28
Episode length: 24.40 +/- 20.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 24.4        |
|    mean_reward          | 16.9        |
| time/                   |             |
|    total_timesteps      | 199800      |
| train/                  |             |
|    approx_kl            | 0.002792957 |
|    clip_fraction        | 0.0786      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.611      |
|    explained_variance   | 0.454       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.679       |
|    n_updates            | 4990        |
|    policy_gradient_loss | -0.00628    |
|    value_loss           | 1.32        |
-----------------------------------------
Eval num_timesteps=200000, episode_reward=4.20 +/- 2.26
Episode length: 7.00 +/- 3.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7        |
|    mean_reward     | 4.2      |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.3     |
|    ep_rew_mean     | 16       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 500      |
|    time_elapsed    | 55563    |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=200200, episode_reward=22.18 +/- 15.11
Episode length: 32.60 +/- 21.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.6         |
|    mean_reward          | 22.2         |
| time/                   |              |
|    total_timesteps      | 200200       |
| train/                  |              |
|    approx_kl            | 0.0017494962 |
|    clip_fraction        | 0.052        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.586       |
|    explained_variance   | 0.444        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.175        |
|    n_updates            | 5000         |
|    policy_gradient_loss | -0.00491     |
|    value_loss           | 1.58         |
------------------------------------------
Eval num_timesteps=200400, episode_reward=16.29 +/- 16.31
Episode length: 23.20 +/- 22.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 16.3     |
| time/              |          |
|    total_timesteps | 200400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.2     |
|    ep_rew_mean     | 16.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 501      |
|    time_elapsed    | 55692    |
|    total_timesteps | 200400   |
---------------------------------
Eval num_timesteps=200600, episode_reward=19.68 +/- 13.33
Episode length: 28.20 +/- 18.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 28.2         |
|    mean_reward          | 19.7         |
| time/                   |              |
|    total_timesteps      | 200600       |
| train/                  |              |
|    approx_kl            | 0.0019848384 |
|    clip_fraction        | 0.0393       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.591       |
|    explained_variance   | 0.368        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.411        |
|    n_updates            | 5010         |
|    policy_gradient_loss | -0.00464     |
|    value_loss           | 1.34         |
------------------------------------------
Eval num_timesteps=200800, episode_reward=22.20 +/- 15.53
Episode length: 32.20 +/- 21.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 22.2     |
| time/              |          |
|    total_timesteps | 200800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.7     |
|    ep_rew_mean     | 17       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 502      |
|    time_elapsed    | 55801    |
|    total_timesteps | 200800   |
---------------------------------
Eval num_timesteps=201000, episode_reward=17.64 +/- 14.89
Episode length: 26.00 +/- 20.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 26          |
|    mean_reward          | 17.6        |
| time/                   |             |
|    total_timesteps      | 201000      |
| train/                  |             |
|    approx_kl            | 0.001240716 |
|    clip_fraction        | 0.0455      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.614      |
|    explained_variance   | 0.41        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.475       |
|    n_updates            | 5020        |
|    policy_gradient_loss | -0.00324    |
|    value_loss           | 1.74        |
-----------------------------------------
Eval num_timesteps=201200, episode_reward=15.99 +/- 14.64
Episode length: 24.00 +/- 21.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 201200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | 17.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 503      |
|    time_elapsed    | 55908    |
|    total_timesteps | 201200   |
---------------------------------
Eval num_timesteps=201400, episode_reward=14.00 +/- 10.86
Episode length: 20.80 +/- 15.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 20.8        |
|    mean_reward          | 14          |
| time/                   |             |
|    total_timesteps      | 201400      |
| train/                  |             |
|    approx_kl            | 0.002947529 |
|    clip_fraction        | 0.0813      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.619      |
|    explained_variance   | 0.527       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.385       |
|    n_updates            | 5030        |
|    policy_gradient_loss | -0.0057     |
|    value_loss           | 1.8         |
-----------------------------------------
Eval num_timesteps=201600, episode_reward=8.85 +/- 13.94
Episode length: 13.00 +/- 18.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13       |
|    mean_reward     | 8.85     |
| time/              |          |
|    total_timesteps | 201600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | 17.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 504      |
|    time_elapsed    | 56011    |
|    total_timesteps | 201600   |
---------------------------------
Eval num_timesteps=201800, episode_reward=19.62 +/- 14.10
Episode length: 28.60 +/- 19.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 28.6         |
|    mean_reward          | 19.6         |
| time/                   |              |
|    total_timesteps      | 201800       |
| train/                  |              |
|    approx_kl            | 0.0013158976 |
|    clip_fraction        | 0.0437       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.605       |
|    explained_variance   | 0.463        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.24         |
|    n_updates            | 5040         |
|    policy_gradient_loss | -0.00429     |
|    value_loss           | 1.45         |
------------------------------------------
Eval num_timesteps=202000, episode_reward=23.36 +/- 14.08
Episode length: 33.80 +/- 19.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 23.4     |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | 16.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 505      |
|    time_elapsed    | 56123    |
|    total_timesteps | 202000   |
---------------------------------
Eval num_timesteps=202200, episode_reward=29.06 +/- 10.84
Episode length: 42.20 +/- 15.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.2         |
|    mean_reward          | 29.1         |
| time/                   |              |
|    total_timesteps      | 202200       |
| train/                  |              |
|    approx_kl            | 0.0018757203 |
|    clip_fraction        | 0.0442       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.596       |
|    explained_variance   | 0.404        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.789        |
|    n_updates            | 5050         |
|    policy_gradient_loss | -0.00406     |
|    value_loss           | 2.76         |
------------------------------------------
Eval num_timesteps=202400, episode_reward=17.46 +/- 14.50
Episode length: 25.40 +/- 20.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | 17.5     |
| time/              |          |
|    total_timesteps | 202400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.2     |
|    ep_rew_mean     | 16.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 506      |
|    time_elapsed    | 56254    |
|    total_timesteps | 202400   |
---------------------------------
Eval num_timesteps=202600, episode_reward=22.52 +/- 15.16
Episode length: 32.80 +/- 21.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 32.8        |
|    mean_reward          | 22.5        |
| time/                   |             |
|    total_timesteps      | 202600      |
| train/                  |             |
|    approx_kl            | 0.001205176 |
|    clip_fraction        | 0.0346      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.608      |
|    explained_variance   | 0.317       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.12        |
|    n_updates            | 5060        |
|    policy_gradient_loss | -0.00405    |
|    value_loss           | 0.793       |
-----------------------------------------
Eval num_timesteps=202800, episode_reward=17.65 +/- 14.51
Episode length: 25.60 +/- 20.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 202800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.5     |
|    ep_rew_mean     | 16.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 507      |
|    time_elapsed    | 56385    |
|    total_timesteps | 202800   |
---------------------------------
Eval num_timesteps=203000, episode_reward=9.71 +/- 8.36
Episode length: 15.00 +/- 12.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15          |
|    mean_reward          | 9.71        |
| time/                   |             |
|    total_timesteps      | 203000      |
| train/                  |             |
|    approx_kl            | 0.001455293 |
|    clip_fraction        | 0.0462      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.622      |
|    explained_variance   | 0.479       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.323       |
|    n_updates            | 5070        |
|    policy_gradient_loss | -0.00613    |
|    value_loss           | 1.63        |
-----------------------------------------
Eval num_timesteps=203200, episode_reward=10.66 +/- 13.05
Episode length: 15.60 +/- 17.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 10.7     |
| time/              |          |
|    total_timesteps | 203200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.6     |
|    ep_rew_mean     | 15.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 508      |
|    time_elapsed    | 56481    |
|    total_timesteps | 203200   |
---------------------------------
Eval num_timesteps=203400, episode_reward=15.12 +/- 10.45
Episode length: 22.00 +/- 14.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22           |
|    mean_reward          | 15.1         |
| time/                   |              |
|    total_timesteps      | 203400       |
| train/                  |              |
|    approx_kl            | 0.0011529851 |
|    clip_fraction        | 0.0368       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.612       |
|    explained_variance   | 0.314        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.12         |
|    n_updates            | 5080         |
|    policy_gradient_loss | -0.00511     |
|    value_loss           | 3.33         |
------------------------------------------
Eval num_timesteps=203600, episode_reward=11.24 +/- 12.87
Episode length: 17.20 +/- 18.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | 11.2     |
| time/              |          |
|    total_timesteps | 203600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.3     |
|    ep_rew_mean     | 15.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 509      |
|    time_elapsed    | 56589    |
|    total_timesteps | 203600   |
---------------------------------
Eval num_timesteps=203800, episode_reward=20.48 +/- 12.44
Episode length: 29.40 +/- 16.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29.4         |
|    mean_reward          | 20.5         |
| time/                   |              |
|    total_timesteps      | 203800       |
| train/                  |              |
|    approx_kl            | 0.0028938267 |
|    clip_fraction        | 0.0788       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.612       |
|    explained_variance   | 0.309        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.37         |
|    n_updates            | 5090         |
|    policy_gradient_loss | -0.00377     |
|    value_loss           | 1.73         |
------------------------------------------
Eval num_timesteps=204000, episode_reward=15.37 +/- 15.34
Episode length: 23.20 +/- 22.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 15.4     |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.1     |
|    ep_rew_mean     | 15       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 510      |
|    time_elapsed    | 56698    |
|    total_timesteps | 204000   |
---------------------------------
Eval num_timesteps=204200, episode_reward=22.41 +/- 16.16
Episode length: 31.80 +/- 22.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 31.8         |
|    mean_reward          | 22.4         |
| time/                   |              |
|    total_timesteps      | 204200       |
| train/                  |              |
|    approx_kl            | 0.0017688145 |
|    clip_fraction        | 0.0469       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.606       |
|    explained_variance   | 0.377        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.492        |
|    n_updates            | 5100         |
|    policy_gradient_loss | -0.00458     |
|    value_loss           | 2.51         |
------------------------------------------
Eval num_timesteps=204400, episode_reward=16.40 +/- 15.22
Episode length: 24.00 +/- 21.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16.4     |
| time/              |          |
|    total_timesteps | 204400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.7     |
|    ep_rew_mean     | 14.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 511      |
|    time_elapsed    | 56801    |
|    total_timesteps | 204400   |
---------------------------------
Eval num_timesteps=204600, episode_reward=19.04 +/- 12.08
Episode length: 28.60 +/- 18.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 28.6         |
|    mean_reward          | 19           |
| time/                   |              |
|    total_timesteps      | 204600       |
| train/                  |              |
|    approx_kl            | 0.0015810796 |
|    clip_fraction        | 0.0446       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.614       |
|    explained_variance   | 0.321        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.06         |
|    n_updates            | 5110         |
|    policy_gradient_loss | -0.00541     |
|    value_loss           | 2.53         |
------------------------------------------
Eval num_timesteps=204800, episode_reward=17.51 +/- 13.64
Episode length: 26.20 +/- 20.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | 17.5     |
| time/              |          |
|    total_timesteps | 204800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | 13.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 512      |
|    time_elapsed    | 56913    |
|    total_timesteps | 204800   |
---------------------------------
Eval num_timesteps=205000, episode_reward=12.56 +/- 12.64
Episode length: 18.00 +/- 16.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18           |
|    mean_reward          | 12.6         |
| time/                   |              |
|    total_timesteps      | 205000       |
| train/                  |              |
|    approx_kl            | 0.0022049216 |
|    clip_fraction        | 0.0908       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.624       |
|    explained_variance   | 0.497        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.383        |
|    n_updates            | 5120         |
|    policy_gradient_loss | -0.00702     |
|    value_loss           | 1.74         |
------------------------------------------
Eval num_timesteps=205200, episode_reward=18.10 +/- 14.56
Episode length: 26.00 +/- 19.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | 18.1     |
| time/              |          |
|    total_timesteps | 205200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.6     |
|    ep_rew_mean     | 14.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 513      |
|    time_elapsed    | 57022    |
|    total_timesteps | 205200   |
---------------------------------
Eval num_timesteps=205400, episode_reward=19.06 +/- 14.35
Episode length: 27.20 +/- 19.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 27.2         |
|    mean_reward          | 19.1         |
| time/                   |              |
|    total_timesteps      | 205400       |
| train/                  |              |
|    approx_kl            | 0.0015763693 |
|    clip_fraction        | 0.0712       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.613       |
|    explained_variance   | 0.27         |
|    learning_rate        | 0.0001       |
|    loss                 | 1.87         |
|    n_updates            | 5130         |
|    policy_gradient_loss | -0.00605     |
|    value_loss           | 1.92         |
------------------------------------------
Eval num_timesteps=205600, episode_reward=17.53 +/- 15.34
Episode length: 25.00 +/- 20.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | 17.5     |
| time/              |          |
|    total_timesteps | 205600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | 14       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 514      |
|    time_elapsed    | 57136    |
|    total_timesteps | 205600   |
---------------------------------
Eval num_timesteps=205800, episode_reward=21.24 +/- 11.96
Episode length: 30.60 +/- 16.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30.6         |
|    mean_reward          | 21.2         |
| time/                   |              |
|    total_timesteps      | 205800       |
| train/                  |              |
|    approx_kl            | 0.0024284092 |
|    clip_fraction        | 0.0286       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.604       |
|    explained_variance   | 0.53         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.362        |
|    n_updates            | 5140         |
|    policy_gradient_loss | -0.00437     |
|    value_loss           | 1.75         |
------------------------------------------
Eval num_timesteps=206000, episode_reward=16.90 +/- 15.78
Episode length: 24.20 +/- 21.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.9     |
|    ep_rew_mean     | 14.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 515      |
|    time_elapsed    | 57244    |
|    total_timesteps | 206000   |
---------------------------------
Eval num_timesteps=206200, episode_reward=9.45 +/- 9.65
Episode length: 14.00 +/- 13.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14           |
|    mean_reward          | 9.45         |
| time/                   |              |
|    total_timesteps      | 206200       |
| train/                  |              |
|    approx_kl            | 0.0012780711 |
|    clip_fraction        | 0.0768       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.605       |
|    explained_variance   | 0.484        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.686        |
|    n_updates            | 5150         |
|    policy_gradient_loss | -0.00436     |
|    value_loss           | 2.05         |
------------------------------------------
Eval num_timesteps=206400, episode_reward=4.27 +/- 2.28
Episode length: 7.00 +/- 3.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7        |
|    mean_reward     | 4.27     |
| time/              |          |
|    total_timesteps | 206400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.4     |
|    ep_rew_mean     | 14.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 516      |
|    time_elapsed    | 57328    |
|    total_timesteps | 206400   |
---------------------------------
Eval num_timesteps=206600, episode_reward=11.75 +/- 11.92
Episode length: 17.60 +/- 16.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.6         |
|    mean_reward          | 11.8         |
| time/                   |              |
|    total_timesteps      | 206600       |
| train/                  |              |
|    approx_kl            | 0.0017778996 |
|    clip_fraction        | 0.0328       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.614       |
|    explained_variance   | 0.421        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.862        |
|    n_updates            | 5160         |
|    policy_gradient_loss | -0.00413     |
|    value_loss           | 1.53         |
------------------------------------------
Eval num_timesteps=206800, episode_reward=17.11 +/- 14.77
Episode length: 25.20 +/- 21.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 17.1     |
| time/              |          |
|    total_timesteps | 206800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23       |
|    ep_rew_mean     | 15.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 517      |
|    time_elapsed    | 57445    |
|    total_timesteps | 206800   |
---------------------------------
Eval num_timesteps=207000, episode_reward=9.96 +/- 13.48
Episode length: 14.40 +/- 17.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.4         |
|    mean_reward          | 9.96         |
| time/                   |              |
|    total_timesteps      | 207000       |
| train/                  |              |
|    approx_kl            | 0.0046004406 |
|    clip_fraction        | 0.153        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.644       |
|    explained_variance   | 0.143        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.455        |
|    n_updates            | 5170         |
|    policy_gradient_loss | -0.00717     |
|    value_loss           | 2.23         |
------------------------------------------
Eval num_timesteps=207200, episode_reward=17.16 +/- 16.10
Episode length: 24.00 +/- 21.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 17.2     |
| time/              |          |
|    total_timesteps | 207200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.7     |
|    ep_rew_mean     | 15.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 518      |
|    time_elapsed    | 57555    |
|    total_timesteps | 207200   |
---------------------------------
Eval num_timesteps=207400, episode_reward=15.68 +/- 16.29
Episode length: 22.60 +/- 22.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22.6         |
|    mean_reward          | 15.7         |
| time/                   |              |
|    total_timesteps      | 207400       |
| train/                  |              |
|    approx_kl            | 0.0033294023 |
|    clip_fraction        | 0.112        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.644       |
|    explained_variance   | 0.211        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.18         |
|    n_updates            | 5180         |
|    policy_gradient_loss | -0.00566     |
|    value_loss           | 1.93         |
------------------------------------------
Eval num_timesteps=207600, episode_reward=4.60 +/- 2.69
Episode length: 7.40 +/- 3.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7.4      |
|    mean_reward     | 4.6      |
| time/              |          |
|    total_timesteps | 207600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.2     |
|    ep_rew_mean     | 15.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 519      |
|    time_elapsed    | 57646    |
|    total_timesteps | 207600   |
---------------------------------
Eval num_timesteps=207800, episode_reward=29.30 +/- 12.60
Episode length: 41.40 +/- 17.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.4         |
|    mean_reward          | 29.3         |
| time/                   |              |
|    total_timesteps      | 207800       |
| train/                  |              |
|    approx_kl            | 0.0027154658 |
|    clip_fraction        | 0.085        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.637       |
|    explained_variance   | 0.365        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.2          |
|    n_updates            | 5190         |
|    policy_gradient_loss | -0.00881     |
|    value_loss           | 2.09         |
------------------------------------------
Eval num_timesteps=208000, episode_reward=22.85 +/- 15.34
Episode length: 32.80 +/- 21.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 22.9     |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24       |
|    ep_rew_mean     | 16.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 520      |
|    time_elapsed    | 57776    |
|    total_timesteps | 208000   |
---------------------------------
Eval num_timesteps=208200, episode_reward=11.54 +/- 12.11
Episode length: 17.00 +/- 16.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17           |
|    mean_reward          | 11.5         |
| time/                   |              |
|    total_timesteps      | 208200       |
| train/                  |              |
|    approx_kl            | 0.0026536915 |
|    clip_fraction        | 0.102        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.644       |
|    explained_variance   | 0.154        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.629        |
|    n_updates            | 5200         |
|    policy_gradient_loss | -0.0091      |
|    value_loss           | 2.35         |
------------------------------------------
Eval num_timesteps=208400, episode_reward=15.72 +/- 14.94
Episode length: 23.60 +/- 21.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 15.7     |
| time/              |          |
|    total_timesteps | 208400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | 17       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 521      |
|    time_elapsed    | 57882    |
|    total_timesteps | 208400   |
---------------------------------
Eval num_timesteps=208600, episode_reward=16.15 +/- 16.43
Episode length: 23.00 +/- 22.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23           |
|    mean_reward          | 16.2         |
| time/                   |              |
|    total_timesteps      | 208600       |
| train/                  |              |
|    approx_kl            | 0.0028266401 |
|    clip_fraction        | 0.0712       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.643       |
|    explained_variance   | 0.231        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.482        |
|    n_updates            | 5210         |
|    policy_gradient_loss | -0.00631     |
|    value_loss           | 1.69         |
------------------------------------------
Eval num_timesteps=208800, episode_reward=19.25 +/- 13.78
Episode length: 27.80 +/- 18.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.8     |
|    mean_reward     | 19.3     |
| time/              |          |
|    total_timesteps | 208800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.5     |
|    ep_rew_mean     | 16.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 522      |
|    time_elapsed    | 57984    |
|    total_timesteps | 208800   |
---------------------------------
Eval num_timesteps=209000, episode_reward=19.05 +/- 14.44
Episode length: 27.40 +/- 19.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 27.4         |
|    mean_reward          | 19           |
| time/                   |              |
|    total_timesteps      | 209000       |
| train/                  |              |
|    approx_kl            | 0.0016062582 |
|    clip_fraction        | 0.0502       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.63        |
|    explained_variance   | 0.321        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.763        |
|    n_updates            | 5220         |
|    policy_gradient_loss | -0.00568     |
|    value_loss           | 2.55         |
------------------------------------------
Eval num_timesteps=209200, episode_reward=18.61 +/- 12.75
Episode length: 27.60 +/- 18.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.6     |
|    mean_reward     | 18.6     |
| time/              |          |
|    total_timesteps | 209200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.1     |
|    ep_rew_mean     | 15.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 523      |
|    time_elapsed    | 58094    |
|    total_timesteps | 209200   |
---------------------------------
Eval num_timesteps=209400, episode_reward=17.55 +/- 14.91
Episode length: 25.20 +/- 20.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.2         |
|    mean_reward          | 17.5         |
| time/                   |              |
|    total_timesteps      | 209400       |
| train/                  |              |
|    approx_kl            | 0.0026490737 |
|    clip_fraction        | 0.117        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.62        |
|    explained_variance   | 0.324        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.979        |
|    n_updates            | 5230         |
|    policy_gradient_loss | -0.00855     |
|    value_loss           | 2.64         |
------------------------------------------
Eval num_timesteps=209600, episode_reward=17.77 +/- 13.86
Episode length: 26.20 +/- 19.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | 17.8     |
| time/              |          |
|    total_timesteps | 209600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.6     |
|    ep_rew_mean     | 15.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 524      |
|    time_elapsed    | 58223    |
|    total_timesteps | 209600   |
---------------------------------
Eval num_timesteps=209800, episode_reward=16.20 +/- 14.99
Episode length: 24.00 +/- 21.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24           |
|    mean_reward          | 16.2         |
| time/                   |              |
|    total_timesteps      | 209800       |
| train/                  |              |
|    approx_kl            | 0.0022648044 |
|    clip_fraction        | 0.0665       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.622       |
|    explained_variance   | 0.242        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.17         |
|    n_updates            | 5240         |
|    policy_gradient_loss | -0.00651     |
|    value_loss           | 2.25         |
------------------------------------------
Eval num_timesteps=210000, episode_reward=24.09 +/- 12.84
Episode length: 35.20 +/- 18.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 24.1     |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.3     |
|    ep_rew_mean     | 14.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 525      |
|    time_elapsed    | 58332    |
|    total_timesteps | 210000   |
---------------------------------
Eval num_timesteps=210200, episode_reward=23.30 +/- 14.65
Episode length: 33.60 +/- 20.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.6         |
|    mean_reward          | 23.3         |
| time/                   |              |
|    total_timesteps      | 210200       |
| train/                  |              |
|    approx_kl            | 0.0028237435 |
|    clip_fraction        | 0.144        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.615       |
|    explained_variance   | 0.446        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.577        |
|    n_updates            | 5250         |
|    policy_gradient_loss | -0.00908     |
|    value_loss           | 2.3          |
------------------------------------------
Eval num_timesteps=210400, episode_reward=25.44 +/- 13.88
Episode length: 35.60 +/- 18.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 25.4     |
| time/              |          |
|    total_timesteps | 210400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.8     |
|    ep_rew_mean     | 13.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 526      |
|    time_elapsed    | 58441    |
|    total_timesteps | 210400   |
---------------------------------
Eval num_timesteps=210600, episode_reward=9.05 +/- 12.20
Episode length: 14.20 +/- 17.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.2        |
|    mean_reward          | 9.05        |
| time/                   |             |
|    total_timesteps      | 210600      |
| train/                  |             |
|    approx_kl            | 0.001874162 |
|    clip_fraction        | 0.0621      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.634      |
|    explained_variance   | 0.434       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.24        |
|    n_updates            | 5260        |
|    policy_gradient_loss | -0.00726    |
|    value_loss           | 2.39        |
-----------------------------------------
Eval num_timesteps=210800, episode_reward=11.41 +/- 11.82
Episode length: 17.20 +/- 16.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | 11.4     |
| time/              |          |
|    total_timesteps | 210800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.8     |
|    ep_rew_mean     | 13.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 527      |
|    time_elapsed    | 58546    |
|    total_timesteps | 210800   |
---------------------------------
Eval num_timesteps=211000, episode_reward=6.73 +/- 3.43
Episode length: 10.60 +/- 4.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 10.6         |
|    mean_reward          | 6.73         |
| time/                   |              |
|    total_timesteps      | 211000       |
| train/                  |              |
|    approx_kl            | 0.0017426583 |
|    clip_fraction        | 0.0549       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.621       |
|    explained_variance   | 0.239        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.54         |
|    n_updates            | 5270         |
|    policy_gradient_loss | -0.00668     |
|    value_loss           | 2.78         |
------------------------------------------
Eval num_timesteps=211200, episode_reward=18.05 +/- 13.10
Episode length: 27.60 +/- 19.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.6     |
|    mean_reward     | 18.1     |
| time/              |          |
|    total_timesteps | 211200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.4     |
|    ep_rew_mean     | 14.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 528      |
|    time_elapsed    | 58632    |
|    total_timesteps | 211200   |
---------------------------------
Eval num_timesteps=211400, episode_reward=17.95 +/- 13.86
Episode length: 26.20 +/- 19.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.2         |
|    mean_reward          | 18           |
| time/                   |              |
|    total_timesteps      | 211400       |
| train/                  |              |
|    approx_kl            | 0.0034673538 |
|    clip_fraction        | 0.0953       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.638       |
|    explained_variance   | 0.266        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.017        |
|    n_updates            | 5280         |
|    policy_gradient_loss | -0.00764     |
|    value_loss           | 1.17         |
------------------------------------------
Eval num_timesteps=211600, episode_reward=13.86 +/- 9.91
Episode length: 21.00 +/- 14.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | 13.9     |
| time/              |          |
|    total_timesteps | 211600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.8     |
|    ep_rew_mean     | 15       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 529      |
|    time_elapsed    | 58735    |
|    total_timesteps | 211600   |
---------------------------------
Eval num_timesteps=211800, episode_reward=18.36 +/- 14.56
Episode length: 26.00 +/- 19.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26           |
|    mean_reward          | 18.4         |
| time/                   |              |
|    total_timesteps      | 211800       |
| train/                  |              |
|    approx_kl            | 0.0041563963 |
|    clip_fraction        | 0.152        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.636       |
|    explained_variance   | 0.354        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.57         |
|    n_updates            | 5290         |
|    policy_gradient_loss | -0.00587     |
|    value_loss           | 2.34         |
------------------------------------------
Eval num_timesteps=212000, episode_reward=8.67 +/- 12.45
Episode length: 13.60 +/- 18.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.6     |
|    mean_reward     | 8.67     |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.4     |
|    ep_rew_mean     | 13.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 530      |
|    time_elapsed    | 58864    |
|    total_timesteps | 212000   |
---------------------------------
Eval num_timesteps=212200, episode_reward=23.85 +/- 13.81
Episode length: 34.80 +/- 19.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.8         |
|    mean_reward          | 23.8         |
| time/                   |              |
|    total_timesteps      | 212200       |
| train/                  |              |
|    approx_kl            | 0.0018193228 |
|    clip_fraction        | 0.0451       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.623       |
|    explained_variance   | 0.457        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.344        |
|    n_updates            | 5300         |
|    policy_gradient_loss | -0.00624     |
|    value_loss           | 2.17         |
------------------------------------------
Eval num_timesteps=212400, episode_reward=8.60 +/- 12.98
Episode length: 13.20 +/- 18.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.2     |
|    mean_reward     | 8.6      |
| time/              |          |
|    total_timesteps | 212400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.4     |
|    ep_rew_mean     | 14.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 531      |
|    time_elapsed    | 58970    |
|    total_timesteps | 212400   |
---------------------------------
Eval num_timesteps=212600, episode_reward=29.36 +/- 9.70
Episode length: 43.00 +/- 14.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 43           |
|    mean_reward          | 29.4         |
| time/                   |              |
|    total_timesteps      | 212600       |
| train/                  |              |
|    approx_kl            | 0.0028557794 |
|    clip_fraction        | 0.123        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.624       |
|    explained_variance   | 0.486        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.843        |
|    n_updates            | 5310         |
|    policy_gradient_loss | -0.00775     |
|    value_loss           | 1.72         |
------------------------------------------
Eval num_timesteps=212800, episode_reward=28.09 +/- 10.72
Episode length: 40.40 +/- 14.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.4     |
|    mean_reward     | 28.1     |
| time/              |          |
|    total_timesteps | 212800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.9     |
|    ep_rew_mean     | 14.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 532      |
|    time_elapsed    | 59123    |
|    total_timesteps | 212800   |
---------------------------------
Eval num_timesteps=213000, episode_reward=25.29 +/- 13.60
Episode length: 35.80 +/- 18.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.8         |
|    mean_reward          | 25.3         |
| time/                   |              |
|    total_timesteps      | 213000       |
| train/                  |              |
|    approx_kl            | 0.0021231833 |
|    clip_fraction        | 0.0886       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.612       |
|    explained_variance   | 0.475        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.02         |
|    n_updates            | 5320         |
|    policy_gradient_loss | -0.00556     |
|    value_loss           | 2.57         |
------------------------------------------
Eval num_timesteps=213200, episode_reward=11.04 +/- 12.07
Episode length: 16.60 +/- 17.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | 11       |
| time/              |          |
|    total_timesteps | 213200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | 14.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 533      |
|    time_elapsed    | 59238    |
|    total_timesteps | 213200   |
---------------------------------
Eval num_timesteps=213400, episode_reward=15.89 +/- 14.41
Episode length: 24.20 +/- 21.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.2         |
|    mean_reward          | 15.9         |
| time/                   |              |
|    total_timesteps      | 213400       |
| train/                  |              |
|    approx_kl            | 0.0029266882 |
|    clip_fraction        | 0.118        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.646       |
|    explained_variance   | 0.279        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.869        |
|    n_updates            | 5330         |
|    policy_gradient_loss | -0.00642     |
|    value_loss           | 1.5          |
------------------------------------------
Eval num_timesteps=213600, episode_reward=12.35 +/- 11.82
Episode length: 18.00 +/- 16.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | 12.3     |
| time/              |          |
|    total_timesteps | 213600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.2     |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 534      |
|    time_elapsed    | 59350    |
|    total_timesteps | 213600   |
---------------------------------
Eval num_timesteps=213800, episode_reward=5.52 +/- 4.57
Episode length: 9.00 +/- 6.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 9            |
|    mean_reward          | 5.52         |
| time/                   |              |
|    total_timesteps      | 213800       |
| train/                  |              |
|    approx_kl            | 0.0020503183 |
|    clip_fraction        | 0.0717       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.649       |
|    explained_variance   | 0.427        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.56         |
|    n_updates            | 5340         |
|    policy_gradient_loss | -0.00617     |
|    value_loss           | 1.07         |
------------------------------------------
Eval num_timesteps=214000, episode_reward=13.18 +/- 12.95
Episode length: 19.00 +/- 17.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 13.2     |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.1     |
|    ep_rew_mean     | 15.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 535      |
|    time_elapsed    | 59439    |
|    total_timesteps | 214000   |
---------------------------------
Eval num_timesteps=214200, episode_reward=4.28 +/- 0.82
Episode length: 6.80 +/- 1.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 6.8          |
|    mean_reward          | 4.28         |
| time/                   |              |
|    total_timesteps      | 214200       |
| train/                  |              |
|    approx_kl            | 0.0029813081 |
|    clip_fraction        | 0.148        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.624       |
|    explained_variance   | 0.339        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.51         |
|    n_updates            | 5350         |
|    policy_gradient_loss | -0.00782     |
|    value_loss           | 2.25         |
------------------------------------------
Eval num_timesteps=214400, episode_reward=29.49 +/- 13.30
Episode length: 41.00 +/- 18.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41       |
|    mean_reward     | 29.5     |
| time/              |          |
|    total_timesteps | 214400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.1     |
|    ep_rew_mean     | 14.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 536      |
|    time_elapsed    | 59549    |
|    total_timesteps | 214400   |
---------------------------------
Eval num_timesteps=214600, episode_reward=23.41 +/- 14.72
Episode length: 33.80 +/- 20.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.8         |
|    mean_reward          | 23.4         |
| time/                   |              |
|    total_timesteps      | 214600       |
| train/                  |              |
|    approx_kl            | 0.0019011451 |
|    clip_fraction        | 0.0712       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.629       |
|    explained_variance   | 0.469        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.624        |
|    n_updates            | 5360         |
|    policy_gradient_loss | -0.00403     |
|    value_loss           | 2.05         |
------------------------------------------
Eval num_timesteps=214800, episode_reward=10.12 +/- 11.78
Episode length: 15.60 +/- 17.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 10.1     |
| time/              |          |
|    total_timesteps | 214800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.2     |
|    ep_rew_mean     | 15.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 537      |
|    time_elapsed    | 59684    |
|    total_timesteps | 214800   |
---------------------------------
Eval num_timesteps=215000, episode_reward=12.98 +/- 12.26
Episode length: 18.80 +/- 16.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.8         |
|    mean_reward          | 13           |
| time/                   |              |
|    total_timesteps      | 215000       |
| train/                  |              |
|    approx_kl            | 0.0019567658 |
|    clip_fraction        | 0.0915       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.643       |
|    explained_variance   | 0.493        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.269        |
|    n_updates            | 5370         |
|    policy_gradient_loss | -0.00848     |
|    value_loss           | 1.12         |
------------------------------------------
Eval num_timesteps=215200, episode_reward=20.75 +/- 11.78
Episode length: 30.60 +/- 16.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.6     |
|    mean_reward     | 20.8     |
| time/              |          |
|    total_timesteps | 215200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.5     |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 538      |
|    time_elapsed    | 59787    |
|    total_timesteps | 215200   |
---------------------------------
Eval num_timesteps=215400, episode_reward=3.12 +/- 3.13
Episode length: 5.40 +/- 4.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 5.4          |
|    mean_reward          | 3.12         |
| time/                   |              |
|    total_timesteps      | 215400       |
| train/                  |              |
|    approx_kl            | 0.0027045084 |
|    clip_fraction        | 0.119        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.636       |
|    explained_variance   | 0.269        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.43         |
|    n_updates            | 5380         |
|    policy_gradient_loss | -0.00537     |
|    value_loss           | 1.87         |
------------------------------------------
Eval num_timesteps=215600, episode_reward=10.93 +/- 12.34
Episode length: 16.00 +/- 17.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | 10.9     |
| time/              |          |
|    total_timesteps | 215600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.7     |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 539      |
|    time_elapsed    | 59871    |
|    total_timesteps | 215600   |
---------------------------------
Eval num_timesteps=215800, episode_reward=15.64 +/- 15.87
Episode length: 22.80 +/- 22.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 22.8        |
|    mean_reward          | 15.6        |
| time/                   |             |
|    total_timesteps      | 215800      |
| train/                  |             |
|    approx_kl            | 0.002688248 |
|    clip_fraction        | 0.0627      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.658      |
|    explained_variance   | 0.232       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.741       |
|    n_updates            | 5390        |
|    policy_gradient_loss | -0.00336    |
|    value_loss           | 1.56        |
-----------------------------------------
Eval num_timesteps=216000, episode_reward=28.41 +/- 13.80
Episode length: 40.40 +/- 19.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.4     |
|    mean_reward     | 28.4     |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.1     |
|    ep_rew_mean     | 18       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 540      |
|    time_elapsed    | 60002    |
|    total_timesteps | 216000   |
---------------------------------
Eval num_timesteps=216200, episode_reward=18.05 +/- 14.84
Episode length: 25.80 +/- 19.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.8         |
|    mean_reward          | 18           |
| time/                   |              |
|    total_timesteps      | 216200       |
| train/                  |              |
|    approx_kl            | 0.0035909626 |
|    clip_fraction        | 0.107        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.641       |
|    explained_variance   | 0.185        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.139        |
|    n_updates            | 5400         |
|    policy_gradient_loss | -0.00554     |
|    value_loss           | 0.764        |
------------------------------------------
Eval num_timesteps=216400, episode_reward=13.93 +/- 11.68
Episode length: 20.40 +/- 15.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 13.9     |
| time/              |          |
|    total_timesteps | 216400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.5     |
|    ep_rew_mean     | 18.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 541      |
|    time_elapsed    | 60141    |
|    total_timesteps | 216400   |
---------------------------------
Eval num_timesteps=216600, episode_reward=29.29 +/- 12.10
Episode length: 41.60 +/- 16.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.6         |
|    mean_reward          | 29.3         |
| time/                   |              |
|    total_timesteps      | 216600       |
| train/                  |              |
|    approx_kl            | 0.0020793409 |
|    clip_fraction        | 0.0509       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.649       |
|    explained_variance   | 0.271        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0525       |
|    n_updates            | 5410         |
|    policy_gradient_loss | -0.00462     |
|    value_loss           | 2.05         |
------------------------------------------
Eval num_timesteps=216800, episode_reward=16.97 +/- 14.78
Episode length: 24.80 +/- 20.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 216800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.2     |
|    ep_rew_mean     | 18.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 542      |
|    time_elapsed    | 60251    |
|    total_timesteps | 216800   |
---------------------------------
Eval num_timesteps=217000, episode_reward=8.99 +/- 5.66
Episode length: 13.80 +/- 8.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 13.8         |
|    mean_reward          | 8.99         |
| time/                   |              |
|    total_timesteps      | 217000       |
| train/                  |              |
|    approx_kl            | 0.0017963162 |
|    clip_fraction        | 0.0621       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.632       |
|    explained_variance   | 0.426        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.83         |
|    n_updates            | 5420         |
|    policy_gradient_loss | -0.00581     |
|    value_loss           | 2.15         |
------------------------------------------
Eval num_timesteps=217200, episode_reward=11.32 +/- 11.61
Episode length: 17.00 +/- 16.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | 11.3     |
| time/              |          |
|    total_timesteps | 217200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.1     |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 543      |
|    time_elapsed    | 60349    |
|    total_timesteps | 217200   |
---------------------------------
Eval num_timesteps=217400, episode_reward=9.13 +/- 13.36
Episode length: 13.60 +/- 18.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 13.6         |
|    mean_reward          | 9.13         |
| time/                   |              |
|    total_timesteps      | 217400       |
| train/                  |              |
|    approx_kl            | 0.0016937157 |
|    clip_fraction        | 0.073        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.638       |
|    explained_variance   | 0.396        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.74         |
|    n_updates            | 5430         |
|    policy_gradient_loss | -0.00513     |
|    value_loss           | 2.13         |
------------------------------------------
Eval num_timesteps=217600, episode_reward=10.32 +/- 13.37
Episode length: 15.00 +/- 17.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | 10.3     |
| time/              |          |
|    total_timesteps | 217600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | 17.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 544      |
|    time_elapsed    | 60452    |
|    total_timesteps | 217600   |
---------------------------------
Eval num_timesteps=217800, episode_reward=10.17 +/- 12.76
Episode length: 15.20 +/- 17.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.2         |
|    mean_reward          | 10.2         |
| time/                   |              |
|    total_timesteps      | 217800       |
| train/                  |              |
|    approx_kl            | 0.0021680389 |
|    clip_fraction        | 0.0844       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.619       |
|    explained_variance   | 0.275        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.4          |
|    n_updates            | 5440         |
|    policy_gradient_loss | -0.00712     |
|    value_loss           | 2.71         |
------------------------------------------
Eval num_timesteps=218000, episode_reward=8.24 +/- 12.61
Episode length: 13.00 +/- 18.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13       |
|    mean_reward     | 8.24     |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24       |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 545      |
|    time_elapsed    | 60559    |
|    total_timesteps | 218000   |
---------------------------------
Eval num_timesteps=218200, episode_reward=29.09 +/- 11.90
Episode length: 41.80 +/- 16.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.8         |
|    mean_reward          | 29.1         |
| time/                   |              |
|    total_timesteps      | 218200       |
| train/                  |              |
|    approx_kl            | 0.0038313586 |
|    clip_fraction        | 0.11         |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.622       |
|    explained_variance   | 0.391        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.712        |
|    n_updates            | 5450         |
|    policy_gradient_loss | -0.00751     |
|    value_loss           | 2.16         |
------------------------------------------
Eval num_timesteps=218400, episode_reward=21.68 +/- 16.15
Episode length: 31.40 +/- 22.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.4     |
|    mean_reward     | 21.7     |
| time/              |          |
|    total_timesteps | 218400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.4     |
|    ep_rew_mean     | 14.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 546      |
|    time_elapsed    | 60667    |
|    total_timesteps | 218400   |
---------------------------------
Eval num_timesteps=218600, episode_reward=9.88 +/- 12.58
Episode length: 15.00 +/- 17.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15          |
|    mean_reward          | 9.88        |
| time/                   |             |
|    total_timesteps      | 218600      |
| train/                  |             |
|    approx_kl            | 0.002975403 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.626      |
|    explained_variance   | 0.26        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.953       |
|    n_updates            | 5460        |
|    policy_gradient_loss | -0.00655    |
|    value_loss           | 1.88        |
-----------------------------------------
Eval num_timesteps=218800, episode_reward=18.95 +/- 14.33
Episode length: 27.60 +/- 19.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.6     |
|    mean_reward     | 19       |
| time/              |          |
|    total_timesteps | 218800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21       |
|    ep_rew_mean     | 14.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 547      |
|    time_elapsed    | 60776    |
|    total_timesteps | 218800   |
---------------------------------
Eval num_timesteps=219000, episode_reward=16.62 +/- 15.63
Episode length: 24.00 +/- 21.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24           |
|    mean_reward          | 16.6         |
| time/                   |              |
|    total_timesteps      | 219000       |
| train/                  |              |
|    approx_kl            | 0.0014500323 |
|    clip_fraction        | 0.0453       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.58        |
|    explained_variance   | 0.327        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.4          |
|    n_updates            | 5470         |
|    policy_gradient_loss | -0.00422     |
|    value_loss           | 3.21         |
------------------------------------------
Eval num_timesteps=219200, episode_reward=15.14 +/- 15.82
Episode length: 22.40 +/- 22.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 15.1     |
| time/              |          |
|    total_timesteps | 219200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.7     |
|    ep_rew_mean     | 14.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 548      |
|    time_elapsed    | 60879    |
|    total_timesteps | 219200   |
---------------------------------
Eval num_timesteps=219400, episode_reward=9.47 +/- 13.16
Episode length: 14.20 +/- 18.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.2         |
|    mean_reward          | 9.47         |
| time/                   |              |
|    total_timesteps      | 219400       |
| train/                  |              |
|    approx_kl            | 0.0026880726 |
|    clip_fraction        | 0.0583       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.638       |
|    explained_variance   | 0.469        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.721        |
|    n_updates            | 5480         |
|    policy_gradient_loss | -0.00524     |
|    value_loss           | 1.2          |
------------------------------------------
Eval num_timesteps=219600, episode_reward=11.01 +/- 11.39
Episode length: 17.00 +/- 16.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | 11       |
| time/              |          |
|    total_timesteps | 219600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.4     |
|    ep_rew_mean     | 13.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 549      |
|    time_elapsed    | 60982    |
|    total_timesteps | 219600   |
---------------------------------
Eval num_timesteps=219800, episode_reward=11.86 +/- 11.67
Episode length: 18.00 +/- 16.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18           |
|    mean_reward          | 11.9         |
| time/                   |              |
|    total_timesteps      | 219800       |
| train/                  |              |
|    approx_kl            | 0.0024189886 |
|    clip_fraction        | 0.0719       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.609       |
|    explained_variance   | 0.382        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.03         |
|    n_updates            | 5490         |
|    policy_gradient_loss | -0.00837     |
|    value_loss           | 2.86         |
------------------------------------------
Eval num_timesteps=220000, episode_reward=16.74 +/- 14.63
Episode length: 24.80 +/- 20.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.9     |
|    ep_rew_mean     | 14.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 550      |
|    time_elapsed    | 61095    |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=220200, episode_reward=14.23 +/- 12.02
Episode length: 20.40 +/- 15.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 20.4         |
|    mean_reward          | 14.2         |
| time/                   |              |
|    total_timesteps      | 220200       |
| train/                  |              |
|    approx_kl            | 0.0020748158 |
|    clip_fraction        | 0.0714       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.618       |
|    explained_variance   | 0.472        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.01         |
|    n_updates            | 5500         |
|    policy_gradient_loss | -0.00587     |
|    value_loss           | 1.65         |
------------------------------------------
Eval num_timesteps=220400, episode_reward=24.85 +/- 12.70
Episode length: 35.80 +/- 17.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 24.9     |
| time/              |          |
|    total_timesteps | 220400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.4     |
|    ep_rew_mean     | 14.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 551      |
|    time_elapsed    | 61228    |
|    total_timesteps | 220400   |
---------------------------------
Eval num_timesteps=220600, episode_reward=24.76 +/- 11.45
Episode length: 36.40 +/- 16.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.4         |
|    mean_reward          | 24.8         |
| time/                   |              |
|    total_timesteps      | 220600       |
| train/                  |              |
|    approx_kl            | 0.0018647818 |
|    clip_fraction        | 0.0511       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.631       |
|    explained_variance   | 0.345        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.528        |
|    n_updates            | 5510         |
|    policy_gradient_loss | -0.00658     |
|    value_loss           | 1.86         |
------------------------------------------
Eval num_timesteps=220800, episode_reward=15.29 +/- 15.71
Episode length: 22.60 +/- 22.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 15.3     |
| time/              |          |
|    total_timesteps | 220800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.7     |
|    ep_rew_mean     | 15.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 552      |
|    time_elapsed    | 61338    |
|    total_timesteps | 220800   |
---------------------------------
Eval num_timesteps=221000, episode_reward=17.12 +/- 15.14
Episode length: 24.60 +/- 20.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.6         |
|    mean_reward          | 17.1         |
| time/                   |              |
|    total_timesteps      | 221000       |
| train/                  |              |
|    approx_kl            | 0.0029444115 |
|    clip_fraction        | 0.0888       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.64        |
|    explained_variance   | 0.466        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.555        |
|    n_updates            | 5520         |
|    policy_gradient_loss | -0.00695     |
|    value_loss           | 1.67         |
------------------------------------------
Eval num_timesteps=221200, episode_reward=19.40 +/- 12.44
Episode length: 28.60 +/- 17.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.6     |
|    mean_reward     | 19.4     |
| time/              |          |
|    total_timesteps | 221200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | 16.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 553      |
|    time_elapsed    | 61441    |
|    total_timesteps | 221200   |
---------------------------------
Eval num_timesteps=221400, episode_reward=10.69 +/- 12.01
Episode length: 16.20 +/- 17.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.2         |
|    mean_reward          | 10.7         |
| time/                   |              |
|    total_timesteps      | 221400       |
| train/                  |              |
|    approx_kl            | 0.0016472144 |
|    clip_fraction        | 0.0571       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.65        |
|    explained_variance   | 0.108        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.45         |
|    n_updates            | 5530         |
|    policy_gradient_loss | -0.0043      |
|    value_loss           | 2.14         |
------------------------------------------
Eval num_timesteps=221600, episode_reward=13.81 +/- 12.73
Episode length: 20.60 +/- 18.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 13.8     |
| time/              |          |
|    total_timesteps | 221600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.2     |
|    ep_rew_mean     | 16       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 554      |
|    time_elapsed    | 61561    |
|    total_timesteps | 221600   |
---------------------------------
Eval num_timesteps=221800, episode_reward=12.38 +/- 10.98
Episode length: 18.80 +/- 16.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.8         |
|    mean_reward          | 12.4         |
| time/                   |              |
|    total_timesteps      | 221800       |
| train/                  |              |
|    approx_kl            | 0.0027100886 |
|    clip_fraction        | 0.111        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.633       |
|    explained_variance   | 0.304        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.68         |
|    n_updates            | 5540         |
|    policy_gradient_loss | -0.00783     |
|    value_loss           | 2.1          |
------------------------------------------
Eval num_timesteps=222000, episode_reward=10.94 +/- 11.94
Episode length: 16.40 +/- 17.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | 10.9     |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.7     |
|    ep_rew_mean     | 15.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 555      |
|    time_elapsed    | 61670    |
|    total_timesteps | 222000   |
---------------------------------
Eval num_timesteps=222200, episode_reward=14.26 +/- 12.30
Episode length: 22.00 +/- 18.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22           |
|    mean_reward          | 14.3         |
| time/                   |              |
|    total_timesteps      | 222200       |
| train/                  |              |
|    approx_kl            | 0.0028647562 |
|    clip_fraction        | 0.0625       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.615       |
|    explained_variance   | 0.468        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.469        |
|    n_updates            | 5550         |
|    policy_gradient_loss | -0.00691     |
|    value_loss           | 2.22         |
------------------------------------------
Eval num_timesteps=222400, episode_reward=12.66 +/- 11.71
Episode length: 19.20 +/- 17.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.2     |
|    mean_reward     | 12.7     |
| time/              |          |
|    total_timesteps | 222400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24       |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 556      |
|    time_elapsed    | 61774    |
|    total_timesteps | 222400   |
---------------------------------
Eval num_timesteps=222600, episode_reward=17.82 +/- 14.39
Episode length: 26.00 +/- 20.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26           |
|    mean_reward          | 17.8         |
| time/                   |              |
|    total_timesteps      | 222600       |
| train/                  |              |
|    approx_kl            | 0.0045744455 |
|    clip_fraction        | 0.134        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.634       |
|    explained_variance   | 0.176        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.966        |
|    n_updates            | 5560         |
|    policy_gradient_loss | -0.00996     |
|    value_loss           | 1.26         |
------------------------------------------
Eval num_timesteps=222800, episode_reward=16.69 +/- 13.67
Episode length: 25.40 +/- 20.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 222800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22       |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 557      |
|    time_elapsed    | 61881    |
|    total_timesteps | 222800   |
---------------------------------
Eval num_timesteps=223000, episode_reward=22.76 +/- 14.27
Episode length: 33.60 +/- 20.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.6        |
|    mean_reward          | 22.8        |
| time/                   |             |
|    total_timesteps      | 223000      |
| train/                  |             |
|    approx_kl            | 0.002397248 |
|    clip_fraction        | 0.0846      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.616      |
|    explained_variance   | 0.273       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.64        |
|    n_updates            | 5570        |
|    policy_gradient_loss | -0.00691    |
|    value_loss           | 3.41        |
-----------------------------------------
Eval num_timesteps=223200, episode_reward=18.82 +/- 13.38
Episode length: 27.40 +/- 18.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.4     |
|    mean_reward     | 18.8     |
| time/              |          |
|    total_timesteps | 223200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.7     |
|    ep_rew_mean     | 15.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 558      |
|    time_elapsed    | 61992    |
|    total_timesteps | 223200   |
---------------------------------
Eval num_timesteps=223400, episode_reward=10.83 +/- 11.44
Episode length: 16.60 +/- 16.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.6        |
|    mean_reward          | 10.8        |
| time/                   |             |
|    total_timesteps      | 223400      |
| train/                  |             |
|    approx_kl            | 0.004494061 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.638      |
|    explained_variance   | 0.217       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.415       |
|    n_updates            | 5580        |
|    policy_gradient_loss | -0.00792    |
|    value_loss           | 1.64        |
-----------------------------------------
Eval num_timesteps=223600, episode_reward=16.52 +/- 16.13
Episode length: 23.40 +/- 21.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 223600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.6     |
|    ep_rew_mean     | 15.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 559      |
|    time_elapsed    | 62100    |
|    total_timesteps | 223600   |
---------------------------------
Eval num_timesteps=223800, episode_reward=14.66 +/- 11.62
Episode length: 21.40 +/- 15.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 21.4         |
|    mean_reward          | 14.7         |
| time/                   |              |
|    total_timesteps      | 223800       |
| train/                  |              |
|    approx_kl            | 0.0037911076 |
|    clip_fraction        | 0.118        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.606       |
|    explained_variance   | 0.298        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.206        |
|    n_updates            | 5590         |
|    policy_gradient_loss | -0.0083      |
|    value_loss           | 2.59         |
------------------------------------------
Eval num_timesteps=224000, episode_reward=9.17 +/- 12.71
Episode length: 14.00 +/- 18.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14       |
|    mean_reward     | 9.17     |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.7     |
|    ep_rew_mean     | 15.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 560      |
|    time_elapsed    | 62204    |
|    total_timesteps | 224000   |
---------------------------------
Eval num_timesteps=224200, episode_reward=17.76 +/- 13.74
Episode length: 26.80 +/- 19.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.8         |
|    mean_reward          | 17.8         |
| time/                   |              |
|    total_timesteps      | 224200       |
| train/                  |              |
|    approx_kl            | 0.0028319615 |
|    clip_fraction        | 0.112        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.645       |
|    explained_variance   | 0.392        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.844        |
|    n_updates            | 5600         |
|    policy_gradient_loss | -0.00568     |
|    value_loss           | 1.81         |
------------------------------------------
Eval num_timesteps=224400, episode_reward=17.75 +/- 14.12
Episode length: 26.20 +/- 20.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | 17.8     |
| time/              |          |
|    total_timesteps | 224400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.8     |
|    ep_rew_mean     | 16.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 561      |
|    time_elapsed    | 62311    |
|    total_timesteps | 224400   |
---------------------------------
Eval num_timesteps=224600, episode_reward=17.09 +/- 15.75
Episode length: 24.20 +/- 21.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 24.2        |
|    mean_reward          | 17.1        |
| time/                   |             |
|    total_timesteps      | 224600      |
| train/                  |             |
|    approx_kl            | 0.005751167 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.625      |
|    explained_variance   | -0.0967     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.42        |
|    n_updates            | 5610        |
|    policy_gradient_loss | -0.00838    |
|    value_loss           | 3.3         |
-----------------------------------------
Eval num_timesteps=224800, episode_reward=18.33 +/- 14.18
Episode length: 26.60 +/- 19.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.6     |
|    mean_reward     | 18.3     |
| time/              |          |
|    total_timesteps | 224800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | 16.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 562      |
|    time_elapsed    | 62424    |
|    total_timesteps | 224800   |
---------------------------------
Eval num_timesteps=225000, episode_reward=23.27 +/- 14.78
Episode length: 33.60 +/- 20.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.6         |
|    mean_reward          | 23.3         |
| time/                   |              |
|    total_timesteps      | 225000       |
| train/                  |              |
|    approx_kl            | 0.0027739964 |
|    clip_fraction        | 0.113        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.63        |
|    explained_variance   | 0.253        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.974        |
|    n_updates            | 5620         |
|    policy_gradient_loss | -0.00641     |
|    value_loss           | 1.45         |
------------------------------------------
Eval num_timesteps=225200, episode_reward=16.93 +/- 15.27
Episode length: 24.40 +/- 20.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 225200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.8     |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 563      |
|    time_elapsed    | 62529    |
|    total_timesteps | 225200   |
---------------------------------
Eval num_timesteps=225400, episode_reward=11.50 +/- 12.25
Episode length: 17.60 +/- 17.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.6         |
|    mean_reward          | 11.5         |
| time/                   |              |
|    total_timesteps      | 225400       |
| train/                  |              |
|    approx_kl            | 0.0020129103 |
|    clip_fraction        | 0.118        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.64        |
|    explained_variance   | 0.337        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.67         |
|    n_updates            | 5630         |
|    policy_gradient_loss | -0.00577     |
|    value_loss           | 2.11         |
------------------------------------------
Eval num_timesteps=225600, episode_reward=22.77 +/- 14.35
Episode length: 33.40 +/- 20.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 22.8     |
| time/              |          |
|    total_timesteps | 225600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25       |
|    ep_rew_mean     | 17       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 564      |
|    time_elapsed    | 62659    |
|    total_timesteps | 225600   |
---------------------------------
Eval num_timesteps=225800, episode_reward=17.16 +/- 15.97
Episode length: 24.00 +/- 21.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 24         |
|    mean_reward          | 17.2       |
| time/                   |            |
|    total_timesteps      | 225800     |
| train/                  |            |
|    approx_kl            | 0.00189359 |
|    clip_fraction        | 0.0357     |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.637     |
|    explained_variance   | 0.165      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.412      |
|    n_updates            | 5640       |
|    policy_gradient_loss | -0.00389   |
|    value_loss           | 1.79       |
----------------------------------------
Eval num_timesteps=226000, episode_reward=14.38 +/- 12.25
Episode length: 20.80 +/- 16.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.8     |
|    mean_reward     | 14.4     |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.3     |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 565      |
|    time_elapsed    | 62765    |
|    total_timesteps | 226000   |
---------------------------------
Eval num_timesteps=226200, episode_reward=12.58 +/- 11.69
Episode length: 18.60 +/- 15.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.6        |
|    mean_reward          | 12.6        |
| time/                   |             |
|    total_timesteps      | 226200      |
| train/                  |             |
|    approx_kl            | 0.003017143 |
|    clip_fraction        | 0.0788      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.625      |
|    explained_variance   | 0.44        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.26        |
|    n_updates            | 5650        |
|    policy_gradient_loss | -0.00796    |
|    value_loss           | 2.08        |
-----------------------------------------
Eval num_timesteps=226400, episode_reward=15.19 +/- 15.36
Episode length: 22.80 +/- 22.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 15.2     |
| time/              |          |
|    total_timesteps | 226400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | 16       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 566      |
|    time_elapsed    | 62876    |
|    total_timesteps | 226400   |
---------------------------------
Eval num_timesteps=226600, episode_reward=23.03 +/- 15.00
Episode length: 33.00 +/- 20.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33           |
|    mean_reward          | 23           |
| time/                   |              |
|    total_timesteps      | 226600       |
| train/                  |              |
|    approx_kl            | 0.0024977182 |
|    clip_fraction        | 0.0721       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.624       |
|    explained_variance   | 0.459        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.196        |
|    n_updates            | 5660         |
|    policy_gradient_loss | -0.00526     |
|    value_loss           | 1.53         |
------------------------------------------
Eval num_timesteps=226800, episode_reward=12.70 +/- 11.98
Episode length: 18.60 +/- 16.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 12.7     |
| time/              |          |
|    total_timesteps | 226800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.7     |
|    ep_rew_mean     | 16.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 567      |
|    time_elapsed    | 63009    |
|    total_timesteps | 226800   |
---------------------------------
Eval num_timesteps=227000, episode_reward=15.81 +/- 15.73
Episode length: 23.20 +/- 21.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.2         |
|    mean_reward          | 15.8         |
| time/                   |              |
|    total_timesteps      | 227000       |
| train/                  |              |
|    approx_kl            | 0.0040910724 |
|    clip_fraction        | 0.117        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.634       |
|    explained_variance   | 0.0689       |
|    learning_rate        | 0.0001       |
|    loss                 | 2.3          |
|    n_updates            | 5670         |
|    policy_gradient_loss | -0.00696     |
|    value_loss           | 2.11         |
------------------------------------------
Eval num_timesteps=227200, episode_reward=25.45 +/- 12.88
Episode length: 36.00 +/- 17.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 25.4     |
| time/              |          |
|    total_timesteps | 227200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.3     |
|    ep_rew_mean     | 17.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 568      |
|    time_elapsed    | 63120    |
|    total_timesteps | 227200   |
---------------------------------
Eval num_timesteps=227400, episode_reward=9.52 +/- 8.37
Episode length: 14.20 +/- 11.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.2         |
|    mean_reward          | 9.52         |
| time/                   |              |
|    total_timesteps      | 227400       |
| train/                  |              |
|    approx_kl            | 0.0019196734 |
|    clip_fraction        | 0.104        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.632       |
|    explained_variance   | 0.193        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.161        |
|    n_updates            | 5680         |
|    policy_gradient_loss | -0.00715     |
|    value_loss           | 1.13         |
------------------------------------------
Eval num_timesteps=227600, episode_reward=23.07 +/- 15.90
Episode length: 32.40 +/- 21.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.4     |
|    mean_reward     | 23.1     |
| time/              |          |
|    total_timesteps | 227600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.6     |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 569      |
|    time_elapsed    | 63221    |
|    total_timesteps | 227600   |
---------------------------------
Eval num_timesteps=227800, episode_reward=21.74 +/- 11.44
Episode length: 32.00 +/- 16.30
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32            |
|    mean_reward          | 21.7          |
| time/                   |               |
|    total_timesteps      | 227800        |
| train/                  |               |
|    approx_kl            | 0.00050543837 |
|    clip_fraction        | 0.0152        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.628        |
|    explained_variance   | 0.283         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.441         |
|    n_updates            | 5690          |
|    policy_gradient_loss | -0.00268      |
|    value_loss           | 1.83          |
-------------------------------------------
Eval num_timesteps=228000, episode_reward=22.47 +/- 16.10
Episode length: 32.00 +/- 22.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32       |
|    mean_reward     | 22.5     |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26       |
|    ep_rew_mean     | 18       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 570      |
|    time_elapsed    | 63331    |
|    total_timesteps | 228000   |
---------------------------------
Eval num_timesteps=228200, episode_reward=16.31 +/- 14.62
Episode length: 24.60 +/- 21.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.6         |
|    mean_reward          | 16.3         |
| time/                   |              |
|    total_timesteps      | 228200       |
| train/                  |              |
|    approx_kl            | 0.0029347015 |
|    clip_fraction        | 0.162        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.629       |
|    explained_variance   | 0.405        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.61         |
|    n_updates            | 5700         |
|    policy_gradient_loss | -0.00875     |
|    value_loss           | 1.61         |
------------------------------------------
Eval num_timesteps=228400, episode_reward=16.95 +/- 14.32
Episode length: 25.00 +/- 20.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 228400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.9     |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 571      |
|    time_elapsed    | 63462    |
|    total_timesteps | 228400   |
---------------------------------
Eval num_timesteps=228600, episode_reward=12.36 +/- 12.24
Episode length: 18.00 +/- 16.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18          |
|    mean_reward          | 12.4        |
| time/                   |             |
|    total_timesteps      | 228600      |
| train/                  |             |
|    approx_kl            | 0.004778261 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.638      |
|    explained_variance   | -0.02       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.378       |
|    n_updates            | 5710        |
|    policy_gradient_loss | -0.00621    |
|    value_loss           | 2.11        |
-----------------------------------------
Eval num_timesteps=228800, episode_reward=2.63 +/- 1.32
Episode length: 4.60 +/- 1.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.6      |
|    mean_reward     | 2.63     |
| time/              |          |
|    total_timesteps | 228800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27       |
|    ep_rew_mean     | 18.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 572      |
|    time_elapsed    | 63545    |
|    total_timesteps | 228800   |
---------------------------------
Eval num_timesteps=229000, episode_reward=26.13 +/- 13.83
Episode length: 36.80 +/- 18.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.8         |
|    mean_reward          | 26.1         |
| time/                   |              |
|    total_timesteps      | 229000       |
| train/                  |              |
|    approx_kl            | 0.0019296948 |
|    clip_fraction        | 0.0471       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.627       |
|    explained_variance   | 0.33         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.51         |
|    n_updates            | 5720         |
|    policy_gradient_loss | -0.00558     |
|    value_loss           | 2            |
------------------------------------------
Eval num_timesteps=229200, episode_reward=16.67 +/- 14.37
Episode length: 25.00 +/- 20.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 229200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.9     |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 573      |
|    time_elapsed    | 63648    |
|    total_timesteps | 229200   |
---------------------------------
Eval num_timesteps=229400, episode_reward=9.69 +/- 12.41
Episode length: 14.80 +/- 17.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.8        |
|    mean_reward          | 9.69        |
| time/                   |             |
|    total_timesteps      | 229400      |
| train/                  |             |
|    approx_kl            | 0.003106753 |
|    clip_fraction        | 0.0618      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.619      |
|    explained_variance   | 0.47        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.698       |
|    n_updates            | 5730        |
|    policy_gradient_loss | -0.00679    |
|    value_loss           | 1.81        |
-----------------------------------------
Eval num_timesteps=229600, episode_reward=11.09 +/- 13.12
Episode length: 16.20 +/- 17.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | 11.1     |
| time/              |          |
|    total_timesteps | 229600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.6     |
|    ep_rew_mean     | 18.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 574      |
|    time_elapsed    | 63761    |
|    total_timesteps | 229600   |
---------------------------------
Eval num_timesteps=229800, episode_reward=22.70 +/- 14.97
Episode length: 33.00 +/- 20.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33           |
|    mean_reward          | 22.7         |
| time/                   |              |
|    total_timesteps      | 229800       |
| train/                  |              |
|    approx_kl            | 0.0015649804 |
|    clip_fraction        | 0.0779       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.622       |
|    explained_variance   | 0.396        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.145        |
|    n_updates            | 5740         |
|    policy_gradient_loss | -0.00795     |
|    value_loss           | 1.46         |
------------------------------------------
Eval num_timesteps=230000, episode_reward=19.05 +/- 14.39
Episode length: 27.20 +/- 19.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.2     |
|    mean_reward     | 19       |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.9     |
|    ep_rew_mean     | 18.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 575      |
|    time_elapsed    | 63869    |
|    total_timesteps | 230000   |
---------------------------------
Eval num_timesteps=230200, episode_reward=23.40 +/- 14.06
Episode length: 33.80 +/- 19.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.8         |
|    mean_reward          | 23.4         |
| time/                   |              |
|    total_timesteps      | 230200       |
| train/                  |              |
|    approx_kl            | 0.0036645005 |
|    clip_fraction        | 0.104        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.645       |
|    explained_variance   | -0.0198      |
|    learning_rate        | 0.0001       |
|    loss                 | 0.14         |
|    n_updates            | 5750         |
|    policy_gradient_loss | -0.00781     |
|    value_loss           | 0.989        |
------------------------------------------
Eval num_timesteps=230400, episode_reward=25.37 +/- 12.53
Episode length: 36.20 +/- 16.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 25.4     |
| time/              |          |
|    total_timesteps | 230400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.6     |
|    ep_rew_mean     | 17.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 576      |
|    time_elapsed    | 64023    |
|    total_timesteps | 230400   |
---------------------------------
Eval num_timesteps=230600, episode_reward=10.83 +/- 13.31
Episode length: 15.60 +/- 17.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.6         |
|    mean_reward          | 10.8         |
| time/                   |              |
|    total_timesteps      | 230600       |
| train/                  |              |
|    approx_kl            | 0.0015757622 |
|    clip_fraction        | 0.0502       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.605       |
|    explained_variance   | 0.383        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.3          |
|    n_updates            | 5760         |
|    policy_gradient_loss | -0.00434     |
|    value_loss           | 2.51         |
------------------------------------------
Eval num_timesteps=230800, episode_reward=34.47 +/- 0.99
Episode length: 50.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 34.5     |
| time/              |          |
|    total_timesteps | 230800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | 16.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 577      |
|    time_elapsed    | 64150    |
|    total_timesteps | 230800   |
---------------------------------
Eval num_timesteps=231000, episode_reward=17.86 +/- 14.91
Episode length: 25.60 +/- 20.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.6         |
|    mean_reward          | 17.9         |
| time/                   |              |
|    total_timesteps      | 231000       |
| train/                  |              |
|    approx_kl            | 0.0018198425 |
|    clip_fraction        | 0.0717       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.624       |
|    explained_variance   | 0.228        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.665        |
|    n_updates            | 5770         |
|    policy_gradient_loss | -0.00548     |
|    value_loss           | 2.29         |
------------------------------------------
Eval num_timesteps=231200, episode_reward=17.81 +/- 15.11
Episode length: 25.80 +/- 20.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | 17.8     |
| time/              |          |
|    total_timesteps | 231200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.8     |
|    ep_rew_mean     | 16.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 578      |
|    time_elapsed    | 64257    |
|    total_timesteps | 231200   |
---------------------------------
Eval num_timesteps=231400, episode_reward=16.82 +/- 14.44
Episode length: 25.00 +/- 20.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25           |
|    mean_reward          | 16.8         |
| time/                   |              |
|    total_timesteps      | 231400       |
| train/                  |              |
|    approx_kl            | 0.0016515023 |
|    clip_fraction        | 0.0458       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.621       |
|    explained_variance   | 0.473        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0391       |
|    n_updates            | 5780         |
|    policy_gradient_loss | -0.00449     |
|    value_loss           | 1.57         |
------------------------------------------
Eval num_timesteps=231600, episode_reward=23.62 +/- 14.63
Episode length: 34.00 +/- 20.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 23.6     |
| time/              |          |
|    total_timesteps | 231600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 579      |
|    time_elapsed    | 64366    |
|    total_timesteps | 231600   |
---------------------------------
Eval num_timesteps=231800, episode_reward=11.42 +/- 12.81
Episode length: 16.40 +/- 17.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | 11.4         |
| time/                   |              |
|    total_timesteps      | 231800       |
| train/                  |              |
|    approx_kl            | 0.0019240358 |
|    clip_fraction        | 0.0589       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.597       |
|    explained_variance   | 0.406        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.909        |
|    n_updates            | 5790         |
|    policy_gradient_loss | -0.00451     |
|    value_loss           | 2.06         |
------------------------------------------
Eval num_timesteps=232000, episode_reward=10.38 +/- 12.20
Episode length: 15.60 +/- 17.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 10.4     |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.8     |
|    ep_rew_mean     | 14.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 580      |
|    time_elapsed    | 64473    |
|    total_timesteps | 232000   |
---------------------------------
Eval num_timesteps=232200, episode_reward=11.52 +/- 11.59
Episode length: 17.40 +/- 16.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.4         |
|    mean_reward          | 11.5         |
| time/                   |              |
|    total_timesteps      | 232200       |
| train/                  |              |
|    approx_kl            | 0.0028573382 |
|    clip_fraction        | 0.0431       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.612       |
|    explained_variance   | 0.372        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.46         |
|    n_updates            | 5800         |
|    policy_gradient_loss | -0.00454     |
|    value_loss           | 2.75         |
------------------------------------------
Eval num_timesteps=232400, episode_reward=18.98 +/- 14.76
Episode length: 27.00 +/- 20.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27       |
|    mean_reward     | 19       |
| time/              |          |
|    total_timesteps | 232400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.1     |
|    ep_rew_mean     | 14.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 581      |
|    time_elapsed    | 64579    |
|    total_timesteps | 232400   |
---------------------------------
Eval num_timesteps=232600, episode_reward=16.37 +/- 15.34
Episode length: 24.00 +/- 21.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24           |
|    mean_reward          | 16.4         |
| time/                   |              |
|    total_timesteps      | 232600       |
| train/                  |              |
|    approx_kl            | 0.0026626678 |
|    clip_fraction        | 0.0795       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.62        |
|    explained_variance   | 0.279        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.462        |
|    n_updates            | 5810         |
|    policy_gradient_loss | -0.00558     |
|    value_loss           | 1.66         |
------------------------------------------
Eval num_timesteps=232800, episode_reward=15.83 +/- 16.61
Episode length: 22.60 +/- 22.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 15.8     |
| time/              |          |
|    total_timesteps | 232800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.7     |
|    ep_rew_mean     | 14.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 582      |
|    time_elapsed    | 64708    |
|    total_timesteps | 232800   |
---------------------------------
Eval num_timesteps=233000, episode_reward=9.98 +/- 12.93
Episode length: 14.80 +/- 17.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.8        |
|    mean_reward          | 9.98        |
| time/                   |             |
|    total_timesteps      | 233000      |
| train/                  |             |
|    approx_kl            | 0.002123504 |
|    clip_fraction        | 0.0763      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.625      |
|    explained_variance   | 0.378       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.502       |
|    n_updates            | 5820        |
|    policy_gradient_loss | -0.0043     |
|    value_loss           | 1.81        |
-----------------------------------------
Eval num_timesteps=233200, episode_reward=10.14 +/- 12.22
Episode length: 15.40 +/- 17.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | 10.1     |
| time/              |          |
|    total_timesteps | 233200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.4     |
|    ep_rew_mean     | 15.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 583      |
|    time_elapsed    | 64811    |
|    total_timesteps | 233200   |
---------------------------------
Eval num_timesteps=233400, episode_reward=15.92 +/- 14.25
Episode length: 24.20 +/- 21.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 24.2        |
|    mean_reward          | 15.9        |
| time/                   |             |
|    total_timesteps      | 233400      |
| train/                  |             |
|    approx_kl            | 0.001814995 |
|    clip_fraction        | 0.0763      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.613      |
|    explained_variance   | 0.316       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.14        |
|    n_updates            | 5830        |
|    policy_gradient_loss | -0.00604    |
|    value_loss           | 2.45        |
-----------------------------------------
Eval num_timesteps=233600, episode_reward=22.63 +/- 15.56
Episode length: 32.60 +/- 21.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.6     |
|    mean_reward     | 22.6     |
| time/              |          |
|    total_timesteps | 233600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.8     |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 584      |
|    time_elapsed    | 64919    |
|    total_timesteps | 233600   |
---------------------------------
Eval num_timesteps=233800, episode_reward=6.96 +/- 6.53
Episode length: 11.00 +/- 9.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 11           |
|    mean_reward          | 6.96         |
| time/                   |              |
|    total_timesteps      | 233800       |
| train/                  |              |
|    approx_kl            | 0.0033739428 |
|    clip_fraction        | 0.194        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.637       |
|    explained_variance   | 0.38         |
|    learning_rate        | 0.0001       |
|    loss                 | 1.2          |
|    n_updates            | 5840         |
|    policy_gradient_loss | -0.00787     |
|    value_loss           | 1.65         |
------------------------------------------
Eval num_timesteps=234000, episode_reward=22.25 +/- 11.93
Episode length: 32.00 +/- 16.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32       |
|    mean_reward     | 22.2     |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 585      |
|    time_elapsed    | 65012    |
|    total_timesteps | 234000   |
---------------------------------
Eval num_timesteps=234200, episode_reward=22.13 +/- 16.03
Episode length: 31.80 +/- 22.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 31.8         |
|    mean_reward          | 22.1         |
| time/                   |              |
|    total_timesteps      | 234200       |
| train/                  |              |
|    approx_kl            | 0.0013633596 |
|    clip_fraction        | 0.0502       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.603       |
|    explained_variance   | 0.289        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.403        |
|    n_updates            | 5850         |
|    policy_gradient_loss | -0.00657     |
|    value_loss           | 2.74         |
------------------------------------------
Eval num_timesteps=234400, episode_reward=17.11 +/- 14.65
Episode length: 25.00 +/- 20.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | 17.1     |
| time/              |          |
|    total_timesteps | 234400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.9     |
|    ep_rew_mean     | 15.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 586      |
|    time_elapsed    | 65139    |
|    total_timesteps | 234400   |
---------------------------------
Eval num_timesteps=234600, episode_reward=17.22 +/- 15.16
Episode length: 24.80 +/- 20.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.8         |
|    mean_reward          | 17.2         |
| time/                   |              |
|    total_timesteps      | 234600       |
| train/                  |              |
|    approx_kl            | 0.0028025475 |
|    clip_fraction        | 0.0703       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.62        |
|    explained_variance   | 0.255        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.25         |
|    n_updates            | 5860         |
|    policy_gradient_loss | -0.0069      |
|    value_loss           | 2.34         |
------------------------------------------
Eval num_timesteps=234800, episode_reward=5.61 +/- 2.97
Episode length: 9.20 +/- 4.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.2      |
|    mean_reward     | 5.61     |
| time/              |          |
|    total_timesteps | 234800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.4     |
|    ep_rew_mean     | 16.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 587      |
|    time_elapsed    | 65249    |
|    total_timesteps | 234800   |
---------------------------------
Eval num_timesteps=235000, episode_reward=17.31 +/- 10.86
Episode length: 24.40 +/- 14.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.4         |
|    mean_reward          | 17.3         |
| time/                   |              |
|    total_timesteps      | 235000       |
| train/                  |              |
|    approx_kl            | 0.0040580123 |
|    clip_fraction        | 0.153        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.634       |
|    explained_variance   | 0.358        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.469        |
|    n_updates            | 5870         |
|    policy_gradient_loss | -0.0126      |
|    value_loss           | 0.995        |
------------------------------------------
Eval num_timesteps=235200, episode_reward=10.79 +/- 9.34
Episode length: 16.60 +/- 13.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | 10.8     |
| time/              |          |
|    total_timesteps | 235200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.5     |
|    ep_rew_mean     | 16.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 588      |
|    time_elapsed    | 65347    |
|    total_timesteps | 235200   |
---------------------------------
Eval num_timesteps=235400, episode_reward=7.37 +/- 3.63
Episode length: 11.20 +/- 5.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 11.2         |
|    mean_reward          | 7.37         |
| time/                   |              |
|    total_timesteps      | 235400       |
| train/                  |              |
|    approx_kl            | 0.0011815892 |
|    clip_fraction        | 0.0801       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.601       |
|    explained_variance   | 0.148        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.09         |
|    n_updates            | 5880         |
|    policy_gradient_loss | -0.00478     |
|    value_loss           | 2.91         |
------------------------------------------
Eval num_timesteps=235600, episode_reward=18.88 +/- 14.34
Episode length: 27.20 +/- 19.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.2     |
|    mean_reward     | 18.9     |
| time/              |          |
|    total_timesteps | 235600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24       |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 589      |
|    time_elapsed    | 65440    |
|    total_timesteps | 235600   |
---------------------------------
Eval num_timesteps=235800, episode_reward=23.00 +/- 15.88
Episode length: 32.40 +/- 21.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.4         |
|    mean_reward          | 23           |
| time/                   |              |
|    total_timesteps      | 235800       |
| train/                  |              |
|    approx_kl            | 0.0023467161 |
|    clip_fraction        | 0.0922       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.609       |
|    explained_variance   | -0.0296      |
|    learning_rate        | 0.0001       |
|    loss                 | 2.32         |
|    n_updates            | 5890         |
|    policy_gradient_loss | -0.00744     |
|    value_loss           | 2.52         |
------------------------------------------
Eval num_timesteps=236000, episode_reward=9.00 +/- 13.87
Episode length: 13.20 +/- 18.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.2     |
|    mean_reward     | 9        |
| time/              |          |
|    total_timesteps | 236000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.4     |
|    ep_rew_mean     | 16.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 590      |
|    time_elapsed    | 65570    |
|    total_timesteps | 236000   |
---------------------------------
Eval num_timesteps=236200, episode_reward=12.98 +/- 10.79
Episode length: 19.80 +/- 15.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 19.8         |
|    mean_reward          | 13           |
| time/                   |              |
|    total_timesteps      | 236200       |
| train/                  |              |
|    approx_kl            | 0.0028197193 |
|    clip_fraction        | 0.0795       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.617       |
|    explained_variance   | 0.261        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.435        |
|    n_updates            | 5900         |
|    policy_gradient_loss | -0.00532     |
|    value_loss           | 1.52         |
------------------------------------------
Eval num_timesteps=236400, episode_reward=28.43 +/- 9.87
Episode length: 42.80 +/- 14.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.8     |
|    mean_reward     | 28.4     |
| time/              |          |
|    total_timesteps | 236400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.2     |
|    ep_rew_mean     | 17.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 591      |
|    time_elapsed    | 65685    |
|    total_timesteps | 236400   |
---------------------------------
Eval num_timesteps=236600, episode_reward=28.22 +/- 11.95
Episode length: 41.40 +/- 17.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.4         |
|    mean_reward          | 28.2         |
| time/                   |              |
|    total_timesteps      | 236600       |
| train/                  |              |
|    approx_kl            | 0.0034224626 |
|    clip_fraction        | 0.11         |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.63        |
|    explained_variance   | 0.368        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.861        |
|    n_updates            | 5910         |
|    policy_gradient_loss | -0.00735     |
|    value_loss           | 1.96         |
------------------------------------------
Eval num_timesteps=236800, episode_reward=23.68 +/- 15.05
Episode length: 33.40 +/- 20.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 23.7     |
| time/              |          |
|    total_timesteps | 236800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.6     |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 592      |
|    time_elapsed    | 65837    |
|    total_timesteps | 236800   |
---------------------------------
Eval num_timesteps=237000, episode_reward=16.43 +/- 15.27
Episode length: 24.00 +/- 21.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24           |
|    mean_reward          | 16.4         |
| time/                   |              |
|    total_timesteps      | 237000       |
| train/                  |              |
|    approx_kl            | 0.0021445358 |
|    clip_fraction        | 0.0864       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.621       |
|    explained_variance   | 0.254        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.797        |
|    n_updates            | 5920         |
|    policy_gradient_loss | -0.00445     |
|    value_loss           | 1.84         |
------------------------------------------
Eval num_timesteps=237200, episode_reward=11.29 +/- 11.69
Episode length: 17.20 +/- 16.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | 11.3     |
| time/              |          |
|    total_timesteps | 237200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | 17.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 593      |
|    time_elapsed    | 65943    |
|    total_timesteps | 237200   |
---------------------------------
Eval num_timesteps=237400, episode_reward=22.16 +/- 16.01
Episode length: 31.80 +/- 22.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 31.8         |
|    mean_reward          | 22.2         |
| time/                   |              |
|    total_timesteps      | 237400       |
| train/                  |              |
|    approx_kl            | 0.0031872874 |
|    clip_fraction        | 0.145        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.617       |
|    explained_variance   | 0.464        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.433        |
|    n_updates            | 5930         |
|    policy_gradient_loss | -0.00967     |
|    value_loss           | 1.57         |
------------------------------------------
Eval num_timesteps=237600, episode_reward=17.36 +/- 14.49
Episode length: 25.60 +/- 20.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 237600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.2     |
|    ep_rew_mean     | 17.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 594      |
|    time_elapsed    | 66054    |
|    total_timesteps | 237600   |
---------------------------------
Eval num_timesteps=237800, episode_reward=9.72 +/- 8.26
Episode length: 15.00 +/- 11.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15          |
|    mean_reward          | 9.72        |
| time/                   |             |
|    total_timesteps      | 237800      |
| train/                  |             |
|    approx_kl            | 0.002092468 |
|    clip_fraction        | 0.0569      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.596      |
|    explained_variance   | 0.199       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.512       |
|    n_updates            | 5940        |
|    policy_gradient_loss | -0.00653    |
|    value_loss           | 2.39        |
-----------------------------------------
Eval num_timesteps=238000, episode_reward=15.58 +/- 16.35
Episode length: 22.60 +/- 22.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 15.6     |
| time/              |          |
|    total_timesteps | 238000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | 17.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 595      |
|    time_elapsed    | 66148    |
|    total_timesteps | 238000   |
---------------------------------
Eval num_timesteps=238200, episode_reward=15.13 +/- 15.35
Episode length: 22.80 +/- 22.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22.8         |
|    mean_reward          | 15.1         |
| time/                   |              |
|    total_timesteps      | 238200       |
| train/                  |              |
|    approx_kl            | 0.0028463353 |
|    clip_fraction        | 0.115        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.6         |
|    explained_variance   | 0.336        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.538        |
|    n_updates            | 5950         |
|    policy_gradient_loss | -0.0083      |
|    value_loss           | 2.44         |
------------------------------------------
Eval num_timesteps=238400, episode_reward=11.03 +/- 13.10
Episode length: 15.80 +/- 17.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | 11       |
| time/              |          |
|    total_timesteps | 238400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.1     |
|    ep_rew_mean     | 15.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 596      |
|    time_elapsed    | 66250    |
|    total_timesteps | 238400   |
---------------------------------
Eval num_timesteps=238600, episode_reward=17.76 +/- 14.42
Episode length: 25.80 +/- 20.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.8         |
|    mean_reward          | 17.8         |
| time/                   |              |
|    total_timesteps      | 238600       |
| train/                  |              |
|    approx_kl            | 0.0027763115 |
|    clip_fraction        | 0.0933       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.603       |
|    explained_variance   | 0.258        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.59         |
|    n_updates            | 5960         |
|    policy_gradient_loss | -0.00516     |
|    value_loss           | 2.6          |
------------------------------------------
Eval num_timesteps=238800, episode_reward=16.56 +/- 15.28
Episode length: 24.20 +/- 21.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 238800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.6     |
|    ep_rew_mean     | 14.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 597      |
|    time_elapsed    | 66356    |
|    total_timesteps | 238800   |
---------------------------------
Eval num_timesteps=239000, episode_reward=12.63 +/- 12.98
Episode length: 18.00 +/- 17.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18           |
|    mean_reward          | 12.6         |
| time/                   |              |
|    total_timesteps      | 239000       |
| train/                  |              |
|    approx_kl            | 0.0012572771 |
|    clip_fraction        | 0.069        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.59        |
|    explained_variance   | 0.391        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.994        |
|    n_updates            | 5970         |
|    policy_gradient_loss | -0.00605     |
|    value_loss           | 2.51         |
------------------------------------------
Eval num_timesteps=239200, episode_reward=12.60 +/- 12.09
Episode length: 18.40 +/- 16.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.4     |
|    mean_reward     | 12.6     |
| time/              |          |
|    total_timesteps | 239200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.2     |
|    ep_rew_mean     | 13.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 598      |
|    time_elapsed    | 66459    |
|    total_timesteps | 239200   |
---------------------------------
Eval num_timesteps=239400, episode_reward=15.81 +/- 15.26
Episode length: 23.40 +/- 21.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.4         |
|    mean_reward          | 15.8         |
| time/                   |              |
|    total_timesteps      | 239400       |
| train/                  |              |
|    approx_kl            | 0.0014029782 |
|    clip_fraction        | 0.0411       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.595       |
|    explained_variance   | 0.433        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.705        |
|    n_updates            | 5980         |
|    policy_gradient_loss | -0.00519     |
|    value_loss           | 2.57         |
------------------------------------------
Eval num_timesteps=239600, episode_reward=19.13 +/- 14.34
Episode length: 26.80 +/- 18.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.8     |
|    mean_reward     | 19.1     |
| time/              |          |
|    total_timesteps | 239600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.8     |
|    ep_rew_mean     | 13.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 599      |
|    time_elapsed    | 66564    |
|    total_timesteps | 239600   |
---------------------------------
Eval num_timesteps=239800, episode_reward=9.18 +/- 12.69
Episode length: 14.00 +/- 18.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14           |
|    mean_reward          | 9.18         |
| time/                   |              |
|    total_timesteps      | 239800       |
| train/                  |              |
|    approx_kl            | 0.0018591274 |
|    clip_fraction        | 0.0694       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.629       |
|    explained_variance   | 0.34         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.158        |
|    n_updates            | 5990         |
|    policy_gradient_loss | -0.00664     |
|    value_loss           | 1.78         |
------------------------------------------
Eval num_timesteps=240000, episode_reward=17.92 +/- 15.34
Episode length: 25.20 +/- 20.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 240000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.4     |
|    ep_rew_mean     | 14.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 600      |
|    time_elapsed    | 66670    |
|    total_timesteps | 240000   |
---------------------------------
Eval num_timesteps=240200, episode_reward=16.59 +/- 14.02
Episode length: 25.00 +/- 20.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25           |
|    mean_reward          | 16.6         |
| time/                   |              |
|    total_timesteps      | 240200       |
| train/                  |              |
|    approx_kl            | 0.0023880017 |
|    clip_fraction        | 0.104        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.624       |
|    explained_variance   | 0.471        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.497        |
|    n_updates            | 6000         |
|    policy_gradient_loss | -0.00595     |
|    value_loss           | 1.01         |
------------------------------------------
Eval num_timesteps=240400, episode_reward=23.13 +/- 14.41
Episode length: 33.60 +/- 20.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 23.1     |
| time/              |          |
|    total_timesteps | 240400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.9     |
|    ep_rew_mean     | 14.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 601      |
|    time_elapsed    | 66780    |
|    total_timesteps | 240400   |
---------------------------------
Eval num_timesteps=240600, episode_reward=19.77 +/- 13.42
Episode length: 28.80 +/- 19.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 28.8         |
|    mean_reward          | 19.8         |
| time/                   |              |
|    total_timesteps      | 240600       |
| train/                  |              |
|    approx_kl            | 0.0034238158 |
|    clip_fraction        | 0.12         |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.603       |
|    explained_variance   | 0.371        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.86         |
|    n_updates            | 6010         |
|    policy_gradient_loss | -0.00701     |
|    value_loss           | 2.36         |
------------------------------------------
Eval num_timesteps=240800, episode_reward=5.22 +/- 2.94
Episode length: 8.40 +/- 4.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8.4      |
|    mean_reward     | 5.22     |
| time/              |          |
|    total_timesteps | 240800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20       |
|    ep_rew_mean     | 13.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 602      |
|    time_elapsed    | 66869    |
|    total_timesteps | 240800   |
---------------------------------
Eval num_timesteps=241000, episode_reward=11.44 +/- 11.57
Episode length: 17.40 +/- 16.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.4         |
|    mean_reward          | 11.4         |
| time/                   |              |
|    total_timesteps      | 241000       |
| train/                  |              |
|    approx_kl            | 0.0034677484 |
|    clip_fraction        | 0.0757       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.579       |
|    explained_variance   | 0.231        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.05         |
|    n_updates            | 6020         |
|    policy_gradient_loss | -0.00567     |
|    value_loss           | 3.13         |
------------------------------------------
Eval num_timesteps=241200, episode_reward=28.67 +/- 12.19
Episode length: 41.40 +/- 17.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | 28.7     |
| time/              |          |
|    total_timesteps | 241200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.8     |
|    ep_rew_mean     | 14       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 603      |
|    time_elapsed    | 66974    |
|    total_timesteps | 241200   |
---------------------------------
Eval num_timesteps=241400, episode_reward=16.26 +/- 14.15
Episode length: 24.60 +/- 20.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.6         |
|    mean_reward          | 16.3         |
| time/                   |              |
|    total_timesteps      | 241400       |
| train/                  |              |
|    approx_kl            | 0.0014213115 |
|    clip_fraction        | 0.0672       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.604       |
|    explained_variance   | 0.514        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.112        |
|    n_updates            | 6030         |
|    policy_gradient_loss | -0.00323     |
|    value_loss           | 1.76         |
------------------------------------------
Eval num_timesteps=241600, episode_reward=14.96 +/- 15.94
Episode length: 22.20 +/- 22.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 15       |
| time/              |          |
|    total_timesteps | 241600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.7     |
|    ep_rew_mean     | 14.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 604      |
|    time_elapsed    | 67076    |
|    total_timesteps | 241600   |
---------------------------------
Eval num_timesteps=241800, episode_reward=16.41 +/- 15.38
Episode length: 24.00 +/- 21.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 24          |
|    mean_reward          | 16.4        |
| time/                   |             |
|    total_timesteps      | 241800      |
| train/                  |             |
|    approx_kl            | 0.002019872 |
|    clip_fraction        | 0.0583      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.608      |
|    explained_variance   | 0.475       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.09        |
|    n_updates            | 6040        |
|    policy_gradient_loss | -0.00451    |
|    value_loss           | 2.06        |
-----------------------------------------
Eval num_timesteps=242000, episode_reward=16.77 +/- 13.56
Episode length: 25.60 +/- 19.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 242000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.8     |
|    ep_rew_mean     | 14.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 605      |
|    time_elapsed    | 67209    |
|    total_timesteps | 242000   |
---------------------------------
Eval num_timesteps=242200, episode_reward=10.13 +/- 13.47
Episode length: 14.60 +/- 17.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.6        |
|    mean_reward          | 10.1        |
| time/                   |             |
|    total_timesteps      | 242200      |
| train/                  |             |
|    approx_kl            | 0.003038134 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.616      |
|    explained_variance   | 0.272       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.449       |
|    n_updates            | 6050        |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 2.4         |
-----------------------------------------
Eval num_timesteps=242400, episode_reward=17.99 +/- 13.34
Episode length: 27.00 +/- 19.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27       |
|    mean_reward     | 18       |
| time/              |          |
|    total_timesteps | 242400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.9     |
|    ep_rew_mean     | 14.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 606      |
|    time_elapsed    | 67318    |
|    total_timesteps | 242400   |
---------------------------------
Eval num_timesteps=242600, episode_reward=3.52 +/- 0.98
Episode length: 6.00 +/- 1.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 6            |
|    mean_reward          | 3.52         |
| time/                   |              |
|    total_timesteps      | 242600       |
| train/                  |              |
|    approx_kl            | 0.0022449258 |
|    clip_fraction        | 0.0596       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.599       |
|    explained_variance   | 0.424        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.482        |
|    n_updates            | 6060         |
|    policy_gradient_loss | -0.00437     |
|    value_loss           | 1.64         |
------------------------------------------
Eval num_timesteps=242800, episode_reward=11.55 +/- 12.25
Episode length: 17.20 +/- 16.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | 11.5     |
| time/              |          |
|    total_timesteps | 242800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.3     |
|    ep_rew_mean     | 15.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 607      |
|    time_elapsed    | 67400    |
|    total_timesteps | 242800   |
---------------------------------
Eval num_timesteps=243000, episode_reward=17.56 +/- 14.54
Episode length: 25.60 +/- 20.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.6         |
|    mean_reward          | 17.6         |
| time/                   |              |
|    total_timesteps      | 243000       |
| train/                  |              |
|    approx_kl            | 0.0044962387 |
|    clip_fraction        | 0.181        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.634       |
|    explained_variance   | 0.325        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.78         |
|    n_updates            | 6070         |
|    policy_gradient_loss | -0.00711     |
|    value_loss           | 0.954        |
------------------------------------------
Eval num_timesteps=243200, episode_reward=12.19 +/- 12.44
Episode length: 17.40 +/- 16.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 12.2     |
| time/              |          |
|    total_timesteps | 243200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.4     |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 608      |
|    time_elapsed    | 67503    |
|    total_timesteps | 243200   |
---------------------------------
Eval num_timesteps=243400, episode_reward=21.78 +/- 16.92
Episode length: 31.00 +/- 23.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 31           |
|    mean_reward          | 21.8         |
| time/                   |              |
|    total_timesteps      | 243400       |
| train/                  |              |
|    approx_kl            | 0.0022503787 |
|    clip_fraction        | 0.0786       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.614       |
|    explained_variance   | 0.333        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.328        |
|    n_updates            | 6080         |
|    policy_gradient_loss | -0.00522     |
|    value_loss           | 1.37         |
------------------------------------------
Eval num_timesteps=243600, episode_reward=10.79 +/- 12.09
Episode length: 16.20 +/- 17.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | 10.8     |
| time/              |          |
|    total_timesteps | 243600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.2     |
|    ep_rew_mean     | 17.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 609      |
|    time_elapsed    | 67606    |
|    total_timesteps | 243600   |
---------------------------------
Eval num_timesteps=243800, episode_reward=12.49 +/- 11.74
Episode length: 18.60 +/- 16.04
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 18.6       |
|    mean_reward          | 12.5       |
| time/                   |            |
|    total_timesteps      | 243800     |
| train/                  |            |
|    approx_kl            | 0.00234379 |
|    clip_fraction        | 0.0699     |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.586     |
|    explained_variance   | 0.252      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.737      |
|    n_updates            | 6090       |
|    policy_gradient_loss | -0.00557   |
|    value_loss           | 2.35       |
----------------------------------------
Eval num_timesteps=244000, episode_reward=11.26 +/- 11.43
Episode length: 17.40 +/- 16.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 11.3     |
| time/              |          |
|    total_timesteps | 244000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.4     |
|    ep_rew_mean     | 18.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 610      |
|    time_elapsed    | 67712    |
|    total_timesteps | 244000   |
---------------------------------
Eval num_timesteps=244200, episode_reward=21.65 +/- 15.73
Episode length: 31.80 +/- 22.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 31.8        |
|    mean_reward          | 21.7        |
| time/                   |             |
|    total_timesteps      | 244200      |
| train/                  |             |
|    approx_kl            | 0.003295973 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.616      |
|    explained_variance   | 0.183       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.132       |
|    n_updates            | 6100        |
|    policy_gradient_loss | -0.009      |
|    value_loss           | 1.24        |
-----------------------------------------
Eval num_timesteps=244400, episode_reward=23.02 +/- 14.05
Episode length: 33.60 +/- 20.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 23       |
| time/              |          |
|    total_timesteps | 244400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.3     |
|    ep_rew_mean     | 19.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 611      |
|    time_elapsed    | 67823    |
|    total_timesteps | 244400   |
---------------------------------
Eval num_timesteps=244600, episode_reward=9.32 +/- 12.63
Episode length: 14.20 +/- 17.98
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 14.2       |
|    mean_reward          | 9.32       |
| time/                   |            |
|    total_timesteps      | 244600     |
| train/                  |            |
|    approx_kl            | 0.00417541 |
|    clip_fraction        | 0.14       |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.611     |
|    explained_variance   | 0.217      |
|    learning_rate        | 0.0001     |
|    loss                 | 1.67       |
|    n_updates            | 6110       |
|    policy_gradient_loss | -0.00598   |
|    value_loss           | 1.28       |
----------------------------------------
Eval num_timesteps=244800, episode_reward=24.42 +/- 12.33
Episode length: 35.80 +/- 17.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 24.4     |
| time/              |          |
|    total_timesteps | 244800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28       |
|    ep_rew_mean     | 19.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 612      |
|    time_elapsed    | 67932    |
|    total_timesteps | 244800   |
---------------------------------
Eval num_timesteps=245000, episode_reward=10.62 +/- 12.59
Episode length: 15.60 +/- 17.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.6        |
|    mean_reward          | 10.6        |
| time/                   |             |
|    total_timesteps      | 245000      |
| train/                  |             |
|    approx_kl            | 0.001657354 |
|    clip_fraction        | 0.0616      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.593      |
|    explained_variance   | 0.33        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.967       |
|    n_updates            | 6120        |
|    policy_gradient_loss | -0.00768    |
|    value_loss           | 2.37        |
-----------------------------------------
Eval num_timesteps=245200, episode_reward=28.17 +/- 13.74
Episode length: 40.40 +/- 19.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.4     |
|    mean_reward     | 28.2     |
| time/              |          |
|    total_timesteps | 245200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.5     |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 613      |
|    time_elapsed    | 68060    |
|    total_timesteps | 245200   |
---------------------------------
Eval num_timesteps=245400, episode_reward=4.46 +/- 4.36
Episode length: 7.40 +/- 6.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 7.4          |
|    mean_reward          | 4.46         |
| time/                   |              |
|    total_timesteps      | 245400       |
| train/                  |              |
|    approx_kl            | 0.0021632502 |
|    clip_fraction        | 0.0645       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.59        |
|    explained_variance   | 0.403        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.739        |
|    n_updates            | 6130         |
|    policy_gradient_loss | -0.00578     |
|    value_loss           | 2.13         |
------------------------------------------
Eval num_timesteps=245600, episode_reward=7.03 +/- 2.56
Episode length: 11.20 +/- 3.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 11.2     |
|    mean_reward     | 7.03     |
| time/              |          |
|    total_timesteps | 245600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.8     |
|    ep_rew_mean     | 16.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 614      |
|    time_elapsed    | 68138    |
|    total_timesteps | 245600   |
---------------------------------
Eval num_timesteps=245800, episode_reward=29.81 +/- 12.67
Episode length: 41.40 +/- 17.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 41.4          |
|    mean_reward          | 29.8          |
| time/                   |               |
|    total_timesteps      | 245800        |
| train/                  |               |
|    approx_kl            | 0.00074361276 |
|    clip_fraction        | 0.0382        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.58         |
|    explained_variance   | 0.442         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.0622        |
|    n_updates            | 6140          |
|    policy_gradient_loss | -0.00317      |
|    value_loss           | 1.93          |
-------------------------------------------
Eval num_timesteps=246000, episode_reward=16.65 +/- 14.69
Episode length: 24.60 +/- 20.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 246000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24       |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 615      |
|    time_elapsed    | 68271    |
|    total_timesteps | 246000   |
---------------------------------
Eval num_timesteps=246200, episode_reward=29.44 +/- 10.14
Episode length: 42.80 +/- 14.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.8        |
|    mean_reward          | 29.4        |
| time/                   |             |
|    total_timesteps      | 246200      |
| train/                  |             |
|    approx_kl            | 0.001895679 |
|    clip_fraction        | 0.0788      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.61       |
|    explained_variance   | 0.564       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.482       |
|    n_updates            | 6150        |
|    policy_gradient_loss | -0.00375    |
|    value_loss           | 1.4         |
-----------------------------------------
Eval num_timesteps=246400, episode_reward=6.78 +/- 3.61
Episode length: 10.80 +/- 5.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10.8     |
|    mean_reward     | 6.78     |
| time/              |          |
|    total_timesteps | 246400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.7     |
|    ep_rew_mean     | 15.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 616      |
|    time_elapsed    | 68386    |
|    total_timesteps | 246400   |
---------------------------------
Eval num_timesteps=246600, episode_reward=3.48 +/- 1.58
Episode length: 5.80 +/- 2.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 5.8         |
|    mean_reward          | 3.48        |
| time/                   |             |
|    total_timesteps      | 246600      |
| train/                  |             |
|    approx_kl            | 0.002047939 |
|    clip_fraction        | 0.0453      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.613      |
|    explained_variance   | -0.00929    |
|    learning_rate        | 0.0001      |
|    loss                 | 1.46        |
|    n_updates            | 6160        |
|    policy_gradient_loss | -0.00561    |
|    value_loss           | 2.23        |
-----------------------------------------
Eval num_timesteps=246800, episode_reward=10.99 +/- 11.22
Episode length: 16.80 +/- 16.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | 11       |
| time/              |          |
|    total_timesteps | 246800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22       |
|    ep_rew_mean     | 15.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 617      |
|    time_elapsed    | 68472    |
|    total_timesteps | 246800   |
---------------------------------
Eval num_timesteps=247000, episode_reward=14.29 +/- 10.88
Episode length: 21.20 +/- 14.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 21.2         |
|    mean_reward          | 14.3         |
| time/                   |              |
|    total_timesteps      | 247000       |
| train/                  |              |
|    approx_kl            | 0.0031756884 |
|    clip_fraction        | 0.0826       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.574       |
|    explained_variance   | 0.371        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.58         |
|    n_updates            | 6170         |
|    policy_gradient_loss | -0.00679     |
|    value_loss           | 2.99         |
------------------------------------------
Eval num_timesteps=247200, episode_reward=9.97 +/- 4.63
Episode length: 15.40 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | 9.97     |
| time/              |          |
|    total_timesteps | 247200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.8     |
|    ep_rew_mean     | 15       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 618      |
|    time_elapsed    | 68580    |
|    total_timesteps | 247200   |
---------------------------------
Eval num_timesteps=247400, episode_reward=19.62 +/- 14.46
Episode length: 27.80 +/- 19.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 27.8         |
|    mean_reward          | 19.6         |
| time/                   |              |
|    total_timesteps      | 247400       |
| train/                  |              |
|    approx_kl            | 0.0016259479 |
|    clip_fraction        | 0.0866       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.573       |
|    explained_variance   | 0.451        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.458        |
|    n_updates            | 6180         |
|    policy_gradient_loss | -0.00519     |
|    value_loss           | 1.92         |
------------------------------------------
Eval num_timesteps=247600, episode_reward=21.86 +/- 15.12
Episode length: 32.40 +/- 21.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.4     |
|    mean_reward     | 21.9     |
| time/              |          |
|    total_timesteps | 247600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.6     |
|    ep_rew_mean     | 15.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 619      |
|    time_elapsed    | 68700    |
|    total_timesteps | 247600   |
---------------------------------
Eval num_timesteps=247800, episode_reward=5.38 +/- 3.26
Episode length: 8.60 +/- 5.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 8.6         |
|    mean_reward          | 5.38        |
| time/                   |             |
|    total_timesteps      | 247800      |
| train/                  |             |
|    approx_kl            | 0.001530187 |
|    clip_fraction        | 0.0408      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.581      |
|    explained_variance   | 0.327       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.781       |
|    n_updates            | 6190        |
|    policy_gradient_loss | -0.00458    |
|    value_loss           | 2.18        |
-----------------------------------------
Eval num_timesteps=248000, episode_reward=16.74 +/- 14.63
Episode length: 24.60 +/- 20.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 248000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.2     |
|    ep_rew_mean     | 15.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 620      |
|    time_elapsed    | 68792    |
|    total_timesteps | 248000   |
---------------------------------
Eval num_timesteps=248200, episode_reward=24.24 +/- 12.73
Episode length: 35.20 +/- 18.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.2         |
|    mean_reward          | 24.2         |
| time/                   |              |
|    total_timesteps      | 248200       |
| train/                  |              |
|    approx_kl            | 0.0027722472 |
|    clip_fraction        | 0.121        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.591       |
|    explained_variance   | 0.192        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.4          |
|    n_updates            | 6200         |
|    policy_gradient_loss | -0.00703     |
|    value_loss           | 2.8          |
------------------------------------------
Eval num_timesteps=248400, episode_reward=4.28 +/- 3.07
Episode length: 6.80 +/- 4.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6.8      |
|    mean_reward     | 4.28     |
| time/              |          |
|    total_timesteps | 248400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22       |
|    ep_rew_mean     | 15.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 621      |
|    time_elapsed    | 68885    |
|    total_timesteps | 248400   |
---------------------------------
Eval num_timesteps=248600, episode_reward=22.60 +/- 15.50
Episode length: 32.40 +/- 21.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.4         |
|    mean_reward          | 22.6         |
| time/                   |              |
|    total_timesteps      | 248600       |
| train/                  |              |
|    approx_kl            | 0.0014803865 |
|    clip_fraction        | 0.0525       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.59        |
|    explained_variance   | 0.179        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.54         |
|    n_updates            | 6210         |
|    policy_gradient_loss | -0.00439     |
|    value_loss           | 2.88         |
------------------------------------------
Eval num_timesteps=248800, episode_reward=17.99 +/- 13.50
Episode length: 26.60 +/- 19.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.6     |
|    mean_reward     | 18       |
| time/              |          |
|    total_timesteps | 248800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.2     |
|    ep_rew_mean     | 15.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 622      |
|    time_elapsed    | 68995    |
|    total_timesteps | 248800   |
---------------------------------
Eval num_timesteps=249000, episode_reward=10.45 +/- 12.39
Episode length: 15.80 +/- 17.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | 10.5         |
| time/                   |              |
|    total_timesteps      | 249000       |
| train/                  |              |
|    approx_kl            | 0.0020212478 |
|    clip_fraction        | 0.0824       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.604       |
|    explained_variance   | 0.252        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.215        |
|    n_updates            | 6220         |
|    policy_gradient_loss | -0.00638     |
|    value_loss           | 1.27         |
------------------------------------------
Eval num_timesteps=249200, episode_reward=14.26 +/- 10.78
Episode length: 21.20 +/- 15.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.2     |
|    mean_reward     | 14.3     |
| time/              |          |
|    total_timesteps | 249200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.3     |
|    ep_rew_mean     | 16       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 623      |
|    time_elapsed    | 69106    |
|    total_timesteps | 249200   |
---------------------------------
Eval num_timesteps=249400, episode_reward=23.15 +/- 15.71
Episode length: 32.60 +/- 21.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.6         |
|    mean_reward          | 23.2         |
| time/                   |              |
|    total_timesteps      | 249400       |
| train/                  |              |
|    approx_kl            | 0.0022285595 |
|    clip_fraction        | 0.0946       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.602       |
|    explained_variance   | 0.155        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.41         |
|    n_updates            | 6230         |
|    policy_gradient_loss | -0.00689     |
|    value_loss           | 2.79         |
------------------------------------------
Eval num_timesteps=249600, episode_reward=17.91 +/- 15.03
Episode length: 25.60 +/- 20.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 249600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | 17.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 624      |
|    time_elapsed    | 69214    |
|    total_timesteps | 249600   |
---------------------------------
Eval num_timesteps=249800, episode_reward=15.48 +/- 15.98
Episode length: 22.60 +/- 22.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22.6         |
|    mean_reward          | 15.5         |
| time/                   |              |
|    total_timesteps      | 249800       |
| train/                  |              |
|    approx_kl            | 0.0032322418 |
|    clip_fraction        | 0.109        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.604       |
|    explained_variance   | 0.173        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.79         |
|    n_updates            | 6240         |
|    policy_gradient_loss | -0.00617     |
|    value_loss           | 1.89         |
------------------------------------------
Eval num_timesteps=250000, episode_reward=18.29 +/- 14.39
Episode length: 26.40 +/- 20.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.4     |
|    mean_reward     | 18.3     |
| time/              |          |
|    total_timesteps | 250000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25       |
|    ep_rew_mean     | 17.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 625      |
|    time_elapsed    | 69327    |
|    total_timesteps | 250000   |
---------------------------------
Eval num_timesteps=250200, episode_reward=18.00 +/- 14.12
Episode length: 26.40 +/- 19.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.4         |
|    mean_reward          | 18           |
| time/                   |              |
|    total_timesteps      | 250200       |
| train/                  |              |
|    approx_kl            | 0.0054521034 |
|    clip_fraction        | 0.156        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.62        |
|    explained_variance   | 0.412        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.573        |
|    n_updates            | 6250         |
|    policy_gradient_loss | -0.00802     |
|    value_loss           | 1.34         |
------------------------------------------
Eval num_timesteps=250400, episode_reward=29.45 +/- 11.78
Episode length: 42.00 +/- 16.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 29.4     |
| time/              |          |
|    total_timesteps | 250400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.2     |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 626      |
|    time_elapsed    | 69459    |
|    total_timesteps | 250400   |
---------------------------------
Eval num_timesteps=250600, episode_reward=24.05 +/- 12.96
Episode length: 35.20 +/- 18.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.2         |
|    mean_reward          | 24           |
| time/                   |              |
|    total_timesteps      | 250600       |
| train/                  |              |
|    approx_kl            | 0.0020426142 |
|    clip_fraction        | 0.0875       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.563       |
|    explained_variance   | 0.381        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.237        |
|    n_updates            | 6260         |
|    policy_gradient_loss | -0.00741     |
|    value_loss           | 1.35         |
------------------------------------------
Eval num_timesteps=250800, episode_reward=16.51 +/- 15.23
Episode length: 24.00 +/- 21.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 250800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.1     |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 627      |
|    time_elapsed    | 69561    |
|    total_timesteps | 250800   |
---------------------------------
Eval num_timesteps=251000, episode_reward=29.36 +/- 13.01
Episode length: 41.20 +/- 17.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.2         |
|    mean_reward          | 29.4         |
| time/                   |              |
|    total_timesteps      | 251000       |
| train/                  |              |
|    approx_kl            | 0.0020048236 |
|    clip_fraction        | 0.0692       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.525       |
|    explained_variance   | 0.247        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.39         |
|    n_updates            | 6270         |
|    policy_gradient_loss | -0.00549     |
|    value_loss           | 3.17         |
------------------------------------------
Eval num_timesteps=251200, episode_reward=13.41 +/- 13.96
Episode length: 19.20 +/- 18.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.2     |
|    mean_reward     | 13.4     |
| time/              |          |
|    total_timesteps | 251200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.6     |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 628      |
|    time_elapsed    | 69691    |
|    total_timesteps | 251200   |
---------------------------------
Eval num_timesteps=251400, episode_reward=5.93 +/- 1.42
Episode length: 9.40 +/- 1.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 9.4          |
|    mean_reward          | 5.93         |
| time/                   |              |
|    total_timesteps      | 251400       |
| train/                  |              |
|    approx_kl            | 0.0021184548 |
|    clip_fraction        | 0.0415       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.539       |
|    explained_variance   | 0.399        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.595        |
|    n_updates            | 6280         |
|    policy_gradient_loss | -0.00554     |
|    value_loss           | 2.09         |
------------------------------------------
Eval num_timesteps=251600, episode_reward=24.77 +/- 13.11
Episode length: 35.80 +/- 17.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 24.8     |
| time/              |          |
|    total_timesteps | 251600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | 17       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 629      |
|    time_elapsed    | 69790    |
|    total_timesteps | 251600   |
---------------------------------
Eval num_timesteps=251800, episode_reward=12.13 +/- 12.37
Episode length: 17.60 +/- 16.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.6         |
|    mean_reward          | 12.1         |
| time/                   |              |
|    total_timesteps      | 251800       |
| train/                  |              |
|    approx_kl            | 0.0014259256 |
|    clip_fraction        | 0.0402       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.564       |
|    explained_variance   | 0.268        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.367        |
|    n_updates            | 6290         |
|    policy_gradient_loss | -0.00295     |
|    value_loss           | 2.13         |
------------------------------------------
Eval num_timesteps=252000, episode_reward=21.41 +/- 15.57
Episode length: 31.80 +/- 22.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.8     |
|    mean_reward     | 21.4     |
| time/              |          |
|    total_timesteps | 252000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.9     |
|    ep_rew_mean     | 15.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 630      |
|    time_elapsed    | 69895    |
|    total_timesteps | 252000   |
---------------------------------
Eval num_timesteps=252200, episode_reward=11.60 +/- 12.01
Episode length: 17.20 +/- 16.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.2         |
|    mean_reward          | 11.6         |
| time/                   |              |
|    total_timesteps      | 252200       |
| train/                  |              |
|    approx_kl            | 0.0016382987 |
|    clip_fraction        | 0.0502       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.532       |
|    explained_variance   | 0.247        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.36         |
|    n_updates            | 6300         |
|    policy_gradient_loss | -0.00412     |
|    value_loss           | 3.1          |
------------------------------------------
Eval num_timesteps=252400, episode_reward=22.67 +/- 15.39
Episode length: 32.60 +/- 21.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.6     |
|    mean_reward     | 22.7     |
| time/              |          |
|    total_timesteps | 252400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.6     |
|    ep_rew_mean     | 15.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 631      |
|    time_elapsed    | 70002    |
|    total_timesteps | 252400   |
---------------------------------
Eval num_timesteps=252600, episode_reward=29.22 +/- 12.18
Episode length: 41.60 +/- 16.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.6         |
|    mean_reward          | 29.2         |
| time/                   |              |
|    total_timesteps      | 252600       |
| train/                  |              |
|    approx_kl            | 0.0014076914 |
|    clip_fraction        | 0.0393       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.591       |
|    explained_variance   | 0.347        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.852        |
|    n_updates            | 6310         |
|    policy_gradient_loss | -0.00444     |
|    value_loss           | 1.58         |
------------------------------------------
Eval num_timesteps=252800, episode_reward=22.09 +/- 14.86
Episode length: 32.60 +/- 21.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.6     |
|    mean_reward     | 22.1     |
| time/              |          |
|    total_timesteps | 252800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 632      |
|    time_elapsed    | 70113    |
|    total_timesteps | 252800   |
---------------------------------
Eval num_timesteps=253000, episode_reward=20.22 +/- 14.63
Episode length: 29.80 +/- 20.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29.8         |
|    mean_reward          | 20.2         |
| time/                   |              |
|    total_timesteps      | 253000       |
| train/                  |              |
|    approx_kl            | 0.0032805954 |
|    clip_fraction        | 0.096        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.576       |
|    explained_variance   | 0.431        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.57         |
|    n_updates            | 6320         |
|    policy_gradient_loss | -0.00877     |
|    value_loss           | 1.95         |
------------------------------------------
Eval num_timesteps=253200, episode_reward=16.93 +/- 15.33
Episode length: 24.40 +/- 21.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 253200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.7     |
|    ep_rew_mean     | 16.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 633      |
|    time_elapsed    | 70216    |
|    total_timesteps | 253200   |
---------------------------------
Eval num_timesteps=253400, episode_reward=24.44 +/- 13.87
Episode length: 35.00 +/- 19.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35           |
|    mean_reward          | 24.4         |
| time/                   |              |
|    total_timesteps      | 253400       |
| train/                  |              |
|    approx_kl            | 0.0017389606 |
|    clip_fraction        | 0.0741       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.581       |
|    explained_variance   | 0.236        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.111        |
|    n_updates            | 6330         |
|    policy_gradient_loss | -0.00465     |
|    value_loss           | 1.13         |
------------------------------------------
Eval num_timesteps=253600, episode_reward=23.36 +/- 14.11
Episode length: 33.80 +/- 19.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 23.4     |
| time/              |          |
|    total_timesteps | 253600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | 16.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 634      |
|    time_elapsed    | 70346    |
|    total_timesteps | 253600   |
---------------------------------
Eval num_timesteps=253800, episode_reward=9.64 +/- 13.66
Episode length: 14.00 +/- 18.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14           |
|    mean_reward          | 9.64         |
| time/                   |              |
|    total_timesteps      | 253800       |
| train/                  |              |
|    approx_kl            | 0.0023910399 |
|    clip_fraction        | 0.0663       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.535       |
|    explained_variance   | 0.367        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.736        |
|    n_updates            | 6340         |
|    policy_gradient_loss | -0.00448     |
|    value_loss           | 2            |
------------------------------------------
Eval num_timesteps=254000, episode_reward=9.14 +/- 12.72
Episode length: 14.00 +/- 18.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14       |
|    mean_reward     | 9.14     |
| time/              |          |
|    total_timesteps | 254000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.4     |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 635      |
|    time_elapsed    | 70450    |
|    total_timesteps | 254000   |
---------------------------------
Eval num_timesteps=254200, episode_reward=17.09 +/- 13.68
Episode length: 25.80 +/- 20.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 25.8        |
|    mean_reward          | 17.1        |
| time/                   |             |
|    total_timesteps      | 254200      |
| train/                  |             |
|    approx_kl            | 0.001826816 |
|    clip_fraction        | 0.0766      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.553      |
|    explained_variance   | 0.312       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.647       |
|    n_updates            | 6350        |
|    policy_gradient_loss | -0.0054     |
|    value_loss           | 1.45        |
-----------------------------------------
Eval num_timesteps=254400, episode_reward=23.34 +/- 15.47
Episode length: 32.80 +/- 21.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 23.3     |
| time/              |          |
|    total_timesteps | 254400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.4     |
|    ep_rew_mean     | 18.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 636      |
|    time_elapsed    | 70565    |
|    total_timesteps | 254400   |
---------------------------------
Eval num_timesteps=254600, episode_reward=18.77 +/- 12.35
Episode length: 28.20 +/- 18.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 28.2        |
|    mean_reward          | 18.8        |
| time/                   |             |
|    total_timesteps      | 254600      |
| train/                  |             |
|    approx_kl            | 0.003298247 |
|    clip_fraction        | 0.0712      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.559      |
|    explained_variance   | 0.172       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.398       |
|    n_updates            | 6360        |
|    policy_gradient_loss | -0.00496    |
|    value_loss           | 1.46        |
-----------------------------------------
Eval num_timesteps=254800, episode_reward=12.34 +/- 11.14
Episode length: 19.20 +/- 16.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.2     |
|    mean_reward     | 12.3     |
| time/              |          |
|    total_timesteps | 254800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.2     |
|    ep_rew_mean     | 18.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 637      |
|    time_elapsed    | 70680    |
|    total_timesteps | 254800   |
---------------------------------
Eval num_timesteps=255000, episode_reward=23.30 +/- 13.76
Episode length: 34.00 +/- 19.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34          |
|    mean_reward          | 23.3        |
| time/                   |             |
|    total_timesteps      | 255000      |
| train/                  |             |
|    approx_kl            | 0.002007198 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.574      |
|    explained_variance   | 0.271       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.646       |
|    n_updates            | 6370        |
|    policy_gradient_loss | -0.0053     |
|    value_loss           | 1.68        |
-----------------------------------------
Eval num_timesteps=255200, episode_reward=17.33 +/- 14.17
Episode length: 25.60 +/- 20.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 17.3     |
| time/              |          |
|    total_timesteps | 255200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26       |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 638      |
|    time_elapsed    | 70792    |
|    total_timesteps | 255200   |
---------------------------------
Eval num_timesteps=255400, episode_reward=23.05 +/- 15.36
Episode length: 32.80 +/- 21.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.8         |
|    mean_reward          | 23.1         |
| time/                   |              |
|    total_timesteps      | 255400       |
| train/                  |              |
|    approx_kl            | 0.0035762296 |
|    clip_fraction        | 0.0913       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.523       |
|    explained_variance   | 0.36         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.797        |
|    n_updates            | 6380         |
|    policy_gradient_loss | -0.00526     |
|    value_loss           | 2.3          |
------------------------------------------
Eval num_timesteps=255600, episode_reward=22.45 +/- 16.55
Episode length: 31.60 +/- 22.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.6     |
|    mean_reward     | 22.4     |
| time/              |          |
|    total_timesteps | 255600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.1     |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 639      |
|    time_elapsed    | 70919    |
|    total_timesteps | 255600   |
---------------------------------
Eval num_timesteps=255800, episode_reward=16.40 +/- 15.35
Episode length: 23.80 +/- 21.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.8         |
|    mean_reward          | 16.4         |
| time/                   |              |
|    total_timesteps      | 255800       |
| train/                  |              |
|    approx_kl            | 0.0015716747 |
|    clip_fraction        | 0.0536       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.524       |
|    explained_variance   | 0.341        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.348        |
|    n_updates            | 6390         |
|    policy_gradient_loss | -0.00448     |
|    value_loss           | 1.56         |
------------------------------------------
Eval num_timesteps=256000, episode_reward=11.81 +/- 11.51
Episode length: 18.00 +/- 16.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | 11.8     |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.5     |
|    ep_rew_mean     | 18.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 640      |
|    time_elapsed    | 71023    |
|    total_timesteps | 256000   |
---------------------------------
Eval num_timesteps=256200, episode_reward=6.03 +/- 4.95
Episode length: 9.60 +/- 6.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 9.6          |
|    mean_reward          | 6.03         |
| time/                   |              |
|    total_timesteps      | 256200       |
| train/                  |              |
|    approx_kl            | 0.0035489164 |
|    clip_fraction        | 0.0571       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.547       |
|    explained_variance   | 0.524        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.392        |
|    n_updates            | 6400         |
|    policy_gradient_loss | -0.00398     |
|    value_loss           | 1.56         |
------------------------------------------
Eval num_timesteps=256400, episode_reward=15.34 +/- 16.19
Episode length: 22.40 +/- 22.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 15.3     |
| time/              |          |
|    total_timesteps | 256400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.6     |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 641      |
|    time_elapsed    | 71110    |
|    total_timesteps | 256400   |
---------------------------------
Eval num_timesteps=256600, episode_reward=34.69 +/- 1.09
Episode length: 50.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | 34.7         |
| time/                   |              |
|    total_timesteps      | 256600       |
| train/                  |              |
|    approx_kl            | 0.0040166825 |
|    clip_fraction        | 0.124        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.529       |
|    explained_variance   | 0.458        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.614        |
|    n_updates            | 6410         |
|    policy_gradient_loss | -0.00807     |
|    value_loss           | 1.91         |
------------------------------------------
Eval num_timesteps=256800, episode_reward=7.19 +/- 3.24
Episode length: 11.40 +/- 4.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 11.4     |
|    mean_reward     | 7.19     |
| time/              |          |
|    total_timesteps | 256800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.7     |
|    ep_rew_mean     | 16.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 642      |
|    time_elapsed    | 71223    |
|    total_timesteps | 256800   |
---------------------------------
Eval num_timesteps=257000, episode_reward=23.89 +/- 14.80
Episode length: 33.80 +/- 19.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.8         |
|    mean_reward          | 23.9         |
| time/                   |              |
|    total_timesteps      | 257000       |
| train/                  |              |
|    approx_kl            | 0.0022225657 |
|    clip_fraction        | 0.0625       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.551       |
|    explained_variance   | 0.404        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.784        |
|    n_updates            | 6420         |
|    policy_gradient_loss | -0.00538     |
|    value_loss           | 2.01         |
------------------------------------------
Eval num_timesteps=257200, episode_reward=22.33 +/- 14.90
Episode length: 32.80 +/- 21.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 22.3     |
| time/              |          |
|    total_timesteps | 257200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.4     |
|    ep_rew_mean     | 15.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 643      |
|    time_elapsed    | 71351    |
|    total_timesteps | 257200   |
---------------------------------
Eval num_timesteps=257400, episode_reward=8.72 +/- 4.93
Episode length: 13.20 +/- 6.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 13.2         |
|    mean_reward          | 8.72         |
| time/                   |              |
|    total_timesteps      | 257400       |
| train/                  |              |
|    approx_kl            | 0.0018125286 |
|    clip_fraction        | 0.0768       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.541       |
|    explained_variance   | 0.343        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.471        |
|    n_updates            | 6430         |
|    policy_gradient_loss | -0.00486     |
|    value_loss           | 2.53         |
------------------------------------------
Eval num_timesteps=257600, episode_reward=24.30 +/- 13.08
Episode length: 35.20 +/- 18.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 24.3     |
| time/              |          |
|    total_timesteps | 257600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.6     |
|    ep_rew_mean     | 15.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 644      |
|    time_elapsed    | 71468    |
|    total_timesteps | 257600   |
---------------------------------
Eval num_timesteps=257800, episode_reward=16.13 +/- 14.53
Episode length: 24.20 +/- 21.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.2         |
|    mean_reward          | 16.1         |
| time/                   |              |
|    total_timesteps      | 257800       |
| train/                  |              |
|    approx_kl            | 0.0031077329 |
|    clip_fraction        | 0.0768       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.552       |
|    explained_variance   | 0.409        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.279        |
|    n_updates            | 6440         |
|    policy_gradient_loss | -0.00636     |
|    value_loss           | 1.83         |
------------------------------------------
Eval num_timesteps=258000, episode_reward=11.75 +/- 13.25
Episode length: 17.00 +/- 17.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | 11.7     |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.2     |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 645      |
|    time_elapsed    | 71574    |
|    total_timesteps | 258000   |
---------------------------------
Eval num_timesteps=258200, episode_reward=22.24 +/- 15.91
Episode length: 32.00 +/- 22.05
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 32         |
|    mean_reward          | 22.2       |
| time/                   |            |
|    total_timesteps      | 258200     |
| train/                  |            |
|    approx_kl            | 0.00244624 |
|    clip_fraction        | 0.0609     |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.567     |
|    explained_variance   | 0.401      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0326     |
|    n_updates            | 6450       |
|    policy_gradient_loss | -0.00392   |
|    value_loss           | 1.32       |
----------------------------------------
Eval num_timesteps=258400, episode_reward=16.52 +/- 16.05
Episode length: 23.40 +/- 21.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 258400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.8     |
|    ep_rew_mean     | 14.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 646      |
|    time_elapsed    | 71677    |
|    total_timesteps | 258400   |
---------------------------------
Eval num_timesteps=258600, episode_reward=6.99 +/- 2.02
Episode length: 10.80 +/- 2.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 10.8         |
|    mean_reward          | 6.99         |
| time/                   |              |
|    total_timesteps      | 258600       |
| train/                  |              |
|    approx_kl            | 0.0019014217 |
|    clip_fraction        | 0.0605       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.548       |
|    explained_variance   | 0.44         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.136        |
|    n_updates            | 6460         |
|    policy_gradient_loss | -0.00433     |
|    value_loss           | 2.02         |
------------------------------------------
Eval num_timesteps=258800, episode_reward=19.72 +/- 13.11
Episode length: 28.40 +/- 18.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.4     |
|    mean_reward     | 19.7     |
| time/              |          |
|    total_timesteps | 258800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.1     |
|    ep_rew_mean     | 15       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 647      |
|    time_elapsed    | 71769    |
|    total_timesteps | 258800   |
---------------------------------
Eval num_timesteps=259000, episode_reward=13.41 +/- 11.27
Episode length: 20.40 +/- 16.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 20.4         |
|    mean_reward          | 13.4         |
| time/                   |              |
|    total_timesteps      | 259000       |
| train/                  |              |
|    approx_kl            | 0.0009892702 |
|    clip_fraction        | 0.0487       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.536       |
|    explained_variance   | 0.373        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.745        |
|    n_updates            | 6470         |
|    policy_gradient_loss | -0.00533     |
|    value_loss           | 2.4          |
------------------------------------------
Eval num_timesteps=259200, episode_reward=12.38 +/- 10.76
Episode length: 19.00 +/- 15.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 12.4     |
| time/              |          |
|    total_timesteps | 259200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.6     |
|    ep_rew_mean     | 15.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 648      |
|    time_elapsed    | 71877    |
|    total_timesteps | 259200   |
---------------------------------
Eval num_timesteps=259400, episode_reward=17.70 +/- 13.87
Episode length: 26.00 +/- 19.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26           |
|    mean_reward          | 17.7         |
| time/                   |              |
|    total_timesteps      | 259400       |
| train/                  |              |
|    approx_kl            | 0.0028147635 |
|    clip_fraction        | 0.0953       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.589       |
|    explained_variance   | 0.588        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.33         |
|    n_updates            | 6480         |
|    policy_gradient_loss | -0.00504     |
|    value_loss           | 1.07         |
------------------------------------------
Eval num_timesteps=259600, episode_reward=22.71 +/- 14.67
Episode length: 33.40 +/- 20.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 22.7     |
| time/              |          |
|    total_timesteps | 259600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.3     |
|    ep_rew_mean     | 15.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 649      |
|    time_elapsed    | 71984    |
|    total_timesteps | 259600   |
---------------------------------
Eval num_timesteps=259800, episode_reward=10.37 +/- 12.19
Episode length: 15.60 +/- 17.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.6         |
|    mean_reward          | 10.4         |
| time/                   |              |
|    total_timesteps      | 259800       |
| train/                  |              |
|    approx_kl            | 0.0019374769 |
|    clip_fraction        | 0.0567       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.542       |
|    explained_variance   | 0.148        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.135        |
|    n_updates            | 6490         |
|    policy_gradient_loss | -0.00366     |
|    value_loss           | 2.32         |
------------------------------------------
Eval num_timesteps=260000, episode_reward=18.35 +/- 14.30
Episode length: 26.60 +/- 19.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.6     |
|    mean_reward     | 18.4     |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.8     |
|    ep_rew_mean     | 15.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 650      |
|    time_elapsed    | 72097    |
|    total_timesteps | 260000   |
---------------------------------
Eval num_timesteps=260200, episode_reward=15.50 +/- 11.85
Episode length: 22.00 +/- 15.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 22          |
|    mean_reward          | 15.5        |
| time/                   |             |
|    total_timesteps      | 260200      |
| train/                  |             |
|    approx_kl            | 0.003144751 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.543      |
|    explained_variance   | 0.317       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.568       |
|    n_updates            | 6500        |
|    policy_gradient_loss | -0.00648    |
|    value_loss           | 2.39        |
-----------------------------------------
Eval num_timesteps=260400, episode_reward=15.17 +/- 16.24
Episode length: 22.20 +/- 22.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 15.2     |
| time/              |          |
|    total_timesteps | 260400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.1     |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 651      |
|    time_elapsed    | 72202    |
|    total_timesteps | 260400   |
---------------------------------
Eval num_timesteps=260600, episode_reward=10.13 +/- 11.68
Episode length: 15.60 +/- 17.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.6         |
|    mean_reward          | 10.1         |
| time/                   |              |
|    total_timesteps      | 260600       |
| train/                  |              |
|    approx_kl            | 0.0024981832 |
|    clip_fraction        | 0.0922       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.565       |
|    explained_variance   | 0.313        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.526        |
|    n_updates            | 6510         |
|    policy_gradient_loss | -0.00841     |
|    value_loss           | 1.86         |
------------------------------------------
Eval num_timesteps=260800, episode_reward=16.61 +/- 14.16
Episode length: 24.80 +/- 20.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 260800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 652      |
|    time_elapsed    | 72310    |
|    total_timesteps | 260800   |
---------------------------------
Eval num_timesteps=261000, episode_reward=18.23 +/- 13.90
Episode length: 26.60 +/- 19.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.6         |
|    mean_reward          | 18.2         |
| time/                   |              |
|    total_timesteps      | 261000       |
| train/                  |              |
|    approx_kl            | 0.0024117657 |
|    clip_fraction        | 0.0873       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.578       |
|    explained_variance   | 0.264        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.72         |
|    n_updates            | 6520         |
|    policy_gradient_loss | -0.00614     |
|    value_loss           | 2.04         |
------------------------------------------
Eval num_timesteps=261200, episode_reward=21.89 +/- 16.79
Episode length: 31.20 +/- 23.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.2     |
|    mean_reward     | 21.9     |
| time/              |          |
|    total_timesteps | 261200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | 16.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 653      |
|    time_elapsed    | 72417    |
|    total_timesteps | 261200   |
---------------------------------
Eval num_timesteps=261400, episode_reward=18.51 +/- 14.16
Episode length: 27.40 +/- 20.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 27.4        |
|    mean_reward          | 18.5        |
| time/                   |             |
|    total_timesteps      | 261400      |
| train/                  |             |
|    approx_kl            | 0.001836188 |
|    clip_fraction        | 0.0888      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.555      |
|    explained_variance   | 0.359       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.707       |
|    n_updates            | 6530        |
|    policy_gradient_loss | -0.0063     |
|    value_loss           | 2.4         |
-----------------------------------------
Eval num_timesteps=261600, episode_reward=28.97 +/- 11.08
Episode length: 42.20 +/- 15.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 29       |
| time/              |          |
|    total_timesteps | 261600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.2     |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 654      |
|    time_elapsed    | 72526    |
|    total_timesteps | 261600   |
---------------------------------
Eval num_timesteps=261800, episode_reward=17.33 +/- 13.33
Episode length: 26.40 +/- 19.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 26.4        |
|    mean_reward          | 17.3        |
| time/                   |             |
|    total_timesteps      | 261800      |
| train/                  |             |
|    approx_kl            | 0.004279989 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.58       |
|    explained_variance   | 0.296       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.85        |
|    n_updates            | 6540        |
|    policy_gradient_loss | -0.00447    |
|    value_loss           | 1.52        |
-----------------------------------------
Eval num_timesteps=262000, episode_reward=10.71 +/- 12.50
Episode length: 15.80 +/- 17.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | 10.7     |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24       |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 655      |
|    time_elapsed    | 72632    |
|    total_timesteps | 262000   |
---------------------------------
Eval num_timesteps=262200, episode_reward=12.29 +/- 12.48
Episode length: 17.80 +/- 16.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.8         |
|    mean_reward          | 12.3         |
| time/                   |              |
|    total_timesteps      | 262200       |
| train/                  |              |
|    approx_kl            | 0.0022302128 |
|    clip_fraction        | 0.046        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.515       |
|    explained_variance   | 0.322        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.326        |
|    n_updates            | 6550         |
|    policy_gradient_loss | -0.00412     |
|    value_loss           | 1.93         |
------------------------------------------
Eval num_timesteps=262400, episode_reward=7.11 +/- 6.65
Episode length: 10.80 +/- 9.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10.8     |
|    mean_reward     | 7.11     |
| time/              |          |
|    total_timesteps | 262400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.8     |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 656      |
|    time_elapsed    | 72727    |
|    total_timesteps | 262400   |
---------------------------------
Eval num_timesteps=262600, episode_reward=29.25 +/- 12.15
Episode length: 41.60 +/- 16.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.6        |
|    mean_reward          | 29.3        |
| time/                   |             |
|    total_timesteps      | 262600      |
| train/                  |             |
|    approx_kl            | 0.002952757 |
|    clip_fraction        | 0.0897      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.576      |
|    explained_variance   | 0.438       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.93        |
|    n_updates            | 6560        |
|    policy_gradient_loss | -0.0064     |
|    value_loss           | 2.17        |
-----------------------------------------
Eval num_timesteps=262800, episode_reward=12.12 +/- 10.96
Episode length: 18.40 +/- 16.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.4     |
|    mean_reward     | 12.1     |
| time/              |          |
|    total_timesteps | 262800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.7     |
|    ep_rew_mean     | 17       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 657      |
|    time_elapsed    | 72836    |
|    total_timesteps | 262800   |
---------------------------------
Eval num_timesteps=263000, episode_reward=28.25 +/- 11.35
Episode length: 41.80 +/- 16.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.8         |
|    mean_reward          | 28.2         |
| time/                   |              |
|    total_timesteps      | 263000       |
| train/                  |              |
|    approx_kl            | 0.0024345384 |
|    clip_fraction        | 0.0779       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.519       |
|    explained_variance   | 0.394        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.373        |
|    n_updates            | 6570         |
|    policy_gradient_loss | -0.00501     |
|    value_loss           | 1.63         |
------------------------------------------
Eval num_timesteps=263200, episode_reward=18.82 +/- 13.72
Episode length: 27.40 +/- 19.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.4     |
|    mean_reward     | 18.8     |
| time/              |          |
|    total_timesteps | 263200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24       |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 658      |
|    time_elapsed    | 72943    |
|    total_timesteps | 263200   |
---------------------------------
Eval num_timesteps=263400, episode_reward=5.10 +/- 4.53
Episode length: 8.20 +/- 6.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 8.2         |
|    mean_reward          | 5.1         |
| time/                   |             |
|    total_timesteps      | 263400      |
| train/                  |             |
|    approx_kl            | 0.002652146 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.496      |
|    explained_variance   | 0.327       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.405       |
|    n_updates            | 6580        |
|    policy_gradient_loss | -0.0073     |
|    value_loss           | 2.69        |
-----------------------------------------
Eval num_timesteps=263600, episode_reward=11.33 +/- 11.14
Episode length: 17.40 +/- 16.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 11.3     |
| time/              |          |
|    total_timesteps | 263600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | 17.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 659      |
|    time_elapsed    | 73030    |
|    total_timesteps | 263600   |
---------------------------------
Eval num_timesteps=263800, episode_reward=16.17 +/- 15.52
Episode length: 23.60 +/- 21.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.6         |
|    mean_reward          | 16.2         |
| time/                   |              |
|    total_timesteps      | 263800       |
| train/                  |              |
|    approx_kl            | 0.0034149226 |
|    clip_fraction        | 0.0679       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.588       |
|    explained_variance   | 0.275        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.104        |
|    n_updates            | 6590         |
|    policy_gradient_loss | -0.00426     |
|    value_loss           | 1.35         |
------------------------------------------
Eval num_timesteps=264000, episode_reward=24.05 +/- 13.72
Episode length: 34.40 +/- 19.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 24.1     |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | 16.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 660      |
|    time_elapsed    | 73157    |
|    total_timesteps | 264000   |
---------------------------------
Eval num_timesteps=264200, episode_reward=11.05 +/- 11.21
Episode length: 17.00 +/- 16.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17           |
|    mean_reward          | 11           |
| time/                   |              |
|    total_timesteps      | 264200       |
| train/                  |              |
|    approx_kl            | 0.0030449315 |
|    clip_fraction        | 0.081        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.532       |
|    explained_variance   | 0.411        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.35         |
|    n_updates            | 6600         |
|    policy_gradient_loss | -0.00587     |
|    value_loss           | 2.13         |
------------------------------------------
Eval num_timesteps=264400, episode_reward=21.69 +/- 15.67
Episode length: 31.80 +/- 22.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.8     |
|    mean_reward     | 21.7     |
| time/              |          |
|    total_timesteps | 264400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.7     |
|    ep_rew_mean     | 15.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 661      |
|    time_elapsed    | 73265    |
|    total_timesteps | 264400   |
---------------------------------
Eval num_timesteps=264600, episode_reward=23.69 +/- 14.72
Episode length: 33.80 +/- 20.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.8         |
|    mean_reward          | 23.7         |
| time/                   |              |
|    total_timesteps      | 264600       |
| train/                  |              |
|    approx_kl            | 0.0021820087 |
|    clip_fraction        | 0.0688       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.511       |
|    explained_variance   | 0.386        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.48         |
|    n_updates            | 6610         |
|    policy_gradient_loss | -0.00635     |
|    value_loss           | 2.33         |
------------------------------------------
Eval num_timesteps=264800, episode_reward=22.43 +/- 14.94
Episode length: 32.80 +/- 21.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 22.4     |
| time/              |          |
|    total_timesteps | 264800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24       |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 662      |
|    time_elapsed    | 73371    |
|    total_timesteps | 264800   |
---------------------------------
Eval num_timesteps=265000, episode_reward=19.65 +/- 13.72
Episode length: 28.60 +/- 19.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 28.6         |
|    mean_reward          | 19.7         |
| time/                   |              |
|    total_timesteps      | 265000       |
| train/                  |              |
|    approx_kl            | 0.0023038916 |
|    clip_fraction        | 0.06         |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.513       |
|    explained_variance   | 0.288        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0534       |
|    n_updates            | 6620         |
|    policy_gradient_loss | -0.00433     |
|    value_loss           | 1.49         |
------------------------------------------
Eval num_timesteps=265200, episode_reward=16.64 +/- 15.94
Episode length: 23.60 +/- 21.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 265200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 15       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 663      |
|    time_elapsed    | 73473    |
|    total_timesteps | 265200   |
---------------------------------
Eval num_timesteps=265400, episode_reward=22.59 +/- 14.59
Episode length: 33.00 +/- 20.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33          |
|    mean_reward          | 22.6        |
| time/                   |             |
|    total_timesteps      | 265400      |
| train/                  |             |
|    approx_kl            | 0.002509589 |
|    clip_fraction        | 0.0759      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.527      |
|    explained_variance   | 0.416       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.83        |
|    n_updates            | 6630        |
|    policy_gradient_loss | -0.00718    |
|    value_loss           | 3.01        |
-----------------------------------------
Eval num_timesteps=265600, episode_reward=17.56 +/- 14.35
Episode length: 25.80 +/- 19.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 265600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 664      |
|    time_elapsed    | 73580    |
|    total_timesteps | 265600   |
---------------------------------
Eval num_timesteps=265800, episode_reward=17.61 +/- 14.26
Episode length: 25.60 +/- 19.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 25.6        |
|    mean_reward          | 17.6        |
| time/                   |             |
|    total_timesteps      | 265800      |
| train/                  |             |
|    approx_kl            | 0.003746163 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.597      |
|    explained_variance   | 0.428       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.776       |
|    n_updates            | 6640        |
|    policy_gradient_loss | -0.00569    |
|    value_loss           | 1.75        |
-----------------------------------------
Eval num_timesteps=266000, episode_reward=10.53 +/- 13.24
Episode length: 15.40 +/- 17.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | 10.5     |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.9     |
|    ep_rew_mean     | 13.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 665      |
|    time_elapsed    | 73689    |
|    total_timesteps | 266000   |
---------------------------------
Eval num_timesteps=266200, episode_reward=5.65 +/- 3.63
Episode length: 9.00 +/- 5.22
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 9          |
|    mean_reward          | 5.65       |
| time/                   |            |
|    total_timesteps      | 266200     |
| train/                  |            |
|    approx_kl            | 0.00178665 |
|    clip_fraction        | 0.0357     |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.523     |
|    explained_variance   | 0.315      |
|    learning_rate        | 0.0001     |
|    loss                 | 1.95       |
|    n_updates            | 6650       |
|    policy_gradient_loss | -0.00498   |
|    value_loss           | 3.43       |
----------------------------------------
Eval num_timesteps=266400, episode_reward=15.85 +/- 15.35
Episode length: 23.60 +/- 21.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 15.9     |
| time/              |          |
|    total_timesteps | 266400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.8     |
|    ep_rew_mean     | 14.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 666      |
|    time_elapsed    | 73777    |
|    total_timesteps | 266400   |
---------------------------------
Eval num_timesteps=266600, episode_reward=26.76 +/- 9.94
Episode length: 39.00 +/- 13.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 39           |
|    mean_reward          | 26.8         |
| time/                   |              |
|    total_timesteps      | 266600       |
| train/                  |              |
|    approx_kl            | 0.0030748497 |
|    clip_fraction        | 0.067        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.581       |
|    explained_variance   | 0.533        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.195        |
|    n_updates            | 6660         |
|    policy_gradient_loss | -0.0061      |
|    value_loss           | 0.945        |
------------------------------------------
Eval num_timesteps=266800, episode_reward=3.60 +/- 2.40
Episode length: 6.00 +/- 3.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6        |
|    mean_reward     | 3.6      |
| time/              |          |
|    total_timesteps | 266800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.9     |
|    ep_rew_mean     | 14.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 667      |
|    time_elapsed    | 73887    |
|    total_timesteps | 266800   |
---------------------------------
Eval num_timesteps=267000, episode_reward=11.33 +/- 3.49
Episode length: 17.60 +/- 5.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.6         |
|    mean_reward          | 11.3         |
| time/                   |              |
|    total_timesteps      | 267000       |
| train/                  |              |
|    approx_kl            | 0.0037830577 |
|    clip_fraction        | 0.0817       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.574       |
|    explained_variance   | -0.0286      |
|    learning_rate        | 0.0001       |
|    loss                 | 1.92         |
|    n_updates            | 6670         |
|    policy_gradient_loss | -0.00611     |
|    value_loss           | 2.49         |
------------------------------------------
Eval num_timesteps=267200, episode_reward=11.57 +/- 11.11
Episode length: 17.80 +/- 16.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | 11.6     |
| time/              |          |
|    total_timesteps | 267200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 668      |
|    time_elapsed    | 73988    |
|    total_timesteps | 267200   |
---------------------------------
Eval num_timesteps=267400, episode_reward=21.24 +/- 15.34
Episode length: 31.80 +/- 22.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 31.8         |
|    mean_reward          | 21.2         |
| time/                   |              |
|    total_timesteps      | 267400       |
| train/                  |              |
|    approx_kl            | 0.0033948019 |
|    clip_fraction        | 0.0525       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.566       |
|    explained_variance   | 0.181        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.49         |
|    n_updates            | 6680         |
|    policy_gradient_loss | -0.00327     |
|    value_loss           | 1.67         |
------------------------------------------
Eval num_timesteps=267600, episode_reward=23.24 +/- 14.71
Episode length: 33.40 +/- 20.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 23.2     |
| time/              |          |
|    total_timesteps | 267600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 669      |
|    time_elapsed    | 74118    |
|    total_timesteps | 267600   |
---------------------------------
Eval num_timesteps=267800, episode_reward=23.10 +/- 14.07
Episode length: 33.80 +/- 19.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.8         |
|    mean_reward          | 23.1         |
| time/                   |              |
|    total_timesteps      | 267800       |
| train/                  |              |
|    approx_kl            | 0.0031426917 |
|    clip_fraction        | 0.0848       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.598       |
|    explained_variance   | 0.408        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.5          |
|    n_updates            | 6690         |
|    policy_gradient_loss | -0.00496     |
|    value_loss           | 1.18         |
------------------------------------------
Eval num_timesteps=268000, episode_reward=12.30 +/- 12.22
Episode length: 17.80 +/- 16.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | 12.3     |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.4     |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 670      |
|    time_elapsed    | 74246    |
|    total_timesteps | 268000   |
---------------------------------
Eval num_timesteps=268200, episode_reward=23.65 +/- 14.63
Episode length: 33.60 +/- 20.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.6         |
|    mean_reward          | 23.7         |
| time/                   |              |
|    total_timesteps      | 268200       |
| train/                  |              |
|    approx_kl            | 0.0034042688 |
|    clip_fraction        | 0.103        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.596       |
|    explained_variance   | 0.237        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.04         |
|    n_updates            | 6700         |
|    policy_gradient_loss | -0.0053      |
|    value_loss           | 1.61         |
------------------------------------------
Eval num_timesteps=268400, episode_reward=10.52 +/- 13.26
Episode length: 15.20 +/- 17.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | 10.5     |
| time/              |          |
|    total_timesteps | 268400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.8     |
|    ep_rew_mean     | 18.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 671      |
|    time_elapsed    | 74354    |
|    total_timesteps | 268400   |
---------------------------------
Eval num_timesteps=268600, episode_reward=11.51 +/- 12.07
Episode length: 16.80 +/- 16.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.8         |
|    mean_reward          | 11.5         |
| time/                   |              |
|    total_timesteps      | 268600       |
| train/                  |              |
|    approx_kl            | 0.0023174868 |
|    clip_fraction        | 0.0864       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.574       |
|    explained_variance   | 0.331        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.416        |
|    n_updates            | 6710         |
|    policy_gradient_loss | -0.00477     |
|    value_loss           | 1.49         |
------------------------------------------
Eval num_timesteps=268800, episode_reward=29.81 +/- 7.66
Episode length: 44.40 +/- 11.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.4     |
|    mean_reward     | 29.8     |
| time/              |          |
|    total_timesteps | 268800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.3     |
|    ep_rew_mean     | 19.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 672      |
|    time_elapsed    | 74469    |
|    total_timesteps | 268800   |
---------------------------------
Eval num_timesteps=269000, episode_reward=15.18 +/- 15.35
Episode length: 22.80 +/- 22.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22.8         |
|    mean_reward          | 15.2         |
| time/                   |              |
|    total_timesteps      | 269000       |
| train/                  |              |
|    approx_kl            | 0.0021115679 |
|    clip_fraction        | 0.0886       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.586       |
|    explained_variance   | 0.503        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.279        |
|    n_updates            | 6720         |
|    policy_gradient_loss | -0.00617     |
|    value_loss           | 1.23         |
------------------------------------------
Eval num_timesteps=269200, episode_reward=21.32 +/- 15.25
Episode length: 32.00 +/- 22.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32       |
|    mean_reward     | 21.3     |
| time/              |          |
|    total_timesteps | 269200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.4     |
|    ep_rew_mean     | 18.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 673      |
|    time_elapsed    | 74576    |
|    total_timesteps | 269200   |
---------------------------------
Eval num_timesteps=269400, episode_reward=15.16 +/- 15.32
Episode length: 22.80 +/- 22.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22.8         |
|    mean_reward          | 15.2         |
| time/                   |              |
|    total_timesteps      | 269400       |
| train/                  |              |
|    approx_kl            | 0.0019538035 |
|    clip_fraction        | 0.0605       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.596       |
|    explained_variance   | 0.453        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.256        |
|    n_updates            | 6730         |
|    policy_gradient_loss | -0.00414     |
|    value_loss           | 1.42         |
------------------------------------------
Eval num_timesteps=269600, episode_reward=20.27 +/- 13.20
Episode length: 29.20 +/- 18.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.2     |
|    mean_reward     | 20.3     |
| time/              |          |
|    total_timesteps | 269600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.7     |
|    ep_rew_mean     | 16.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 674      |
|    time_elapsed    | 74684    |
|    total_timesteps | 269600   |
---------------------------------
Eval num_timesteps=269800, episode_reward=10.28 +/- 12.30
Episode length: 15.80 +/- 18.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | 10.3         |
| time/                   |              |
|    total_timesteps      | 269800       |
| train/                  |              |
|    approx_kl            | 0.0029822823 |
|    clip_fraction        | 0.0634       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.552       |
|    explained_variance   | 0.315        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.18         |
|    n_updates            | 6740         |
|    policy_gradient_loss | -0.00687     |
|    value_loss           | 2.99         |
------------------------------------------
Eval num_timesteps=270000, episode_reward=23.81 +/- 14.00
Episode length: 34.40 +/- 19.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 23.8     |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | 16       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 675      |
|    time_elapsed    | 74796    |
|    total_timesteps | 270000   |
---------------------------------
Eval num_timesteps=270200, episode_reward=7.68 +/- 8.13
Episode length: 11.60 +/- 10.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 11.6        |
|    mean_reward          | 7.68        |
| time/                   |             |
|    total_timesteps      | 270200      |
| train/                  |             |
|    approx_kl            | 0.002993467 |
|    clip_fraction        | 0.0529      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.588      |
|    explained_variance   | 0.336       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.324       |
|    n_updates            | 6750        |
|    policy_gradient_loss | -0.00414    |
|    value_loss           | 2.29        |
-----------------------------------------
Eval num_timesteps=270400, episode_reward=10.59 +/- 13.39
Episode length: 15.20 +/- 17.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | 10.6     |
| time/              |          |
|    total_timesteps | 270400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.5     |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 676      |
|    time_elapsed    | 74889    |
|    total_timesteps | 270400   |
---------------------------------
Eval num_timesteps=270600, episode_reward=18.54 +/- 14.68
Episode length: 26.60 +/- 19.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 26.6        |
|    mean_reward          | 18.5        |
| time/                   |             |
|    total_timesteps      | 270600      |
| train/                  |             |
|    approx_kl            | 0.003026416 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.608      |
|    explained_variance   | 0.373       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.22        |
|    n_updates            | 6760        |
|    policy_gradient_loss | -0.00415    |
|    value_loss           | 1.58        |
-----------------------------------------
Eval num_timesteps=270800, episode_reward=29.01 +/- 9.86
Episode length: 43.00 +/- 14.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43       |
|    mean_reward     | 29       |
| time/              |          |
|    total_timesteps | 270800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 677      |
|    time_elapsed    | 74998    |
|    total_timesteps | 270800   |
---------------------------------
Eval num_timesteps=271000, episode_reward=15.91 +/- 15.24
Episode length: 23.60 +/- 21.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.6         |
|    mean_reward          | 15.9         |
| time/                   |              |
|    total_timesteps      | 271000       |
| train/                  |              |
|    approx_kl            | 0.0037607194 |
|    clip_fraction        | 0.0996       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.614       |
|    explained_variance   | 0.184        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.25         |
|    n_updates            | 6770         |
|    policy_gradient_loss | -0.00703     |
|    value_loss           | 1.71         |
------------------------------------------
Eval num_timesteps=271200, episode_reward=17.70 +/- 15.23
Episode length: 25.20 +/- 20.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 17.7     |
| time/              |          |
|    total_timesteps | 271200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.7     |
|    ep_rew_mean     | 14.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 678      |
|    time_elapsed    | 75126    |
|    total_timesteps | 271200   |
---------------------------------
Eval num_timesteps=271400, episode_reward=5.60 +/- 2.61
Episode length: 9.00 +/- 3.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 5.6         |
| time/                   |             |
|    total_timesteps      | 271400      |
| train/                  |             |
|    approx_kl            | 0.003893603 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.575      |
|    explained_variance   | 0.416       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.703       |
|    n_updates            | 6780        |
|    policy_gradient_loss | -0.00853    |
|    value_loss           | 2.28        |
-----------------------------------------
Eval num_timesteps=271600, episode_reward=15.34 +/- 15.27
Episode length: 23.00 +/- 22.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 15.3     |
| time/              |          |
|    total_timesteps | 271600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.2     |
|    ep_rew_mean     | 15.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 679      |
|    time_elapsed    | 75211    |
|    total_timesteps | 271600   |
---------------------------------
Eval num_timesteps=271800, episode_reward=11.35 +/- 11.98
Episode length: 17.20 +/- 16.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.2         |
|    mean_reward          | 11.4         |
| time/                   |              |
|    total_timesteps      | 271800       |
| train/                  |              |
|    approx_kl            | 0.0026060122 |
|    clip_fraction        | 0.081        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.559       |
|    explained_variance   | 0.382        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.796        |
|    n_updates            | 6790         |
|    policy_gradient_loss | -0.00745     |
|    value_loss           | 2.46         |
------------------------------------------
Eval num_timesteps=272000, episode_reward=21.97 +/- 15.80
Episode length: 31.80 +/- 22.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.8     |
|    mean_reward     | 22       |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | 16       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 680      |
|    time_elapsed    | 75345    |
|    total_timesteps | 272000   |
---------------------------------
Eval num_timesteps=272200, episode_reward=13.21 +/- 12.39
Episode length: 20.00 +/- 18.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 20          |
|    mean_reward          | 13.2        |
| time/                   |             |
|    total_timesteps      | 272200      |
| train/                  |             |
|    approx_kl            | 0.002367586 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.546      |
|    explained_variance   | 0.353       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.343       |
|    n_updates            | 6800        |
|    policy_gradient_loss | -0.00764    |
|    value_loss           | 2.24        |
-----------------------------------------
Eval num_timesteps=272400, episode_reward=17.43 +/- 16.09
Episode length: 24.60 +/- 21.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 272400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | 16.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 681      |
|    time_elapsed    | 75455    |
|    total_timesteps | 272400   |
---------------------------------
Eval num_timesteps=272600, episode_reward=11.15 +/- 12.99
Episode length: 16.20 +/- 17.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.2        |
|    mean_reward          | 11.1        |
| time/                   |             |
|    total_timesteps      | 272600      |
| train/                  |             |
|    approx_kl            | 0.002669327 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.586      |
|    explained_variance   | 0.32        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.739       |
|    n_updates            | 6810        |
|    policy_gradient_loss | -0.00713    |
|    value_loss           | 1.3         |
-----------------------------------------
Eval num_timesteps=272800, episode_reward=29.49 +/- 11.69
Episode length: 42.00 +/- 16.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 29.5     |
| time/              |          |
|    total_timesteps | 272800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.7     |
|    ep_rew_mean     | 15.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 682      |
|    time_elapsed    | 75568    |
|    total_timesteps | 272800   |
---------------------------------
Eval num_timesteps=273000, episode_reward=17.71 +/- 15.60
Episode length: 25.00 +/- 20.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25           |
|    mean_reward          | 17.7         |
| time/                   |              |
|    total_timesteps      | 273000       |
| train/                  |              |
|    approx_kl            | 0.0031204037 |
|    clip_fraction        | 0.0801       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.573       |
|    explained_variance   | 0.316        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.58         |
|    n_updates            | 6820         |
|    policy_gradient_loss | -0.00765     |
|    value_loss           | 2.77         |
------------------------------------------
Eval num_timesteps=273200, episode_reward=18.25 +/- 13.92
Episode length: 26.60 +/- 19.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.6     |
|    mean_reward     | 18.2     |
| time/              |          |
|    total_timesteps | 273200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.1     |
|    ep_rew_mean     | 15.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 683      |
|    time_elapsed    | 75699    |
|    total_timesteps | 273200   |
---------------------------------
Eval num_timesteps=273400, episode_reward=17.28 +/- 15.26
Episode length: 24.80 +/- 20.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.8         |
|    mean_reward          | 17.3         |
| time/                   |              |
|    total_timesteps      | 273400       |
| train/                  |              |
|    approx_kl            | 0.0016978652 |
|    clip_fraction        | 0.0761       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.567       |
|    explained_variance   | 0.424        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.55         |
|    n_updates            | 6830         |
|    policy_gradient_loss | -0.00622     |
|    value_loss           | 1.71         |
------------------------------------------
Eval num_timesteps=273600, episode_reward=20.98 +/- 13.07
Episode length: 29.40 +/- 17.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.4     |
|    mean_reward     | 21       |
| time/              |          |
|    total_timesteps | 273600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.8     |
|    ep_rew_mean     | 15.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 684      |
|    time_elapsed    | 75812    |
|    total_timesteps | 273600   |
---------------------------------
Eval num_timesteps=273800, episode_reward=22.91 +/- 13.76
Episode length: 34.00 +/- 19.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34           |
|    mean_reward          | 22.9         |
| time/                   |              |
|    total_timesteps      | 273800       |
| train/                  |              |
|    approx_kl            | 0.0029678862 |
|    clip_fraction        | 0.112        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.503       |
|    explained_variance   | 0.234        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.928        |
|    n_updates            | 6840         |
|    policy_gradient_loss | -0.0102      |
|    value_loss           | 2.8          |
------------------------------------------
Eval num_timesteps=274000, episode_reward=12.52 +/- 11.50
Episode length: 19.00 +/- 16.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 12.5     |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.3     |
|    ep_rew_mean     | 15.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 685      |
|    time_elapsed    | 75926    |
|    total_timesteps | 274000   |
---------------------------------
Eval num_timesteps=274200, episode_reward=10.77 +/- 13.09
Episode length: 15.60 +/- 17.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.6        |
|    mean_reward          | 10.8        |
| time/                   |             |
|    total_timesteps      | 274200      |
| train/                  |             |
|    approx_kl            | 0.006037399 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.572      |
|    explained_variance   | 0.313       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.164       |
|    n_updates            | 6850        |
|    policy_gradient_loss | -0.0061     |
|    value_loss           | 1.26        |
-----------------------------------------
Eval num_timesteps=274400, episode_reward=23.73 +/- 12.73
Episode length: 35.20 +/- 18.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 23.7     |
| time/              |          |
|    total_timesteps | 274400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.3     |
|    ep_rew_mean     | 15.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 686      |
|    time_elapsed    | 76034    |
|    total_timesteps | 274400   |
---------------------------------
Eval num_timesteps=274600, episode_reward=23.49 +/- 15.31
Episode length: 33.20 +/- 20.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.2         |
|    mean_reward          | 23.5         |
| time/                   |              |
|    total_timesteps      | 274600       |
| train/                  |              |
|    approx_kl            | 0.0027547856 |
|    clip_fraction        | 0.146        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.542       |
|    explained_variance   | 0.441        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.258        |
|    n_updates            | 6860         |
|    policy_gradient_loss | -0.00813     |
|    value_loss           | 2.35         |
------------------------------------------
Eval num_timesteps=274800, episode_reward=25.18 +/- 13.28
Episode length: 35.80 +/- 17.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 25.2     |
| time/              |          |
|    total_timesteps | 274800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.2     |
|    ep_rew_mean     | 14.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 687      |
|    time_elapsed    | 76142    |
|    total_timesteps | 274800   |
---------------------------------
Eval num_timesteps=275000, episode_reward=31.01 +/- 9.24
Episode length: 44.00 +/- 12.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 44          |
|    mean_reward          | 31          |
| time/                   |             |
|    total_timesteps      | 275000      |
| train/                  |             |
|    approx_kl            | 0.002689492 |
|    clip_fraction        | 0.0674      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.557      |
|    explained_variance   | 0.304       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.794       |
|    n_updates            | 6870        |
|    policy_gradient_loss | -0.00692    |
|    value_loss           | 2.58        |
-----------------------------------------
Eval num_timesteps=275200, episode_reward=12.58 +/- 12.13
Episode length: 18.40 +/- 15.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.4     |
|    mean_reward     | 12.6     |
| time/              |          |
|    total_timesteps | 275200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.8     |
|    ep_rew_mean     | 14.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 688      |
|    time_elapsed    | 76269    |
|    total_timesteps | 275200   |
---------------------------------
Eval num_timesteps=275400, episode_reward=18.14 +/- 14.75
Episode length: 25.80 +/- 19.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 25.8        |
|    mean_reward          | 18.1        |
| time/                   |             |
|    total_timesteps      | 275400      |
| train/                  |             |
|    approx_kl            | 0.005083886 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.577      |
|    explained_variance   | 0.234       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.94        |
|    n_updates            | 6880        |
|    policy_gradient_loss | -0.00944    |
|    value_loss           | 2.09        |
-----------------------------------------
Eval num_timesteps=275600, episode_reward=15.89 +/- 15.23
Episode length: 23.60 +/- 21.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 15.9     |
| time/              |          |
|    total_timesteps | 275600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 689      |
|    time_elapsed    | 76373    |
|    total_timesteps | 275600   |
---------------------------------
Eval num_timesteps=275800, episode_reward=17.72 +/- 14.68
Episode length: 25.80 +/- 19.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 25.8        |
|    mean_reward          | 17.7        |
| time/                   |             |
|    total_timesteps      | 275800      |
| train/                  |             |
|    approx_kl            | 0.002810162 |
|    clip_fraction        | 0.094       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.578      |
|    explained_variance   | 0.409       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.376       |
|    n_updates            | 6890        |
|    policy_gradient_loss | -0.00777    |
|    value_loss           | 2.07        |
-----------------------------------------
Eval num_timesteps=276000, episode_reward=22.03 +/- 15.65
Episode length: 31.80 +/- 21.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.8     |
|    mean_reward     | 22       |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.6     |
|    ep_rew_mean     | 15.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 690      |
|    time_elapsed    | 76479    |
|    total_timesteps | 276000   |
---------------------------------
Eval num_timesteps=276200, episode_reward=5.34 +/- 2.97
Episode length: 8.40 +/- 4.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 8.4          |
|    mean_reward          | 5.34         |
| time/                   |              |
|    total_timesteps      | 276200       |
| train/                  |              |
|    approx_kl            | 0.0028042062 |
|    clip_fraction        | 0.102        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.559       |
|    explained_variance   | 0.524        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.434        |
|    n_updates            | 6900         |
|    policy_gradient_loss | -0.0067      |
|    value_loss           | 1.35         |
------------------------------------------
Eval num_timesteps=276400, episode_reward=24.03 +/- 15.27
Episode length: 33.40 +/- 20.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 24       |
| time/              |          |
|    total_timesteps | 276400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.6     |
|    ep_rew_mean     | 14.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 691      |
|    time_elapsed    | 76589    |
|    total_timesteps | 276400   |
---------------------------------
Eval num_timesteps=276600, episode_reward=22.74 +/- 14.91
Episode length: 33.00 +/- 20.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33           |
|    mean_reward          | 22.7         |
| time/                   |              |
|    total_timesteps      | 276600       |
| train/                  |              |
|    approx_kl            | 0.0030699626 |
|    clip_fraction        | 0.0801       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.577       |
|    explained_variance   | 0.177        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.02         |
|    n_updates            | 6910         |
|    policy_gradient_loss | -0.00675     |
|    value_loss           | 3.36         |
------------------------------------------
Eval num_timesteps=276800, episode_reward=9.12 +/- 12.83
Episode length: 13.80 +/- 18.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.8     |
|    mean_reward     | 9.12     |
| time/              |          |
|    total_timesteps | 276800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.7     |
|    ep_rew_mean     | 14.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 692      |
|    time_elapsed    | 76692    |
|    total_timesteps | 276800   |
---------------------------------
Eval num_timesteps=277000, episode_reward=23.63 +/- 15.10
Episode length: 33.20 +/- 20.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.2         |
|    mean_reward          | 23.6         |
| time/                   |              |
|    total_timesteps      | 277000       |
| train/                  |              |
|    approx_kl            | 0.0020516687 |
|    clip_fraction        | 0.0583       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.555       |
|    explained_variance   | 0.257        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.07         |
|    n_updates            | 6920         |
|    policy_gradient_loss | -0.00516     |
|    value_loss           | 2.91         |
------------------------------------------
Eval num_timesteps=277200, episode_reward=21.89 +/- 15.88
Episode length: 31.80 +/- 22.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.8     |
|    mean_reward     | 21.9     |
| time/              |          |
|    total_timesteps | 277200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.3     |
|    ep_rew_mean     | 14.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 693      |
|    time_elapsed    | 76822    |
|    total_timesteps | 277200   |
---------------------------------
Eval num_timesteps=277400, episode_reward=21.99 +/- 16.25
Episode length: 31.60 +/- 22.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 31.6         |
|    mean_reward          | 22           |
| time/                   |              |
|    total_timesteps      | 277400       |
| train/                  |              |
|    approx_kl            | 0.0029188653 |
|    clip_fraction        | 0.18         |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.592       |
|    explained_variance   | 0.445        |
|    learning_rate        | 0.0001       |
|    loss                 | 1            |
|    n_updates            | 6930         |
|    policy_gradient_loss | -0.0123      |
|    value_loss           | 1.54         |
------------------------------------------
Eval num_timesteps=277600, episode_reward=22.77 +/- 10.88
Episode length: 33.00 +/- 14.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 22.8     |
| time/              |          |
|    total_timesteps | 277600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | 14       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 694      |
|    time_elapsed    | 76951    |
|    total_timesteps | 277600   |
---------------------------------
Eval num_timesteps=277800, episode_reward=11.21 +/- 12.27
Episode length: 16.60 +/- 17.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.6         |
|    mean_reward          | 11.2         |
| time/                   |              |
|    total_timesteps      | 277800       |
| train/                  |              |
|    approx_kl            | 0.0020340686 |
|    clip_fraction        | 0.0895       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.537       |
|    explained_variance   | 0.327        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.25         |
|    n_updates            | 6940         |
|    policy_gradient_loss | -0.00474     |
|    value_loss           | 3.14         |
------------------------------------------
Eval num_timesteps=278000, episode_reward=24.26 +/- 13.01
Episode length: 35.00 +/- 18.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 24.3     |
| time/              |          |
|    total_timesteps | 278000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20       |
|    ep_rew_mean     | 13.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 695      |
|    time_elapsed    | 77059    |
|    total_timesteps | 278000   |
---------------------------------
Eval num_timesteps=278200, episode_reward=15.92 +/- 15.64
Episode length: 23.40 +/- 21.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.4         |
|    mean_reward          | 15.9         |
| time/                   |              |
|    total_timesteps      | 278200       |
| train/                  |              |
|    approx_kl            | 0.0026278913 |
|    clip_fraction        | 0.098        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.575       |
|    explained_variance   | 0.305        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.862        |
|    n_updates            | 6950         |
|    policy_gradient_loss | -0.00601     |
|    value_loss           | 2.17         |
------------------------------------------
Eval num_timesteps=278400, episode_reward=4.94 +/- 1.30
Episode length: 8.00 +/- 1.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8        |
|    mean_reward     | 4.94     |
| time/              |          |
|    total_timesteps | 278400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.2     |
|    ep_rew_mean     | 13.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 696      |
|    time_elapsed    | 77149    |
|    total_timesteps | 278400   |
---------------------------------
Eval num_timesteps=278600, episode_reward=3.70 +/- 1.45
Episode length: 6.40 +/- 2.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 6.4          |
|    mean_reward          | 3.7          |
| time/                   |              |
|    total_timesteps      | 278600       |
| train/                  |              |
|    approx_kl            | 0.0031937927 |
|    clip_fraction        | 0.114        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.554       |
|    explained_variance   | 0.408        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.964        |
|    n_updates            | 6960         |
|    policy_gradient_loss | -0.00805     |
|    value_loss           | 2.47         |
------------------------------------------
Eval num_timesteps=278800, episode_reward=18.00 +/- 13.13
Episode length: 27.00 +/- 19.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27       |
|    mean_reward     | 18       |
| time/              |          |
|    total_timesteps | 278800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.7     |
|    ep_rew_mean     | 14       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 697      |
|    time_elapsed    | 77238    |
|    total_timesteps | 278800   |
---------------------------------
Eval num_timesteps=279000, episode_reward=17.07 +/- 13.50
Episode length: 26.00 +/- 19.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26           |
|    mean_reward          | 17.1         |
| time/                   |              |
|    total_timesteps      | 279000       |
| train/                  |              |
|    approx_kl            | 0.0030591076 |
|    clip_fraction        | 0.132        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.572       |
|    explained_variance   | 0.351        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.38         |
|    n_updates            | 6970         |
|    policy_gradient_loss | -0.00868     |
|    value_loss           | 1.98         |
------------------------------------------
Eval num_timesteps=279200, episode_reward=16.13 +/- 14.08
Episode length: 24.60 +/- 20.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 16.1     |
| time/              |          |
|    total_timesteps | 279200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.3     |
|    ep_rew_mean     | 13       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 698      |
|    time_elapsed    | 77340    |
|    total_timesteps | 279200   |
---------------------------------
Eval num_timesteps=279400, episode_reward=17.87 +/- 13.92
Episode length: 26.20 +/- 19.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.2         |
|    mean_reward          | 17.9         |
| time/                   |              |
|    total_timesteps      | 279400       |
| train/                  |              |
|    approx_kl            | 0.0027530063 |
|    clip_fraction        | 0.0732       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.559       |
|    explained_variance   | 0.481        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.715        |
|    n_updates            | 6980         |
|    policy_gradient_loss | -0.00734     |
|    value_loss           | 2.11         |
------------------------------------------
Eval num_timesteps=279600, episode_reward=4.64 +/- 3.42
Episode length: 7.40 +/- 4.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7.4      |
|    mean_reward     | 4.64     |
| time/              |          |
|    total_timesteps | 279600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 699      |
|    time_elapsed    | 77434    |
|    total_timesteps | 279600   |
---------------------------------
Eval num_timesteps=279800, episode_reward=17.41 +/- 15.39
Episode length: 24.80 +/- 20.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 24.8        |
|    mean_reward          | 17.4        |
| time/                   |             |
|    total_timesteps      | 279800      |
| train/                  |             |
|    approx_kl            | 0.003942541 |
|    clip_fraction        | 0.0663      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.528      |
|    explained_variance   | 0.32        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.432       |
|    n_updates            | 6990        |
|    policy_gradient_loss | -0.00398    |
|    value_loss           | 1.64        |
-----------------------------------------
Eval num_timesteps=280000, episode_reward=22.52 +/- 14.23
Episode length: 33.40 +/- 20.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 22.5     |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.5     |
|    ep_rew_mean     | 16.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 700      |
|    time_elapsed    | 77540    |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=280200, episode_reward=16.85 +/- 13.56
Episode length: 25.60 +/- 20.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 25.6        |
|    mean_reward          | 16.9        |
| time/                   |             |
|    total_timesteps      | 280200      |
| train/                  |             |
|    approx_kl            | 0.003314829 |
|    clip_fraction        | 0.0987      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.567      |
|    explained_variance   | 0.0353      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.386       |
|    n_updates            | 7000        |
|    policy_gradient_loss | -0.00354    |
|    value_loss           | 1.34        |
-----------------------------------------
Eval num_timesteps=280400, episode_reward=22.02 +/- 15.28
Episode length: 32.20 +/- 21.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 22       |
| time/              |          |
|    total_timesteps | 280400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23       |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 701      |
|    time_elapsed    | 77646    |
|    total_timesteps | 280400   |
---------------------------------
Eval num_timesteps=280600, episode_reward=16.20 +/- 15.85
Episode length: 23.40 +/- 21.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.4         |
|    mean_reward          | 16.2         |
| time/                   |              |
|    total_timesteps      | 280600       |
| train/                  |              |
|    approx_kl            | 0.0018870804 |
|    clip_fraction        | 0.0518       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.562       |
|    explained_variance   | 0.312        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.59         |
|    n_updates            | 7010         |
|    policy_gradient_loss | -0.00446     |
|    value_loss           | 2.53         |
------------------------------------------
Eval num_timesteps=280800, episode_reward=16.72 +/- 15.44
Episode length: 24.00 +/- 21.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 280800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.1     |
|    ep_rew_mean     | 15.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 702      |
|    time_elapsed    | 77755    |
|    total_timesteps | 280800   |
---------------------------------
Eval num_timesteps=281000, episode_reward=3.30 +/- 1.59
Episode length: 5.60 +/- 2.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 5.6          |
|    mean_reward          | 3.3          |
| time/                   |              |
|    total_timesteps      | 281000       |
| train/                  |              |
|    approx_kl            | 0.0018207596 |
|    clip_fraction        | 0.0658       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.579       |
|    explained_variance   | 0.42         |
|    learning_rate        | 0.0001       |
|    loss                 | 1.14         |
|    n_updates            | 7020         |
|    policy_gradient_loss | -0.00536     |
|    value_loss           | 2.13         |
------------------------------------------
Eval num_timesteps=281200, episode_reward=10.94 +/- 13.20
Episode length: 16.60 +/- 18.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | 10.9     |
| time/              |          |
|    total_timesteps | 281200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 703      |
|    time_elapsed    | 77838    |
|    total_timesteps | 281200   |
---------------------------------
Eval num_timesteps=281400, episode_reward=28.92 +/- 12.26
Episode length: 41.40 +/- 17.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.4         |
|    mean_reward          | 28.9         |
| time/                   |              |
|    total_timesteps      | 281400       |
| train/                  |              |
|    approx_kl            | 0.0032217863 |
|    clip_fraction        | 0.0908       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.578       |
|    explained_variance   | 0.345        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.137        |
|    n_updates            | 7030         |
|    policy_gradient_loss | -0.00495     |
|    value_loss           | 1.58         |
------------------------------------------
Eval num_timesteps=281600, episode_reward=26.83 +/- 12.54
Episode length: 37.80 +/- 16.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.8     |
|    mean_reward     | 26.8     |
| time/              |          |
|    total_timesteps | 281600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.5     |
|    ep_rew_mean     | 16.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 704      |
|    time_elapsed    | 77982    |
|    total_timesteps | 281600   |
---------------------------------
Eval num_timesteps=281800, episode_reward=22.97 +/- 16.00
Episode length: 32.40 +/- 21.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.4         |
|    mean_reward          | 23           |
| time/                   |              |
|    total_timesteps      | 281800       |
| train/                  |              |
|    approx_kl            | 0.0032544674 |
|    clip_fraction        | 0.124        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.55        |
|    explained_variance   | 0.533        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.614        |
|    n_updates            | 7040         |
|    policy_gradient_loss | -0.00817     |
|    value_loss           | 1.61         |
------------------------------------------
Eval num_timesteps=282000, episode_reward=8.70 +/- 12.36
Episode length: 13.60 +/- 18.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.6     |
|    mean_reward     | 8.7      |
| time/              |          |
|    total_timesteps | 282000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.9     |
|    ep_rew_mean     | 15.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 705      |
|    time_elapsed    | 78085    |
|    total_timesteps | 282000   |
---------------------------------
Eval num_timesteps=282200, episode_reward=11.64 +/- 12.97
Episode length: 16.80 +/- 17.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.8         |
|    mean_reward          | 11.6         |
| time/                   |              |
|    total_timesteps      | 282200       |
| train/                  |              |
|    approx_kl            | 0.0017898852 |
|    clip_fraction        | 0.0645       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.546       |
|    explained_variance   | 0.407        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.367        |
|    n_updates            | 7050         |
|    policy_gradient_loss | -0.00722     |
|    value_loss           | 2.3          |
------------------------------------------
Eval num_timesteps=282400, episode_reward=18.37 +/- 13.73
Episode length: 27.20 +/- 19.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.2     |
|    mean_reward     | 18.4     |
| time/              |          |
|    total_timesteps | 282400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.2     |
|    ep_rew_mean     | 14.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 706      |
|    time_elapsed    | 78199    |
|    total_timesteps | 282400   |
---------------------------------
Eval num_timesteps=282600, episode_reward=10.65 +/- 13.20
Episode length: 15.40 +/- 17.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.4         |
|    mean_reward          | 10.7         |
| time/                   |              |
|    total_timesteps      | 282600       |
| train/                  |              |
|    approx_kl            | 0.0038957985 |
|    clip_fraction        | 0.146        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.504       |
|    explained_variance   | 0.298        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.34         |
|    n_updates            | 7060         |
|    policy_gradient_loss | -0.00891     |
|    value_loss           | 3.29         |
------------------------------------------
Eval num_timesteps=282800, episode_reward=16.97 +/- 15.26
Episode length: 24.40 +/- 20.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 282800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.1     |
|    ep_rew_mean     | 15       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 707      |
|    time_elapsed    | 78305    |
|    total_timesteps | 282800   |
---------------------------------
Eval num_timesteps=283000, episode_reward=4.21 +/- 3.26
Episode length: 7.00 +/- 4.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 7           |
|    mean_reward          | 4.21        |
| time/                   |             |
|    total_timesteps      | 283000      |
| train/                  |             |
|    approx_kl            | 0.001682842 |
|    clip_fraction        | 0.0857      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.499      |
|    explained_variance   | 0.305       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.07        |
|    n_updates            | 7070        |
|    policy_gradient_loss | -0.00603    |
|    value_loss           | 2.41        |
-----------------------------------------
Eval num_timesteps=283200, episode_reward=16.04 +/- 15.62
Episode length: 23.40 +/- 21.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 283200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 14.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 708      |
|    time_elapsed    | 78393    |
|    total_timesteps | 283200   |
---------------------------------
Eval num_timesteps=283400, episode_reward=10.39 +/- 12.17
Episode length: 16.00 +/- 18.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | 10.4         |
| time/                   |              |
|    total_timesteps      | 283400       |
| train/                  |              |
|    approx_kl            | 0.0034773068 |
|    clip_fraction        | 0.115        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.571       |
|    explained_variance   | -0.317       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.785        |
|    n_updates            | 7080         |
|    policy_gradient_loss | -0.00551     |
|    value_loss           | 1.98         |
------------------------------------------
Eval num_timesteps=283600, episode_reward=11.77 +/- 11.94
Episode length: 17.40 +/- 16.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 11.8     |
| time/              |          |
|    total_timesteps | 283600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.5     |
|    ep_rew_mean     | 14.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 709      |
|    time_elapsed    | 78497    |
|    total_timesteps | 283600   |
---------------------------------
Eval num_timesteps=283800, episode_reward=22.32 +/- 15.80
Episode length: 32.00 +/- 22.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32           |
|    mean_reward          | 22.3         |
| time/                   |              |
|    total_timesteps      | 283800       |
| train/                  |              |
|    approx_kl            | 0.0012955848 |
|    clip_fraction        | 0.0813       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.526       |
|    explained_variance   | 0.265        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.681        |
|    n_updates            | 7090         |
|    policy_gradient_loss | -0.00469     |
|    value_loss           | 2.34         |
------------------------------------------
Eval num_timesteps=284000, episode_reward=17.38 +/- 14.54
Episode length: 25.20 +/- 20.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 284000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.2     |
|    ep_rew_mean     | 15.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 710      |
|    time_elapsed    | 78627    |
|    total_timesteps | 284000   |
---------------------------------
Eval num_timesteps=284200, episode_reward=8.73 +/- 12.92
Episode length: 13.40 +/- 18.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.4        |
|    mean_reward          | 8.73        |
| time/                   |             |
|    total_timesteps      | 284200      |
| train/                  |             |
|    approx_kl            | 0.004562888 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.523      |
|    explained_variance   | 0.136       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.01        |
|    n_updates            | 7100        |
|    policy_gradient_loss | -0.0064     |
|    value_loss           | 2.62        |
-----------------------------------------
Eval num_timesteps=284400, episode_reward=22.82 +/- 16.13
Episode length: 32.20 +/- 21.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 22.8     |
| time/              |          |
|    total_timesteps | 284400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.1     |
|    ep_rew_mean     | 15.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 711      |
|    time_elapsed    | 78733    |
|    total_timesteps | 284400   |
---------------------------------
Eval num_timesteps=284600, episode_reward=34.92 +/- 0.54
Episode length: 50.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | 34.9         |
| time/                   |              |
|    total_timesteps      | 284600       |
| train/                  |              |
|    approx_kl            | 0.0014308199 |
|    clip_fraction        | 0.05         |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.496       |
|    explained_variance   | 0.301        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.28         |
|    n_updates            | 7110         |
|    policy_gradient_loss | -0.00479     |
|    value_loss           | 1.91         |
------------------------------------------
Eval num_timesteps=284800, episode_reward=8.38 +/- 6.32
Episode length: 13.20 +/- 9.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.2     |
|    mean_reward     | 8.38     |
| time/              |          |
|    total_timesteps | 284800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.6     |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 712      |
|    time_elapsed    | 78852    |
|    total_timesteps | 284800   |
---------------------------------
Eval num_timesteps=285000, episode_reward=22.28 +/- 11.34
Episode length: 32.40 +/- 16.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.4         |
|    mean_reward          | 22.3         |
| time/                   |              |
|    total_timesteps      | 285000       |
| train/                  |              |
|    approx_kl            | 0.0032031436 |
|    clip_fraction        | 0.159        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.554       |
|    explained_variance   | 0.228        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.441        |
|    n_updates            | 7120         |
|    policy_gradient_loss | -0.00506     |
|    value_loss           | 0.766        |
------------------------------------------
Eval num_timesteps=285200, episode_reward=22.74 +/- 15.35
Episode length: 32.60 +/- 21.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.6     |
|    mean_reward     | 22.7     |
| time/              |          |
|    total_timesteps | 285200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.2     |
|    ep_rew_mean     | 18       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 713      |
|    time_elapsed    | 78961    |
|    total_timesteps | 285200   |
---------------------------------
Eval num_timesteps=285400, episode_reward=10.59 +/- 12.18
Episode length: 16.00 +/- 17.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | 10.6         |
| time/                   |              |
|    total_timesteps      | 285400       |
| train/                  |              |
|    approx_kl            | 0.0030258794 |
|    clip_fraction        | 0.077        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.539       |
|    explained_variance   | 0.429        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.273        |
|    n_updates            | 7130         |
|    policy_gradient_loss | -0.00377     |
|    value_loss           | 1.51         |
------------------------------------------
Eval num_timesteps=285600, episode_reward=24.69 +/- 12.44
Episode length: 37.00 +/- 18.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 24.7     |
| time/              |          |
|    total_timesteps | 285600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.5     |
|    ep_rew_mean     | 18.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 714      |
|    time_elapsed    | 79083    |
|    total_timesteps | 285600   |
---------------------------------
Eval num_timesteps=285800, episode_reward=16.77 +/- 14.49
Episode length: 24.80 +/- 20.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 24.8        |
|    mean_reward          | 16.8        |
| time/                   |             |
|    total_timesteps      | 285800      |
| train/                  |             |
|    approx_kl            | 0.003813484 |
|    clip_fraction        | 0.0962      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.563      |
|    explained_variance   | 0.315       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.138       |
|    n_updates            | 7140        |
|    policy_gradient_loss | -0.0064     |
|    value_loss           | 1.49        |
-----------------------------------------
Eval num_timesteps=286000, episode_reward=20.66 +/- 16.02
Episode length: 31.00 +/- 23.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31       |
|    mean_reward     | 20.7     |
| time/              |          |
|    total_timesteps | 286000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.4     |
|    ep_rew_mean     | 17.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 715      |
|    time_elapsed    | 79191    |
|    total_timesteps | 286000   |
---------------------------------
Eval num_timesteps=286200, episode_reward=16.33 +/- 14.81
Episode length: 24.20 +/- 21.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.2         |
|    mean_reward          | 16.3         |
| time/                   |              |
|    total_timesteps      | 286200       |
| train/                  |              |
|    approx_kl            | 0.0020443972 |
|    clip_fraction        | 0.0525       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.501       |
|    explained_variance   | 0.446        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.294        |
|    n_updates            | 7150         |
|    policy_gradient_loss | -0.00571     |
|    value_loss           | 2.13         |
------------------------------------------
Eval num_timesteps=286400, episode_reward=16.84 +/- 14.95
Episode length: 24.60 +/- 20.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 286400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.5     |
|    ep_rew_mean     | 18.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 716      |
|    time_elapsed    | 79297    |
|    total_timesteps | 286400   |
---------------------------------
Eval num_timesteps=286600, episode_reward=29.74 +/- 10.63
Episode length: 42.60 +/- 14.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.6         |
|    mean_reward          | 29.7         |
| time/                   |              |
|    total_timesteps      | 286600       |
| train/                  |              |
|    approx_kl            | 0.0019743699 |
|    clip_fraction        | 0.0574       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.527       |
|    explained_variance   | 0.285        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.733        |
|    n_updates            | 7160         |
|    policy_gradient_loss | -0.00469     |
|    value_loss           | 1.93         |
------------------------------------------
Eval num_timesteps=286800, episode_reward=11.89 +/- 11.39
Episode length: 17.80 +/- 16.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | 11.9     |
| time/              |          |
|    total_timesteps | 286800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.5     |
|    ep_rew_mean     | 18.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 717      |
|    time_elapsed    | 79424    |
|    total_timesteps | 286800   |
---------------------------------
Eval num_timesteps=287000, episode_reward=12.24 +/- 13.38
Episode length: 17.80 +/- 17.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.8         |
|    mean_reward          | 12.2         |
| time/                   |              |
|    total_timesteps      | 287000       |
| train/                  |              |
|    approx_kl            | 0.0034146488 |
|    clip_fraction        | 0.0967       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.505       |
|    explained_variance   | 0.418        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.18         |
|    n_updates            | 7170         |
|    policy_gradient_loss | -0.00673     |
|    value_loss           | 1.91         |
------------------------------------------
Eval num_timesteps=287200, episode_reward=17.29 +/- 13.65
Episode length: 26.00 +/- 19.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | 17.3     |
| time/              |          |
|    total_timesteps | 287200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.8     |
|    ep_rew_mean     | 16.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 718      |
|    time_elapsed    | 79529    |
|    total_timesteps | 287200   |
---------------------------------
Eval num_timesteps=287400, episode_reward=12.30 +/- 11.86
Episode length: 18.20 +/- 16.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.2        |
|    mean_reward          | 12.3        |
| time/                   |             |
|    total_timesteps      | 287400      |
| train/                  |             |
|    approx_kl            | 0.001237828 |
|    clip_fraction        | 0.0467      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.517      |
|    explained_variance   | 0.466       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.7         |
|    n_updates            | 7180        |
|    policy_gradient_loss | -0.0046     |
|    value_loss           | 2.26        |
-----------------------------------------
Eval num_timesteps=287600, episode_reward=15.53 +/- 15.99
Episode length: 22.60 +/- 22.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 15.5     |
| time/              |          |
|    total_timesteps | 287600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.6     |
|    ep_rew_mean     | 17       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 719      |
|    time_elapsed    | 79633    |
|    total_timesteps | 287600   |
---------------------------------
Eval num_timesteps=287800, episode_reward=8.72 +/- 12.89
Episode length: 13.40 +/- 18.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.4        |
|    mean_reward          | 8.72        |
| time/                   |             |
|    total_timesteps      | 287800      |
| train/                  |             |
|    approx_kl            | 0.002881409 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.544      |
|    explained_variance   | 0.432       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.74        |
|    n_updates            | 7190        |
|    policy_gradient_loss | -0.00709    |
|    value_loss           | 1.36        |
-----------------------------------------
Eval num_timesteps=288000, episode_reward=4.70 +/- 3.32
Episode length: 7.60 +/- 4.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7.6      |
|    mean_reward     | 4.7      |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.1     |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 720      |
|    time_elapsed    | 79725    |
|    total_timesteps | 288000   |
---------------------------------
Eval num_timesteps=288200, episode_reward=11.69 +/- 11.61
Episode length: 18.00 +/- 17.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18           |
|    mean_reward          | 11.7         |
| time/                   |              |
|    total_timesteps      | 288200       |
| train/                  |              |
|    approx_kl            | 0.0021937103 |
|    clip_fraction        | 0.112        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.484       |
|    explained_variance   | 0.51         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.96         |
|    n_updates            | 7200         |
|    policy_gradient_loss | -0.00681     |
|    value_loss           | 2.05         |
------------------------------------------
Eval num_timesteps=288400, episode_reward=17.87 +/- 14.09
Episode length: 26.00 +/- 19.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 288400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | 16.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 721      |
|    time_elapsed    | 79831    |
|    total_timesteps | 288400   |
---------------------------------
Eval num_timesteps=288600, episode_reward=17.22 +/- 15.02
Episode length: 24.80 +/- 20.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.8         |
|    mean_reward          | 17.2         |
| time/                   |              |
|    total_timesteps      | 288600       |
| train/                  |              |
|    approx_kl            | 0.0022486614 |
|    clip_fraction        | 0.0364       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.507       |
|    explained_variance   | -0.0334      |
|    learning_rate        | 0.0001       |
|    loss                 | 1.02         |
|    n_updates            | 7210         |
|    policy_gradient_loss | -0.005       |
|    value_loss           | 2.96         |
------------------------------------------
Eval num_timesteps=288800, episode_reward=23.10 +/- 14.40
Episode length: 33.40 +/- 20.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 23.1     |
| time/              |          |
|    total_timesteps | 288800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 722      |
|    time_elapsed    | 79938    |
|    total_timesteps | 288800   |
---------------------------------
Eval num_timesteps=289000, episode_reward=11.67 +/- 11.80
Episode length: 17.60 +/- 16.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.6        |
|    mean_reward          | 11.7        |
| time/                   |             |
|    total_timesteps      | 289000      |
| train/                  |             |
|    approx_kl            | 0.005443572 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.545      |
|    explained_variance   | 0.381       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.767       |
|    n_updates            | 7220        |
|    policy_gradient_loss | -0.00585    |
|    value_loss           | 1.06        |
-----------------------------------------
Eval num_timesteps=289200, episode_reward=7.38 +/- 4.01
Episode length: 11.60 +/- 5.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 11.6     |
|    mean_reward     | 7.38     |
| time/              |          |
|    total_timesteps | 289200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.8     |
|    ep_rew_mean     | 15.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 723      |
|    time_elapsed    | 80027    |
|    total_timesteps | 289200   |
---------------------------------
Eval num_timesteps=289400, episode_reward=16.70 +/- 14.58
Episode length: 24.80 +/- 20.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.8         |
|    mean_reward          | 16.7         |
| time/                   |              |
|    total_timesteps      | 289400       |
| train/                  |              |
|    approx_kl            | 0.0015962191 |
|    clip_fraction        | 0.0821       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.479       |
|    explained_variance   | 0.247        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.1          |
|    n_updates            | 7230         |
|    policy_gradient_loss | -0.00569     |
|    value_loss           | 3.08         |
------------------------------------------
Eval num_timesteps=289600, episode_reward=14.96 +/- 15.96
Episode length: 22.20 +/- 22.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 15       |
| time/              |          |
|    total_timesteps | 289600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.6     |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 724      |
|    time_elapsed    | 80160    |
|    total_timesteps | 289600   |
---------------------------------
Eval num_timesteps=289800, episode_reward=10.37 +/- 11.77
Episode length: 16.00 +/- 17.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | 10.4         |
| time/                   |              |
|    total_timesteps      | 289800       |
| train/                  |              |
|    approx_kl            | 0.0030385118 |
|    clip_fraction        | 0.109        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.556       |
|    explained_variance   | 0.28         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.251        |
|    n_updates            | 7240         |
|    policy_gradient_loss | -0.00725     |
|    value_loss           | 1.62         |
------------------------------------------
Eval num_timesteps=290000, episode_reward=16.97 +/- 14.82
Episode length: 24.80 +/- 20.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 290000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.3     |
|    ep_rew_mean     | 16       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 725      |
|    time_elapsed    | 80262    |
|    total_timesteps | 290000   |
---------------------------------
Eval num_timesteps=290200, episode_reward=16.39 +/- 15.06
Episode length: 24.20 +/- 21.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 24.2        |
|    mean_reward          | 16.4        |
| time/                   |             |
|    total_timesteps      | 290200      |
| train/                  |             |
|    approx_kl            | 0.002551655 |
|    clip_fraction        | 0.0855      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.49       |
|    explained_variance   | 0.403       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.538       |
|    n_updates            | 7250        |
|    policy_gradient_loss | -0.00566    |
|    value_loss           | 2.14        |
-----------------------------------------
Eval num_timesteps=290400, episode_reward=28.26 +/- 13.05
Episode length: 40.80 +/- 18.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.8     |
|    mean_reward     | 28.3     |
| time/              |          |
|    total_timesteps | 290400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 726      |
|    time_elapsed    | 80396    |
|    total_timesteps | 290400   |
---------------------------------
Eval num_timesteps=290600, episode_reward=18.80 +/- 13.50
Episode length: 27.20 +/- 18.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 27.2         |
|    mean_reward          | 18.8         |
| time/                   |              |
|    total_timesteps      | 290600       |
| train/                  |              |
|    approx_kl            | 0.0010697931 |
|    clip_fraction        | 0.048        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.501       |
|    explained_variance   | 0.567        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.531        |
|    n_updates            | 7260         |
|    policy_gradient_loss | -0.00328     |
|    value_loss           | 1.55         |
------------------------------------------
Eval num_timesteps=290800, episode_reward=16.26 +/- 14.90
Episode length: 24.20 +/- 21.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 16.3     |
| time/              |          |
|    total_timesteps | 290800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | 16       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 727      |
|    time_elapsed    | 80503    |
|    total_timesteps | 290800   |
---------------------------------
Eval num_timesteps=291000, episode_reward=22.63 +/- 14.58
Episode length: 33.20 +/- 20.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.2         |
|    mean_reward          | 22.6         |
| time/                   |              |
|    total_timesteps      | 291000       |
| train/                  |              |
|    approx_kl            | 0.0014369887 |
|    clip_fraction        | 0.0574       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.505       |
|    explained_variance   | 0.42         |
|    learning_rate        | 0.0001       |
|    loss                 | 1.22         |
|    n_updates            | 7270         |
|    policy_gradient_loss | -0.00273     |
|    value_loss           | 1.16         |
------------------------------------------
Eval num_timesteps=291200, episode_reward=32.95 +/- 4.83
Episode length: 46.80 +/- 6.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | 33       |
| time/              |          |
|    total_timesteps | 291200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.2     |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 728      |
|    time_elapsed    | 80624    |
|    total_timesteps | 291200   |
---------------------------------
Eval num_timesteps=291400, episode_reward=9.39 +/- 12.08
Episode length: 14.40 +/- 17.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.4        |
|    mean_reward          | 9.39        |
| time/                   |             |
|    total_timesteps      | 291400      |
| train/                  |             |
|    approx_kl            | 0.005346554 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.504      |
|    explained_variance   | 0.256       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.5         |
|    n_updates            | 7280        |
|    policy_gradient_loss | -0.0065     |
|    value_loss           | 1.77        |
-----------------------------------------
Eval num_timesteps=291600, episode_reward=13.10 +/- 11.06
Episode length: 19.80 +/- 16.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.8     |
|    mean_reward     | 13.1     |
| time/              |          |
|    total_timesteps | 291600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.2     |
|    ep_rew_mean     | 15.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 729      |
|    time_elapsed    | 80726    |
|    total_timesteps | 291600   |
---------------------------------
Eval num_timesteps=291800, episode_reward=16.96 +/- 16.12
Episode length: 23.80 +/- 21.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.8         |
|    mean_reward          | 17           |
| time/                   |              |
|    total_timesteps      | 291800       |
| train/                  |              |
|    approx_kl            | 0.0016709148 |
|    clip_fraction        | 0.0683       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.479       |
|    explained_variance   | 0.391        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.659        |
|    n_updates            | 7290         |
|    policy_gradient_loss | -0.00462     |
|    value_loss           | 2.27         |
------------------------------------------
Eval num_timesteps=292000, episode_reward=21.92 +/- 14.49
Episode length: 32.80 +/- 21.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 21.9     |
| time/              |          |
|    total_timesteps | 292000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.1     |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 730      |
|    time_elapsed    | 80833    |
|    total_timesteps | 292000   |
---------------------------------
Eval num_timesteps=292200, episode_reward=35.81 +/- 0.83
Episode length: 50.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | 35.8         |
| time/                   |              |
|    total_timesteps      | 292200       |
| train/                  |              |
|    approx_kl            | 0.0034756218 |
|    clip_fraction        | 0.0913       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.494       |
|    explained_variance   | 0.0241       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.668        |
|    n_updates            | 7300         |
|    policy_gradient_loss | -0.00787     |
|    value_loss           | 2.5          |
------------------------------------------
Eval num_timesteps=292400, episode_reward=22.64 +/- 14.99
Episode length: 32.80 +/- 21.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 22.6     |
| time/              |          |
|    total_timesteps | 292400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | 17       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 731      |
|    time_elapsed    | 80986    |
|    total_timesteps | 292400   |
---------------------------------
Eval num_timesteps=292600, episode_reward=11.69 +/- 12.03
Episode length: 17.40 +/- 16.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.4         |
|    mean_reward          | 11.7         |
| time/                   |              |
|    total_timesteps      | 292600       |
| train/                  |              |
|    approx_kl            | 0.0030738264 |
|    clip_fraction        | 0.0772       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.528       |
|    explained_variance   | 0.344        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.162        |
|    n_updates            | 7310         |
|    policy_gradient_loss | -0.0047      |
|    value_loss           | 1.74         |
------------------------------------------
Eval num_timesteps=292800, episode_reward=24.80 +/- 13.54
Episode length: 35.40 +/- 18.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 24.8     |
| time/              |          |
|    total_timesteps | 292800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.2     |
|    ep_rew_mean     | 17.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 732      |
|    time_elapsed    | 81112    |
|    total_timesteps | 292800   |
---------------------------------
Eval num_timesteps=293000, episode_reward=17.19 +/- 14.49
Episode length: 25.20 +/- 20.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.2         |
|    mean_reward          | 17.2         |
| time/                   |              |
|    total_timesteps      | 293000       |
| train/                  |              |
|    approx_kl            | 0.0029245955 |
|    clip_fraction        | 0.0743       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.476       |
|    explained_variance   | 0.262        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.04         |
|    n_updates            | 7320         |
|    policy_gradient_loss | -0.00729     |
|    value_loss           | 1.98         |
------------------------------------------
Eval num_timesteps=293200, episode_reward=13.80 +/- 12.49
Episode length: 20.20 +/- 17.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.2     |
|    mean_reward     | 13.8     |
| time/              |          |
|    total_timesteps | 293200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.2     |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 733      |
|    time_elapsed    | 81214    |
|    total_timesteps | 293200   |
---------------------------------
Eval num_timesteps=293400, episode_reward=17.90 +/- 12.74
Episode length: 27.00 +/- 18.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 27           |
|    mean_reward          | 17.9         |
| time/                   |              |
|    total_timesteps      | 293400       |
| train/                  |              |
|    approx_kl            | 0.0025363197 |
|    clip_fraction        | 0.083        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.501       |
|    explained_variance   | 0.508        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.384        |
|    n_updates            | 7330         |
|    policy_gradient_loss | -0.00636     |
|    value_loss           | 1.76         |
------------------------------------------
Eval num_timesteps=293600, episode_reward=15.88 +/- 15.21
Episode length: 23.40 +/- 21.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 15.9     |
| time/              |          |
|    total_timesteps | 293600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | 16.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 734      |
|    time_elapsed    | 81316    |
|    total_timesteps | 293600   |
---------------------------------
Eval num_timesteps=293800, episode_reward=16.24 +/- 15.84
Episode length: 23.40 +/- 21.76
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 23.4       |
|    mean_reward          | 16.2       |
| time/                   |            |
|    total_timesteps      | 293800     |
| train/                  |            |
|    approx_kl            | 0.00461295 |
|    clip_fraction        | 0.0636     |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.519     |
|    explained_variance   | 0.366      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.454      |
|    n_updates            | 7340       |
|    policy_gradient_loss | -0.00488   |
|    value_loss           | 1.66       |
----------------------------------------
Eval num_timesteps=294000, episode_reward=17.44 +/- 14.19
Episode length: 25.80 +/- 20.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 294000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.2     |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 735      |
|    time_elapsed    | 81419    |
|    total_timesteps | 294000   |
---------------------------------
Eval num_timesteps=294200, episode_reward=11.26 +/- 12.27
Episode length: 16.60 +/- 16.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.6         |
|    mean_reward          | 11.3         |
| time/                   |              |
|    total_timesteps      | 294200       |
| train/                  |              |
|    approx_kl            | 0.0036742107 |
|    clip_fraction        | 0.129        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.568       |
|    explained_variance   | 0.34         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.264        |
|    n_updates            | 7350         |
|    policy_gradient_loss | -0.00862     |
|    value_loss           | 1.45         |
------------------------------------------
Eval num_timesteps=294400, episode_reward=10.24 +/- 13.33
Episode length: 14.80 +/- 17.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 10.2     |
| time/              |          |
|    total_timesteps | 294400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.5     |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 736      |
|    time_elapsed    | 81521    |
|    total_timesteps | 294400   |
---------------------------------
Eval num_timesteps=294600, episode_reward=12.53 +/- 12.18
Episode length: 18.20 +/- 16.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.2         |
|    mean_reward          | 12.5         |
| time/                   |              |
|    total_timesteps      | 294600       |
| train/                  |              |
|    approx_kl            | 0.0025554884 |
|    clip_fraction        | 0.132        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.545       |
|    explained_variance   | 0.524        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.507        |
|    n_updates            | 7360         |
|    policy_gradient_loss | -0.00568     |
|    value_loss           | 1.1          |
------------------------------------------
Eval num_timesteps=294800, episode_reward=17.49 +/- 14.34
Episode length: 25.60 +/- 19.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 17.5     |
| time/              |          |
|    total_timesteps | 294800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | 16.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 737      |
|    time_elapsed    | 81623    |
|    total_timesteps | 294800   |
---------------------------------
Eval num_timesteps=295000, episode_reward=29.13 +/- 11.80
Episode length: 41.80 +/- 16.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.8         |
|    mean_reward          | 29.1         |
| time/                   |              |
|    total_timesteps      | 295000       |
| train/                  |              |
|    approx_kl            | 0.0015330893 |
|    clip_fraction        | 0.0433       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.502       |
|    explained_variance   | 0.359        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.787        |
|    n_updates            | 7370         |
|    policy_gradient_loss | -0.00462     |
|    value_loss           | 2.58         |
------------------------------------------
Eval num_timesteps=295200, episode_reward=23.31 +/- 14.15
Episode length: 33.80 +/- 19.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 23.3     |
| time/              |          |
|    total_timesteps | 295200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 738      |
|    time_elapsed    | 81776    |
|    total_timesteps | 295200   |
---------------------------------
Eval num_timesteps=295400, episode_reward=17.35 +/- 14.12
Episode length: 25.80 +/- 19.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.8         |
|    mean_reward          | 17.4         |
| time/                   |              |
|    total_timesteps      | 295400       |
| train/                  |              |
|    approx_kl            | 0.0025519726 |
|    clip_fraction        | 0.108        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.536       |
|    explained_variance   | 0.485        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0116       |
|    n_updates            | 7380         |
|    policy_gradient_loss | -0.00475     |
|    value_loss           | 1.12         |
------------------------------------------
Eval num_timesteps=295600, episode_reward=14.55 +/- 11.35
Episode length: 21.20 +/- 15.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.2     |
|    mean_reward     | 14.6     |
| time/              |          |
|    total_timesteps | 295600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.2     |
|    ep_rew_mean     | 17.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 739      |
|    time_elapsed    | 81878    |
|    total_timesteps | 295600   |
---------------------------------
Eval num_timesteps=295800, episode_reward=18.33 +/- 15.29
Episode length: 25.60 +/- 20.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.6         |
|    mean_reward          | 18.3         |
| time/                   |              |
|    total_timesteps      | 295800       |
| train/                  |              |
|    approx_kl            | 0.0038893346 |
|    clip_fraction        | 0.118        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.563       |
|    explained_variance   | -0.0146      |
|    learning_rate        | 0.0001       |
|    loss                 | 0.227        |
|    n_updates            | 7390         |
|    policy_gradient_loss | -0.00628     |
|    value_loss           | 1.59         |
------------------------------------------
Eval num_timesteps=296000, episode_reward=12.53 +/- 11.85
Episode length: 18.80 +/- 16.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.8     |
|    mean_reward     | 12.5     |
| time/              |          |
|    total_timesteps | 296000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26       |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 740      |
|    time_elapsed    | 81988    |
|    total_timesteps | 296000   |
---------------------------------
Eval num_timesteps=296200, episode_reward=11.59 +/- 12.23
Episode length: 17.40 +/- 17.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.4         |
|    mean_reward          | 11.6         |
| time/                   |              |
|    total_timesteps      | 296200       |
| train/                  |              |
|    approx_kl            | 0.0021762433 |
|    clip_fraction        | 0.054        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.484       |
|    explained_variance   | 0.449        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.09         |
|    n_updates            | 7400         |
|    policy_gradient_loss | -0.00483     |
|    value_loss           | 1.76         |
------------------------------------------
Eval num_timesteps=296400, episode_reward=15.72 +/- 15.34
Episode length: 23.40 +/- 21.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 15.7     |
| time/              |          |
|    total_timesteps | 296400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.5     |
|    ep_rew_mean     | 18.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 741      |
|    time_elapsed    | 82094    |
|    total_timesteps | 296400   |
---------------------------------
Eval num_timesteps=296600, episode_reward=22.85 +/- 14.73
Episode length: 33.00 +/- 20.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33           |
|    mean_reward          | 22.8         |
| time/                   |              |
|    total_timesteps      | 296600       |
| train/                  |              |
|    approx_kl            | 0.0037859133 |
|    clip_fraction        | 0.0855       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.532       |
|    explained_variance   | 0.0579       |
|    learning_rate        | 0.0001       |
|    loss                 | 2.1          |
|    n_updates            | 7410         |
|    policy_gradient_loss | -0.008       |
|    value_loss           | 2.36         |
------------------------------------------
Eval num_timesteps=296800, episode_reward=29.46 +/- 10.61
Episode length: 42.60 +/- 14.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.6     |
|    mean_reward     | 29.5     |
| time/              |          |
|    total_timesteps | 296800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.4     |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 742      |
|    time_elapsed    | 82205    |
|    total_timesteps | 296800   |
---------------------------------
Eval num_timesteps=297000, episode_reward=23.96 +/- 14.77
Episode length: 33.80 +/- 19.96
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 33.8       |
|    mean_reward          | 24         |
| time/                   |            |
|    total_timesteps      | 297000     |
| train/                  |            |
|    approx_kl            | 0.00245451 |
|    clip_fraction        | 0.0714     |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.495     |
|    explained_variance   | 0.418      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.356      |
|    n_updates            | 7420       |
|    policy_gradient_loss | -0.0055    |
|    value_loss           | 2.02       |
----------------------------------------
Eval num_timesteps=297200, episode_reward=16.22 +/- 14.97
Episode length: 24.00 +/- 21.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16.2     |
| time/              |          |
|    total_timesteps | 297200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.6     |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 743      |
|    time_elapsed    | 82334    |
|    total_timesteps | 297200   |
---------------------------------
Eval num_timesteps=297400, episode_reward=12.68 +/- 10.69
Episode length: 19.20 +/- 15.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 19.2         |
|    mean_reward          | 12.7         |
| time/                   |              |
|    total_timesteps      | 297400       |
| train/                  |              |
|    approx_kl            | 0.0024616043 |
|    clip_fraction        | 0.117        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.515       |
|    explained_variance   | 0.522        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.25         |
|    n_updates            | 7430         |
|    policy_gradient_loss | -0.0059      |
|    value_loss           | 1.29         |
------------------------------------------
Eval num_timesteps=297600, episode_reward=6.53 +/- 4.69
Episode length: 10.00 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 6.53     |
| time/              |          |
|    total_timesteps | 297600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.7     |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 744      |
|    time_elapsed    | 82423    |
|    total_timesteps | 297600   |
---------------------------------
Eval num_timesteps=297800, episode_reward=16.45 +/- 14.75
Episode length: 24.20 +/- 21.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 24.2        |
|    mean_reward          | 16.4        |
| time/                   |             |
|    total_timesteps      | 297800      |
| train/                  |             |
|    approx_kl            | 0.003965103 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.539      |
|    explained_variance   | 0.463       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.92        |
|    n_updates            | 7440        |
|    policy_gradient_loss | -0.00753    |
|    value_loss           | 2.28        |
-----------------------------------------
Eval num_timesteps=298000, episode_reward=8.95 +/- 13.33
Episode length: 13.40 +/- 18.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.4     |
|    mean_reward     | 8.95     |
| time/              |          |
|    total_timesteps | 298000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.3     |
|    ep_rew_mean     | 15.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 745      |
|    time_elapsed    | 82528    |
|    total_timesteps | 298000   |
---------------------------------
Eval num_timesteps=298200, episode_reward=22.29 +/- 14.50
Episode length: 33.00 +/- 20.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33          |
|    mean_reward          | 22.3        |
| time/                   |             |
|    total_timesteps      | 298200      |
| train/                  |             |
|    approx_kl            | 0.004760465 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.578      |
|    explained_variance   | 0.365       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.17        |
|    n_updates            | 7450        |
|    policy_gradient_loss | -0.00574    |
|    value_loss           | 1.87        |
-----------------------------------------
Eval num_timesteps=298400, episode_reward=17.61 +/- 14.49
Episode length: 25.80 +/- 20.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 298400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.4     |
|    ep_rew_mean     | 14.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 746      |
|    time_elapsed    | 82635    |
|    total_timesteps | 298400   |
---------------------------------
Eval num_timesteps=298600, episode_reward=9.10 +/- 13.26
Episode length: 13.60 +/- 18.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 13.6          |
|    mean_reward          | 9.1           |
| time/                   |               |
|    total_timesteps      | 298600        |
| train/                  |               |
|    approx_kl            | 0.00085707713 |
|    clip_fraction        | 0.021         |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.539        |
|    explained_variance   | 0.471         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.581         |
|    n_updates            | 7460          |
|    policy_gradient_loss | -0.00317      |
|    value_loss           | 2.25          |
-------------------------------------------
Eval num_timesteps=298800, episode_reward=15.43 +/- 16.00
Episode length: 22.60 +/- 22.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 15.4     |
| time/              |          |
|    total_timesteps | 298800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.3     |
|    ep_rew_mean     | 14.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 747      |
|    time_elapsed    | 82737    |
|    total_timesteps | 298800   |
---------------------------------
Eval num_timesteps=299000, episode_reward=16.04 +/- 15.15
Episode length: 23.80 +/- 21.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.8         |
|    mean_reward          | 16           |
| time/                   |              |
|    total_timesteps      | 299000       |
| train/                  |              |
|    approx_kl            | 0.0026631132 |
|    clip_fraction        | 0.111        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.573       |
|    explained_variance   | 0.144        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.752        |
|    n_updates            | 7470         |
|    policy_gradient_loss | -0.00753     |
|    value_loss           | 2.17         |
------------------------------------------
Eval num_timesteps=299200, episode_reward=16.03 +/- 15.19
Episode length: 23.80 +/- 21.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 299200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.5     |
|    ep_rew_mean     | 14.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 748      |
|    time_elapsed    | 82865    |
|    total_timesteps | 299200   |
---------------------------------
Eval num_timesteps=299400, episode_reward=17.81 +/- 14.26
Episode length: 26.00 +/- 19.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26           |
|    mean_reward          | 17.8         |
| time/                   |              |
|    total_timesteps      | 299400       |
| train/                  |              |
|    approx_kl            | 0.0022031355 |
|    clip_fraction        | 0.141        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.569       |
|    explained_variance   | 0.295        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.54         |
|    n_updates            | 7480         |
|    policy_gradient_loss | -0.00467     |
|    value_loss           | 1.93         |
------------------------------------------
Eval num_timesteps=299600, episode_reward=16.29 +/- 14.92
Episode length: 24.20 +/- 21.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 16.3     |
| time/              |          |
|    total_timesteps | 299600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.8     |
|    ep_rew_mean     | 14.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 749      |
|    time_elapsed    | 82972    |
|    total_timesteps | 299600   |
---------------------------------
Eval num_timesteps=299800, episode_reward=15.60 +/- 15.48
Episode length: 23.00 +/- 22.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23           |
|    mean_reward          | 15.6         |
| time/                   |              |
|    total_timesteps      | 299800       |
| train/                  |              |
|    approx_kl            | 0.0026750693 |
|    clip_fraction        | 0.0681       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.56        |
|    explained_variance   | 0.2          |
|    learning_rate        | 0.0001       |
|    loss                 | 0.847        |
|    n_updates            | 7490         |
|    policy_gradient_loss | -0.00682     |
|    value_loss           | 1.59         |
------------------------------------------
Eval num_timesteps=300000, episode_reward=26.19 +/- 11.54
Episode length: 37.40 +/- 15.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.4     |
|    mean_reward     | 26.2     |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.3     |
|    ep_rew_mean     | 16       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 750      |
|    time_elapsed    | 83104    |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=300200, episode_reward=11.72 +/- 11.53
Episode length: 17.60 +/- 16.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.6         |
|    mean_reward          | 11.7         |
| time/                   |              |
|    total_timesteps      | 300200       |
| train/                  |              |
|    approx_kl            | 0.0023951535 |
|    clip_fraction        | 0.0538       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.501       |
|    explained_variance   | 0.451        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.946        |
|    n_updates            | 7500         |
|    policy_gradient_loss | -0.00622     |
|    value_loss           | 1.7          |
------------------------------------------
Eval num_timesteps=300400, episode_reward=10.83 +/- 13.11
Episode length: 16.00 +/- 18.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | 10.8     |
| time/              |          |
|    total_timesteps | 300400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23       |
|    ep_rew_mean     | 15.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 751      |
|    time_elapsed    | 83206    |
|    total_timesteps | 300400   |
---------------------------------
Eval num_timesteps=300600, episode_reward=20.17 +/- 12.83
Episode length: 29.00 +/- 17.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29          |
|    mean_reward          | 20.2        |
| time/                   |             |
|    total_timesteps      | 300600      |
| train/                  |             |
|    approx_kl            | 0.003197143 |
|    clip_fraction        | 0.0846      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.523      |
|    explained_variance   | 0.535       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.4         |
|    n_updates            | 7510        |
|    policy_gradient_loss | -0.00596    |
|    value_loss           | 1.27        |
-----------------------------------------
Eval num_timesteps=300800, episode_reward=15.68 +/- 15.82
Episode length: 23.00 +/- 22.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 15.7     |
| time/              |          |
|    total_timesteps | 300800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.6     |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 752      |
|    time_elapsed    | 83319    |
|    total_timesteps | 300800   |
---------------------------------
Eval num_timesteps=301000, episode_reward=22.53 +/- 13.75
Episode length: 33.60 +/- 20.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.6         |
|    mean_reward          | 22.5         |
| time/                   |              |
|    total_timesteps      | 301000       |
| train/                  |              |
|    approx_kl            | 0.0044852807 |
|    clip_fraction        | 0.142        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.551       |
|    explained_variance   | 0.192        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.95         |
|    n_updates            | 7520         |
|    policy_gradient_loss | -0.01        |
|    value_loss           | 2.48         |
------------------------------------------
Eval num_timesteps=301200, episode_reward=22.13 +/- 16.06
Episode length: 31.80 +/- 22.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.8     |
|    mean_reward     | 22.1     |
| time/              |          |
|    total_timesteps | 301200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.5     |
|    ep_rew_mean     | 16.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 753      |
|    time_elapsed    | 83447    |
|    total_timesteps | 301200   |
---------------------------------
Eval num_timesteps=301400, episode_reward=21.97 +/- 15.80
Episode length: 31.80 +/- 22.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 31.8         |
|    mean_reward          | 22           |
| time/                   |              |
|    total_timesteps      | 301400       |
| train/                  |              |
|    approx_kl            | 0.0024072789 |
|    clip_fraction        | 0.0592       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.558       |
|    explained_variance   | 0.357        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.99         |
|    n_updates            | 7530         |
|    policy_gradient_loss | -0.00477     |
|    value_loss           | 2.16         |
------------------------------------------
Eval num_timesteps=301600, episode_reward=16.05 +/- 15.14
Episode length: 23.80 +/- 21.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 301600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.5     |
|    ep_rew_mean     | 16.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 754      |
|    time_elapsed    | 83551    |
|    total_timesteps | 301600   |
---------------------------------
Eval num_timesteps=301800, episode_reward=9.84 +/- 11.92
Episode length: 15.40 +/- 17.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.4         |
|    mean_reward          | 9.84         |
| time/                   |              |
|    total_timesteps      | 301800       |
| train/                  |              |
|    approx_kl            | 0.0023570324 |
|    clip_fraction        | 0.117        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.572       |
|    explained_variance   | 0.379        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.87         |
|    n_updates            | 7540         |
|    policy_gradient_loss | -0.00679     |
|    value_loss           | 1.93         |
------------------------------------------
Eval num_timesteps=302000, episode_reward=17.04 +/- 15.15
Episode length: 24.60 +/- 20.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 302000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.7     |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 755      |
|    time_elapsed    | 83653    |
|    total_timesteps | 302000   |
---------------------------------
Eval num_timesteps=302200, episode_reward=18.45 +/- 13.97
Episode length: 27.20 +/- 19.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 27.2         |
|    mean_reward          | 18.5         |
| time/                   |              |
|    total_timesteps      | 302200       |
| train/                  |              |
|    approx_kl            | 0.0018768493 |
|    clip_fraction        | 0.0701       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.529       |
|    explained_variance   | 0.537        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.871        |
|    n_updates            | 7550         |
|    policy_gradient_loss | -0.00712     |
|    value_loss           | 2.21         |
------------------------------------------
Eval num_timesteps=302400, episode_reward=23.07 +/- 14.05
Episode length: 33.60 +/- 20.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 23.1     |
| time/              |          |
|    total_timesteps | 302400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.5     |
|    ep_rew_mean     | 14.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 756      |
|    time_elapsed    | 83761    |
|    total_timesteps | 302400   |
---------------------------------
Eval num_timesteps=302600, episode_reward=9.98 +/- 11.82
Episode length: 15.40 +/- 17.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.4         |
|    mean_reward          | 9.98         |
| time/                   |              |
|    total_timesteps      | 302600       |
| train/                  |              |
|    approx_kl            | 0.0030698185 |
|    clip_fraction        | 0.102        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.56        |
|    explained_variance   | 0.311        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.92         |
|    n_updates            | 7560         |
|    policy_gradient_loss | -0.00708     |
|    value_loss           | 2.75         |
------------------------------------------
Eval num_timesteps=302800, episode_reward=14.99 +/- 11.48
Episode length: 22.20 +/- 15.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 15       |
| time/              |          |
|    total_timesteps | 302800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.6     |
|    ep_rew_mean     | 14.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 757      |
|    time_elapsed    | 83863    |
|    total_timesteps | 302800   |
---------------------------------
Eval num_timesteps=303000, episode_reward=15.95 +/- 15.64
Episode length: 23.40 +/- 21.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.4         |
|    mean_reward          | 15.9         |
| time/                   |              |
|    total_timesteps      | 303000       |
| train/                  |              |
|    approx_kl            | 0.0032196317 |
|    clip_fraction        | 0.0871       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.561       |
|    explained_variance   | 0.503        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.705        |
|    n_updates            | 7570         |
|    policy_gradient_loss | -0.00574     |
|    value_loss           | 1.6          |
------------------------------------------
Eval num_timesteps=303200, episode_reward=16.31 +/- 14.91
Episode length: 24.20 +/- 21.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 16.3     |
| time/              |          |
|    total_timesteps | 303200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 15.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 758      |
|    time_elapsed    | 83990    |
|    total_timesteps | 303200   |
---------------------------------
Eval num_timesteps=303400, episode_reward=13.71 +/- 11.33
Episode length: 20.20 +/- 15.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 20.2         |
|    mean_reward          | 13.7         |
| time/                   |              |
|    total_timesteps      | 303400       |
| train/                  |              |
|    approx_kl            | 0.0018752965 |
|    clip_fraction        | 0.0839       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.587       |
|    explained_variance   | 0.162        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.205        |
|    n_updates            | 7580         |
|    policy_gradient_loss | -0.00574     |
|    value_loss           | 1.52         |
------------------------------------------
Eval num_timesteps=303600, episode_reward=3.50 +/- 3.40
Episode length: 6.00 +/- 5.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6        |
|    mean_reward     | 3.5      |
| time/              |          |
|    total_timesteps | 303600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.8     |
|    ep_rew_mean     | 14.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 759      |
|    time_elapsed    | 84076    |
|    total_timesteps | 303600   |
---------------------------------
Eval num_timesteps=303800, episode_reward=16.36 +/- 15.70
Episode length: 23.60 +/- 21.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.6         |
|    mean_reward          | 16.4         |
| time/                   |              |
|    total_timesteps      | 303800       |
| train/                  |              |
|    approx_kl            | 0.0026494872 |
|    clip_fraction        | 0.0799       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.516       |
|    explained_variance   | 0.407        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.16         |
|    n_updates            | 7590         |
|    policy_gradient_loss | -0.00759     |
|    value_loss           | 2.39         |
------------------------------------------
Eval num_timesteps=304000, episode_reward=11.90 +/- 12.25
Episode length: 17.40 +/- 16.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 11.9     |
| time/              |          |
|    total_timesteps | 304000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.5     |
|    ep_rew_mean     | 14       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 760      |
|    time_elapsed    | 84181    |
|    total_timesteps | 304000   |
---------------------------------
Eval num_timesteps=304200, episode_reward=12.10 +/- 12.73
Episode length: 17.40 +/- 16.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.4         |
|    mean_reward          | 12.1         |
| time/                   |              |
|    total_timesteps      | 304200       |
| train/                  |              |
|    approx_kl            | 0.0037314151 |
|    clip_fraction        | 0.0821       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.493       |
|    explained_variance   | 0.638        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.02         |
|    n_updates            | 7600         |
|    policy_gradient_loss | -0.00624     |
|    value_loss           | 1.91         |
------------------------------------------
Eval num_timesteps=304400, episode_reward=18.40 +/- 13.55
Episode length: 27.20 +/- 19.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.2     |
|    mean_reward     | 18.4     |
| time/              |          |
|    total_timesteps | 304400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.8     |
|    ep_rew_mean     | 14.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 761      |
|    time_elapsed    | 84289    |
|    total_timesteps | 304400   |
---------------------------------
Eval num_timesteps=304600, episode_reward=5.10 +/- 2.64
Episode length: 8.20 +/- 3.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 8.2         |
|    mean_reward          | 5.1         |
| time/                   |             |
|    total_timesteps      | 304600      |
| train/                  |             |
|    approx_kl            | 0.004606708 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.548      |
|    explained_variance   | 0.168       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.286       |
|    n_updates            | 7610        |
|    policy_gradient_loss | -0.00751    |
|    value_loss           | 1.99        |
-----------------------------------------
Eval num_timesteps=304800, episode_reward=11.15 +/- 12.63
Episode length: 16.60 +/- 17.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | 11.1     |
| time/              |          |
|    total_timesteps | 304800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.5     |
|    ep_rew_mean     | 14.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 762      |
|    time_elapsed    | 84375    |
|    total_timesteps | 304800   |
---------------------------------
Eval num_timesteps=305000, episode_reward=17.13 +/- 15.19
Episode length: 24.60 +/- 20.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 24.6        |
|    mean_reward          | 17.1        |
| time/                   |             |
|    total_timesteps      | 305000      |
| train/                  |             |
|    approx_kl            | 0.002749831 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.504      |
|    explained_variance   | 0.26        |
|    learning_rate        | 0.0001      |
|    loss                 | 1.15        |
|    n_updates            | 7620        |
|    policy_gradient_loss | -0.00838    |
|    value_loss           | 2.62        |
-----------------------------------------
Eval num_timesteps=305200, episode_reward=16.05 +/- 16.09
Episode length: 23.00 +/- 22.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 305200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.7     |
|    ep_rew_mean     | 14.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 763      |
|    time_elapsed    | 84482    |
|    total_timesteps | 305200   |
---------------------------------
Eval num_timesteps=305400, episode_reward=15.93 +/- 14.80
Episode length: 24.00 +/- 21.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24           |
|    mean_reward          | 15.9         |
| time/                   |              |
|    total_timesteps      | 305400       |
| train/                  |              |
|    approx_kl            | 0.0012697737 |
|    clip_fraction        | 0.031        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.519       |
|    explained_variance   | 0.4          |
|    learning_rate        | 0.0001       |
|    loss                 | 0.278        |
|    n_updates            | 7630         |
|    policy_gradient_loss | -0.00275     |
|    value_loss           | 1.57         |
------------------------------------------
Eval num_timesteps=305600, episode_reward=29.14 +/- 12.91
Episode length: 41.20 +/- 17.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | 29.1     |
| time/              |          |
|    total_timesteps | 305600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.1     |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 764      |
|    time_elapsed    | 84588    |
|    total_timesteps | 305600   |
---------------------------------
Eval num_timesteps=305800, episode_reward=12.88 +/- 11.95
Episode length: 19.00 +/- 16.35
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 19        |
|    mean_reward          | 12.9      |
| time/                   |           |
|    total_timesteps      | 305800    |
| train/                  |           |
|    approx_kl            | 0.0042561 |
|    clip_fraction        | 0.096     |
|    clip_range           | 0.1       |
|    entropy_loss         | -0.535    |
|    explained_variance   | 0.102     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.705     |
|    n_updates            | 7640      |
|    policy_gradient_loss | -0.0068   |
|    value_loss           | 1.97      |
---------------------------------------
Eval num_timesteps=306000, episode_reward=17.89 +/- 14.45
Episode length: 26.20 +/- 20.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 306000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.3     |
|    ep_rew_mean     | 15.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 765      |
|    time_elapsed    | 84691    |
|    total_timesteps | 306000   |
---------------------------------
Eval num_timesteps=306200, episode_reward=16.46 +/- 15.08
Episode length: 24.40 +/- 21.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.4         |
|    mean_reward          | 16.5         |
| time/                   |              |
|    total_timesteps      | 306200       |
| train/                  |              |
|    approx_kl            | 0.0041803904 |
|    clip_fraction        | 0.145        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.511       |
|    explained_variance   | 0.445        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.3          |
|    n_updates            | 7650         |
|    policy_gradient_loss | -0.00801     |
|    value_loss           | 1.96         |
------------------------------------------
Eval num_timesteps=306400, episode_reward=24.62 +/- 11.29
Episode length: 36.60 +/- 16.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 24.6     |
| time/              |          |
|    total_timesteps | 306400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.2     |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 766      |
|    time_elapsed    | 84801    |
|    total_timesteps | 306400   |
---------------------------------
Eval num_timesteps=306600, episode_reward=24.71 +/- 14.37
Episode length: 34.60 +/- 19.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.6         |
|    mean_reward          | 24.7         |
| time/                   |              |
|    total_timesteps      | 306600       |
| train/                  |              |
|    approx_kl            | 0.0027561635 |
|    clip_fraction        | 0.0658       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.546       |
|    explained_variance   | 0.244        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.17         |
|    n_updates            | 7660         |
|    policy_gradient_loss | -0.00359     |
|    value_loss           | 2.5          |
------------------------------------------
Eval num_timesteps=306800, episode_reward=15.85 +/- 16.17
Episode length: 22.80 +/- 22.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 15.8     |
| time/              |          |
|    total_timesteps | 306800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.5     |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 767      |
|    time_elapsed    | 84912    |
|    total_timesteps | 306800   |
---------------------------------
Eval num_timesteps=307000, episode_reward=35.36 +/- 0.83
Episode length: 50.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | 35.4         |
| time/                   |              |
|    total_timesteps      | 307000       |
| train/                  |              |
|    approx_kl            | 0.0022186383 |
|    clip_fraction        | 0.0705       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.555       |
|    explained_variance   | 0.145        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.285        |
|    n_updates            | 7670         |
|    policy_gradient_loss | -0.00724     |
|    value_loss           | 2.36         |
------------------------------------------
Eval num_timesteps=307200, episode_reward=22.49 +/- 13.37
Episode length: 34.00 +/- 19.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 22.5     |
| time/              |          |
|    total_timesteps | 307200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | 17.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 768      |
|    time_elapsed    | 85042    |
|    total_timesteps | 307200   |
---------------------------------
Eval num_timesteps=307400, episode_reward=22.80 +/- 14.85
Episode length: 33.00 +/- 20.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33           |
|    mean_reward          | 22.8         |
| time/                   |              |
|    total_timesteps      | 307400       |
| train/                  |              |
|    approx_kl            | 0.0033497927 |
|    clip_fraction        | 0.0853       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.53        |
|    explained_variance   | 0.469        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.126        |
|    n_updates            | 7680         |
|    policy_gradient_loss | -0.00739     |
|    value_loss           | 1.39         |
------------------------------------------
Eval num_timesteps=307600, episode_reward=22.56 +/- 15.14
Episode length: 32.80 +/- 21.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 22.6     |
| time/              |          |
|    total_timesteps | 307600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.7     |
|    ep_rew_mean     | 17       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 769      |
|    time_elapsed    | 85153    |
|    total_timesteps | 307600   |
---------------------------------
Eval num_timesteps=307800, episode_reward=21.00 +/- 13.44
Episode length: 30.40 +/- 18.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.4        |
|    mean_reward          | 21          |
| time/                   |             |
|    total_timesteps      | 307800      |
| train/                  |             |
|    approx_kl            | 0.004023039 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.55       |
|    explained_variance   | 0.449       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0816      |
|    n_updates            | 7690        |
|    policy_gradient_loss | -0.00817    |
|    value_loss           | 1.41        |
-----------------------------------------
Eval num_timesteps=308000, episode_reward=22.00 +/- 16.23
Episode length: 31.60 +/- 22.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.6     |
|    mean_reward     | 22       |
| time/              |          |
|    total_timesteps | 308000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | 16.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 770      |
|    time_elapsed    | 85256    |
|    total_timesteps | 308000   |
---------------------------------
Eval num_timesteps=308200, episode_reward=24.78 +/- 14.06
Episode length: 35.00 +/- 18.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35          |
|    mean_reward          | 24.8        |
| time/                   |             |
|    total_timesteps      | 308200      |
| train/                  |             |
|    approx_kl            | 0.003950088 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.546      |
|    explained_variance   | 0.409       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.567       |
|    n_updates            | 7700        |
|    policy_gradient_loss | -0.00673    |
|    value_loss           | 1.94        |
-----------------------------------------
Eval num_timesteps=308400, episode_reward=17.62 +/- 13.80
Episode length: 26.00 +/- 19.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 308400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.7     |
|    ep_rew_mean     | 16.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 771      |
|    time_elapsed    | 85363    |
|    total_timesteps | 308400   |
---------------------------------
Eval num_timesteps=308600, episode_reward=9.98 +/- 13.50
Episode length: 14.60 +/- 17.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.6        |
|    mean_reward          | 9.98        |
| time/                   |             |
|    total_timesteps      | 308600      |
| train/                  |             |
|    approx_kl            | 0.002257883 |
|    clip_fraction        | 0.0576      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.537      |
|    explained_variance   | 0.383       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.16        |
|    n_updates            | 7710        |
|    policy_gradient_loss | -0.00505    |
|    value_loss           | 2.03        |
-----------------------------------------
Eval num_timesteps=308800, episode_reward=17.91 +/- 13.22
Episode length: 27.20 +/- 19.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.2     |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 308800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.8     |
|    ep_rew_mean     | 16.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 772      |
|    time_elapsed    | 85466    |
|    total_timesteps | 308800   |
---------------------------------
Eval num_timesteps=309000, episode_reward=11.09 +/- 11.29
Episode length: 17.00 +/- 16.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17           |
|    mean_reward          | 11.1         |
| time/                   |              |
|    total_timesteps      | 309000       |
| train/                  |              |
|    approx_kl            | 0.0016676205 |
|    clip_fraction        | 0.0484       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.512       |
|    explained_variance   | 0.433        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.12         |
|    n_updates            | 7720         |
|    policy_gradient_loss | -0.00609     |
|    value_loss           | 2.29         |
------------------------------------------
Eval num_timesteps=309200, episode_reward=14.28 +/- 11.88
Episode length: 20.60 +/- 16.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 14.3     |
| time/              |          |
|    total_timesteps | 309200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 773      |
|    time_elapsed    | 85574    |
|    total_timesteps | 309200   |
---------------------------------
Eval num_timesteps=309400, episode_reward=23.09 +/- 13.96
Episode length: 33.80 +/- 19.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.8         |
|    mean_reward          | 23.1         |
| time/                   |              |
|    total_timesteps      | 309400       |
| train/                  |              |
|    approx_kl            | 0.0033808227 |
|    clip_fraction        | 0.109        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.562       |
|    explained_variance   | 0.328        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.364        |
|    n_updates            | 7730         |
|    policy_gradient_loss | -0.00845     |
|    value_loss           | 1.04         |
------------------------------------------
Eval num_timesteps=309600, episode_reward=11.44 +/- 11.02
Episode length: 17.40 +/- 16.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 11.4     |
| time/              |          |
|    total_timesteps | 309600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.7     |
|    ep_rew_mean     | 15.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 774      |
|    time_elapsed    | 85686    |
|    total_timesteps | 309600   |
---------------------------------
Eval num_timesteps=309800, episode_reward=10.95 +/- 11.92
Episode length: 16.40 +/- 17.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | 11           |
| time/                   |              |
|    total_timesteps      | 309800       |
| train/                  |              |
|    approx_kl            | 0.0020737872 |
|    clip_fraction        | 0.0855       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.532       |
|    explained_variance   | 0.253        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.765        |
|    n_updates            | 7740         |
|    policy_gradient_loss | -0.00589     |
|    value_loss           | 2.36         |
------------------------------------------
Eval num_timesteps=310000, episode_reward=26.77 +/- 9.14
Episode length: 39.20 +/- 13.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39.2     |
|    mean_reward     | 26.8     |
| time/              |          |
|    total_timesteps | 310000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.5     |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 775      |
|    time_elapsed    | 85814    |
|    total_timesteps | 310000   |
---------------------------------
Eval num_timesteps=310200, episode_reward=12.00 +/- 12.00
Episode length: 18.20 +/- 17.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.2         |
|    mean_reward          | 12           |
| time/                   |              |
|    total_timesteps      | 310200       |
| train/                  |              |
|    approx_kl            | 0.0026131335 |
|    clip_fraction        | 0.0484       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.537       |
|    explained_variance   | 0.439        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.17         |
|    n_updates            | 7750         |
|    policy_gradient_loss | -0.00463     |
|    value_loss           | 1.53         |
------------------------------------------
Eval num_timesteps=310400, episode_reward=18.37 +/- 13.63
Episode length: 27.00 +/- 18.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27       |
|    mean_reward     | 18.4     |
| time/              |          |
|    total_timesteps | 310400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.2     |
|    ep_rew_mean     | 15.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 776      |
|    time_elapsed    | 85916    |
|    total_timesteps | 310400   |
---------------------------------
Eval num_timesteps=310600, episode_reward=17.02 +/- 14.03
Episode length: 26.00 +/- 20.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26           |
|    mean_reward          | 17           |
| time/                   |              |
|    total_timesteps      | 310600       |
| train/                  |              |
|    approx_kl            | 0.0016822534 |
|    clip_fraction        | 0.044        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.518       |
|    explained_variance   | 0.346        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.85         |
|    n_updates            | 7760         |
|    policy_gradient_loss | -0.00383     |
|    value_loss           | 2.71         |
------------------------------------------
Eval num_timesteps=310800, episode_reward=15.75 +/- 10.88
Episode length: 23.60 +/- 15.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 15.8     |
| time/              |          |
|    total_timesteps | 310800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23       |
|    ep_rew_mean     | 15.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 777      |
|    time_elapsed    | 86044    |
|    total_timesteps | 310800   |
---------------------------------
Eval num_timesteps=311000, episode_reward=4.49 +/- 4.91
Episode length: 7.20 +/- 6.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 7.2          |
|    mean_reward          | 4.49         |
| time/                   |              |
|    total_timesteps      | 311000       |
| train/                  |              |
|    approx_kl            | 0.0022730227 |
|    clip_fraction        | 0.104        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.532       |
|    explained_variance   | 0.342        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.31         |
|    n_updates            | 7770         |
|    policy_gradient_loss | -0.00737     |
|    value_loss           | 2            |
------------------------------------------
Eval num_timesteps=311200, episode_reward=13.82 +/- 10.24
Episode length: 21.00 +/- 15.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | 13.8     |
| time/              |          |
|    total_timesteps | 311200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.8     |
|    ep_rew_mean     | 16.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 778      |
|    time_elapsed    | 86133    |
|    total_timesteps | 311200   |
---------------------------------
Eval num_timesteps=311400, episode_reward=13.19 +/- 11.05
Episode length: 19.60 +/- 15.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19.6        |
|    mean_reward          | 13.2        |
| time/                   |             |
|    total_timesteps      | 311400      |
| train/                  |             |
|    approx_kl            | 0.004730811 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.551      |
|    explained_variance   | 0.404       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.37        |
|    n_updates            | 7780        |
|    policy_gradient_loss | -0.00818    |
|    value_loss           | 1.33        |
-----------------------------------------
Eval num_timesteps=311600, episode_reward=22.04 +/- 15.71
Episode length: 32.00 +/- 22.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32       |
|    mean_reward     | 22       |
| time/              |          |
|    total_timesteps | 311600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.1     |
|    ep_rew_mean     | 14.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 779      |
|    time_elapsed    | 86236    |
|    total_timesteps | 311600   |
---------------------------------
Eval num_timesteps=311800, episode_reward=9.57 +/- 12.05
Episode length: 15.00 +/- 17.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15           |
|    mean_reward          | 9.57         |
| time/                   |              |
|    total_timesteps      | 311800       |
| train/                  |              |
|    approx_kl            | 0.0013828238 |
|    clip_fraction        | 0.0804       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.522       |
|    explained_variance   | 0.526        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.02         |
|    n_updates            | 7790         |
|    policy_gradient_loss | -0.0047      |
|    value_loss           | 2.11         |
------------------------------------------
Eval num_timesteps=312000, episode_reward=15.72 +/- 15.82
Episode length: 23.00 +/- 22.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 15.7     |
| time/              |          |
|    total_timesteps | 312000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.8     |
|    ep_rew_mean     | 14.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 780      |
|    time_elapsed    | 86341    |
|    total_timesteps | 312000   |
---------------------------------
Eval num_timesteps=312200, episode_reward=16.96 +/- 13.95
Episode length: 25.60 +/- 20.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.6         |
|    mean_reward          | 17           |
| time/                   |              |
|    total_timesteps      | 312200       |
| train/                  |              |
|    approx_kl            | 0.0021911093 |
|    clip_fraction        | 0.166        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.563       |
|    explained_variance   | 0.223        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.783        |
|    n_updates            | 7800         |
|    policy_gradient_loss | -0.00797     |
|    value_loss           | 2.19         |
------------------------------------------
Eval num_timesteps=312400, episode_reward=17.83 +/- 14.58
Episode length: 25.60 +/- 20.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 17.8     |
| time/              |          |
|    total_timesteps | 312400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.2     |
|    ep_rew_mean     | 15.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 781      |
|    time_elapsed    | 86472    |
|    total_timesteps | 312400   |
---------------------------------
Eval num_timesteps=312600, episode_reward=16.76 +/- 15.43
Episode length: 24.20 +/- 21.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.2         |
|    mean_reward          | 16.8         |
| time/                   |              |
|    total_timesteps      | 312600       |
| train/                  |              |
|    approx_kl            | 0.0011650345 |
|    clip_fraction        | 0.0618       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.549       |
|    explained_variance   | 0.218        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.806        |
|    n_updates            | 7810         |
|    policy_gradient_loss | -0.00509     |
|    value_loss           | 2.46         |
------------------------------------------
Eval num_timesteps=312800, episode_reward=21.59 +/- 16.73
Episode length: 31.00 +/- 23.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31       |
|    mean_reward     | 21.6     |
| time/              |          |
|    total_timesteps | 312800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.3     |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 782      |
|    time_elapsed    | 86580    |
|    total_timesteps | 312800   |
---------------------------------
Eval num_timesteps=313000, episode_reward=28.48 +/- 13.18
Episode length: 40.80 +/- 18.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 40.8         |
|    mean_reward          | 28.5         |
| time/                   |              |
|    total_timesteps      | 313000       |
| train/                  |              |
|    approx_kl            | 0.0021593194 |
|    clip_fraction        | 0.114        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.551       |
|    explained_variance   | -0.118       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.42         |
|    n_updates            | 7820         |
|    policy_gradient_loss | -0.00809     |
|    value_loss           | 2.73         |
------------------------------------------
Eval num_timesteps=313200, episode_reward=15.43 +/- 13.14
Episode length: 22.60 +/- 17.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 15.4     |
| time/              |          |
|    total_timesteps | 313200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.7     |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 783      |
|    time_elapsed    | 86685    |
|    total_timesteps | 313200   |
---------------------------------
Eval num_timesteps=313400, episode_reward=17.73 +/- 14.65
Episode length: 25.60 +/- 20.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 25.6        |
|    mean_reward          | 17.7        |
| time/                   |             |
|    total_timesteps      | 313400      |
| train/                  |             |
|    approx_kl            | 0.004150311 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.535      |
|    explained_variance   | 0.208       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.34        |
|    n_updates            | 7830        |
|    policy_gradient_loss | -0.0044     |
|    value_loss           | 1.71        |
-----------------------------------------
Eval num_timesteps=313600, episode_reward=8.93 +/- 12.29
Episode length: 14.00 +/- 18.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14       |
|    mean_reward     | 8.93     |
| time/              |          |
|    total_timesteps | 313600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.4     |
|    ep_rew_mean     | 15.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 784      |
|    time_elapsed    | 86789    |
|    total_timesteps | 313600   |
---------------------------------
Eval num_timesteps=313800, episode_reward=2.53 +/- 1.31
Episode length: 4.40 +/- 1.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 4.4          |
|    mean_reward          | 2.53         |
| time/                   |              |
|    total_timesteps      | 313800       |
| train/                  |              |
|    approx_kl            | 0.0013764324 |
|    clip_fraction        | 0.0578       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.537       |
|    explained_variance   | 0.288        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.771        |
|    n_updates            | 7840         |
|    policy_gradient_loss | -0.00482     |
|    value_loss           | 3.14         |
------------------------------------------
Eval num_timesteps=314000, episode_reward=15.31 +/- 15.78
Episode length: 22.80 +/- 22.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 15.3     |
| time/              |          |
|    total_timesteps | 314000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | 17.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 785      |
|    time_elapsed    | 86869    |
|    total_timesteps | 314000   |
---------------------------------
Eval num_timesteps=314200, episode_reward=10.22 +/- 11.75
Episode length: 15.80 +/- 17.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.8        |
|    mean_reward          | 10.2        |
| time/                   |             |
|    total_timesteps      | 314200      |
| train/                  |             |
|    approx_kl            | 0.003918925 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.567      |
|    explained_variance   | 0.209       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.186       |
|    n_updates            | 7850        |
|    policy_gradient_loss | -0.00842    |
|    value_loss           | 1.04        |
-----------------------------------------
Eval num_timesteps=314400, episode_reward=5.02 +/- 2.84
Episode length: 8.20 +/- 4.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8.2      |
|    mean_reward     | 5.02     |
| time/              |          |
|    total_timesteps | 314400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.4     |
|    ep_rew_mean     | 17.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 786      |
|    time_elapsed    | 86953    |
|    total_timesteps | 314400   |
---------------------------------
Eval num_timesteps=314600, episode_reward=12.45 +/- 13.71
Episode length: 19.00 +/- 19.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 19           |
|    mean_reward          | 12.4         |
| time/                   |              |
|    total_timesteps      | 314600       |
| train/                  |              |
|    approx_kl            | 0.0020276133 |
|    clip_fraction        | 0.054        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.565       |
|    explained_variance   | 0.181        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.82         |
|    n_updates            | 7860         |
|    policy_gradient_loss | -0.00398     |
|    value_loss           | 2.4          |
------------------------------------------
Eval num_timesteps=314800, episode_reward=19.59 +/- 14.61
Episode length: 28.60 +/- 20.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.6     |
|    mean_reward     | 19.6     |
| time/              |          |
|    total_timesteps | 314800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26       |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 787      |
|    time_elapsed    | 87055    |
|    total_timesteps | 314800   |
---------------------------------
Eval num_timesteps=315000, episode_reward=21.92 +/- 15.87
Episode length: 31.80 +/- 22.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 31.8         |
|    mean_reward          | 21.9         |
| time/                   |              |
|    total_timesteps      | 315000       |
| train/                  |              |
|    approx_kl            | 0.0028531584 |
|    clip_fraction        | 0.15         |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.557       |
|    explained_variance   | 0.34         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.735        |
|    n_updates            | 7870         |
|    policy_gradient_loss | -0.00763     |
|    value_loss           | 1.74         |
------------------------------------------
Eval num_timesteps=315200, episode_reward=8.14 +/- 4.47
Episode length: 12.40 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 12.4     |
|    mean_reward     | 8.14     |
| time/              |          |
|    total_timesteps | 315200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.5     |
|    ep_rew_mean     | 18.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 788      |
|    time_elapsed    | 87146    |
|    total_timesteps | 315200   |
---------------------------------
Eval num_timesteps=315400, episode_reward=24.41 +/- 14.63
Episode length: 34.20 +/- 19.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.2         |
|    mean_reward          | 24.4         |
| time/                   |              |
|    total_timesteps      | 315400       |
| train/                  |              |
|    approx_kl            | 0.0022442623 |
|    clip_fraction        | 0.0777       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.568       |
|    explained_variance   | 0.0771       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.35         |
|    n_updates            | 7880         |
|    policy_gradient_loss | -0.00658     |
|    value_loss           | 1.42         |
------------------------------------------
Eval num_timesteps=315600, episode_reward=21.00 +/- 12.01
Episode length: 30.00 +/- 16.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 21       |
| time/              |          |
|    total_timesteps | 315600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26       |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 789      |
|    time_elapsed    | 87299    |
|    total_timesteps | 315600   |
---------------------------------
Eval num_timesteps=315800, episode_reward=16.01 +/- 15.17
Episode length: 23.80 +/- 21.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.8         |
|    mean_reward          | 16           |
| time/                   |              |
|    total_timesteps      | 315800       |
| train/                  |              |
|    approx_kl            | 0.0022671532 |
|    clip_fraction        | 0.0589       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.538       |
|    explained_variance   | 0.178        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.322        |
|    n_updates            | 7890         |
|    policy_gradient_loss | -0.00484     |
|    value_loss           | 2.65         |
------------------------------------------
Eval num_timesteps=316000, episode_reward=3.93 +/- 1.37
Episode length: 6.60 +/- 1.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6.6      |
|    mean_reward     | 3.93     |
| time/              |          |
|    total_timesteps | 316000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.1     |
|    ep_rew_mean     | 18       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 790      |
|    time_elapsed    | 87386    |
|    total_timesteps | 316000   |
---------------------------------
Eval num_timesteps=316200, episode_reward=17.34 +/- 15.04
Episode length: 25.00 +/- 20.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25           |
|    mean_reward          | 17.3         |
| time/                   |              |
|    total_timesteps      | 316200       |
| train/                  |              |
|    approx_kl            | 0.0034061614 |
|    clip_fraction        | 0.13         |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.554       |
|    explained_variance   | 0.351        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.537        |
|    n_updates            | 7900         |
|    policy_gradient_loss | -0.0092      |
|    value_loss           | 1.56         |
------------------------------------------
Eval num_timesteps=316400, episode_reward=28.24 +/- 13.02
Episode length: 40.80 +/- 18.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.8     |
|    mean_reward     | 28.2     |
| time/              |          |
|    total_timesteps | 316400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.3     |
|    ep_rew_mean     | 19.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 791      |
|    time_elapsed    | 87494    |
|    total_timesteps | 316400   |
---------------------------------
Eval num_timesteps=316600, episode_reward=10.36 +/- 12.35
Episode length: 15.80 +/- 17.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | 10.4         |
| time/                   |              |
|    total_timesteps      | 316600       |
| train/                  |              |
|    approx_kl            | 0.0028637755 |
|    clip_fraction        | 0.0667       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.554       |
|    explained_variance   | 0.195        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.555        |
|    n_updates            | 7910         |
|    policy_gradient_loss | -0.0049      |
|    value_loss           | 1.16         |
------------------------------------------
Eval num_timesteps=316800, episode_reward=22.65 +/- 15.92
Episode length: 32.20 +/- 21.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 22.6     |
| time/              |          |
|    total_timesteps | 316800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.3     |
|    ep_rew_mean     | 18       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 792      |
|    time_elapsed    | 87606    |
|    total_timesteps | 316800   |
---------------------------------
Eval num_timesteps=317000, episode_reward=23.14 +/- 13.90
Episode length: 34.00 +/- 19.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34           |
|    mean_reward          | 23.1         |
| time/                   |              |
|    total_timesteps      | 317000       |
| train/                  |              |
|    approx_kl            | 0.0016218408 |
|    clip_fraction        | 0.0699       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.538       |
|    explained_variance   | 0.395        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.731        |
|    n_updates            | 7920         |
|    policy_gradient_loss | -0.0093      |
|    value_loss           | 2.15         |
------------------------------------------
Eval num_timesteps=317200, episode_reward=29.61 +/- 13.09
Episode length: 41.20 +/- 17.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | 29.6     |
| time/              |          |
|    total_timesteps | 317200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.2     |
|    ep_rew_mean     | 18       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 793      |
|    time_elapsed    | 87738    |
|    total_timesteps | 317200   |
---------------------------------
Eval num_timesteps=317400, episode_reward=10.74 +/- 11.97
Episode length: 16.20 +/- 17.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.2         |
|    mean_reward          | 10.7         |
| time/                   |              |
|    total_timesteps      | 317400       |
| train/                  |              |
|    approx_kl            | 0.0040882644 |
|    clip_fraction        | 0.107        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.546       |
|    explained_variance   | 0.301        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.263        |
|    n_updates            | 7930         |
|    policy_gradient_loss | -0.00688     |
|    value_loss           | 1.89         |
------------------------------------------
Eval num_timesteps=317600, episode_reward=15.49 +/- 16.44
Episode length: 22.40 +/- 22.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 15.5     |
| time/              |          |
|    total_timesteps | 317600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.2     |
|    ep_rew_mean     | 18.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 794      |
|    time_elapsed    | 87847    |
|    total_timesteps | 317600   |
---------------------------------
Eval num_timesteps=317800, episode_reward=23.96 +/- 13.44
Episode length: 34.60 +/- 18.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.6        |
|    mean_reward          | 24          |
| time/                   |             |
|    total_timesteps      | 317800      |
| train/                  |             |
|    approx_kl            | 0.004078052 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.571      |
|    explained_variance   | -0.0316     |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0505      |
|    n_updates            | 7940        |
|    policy_gradient_loss | -0.00357    |
|    value_loss           | 1.05        |
-----------------------------------------
Eval num_timesteps=318000, episode_reward=8.75 +/- 3.06
Episode length: 13.20 +/- 4.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.2     |
|    mean_reward     | 8.75     |
| time/              |          |
|    total_timesteps | 318000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.7     |
|    ep_rew_mean     | 18.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 795      |
|    time_elapsed    | 87947    |
|    total_timesteps | 318000   |
---------------------------------
Eval num_timesteps=318200, episode_reward=15.93 +/- 15.82
Episode length: 23.40 +/- 22.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.4         |
|    mean_reward          | 15.9         |
| time/                   |              |
|    total_timesteps      | 318200       |
| train/                  |              |
|    approx_kl            | 0.0028427541 |
|    clip_fraction        | 0.085        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.556       |
|    explained_variance   | 0.313        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.397        |
|    n_updates            | 7950         |
|    policy_gradient_loss | -0.00553     |
|    value_loss           | 1.63         |
------------------------------------------
Eval num_timesteps=318400, episode_reward=11.48 +/- 12.17
Episode length: 16.80 +/- 16.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | 11.5     |
| time/              |          |
|    total_timesteps | 318400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.1     |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 796      |
|    time_elapsed    | 88055    |
|    total_timesteps | 318400   |
---------------------------------
Eval num_timesteps=318600, episode_reward=23.77 +/- 14.51
Episode length: 33.80 +/- 19.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.8         |
|    mean_reward          | 23.8         |
| time/                   |              |
|    total_timesteps      | 318600       |
| train/                  |              |
|    approx_kl            | 0.0028795025 |
|    clip_fraction        | 0.0871       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.555       |
|    explained_variance   | 0.42         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.456        |
|    n_updates            | 7960         |
|    policy_gradient_loss | -0.00663     |
|    value_loss           | 1.87         |
------------------------------------------
Eval num_timesteps=318800, episode_reward=16.05 +/- 15.98
Episode length: 23.20 +/- 21.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 318800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | 16       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 797      |
|    time_elapsed    | 88161    |
|    total_timesteps | 318800   |
---------------------------------
Eval num_timesteps=319000, episode_reward=9.25 +/- 12.68
Episode length: 14.20 +/- 18.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.2         |
|    mean_reward          | 9.25         |
| time/                   |              |
|    total_timesteps      | 319000       |
| train/                  |              |
|    approx_kl            | 0.0026269832 |
|    clip_fraction        | 0.0688       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.552       |
|    explained_variance   | 0.429        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.44         |
|    n_updates            | 7970         |
|    policy_gradient_loss | -0.00715     |
|    value_loss           | 2.38         |
------------------------------------------
Eval num_timesteps=319200, episode_reward=20.45 +/- 14.00
Episode length: 29.00 +/- 18.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29       |
|    mean_reward     | 20.4     |
| time/              |          |
|    total_timesteps | 319200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | 16.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 798      |
|    time_elapsed    | 88266    |
|    total_timesteps | 319200   |
---------------------------------
Eval num_timesteps=319400, episode_reward=24.29 +/- 13.73
Episode length: 35.00 +/- 18.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35           |
|    mean_reward          | 24.3         |
| time/                   |              |
|    total_timesteps      | 319400       |
| train/                  |              |
|    approx_kl            | 0.0033213315 |
|    clip_fraction        | 0.0489       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.555       |
|    explained_variance   | 0.418        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.844        |
|    n_updates            | 7980         |
|    policy_gradient_loss | -0.00502     |
|    value_loss           | 1.77         |
------------------------------------------
Eval num_timesteps=319600, episode_reward=22.17 +/- 16.50
Episode length: 31.60 +/- 22.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.6     |
|    mean_reward     | 22.2     |
| time/              |          |
|    total_timesteps | 319600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25       |
|    ep_rew_mean     | 17.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 799      |
|    time_elapsed    | 88372    |
|    total_timesteps | 319600   |
---------------------------------
Eval num_timesteps=319800, episode_reward=19.73 +/- 14.09
Episode length: 28.00 +/- 18.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 28           |
|    mean_reward          | 19.7         |
| time/                   |              |
|    total_timesteps      | 319800       |
| train/                  |              |
|    approx_kl            | 0.0023163233 |
|    clip_fraction        | 0.0917       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.583       |
|    explained_variance   | 0.239        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.304        |
|    n_updates            | 7990         |
|    policy_gradient_loss | -0.00637     |
|    value_loss           | 1.2          |
------------------------------------------
Eval num_timesteps=320000, episode_reward=11.12 +/- 11.41
Episode length: 17.20 +/- 16.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | 11.1     |
| time/              |          |
|    total_timesteps | 320000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.3     |
|    ep_rew_mean     | 18.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 800      |
|    time_elapsed    | 88500    |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=320200, episode_reward=21.67 +/- 16.17
Episode length: 31.40 +/- 22.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 31.4        |
|    mean_reward          | 21.7        |
| time/                   |             |
|    total_timesteps      | 320200      |
| train/                  |             |
|    approx_kl            | 0.005705961 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.557      |
|    explained_variance   | 0.273       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.45        |
|    n_updates            | 8000        |
|    policy_gradient_loss | -0.0083     |
|    value_loss           | 1.37        |
-----------------------------------------
Eval num_timesteps=320400, episode_reward=12.29 +/- 11.74
Episode length: 18.60 +/- 16.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 12.3     |
| time/              |          |
|    total_timesteps | 320400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.2     |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 801      |
|    time_elapsed    | 88605    |
|    total_timesteps | 320400   |
---------------------------------
Eval num_timesteps=320600, episode_reward=27.98 +/- 13.03
Episode length: 40.80 +/- 18.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 40.8        |
|    mean_reward          | 28          |
| time/                   |             |
|    total_timesteps      | 320600      |
| train/                  |             |
|    approx_kl            | 0.003535845 |
|    clip_fraction        | 0.0911      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.539      |
|    explained_variance   | 0.278       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.84        |
|    n_updates            | 8010        |
|    policy_gradient_loss | -0.00821    |
|    value_loss           | 2.66        |
-----------------------------------------
Eval num_timesteps=320800, episode_reward=22.32 +/- 15.47
Episode length: 32.40 +/- 21.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.4     |
|    mean_reward     | 22.3     |
| time/              |          |
|    total_timesteps | 320800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.2     |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 802      |
|    time_elapsed    | 88734    |
|    total_timesteps | 320800   |
---------------------------------
Eval num_timesteps=321000, episode_reward=14.41 +/- 10.98
Episode length: 21.40 +/- 14.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 21.4         |
|    mean_reward          | 14.4         |
| time/                   |              |
|    total_timesteps      | 321000       |
| train/                  |              |
|    approx_kl            | 0.0016467631 |
|    clip_fraction        | 0.0498       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.559       |
|    explained_variance   | 0.341        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.925        |
|    n_updates            | 8020         |
|    policy_gradient_loss | -0.00596     |
|    value_loss           | 2.13         |
------------------------------------------
Eval num_timesteps=321200, episode_reward=15.36 +/- 15.69
Episode length: 22.80 +/- 22.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 15.4     |
| time/              |          |
|    total_timesteps | 321200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.6     |
|    ep_rew_mean     | 16.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 803      |
|    time_elapsed    | 88837    |
|    total_timesteps | 321200   |
---------------------------------
Eval num_timesteps=321400, episode_reward=18.39 +/- 14.18
Episode length: 26.60 +/- 19.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.6         |
|    mean_reward          | 18.4         |
| time/                   |              |
|    total_timesteps      | 321400       |
| train/                  |              |
|    approx_kl            | 0.0023109342 |
|    clip_fraction        | 0.0627       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.544       |
|    explained_variance   | 0.361        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.308        |
|    n_updates            | 8030         |
|    policy_gradient_loss | -0.00638     |
|    value_loss           | 2.01         |
------------------------------------------
Eval num_timesteps=321600, episode_reward=10.14 +/- 13.32
Episode length: 14.80 +/- 17.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 10.1     |
| time/              |          |
|    total_timesteps | 321600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.9     |
|    ep_rew_mean     | 17.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 804      |
|    time_elapsed    | 88939    |
|    total_timesteps | 321600   |
---------------------------------
Eval num_timesteps=321800, episode_reward=21.65 +/- 16.19
Episode length: 31.40 +/- 22.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 31.4         |
|    mean_reward          | 21.7         |
| time/                   |              |
|    total_timesteps      | 321800       |
| train/                  |              |
|    approx_kl            | 0.0050173146 |
|    clip_fraction        | 0.154        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.58        |
|    explained_variance   | 0.311        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.115        |
|    n_updates            | 8040         |
|    policy_gradient_loss | -0.00819     |
|    value_loss           | 0.893        |
------------------------------------------
Eval num_timesteps=322000, episode_reward=23.28 +/- 14.67
Episode length: 33.40 +/- 20.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 23.3     |
| time/              |          |
|    total_timesteps | 322000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.2     |
|    ep_rew_mean     | 17.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 805      |
|    time_elapsed    | 89071    |
|    total_timesteps | 322000   |
---------------------------------
Eval num_timesteps=322200, episode_reward=11.43 +/- 11.11
Episode length: 17.60 +/- 16.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.6         |
|    mean_reward          | 11.4         |
| time/                   |              |
|    total_timesteps      | 322200       |
| train/                  |              |
|    approx_kl            | 0.0020001554 |
|    clip_fraction        | 0.117        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.547       |
|    explained_variance   | 0.0521       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.521        |
|    n_updates            | 8050         |
|    policy_gradient_loss | -0.00494     |
|    value_loss           | 2.46         |
------------------------------------------
Eval num_timesteps=322400, episode_reward=15.37 +/- 9.98
Episode length: 22.60 +/- 14.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 15.4     |
| time/              |          |
|    total_timesteps | 322400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.1     |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 806      |
|    time_elapsed    | 89185    |
|    total_timesteps | 322400   |
---------------------------------
Eval num_timesteps=322600, episode_reward=10.84 +/- 11.95
Episode length: 16.60 +/- 16.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.6         |
|    mean_reward          | 10.8         |
| time/                   |              |
|    total_timesteps      | 322600       |
| train/                  |              |
|    approx_kl            | 0.0021079155 |
|    clip_fraction        | 0.0656       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.541       |
|    explained_variance   | 0.375        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.478        |
|    n_updates            | 8060         |
|    policy_gradient_loss | -0.00694     |
|    value_loss           | 2.35         |
------------------------------------------
Eval num_timesteps=322800, episode_reward=10.10 +/- 12.21
Episode length: 15.40 +/- 17.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | 10.1     |
| time/              |          |
|    total_timesteps | 322800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 14.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 807      |
|    time_elapsed    | 89294    |
|    total_timesteps | 322800   |
---------------------------------
Eval num_timesteps=323000, episode_reward=19.97 +/- 14.29
Episode length: 28.00 +/- 18.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 28           |
|    mean_reward          | 20           |
| time/                   |              |
|    total_timesteps      | 323000       |
| train/                  |              |
|    approx_kl            | 0.0023476786 |
|    clip_fraction        | 0.0529       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.536       |
|    explained_variance   | 0.35         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.433        |
|    n_updates            | 8070         |
|    policy_gradient_loss | -0.00574     |
|    value_loss           | 2.22         |
------------------------------------------
Eval num_timesteps=323200, episode_reward=17.34 +/- 15.14
Episode length: 25.00 +/- 20.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | 17.3     |
| time/              |          |
|    total_timesteps | 323200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.8     |
|    ep_rew_mean     | 14.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 808      |
|    time_elapsed    | 89404    |
|    total_timesteps | 323200   |
---------------------------------
Eval num_timesteps=323400, episode_reward=17.86 +/- 13.32
Episode length: 27.00 +/- 19.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 27           |
|    mean_reward          | 17.9         |
| time/                   |              |
|    total_timesteps      | 323400       |
| train/                  |              |
|    approx_kl            | 0.0018070387 |
|    clip_fraction        | 0.0511       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.556       |
|    explained_variance   | 0.452        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.451        |
|    n_updates            | 8080         |
|    policy_gradient_loss | -0.00456     |
|    value_loss           | 1.87         |
------------------------------------------
Eval num_timesteps=323600, episode_reward=4.16 +/- 1.63
Episode length: 6.80 +/- 2.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6.8      |
|    mean_reward     | 4.16     |
| time/              |          |
|    total_timesteps | 323600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.1     |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 809      |
|    time_elapsed    | 89494    |
|    total_timesteps | 323600   |
---------------------------------
Eval num_timesteps=323800, episode_reward=23.66 +/- 13.37
Episode length: 34.60 +/- 19.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.6          |
|    mean_reward          | 23.7          |
| time/                   |               |
|    total_timesteps      | 323800        |
| train/                  |               |
|    approx_kl            | 0.00064868544 |
|    clip_fraction        | 0.0179        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.554        |
|    explained_variance   | 0.468         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.37          |
|    n_updates            | 8090          |
|    policy_gradient_loss | -0.00314      |
|    value_loss           | 1.81          |
-------------------------------------------
Eval num_timesteps=324000, episode_reward=9.71 +/- 13.52
Episode length: 14.20 +/- 17.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.2     |
|    mean_reward     | 9.71     |
| time/              |          |
|    total_timesteps | 324000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.8     |
|    ep_rew_mean     | 13.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 810      |
|    time_elapsed    | 89605    |
|    total_timesteps | 324000   |
---------------------------------
Eval num_timesteps=324200, episode_reward=14.22 +/- 10.63
Episode length: 21.40 +/- 15.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 21.4        |
|    mean_reward          | 14.2        |
| time/                   |             |
|    total_timesteps      | 324200      |
| train/                  |             |
|    approx_kl            | 0.004038815 |
|    clip_fraction        | 0.0848      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.537      |
|    explained_variance   | 0.454       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.16        |
|    n_updates            | 8100        |
|    policy_gradient_loss | -0.00644    |
|    value_loss           | 2.34        |
-----------------------------------------
Eval num_timesteps=324400, episode_reward=17.25 +/- 15.11
Episode length: 25.00 +/- 20.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | 17.2     |
| time/              |          |
|    total_timesteps | 324400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.2     |
|    ep_rew_mean     | 13.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 811      |
|    time_elapsed    | 89714    |
|    total_timesteps | 324400   |
---------------------------------
Eval num_timesteps=324600, episode_reward=11.76 +/- 12.89
Episode length: 17.00 +/- 17.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17           |
|    mean_reward          | 11.8         |
| time/                   |              |
|    total_timesteps      | 324600       |
| train/                  |              |
|    approx_kl            | 0.0011311573 |
|    clip_fraction        | 0.0824       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.557       |
|    explained_variance   | 0.352        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.281        |
|    n_updates            | 8110         |
|    policy_gradient_loss | -0.00476     |
|    value_loss           | 1.21         |
------------------------------------------
Eval num_timesteps=324800, episode_reward=17.68 +/- 13.90
Episode length: 26.00 +/- 19.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | 17.7     |
| time/              |          |
|    total_timesteps | 324800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.3     |
|    ep_rew_mean     | 14.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 812      |
|    time_elapsed    | 89824    |
|    total_timesteps | 324800   |
---------------------------------
Eval num_timesteps=325000, episode_reward=9.66 +/- 13.65
Episode length: 14.00 +/- 18.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14           |
|    mean_reward          | 9.66         |
| time/                   |              |
|    total_timesteps      | 325000       |
| train/                  |              |
|    approx_kl            | 0.0035900674 |
|    clip_fraction        | 0.121        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.528       |
|    explained_variance   | 0.494        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.628        |
|    n_updates            | 8120         |
|    policy_gradient_loss | -0.00682     |
|    value_loss           | 1.74         |
------------------------------------------
Eval num_timesteps=325200, episode_reward=19.20 +/- 14.20
Episode length: 27.60 +/- 18.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.6     |
|    mean_reward     | 19.2     |
| time/              |          |
|    total_timesteps | 325200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | 16.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 813      |
|    time_elapsed    | 89926    |
|    total_timesteps | 325200   |
---------------------------------
Eval num_timesteps=325400, episode_reward=29.01 +/- 11.52
Episode length: 42.00 +/- 16.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42          |
|    mean_reward          | 29          |
| time/                   |             |
|    total_timesteps      | 325400      |
| train/                  |             |
|    approx_kl            | 0.002830546 |
|    clip_fraction        | 0.0862      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.526      |
|    explained_variance   | 0.352       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.22        |
|    n_updates            | 8130        |
|    policy_gradient_loss | -0.0042     |
|    value_loss           | 0.941       |
-----------------------------------------
Eval num_timesteps=325600, episode_reward=13.09 +/- 11.25
Episode length: 19.20 +/- 15.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.2     |
|    mean_reward     | 13.1     |
| time/              |          |
|    total_timesteps | 325600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.9     |
|    ep_rew_mean     | 15.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 814      |
|    time_elapsed    | 90060    |
|    total_timesteps | 325600   |
---------------------------------
Eval num_timesteps=325800, episode_reward=6.93 +/- 5.13
Episode length: 10.80 +/- 7.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 10.8         |
|    mean_reward          | 6.93         |
| time/                   |              |
|    total_timesteps      | 325800       |
| train/                  |              |
|    approx_kl            | 0.0021905657 |
|    clip_fraction        | 0.0625       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.531       |
|    explained_variance   | 0.564        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.52         |
|    n_updates            | 8140         |
|    policy_gradient_loss | -0.0057      |
|    value_loss           | 1.82         |
------------------------------------------
Eval num_timesteps=326000, episode_reward=15.78 +/- 16.18
Episode length: 22.80 +/- 22.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 15.8     |
| time/              |          |
|    total_timesteps | 326000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24       |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 815      |
|    time_elapsed    | 90151    |
|    total_timesteps | 326000   |
---------------------------------
Eval num_timesteps=326200, episode_reward=22.66 +/- 15.41
Episode length: 32.60 +/- 21.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.6         |
|    mean_reward          | 22.7         |
| time/                   |              |
|    total_timesteps      | 326200       |
| train/                  |              |
|    approx_kl            | 0.0031215304 |
|    clip_fraction        | 0.0768       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.552       |
|    explained_variance   | 0.101        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.675        |
|    n_updates            | 8150         |
|    policy_gradient_loss | -0.00694     |
|    value_loss           | 1.34         |
------------------------------------------
Eval num_timesteps=326400, episode_reward=23.38 +/- 15.88
Episode length: 32.60 +/- 21.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.6     |
|    mean_reward     | 23.4     |
| time/              |          |
|    total_timesteps | 326400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.3     |
|    ep_rew_mean     | 17.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 816      |
|    time_elapsed    | 90260    |
|    total_timesteps | 326400   |
---------------------------------
Eval num_timesteps=326600, episode_reward=22.83 +/- 15.20
Episode length: 32.80 +/- 21.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 32.8        |
|    mean_reward          | 22.8        |
| time/                   |             |
|    total_timesteps      | 326600      |
| train/                  |             |
|    approx_kl            | 0.002004828 |
|    clip_fraction        | 0.06        |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.525      |
|    explained_variance   | 0.382       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.664       |
|    n_updates            | 8160        |
|    policy_gradient_loss | -0.00525    |
|    value_loss           | 2.21        |
-----------------------------------------
Eval num_timesteps=326800, episode_reward=12.67 +/- 12.03
Episode length: 18.80 +/- 16.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.8     |
|    mean_reward     | 12.7     |
| time/              |          |
|    total_timesteps | 326800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.4     |
|    ep_rew_mean     | 16.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 817      |
|    time_elapsed    | 90366    |
|    total_timesteps | 326800   |
---------------------------------
Eval num_timesteps=327000, episode_reward=23.86 +/- 14.79
Episode length: 33.80 +/- 20.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.8        |
|    mean_reward          | 23.9        |
| time/                   |             |
|    total_timesteps      | 327000      |
| train/                  |             |
|    approx_kl            | 0.001407662 |
|    clip_fraction        | 0.0355      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.537      |
|    explained_variance   | 0.239       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.19        |
|    n_updates            | 8170        |
|    policy_gradient_loss | -0.00317    |
|    value_loss           | 1.97        |
-----------------------------------------
Eval num_timesteps=327200, episode_reward=9.71 +/- 12.47
Episode length: 14.60 +/- 17.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 9.71     |
| time/              |          |
|    total_timesteps | 327200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.5     |
|    ep_rew_mean     | 16.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 818      |
|    time_elapsed    | 90494    |
|    total_timesteps | 327200   |
---------------------------------
Eval num_timesteps=327400, episode_reward=14.64 +/- 13.24
Episode length: 22.00 +/- 19.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22           |
|    mean_reward          | 14.6         |
| time/                   |              |
|    total_timesteps      | 327400       |
| train/                  |              |
|    approx_kl            | 0.0034634343 |
|    clip_fraction        | 0.109        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.535       |
|    explained_variance   | 0.203        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.02         |
|    n_updates            | 8180         |
|    policy_gradient_loss | -0.00746     |
|    value_loss           | 3.04         |
------------------------------------------
Eval num_timesteps=327600, episode_reward=29.76 +/- 11.67
Episode length: 42.00 +/- 16.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 29.8     |
| time/              |          |
|    total_timesteps | 327600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.8     |
|    ep_rew_mean     | 16.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 819      |
|    time_elapsed    | 90601    |
|    total_timesteps | 327600   |
---------------------------------
Eval num_timesteps=327800, episode_reward=18.98 +/- 12.52
Episode length: 28.80 +/- 18.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 28.8         |
|    mean_reward          | 19           |
| time/                   |              |
|    total_timesteps      | 327800       |
| train/                  |              |
|    approx_kl            | 0.0024306115 |
|    clip_fraction        | 0.0638       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.552       |
|    explained_variance   | -0.0431      |
|    learning_rate        | 0.0001       |
|    loss                 | 1.26         |
|    n_updates            | 8190         |
|    policy_gradient_loss | -0.00586     |
|    value_loss           | 2.04         |
------------------------------------------
Eval num_timesteps=328000, episode_reward=4.32 +/- 2.96
Episode length: 7.20 +/- 4.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7.2      |
|    mean_reward     | 4.32     |
| time/              |          |
|    total_timesteps | 328000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 820      |
|    time_elapsed    | 90686    |
|    total_timesteps | 328000   |
---------------------------------
Eval num_timesteps=328200, episode_reward=12.80 +/- 12.32
Episode length: 18.60 +/- 16.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.6        |
|    mean_reward          | 12.8        |
| time/                   |             |
|    total_timesteps      | 328200      |
| train/                  |             |
|    approx_kl            | 0.002449868 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.564      |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.57        |
|    n_updates            | 8200        |
|    policy_gradient_loss | -0.00771    |
|    value_loss           | 2.08        |
-----------------------------------------
Eval num_timesteps=328400, episode_reward=24.14 +/- 14.56
Episode length: 34.40 +/- 19.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 24.1     |
| time/              |          |
|    total_timesteps | 328400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.1     |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 821      |
|    time_elapsed    | 90823    |
|    total_timesteps | 328400   |
---------------------------------
Eval num_timesteps=328600, episode_reward=17.87 +/- 14.28
Episode length: 26.20 +/- 19.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.2         |
|    mean_reward          | 17.9         |
| time/                   |              |
|    total_timesteps      | 328600       |
| train/                  |              |
|    approx_kl            | 0.0028976642 |
|    clip_fraction        | 0.114        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.551       |
|    explained_variance   | 0.34         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.385        |
|    n_updates            | 8210         |
|    policy_gradient_loss | -0.00762     |
|    value_loss           | 2.17         |
------------------------------------------
Eval num_timesteps=328800, episode_reward=19.38 +/- 13.13
Episode length: 28.60 +/- 19.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.6     |
|    mean_reward     | 19.4     |
| time/              |          |
|    total_timesteps | 328800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.1     |
|    ep_rew_mean     | 15.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 822      |
|    time_elapsed    | 90965    |
|    total_timesteps | 328800   |
---------------------------------
Eval num_timesteps=329000, episode_reward=15.29 +/- 16.17
Episode length: 22.40 +/- 22.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22.4         |
|    mean_reward          | 15.3         |
| time/                   |              |
|    total_timesteps      | 329000       |
| train/                  |              |
|    approx_kl            | 0.0021095232 |
|    clip_fraction        | 0.0609       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.547       |
|    explained_variance   | 0.239        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.82         |
|    n_updates            | 8220         |
|    policy_gradient_loss | -0.00421     |
|    value_loss           | 3.36         |
------------------------------------------
Eval num_timesteps=329200, episode_reward=17.42 +/- 15.51
Episode length: 24.80 +/- 20.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 329200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.8     |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 823      |
|    time_elapsed    | 91070    |
|    total_timesteps | 329200   |
---------------------------------
Eval num_timesteps=329400, episode_reward=18.57 +/- 14.57
Episode length: 26.40 +/- 19.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.4         |
|    mean_reward          | 18.6         |
| time/                   |              |
|    total_timesteps      | 329400       |
| train/                  |              |
|    approx_kl            | 0.0031856236 |
|    clip_fraction        | 0.128        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.564       |
|    explained_variance   | 0.412        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.347        |
|    n_updates            | 8230         |
|    policy_gradient_loss | -0.00887     |
|    value_loss           | 1.93         |
------------------------------------------
Eval num_timesteps=329600, episode_reward=15.10 +/- 13.67
Episode length: 22.40 +/- 19.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 15.1     |
| time/              |          |
|    total_timesteps | 329600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.8     |
|    ep_rew_mean     | 14.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 824      |
|    time_elapsed    | 91177    |
|    total_timesteps | 329600   |
---------------------------------
Eval num_timesteps=329800, episode_reward=10.87 +/- 12.49
Episode length: 16.40 +/- 17.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | 10.9         |
| time/                   |              |
|    total_timesteps      | 329800       |
| train/                  |              |
|    approx_kl            | 0.0019164106 |
|    clip_fraction        | 0.0297       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.577       |
|    explained_variance   | 0.26         |
|    learning_rate        | 0.0001       |
|    loss                 | 1.8          |
|    n_updates            | 8240         |
|    policy_gradient_loss | -0.00318     |
|    value_loss           | 2.49         |
------------------------------------------
Eval num_timesteps=330000, episode_reward=23.89 +/- 12.75
Episode length: 35.40 +/- 18.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 23.9     |
| time/              |          |
|    total_timesteps | 330000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.5     |
|    ep_rew_mean     | 14       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 825      |
|    time_elapsed    | 91283    |
|    total_timesteps | 330000   |
---------------------------------
Eval num_timesteps=330200, episode_reward=16.71 +/- 15.23
Episode length: 24.60 +/- 21.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.6         |
|    mean_reward          | 16.7         |
| time/                   |              |
|    total_timesteps      | 330200       |
| train/                  |              |
|    approx_kl            | 0.0032749376 |
|    clip_fraction        | 0.0964       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.578       |
|    explained_variance   | 0.254        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.68         |
|    n_updates            | 8250         |
|    policy_gradient_loss | -0.00752     |
|    value_loss           | 1.74         |
------------------------------------------
Eval num_timesteps=330400, episode_reward=9.34 +/- 13.23
Episode length: 13.80 +/- 18.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.8     |
|    mean_reward     | 9.34     |
| time/              |          |
|    total_timesteps | 330400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.6     |
|    ep_rew_mean     | 14.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 826      |
|    time_elapsed    | 91385    |
|    total_timesteps | 330400   |
---------------------------------
Eval num_timesteps=330600, episode_reward=12.96 +/- 12.37
Episode length: 19.00 +/- 16.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19          |
|    mean_reward          | 13          |
| time/                   |             |
|    total_timesteps      | 330600      |
| train/                  |             |
|    approx_kl            | 0.002091289 |
|    clip_fraction        | 0.0775      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.587      |
|    explained_variance   | 0.42        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.522       |
|    n_updates            | 8260        |
|    policy_gradient_loss | -0.00601    |
|    value_loss           | 1.83        |
-----------------------------------------
Eval num_timesteps=330800, episode_reward=22.91 +/- 15.11
Episode length: 33.00 +/- 20.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 22.9     |
| time/              |          |
|    total_timesteps | 330800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.4     |
|    ep_rew_mean     | 14.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 827      |
|    time_elapsed    | 91513    |
|    total_timesteps | 330800   |
---------------------------------
Eval num_timesteps=331000, episode_reward=11.82 +/- 12.73
Episode length: 17.00 +/- 16.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17          |
|    mean_reward          | 11.8        |
| time/                   |             |
|    total_timesteps      | 331000      |
| train/                  |             |
|    approx_kl            | 0.001511416 |
|    clip_fraction        | 0.0531      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.569      |
|    explained_variance   | 0.348       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.504       |
|    n_updates            | 8270        |
|    policy_gradient_loss | -0.00601    |
|    value_loss           | 2.84        |
-----------------------------------------
Eval num_timesteps=331200, episode_reward=28.96 +/- 12.18
Episode length: 41.60 +/- 16.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 29       |
| time/              |          |
|    total_timesteps | 331200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.5     |
|    ep_rew_mean     | 14.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 828      |
|    time_elapsed    | 91645    |
|    total_timesteps | 331200   |
---------------------------------
Eval num_timesteps=331400, episode_reward=23.45 +/- 13.97
Episode length: 34.00 +/- 19.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34           |
|    mean_reward          | 23.4         |
| time/                   |              |
|    total_timesteps      | 331400       |
| train/                  |              |
|    approx_kl            | 0.0022781591 |
|    clip_fraction        | 0.0748       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.559       |
|    explained_variance   | 0.46         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.403        |
|    n_updates            | 8280         |
|    policy_gradient_loss | -0.00503     |
|    value_loss           | 2.15         |
------------------------------------------
Eval num_timesteps=331600, episode_reward=10.76 +/- 12.01
Episode length: 16.00 +/- 17.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | 10.8     |
| time/              |          |
|    total_timesteps | 331600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | 16.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 829      |
|    time_elapsed    | 91747    |
|    total_timesteps | 331600   |
---------------------------------
Eval num_timesteps=331800, episode_reward=22.94 +/- 11.30
Episode length: 33.40 +/- 15.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.4         |
|    mean_reward          | 22.9         |
| time/                   |              |
|    total_timesteps      | 331800       |
| train/                  |              |
|    approx_kl            | 0.0020743306 |
|    clip_fraction        | 0.083        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.563       |
|    explained_variance   | 0.18         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.231        |
|    n_updates            | 8290         |
|    policy_gradient_loss | -0.00575     |
|    value_loss           | 1.14         |
------------------------------------------
Eval num_timesteps=332000, episode_reward=22.48 +/- 16.12
Episode length: 32.00 +/- 22.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32       |
|    mean_reward     | 22.5     |
| time/              |          |
|    total_timesteps | 332000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.8     |
|    ep_rew_mean     | 16.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 830      |
|    time_elapsed    | 91865    |
|    total_timesteps | 332000   |
---------------------------------
Eval num_timesteps=332200, episode_reward=22.47 +/- 16.54
Episode length: 31.60 +/- 22.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 31.6         |
|    mean_reward          | 22.5         |
| time/                   |              |
|    total_timesteps      | 332200       |
| train/                  |              |
|    approx_kl            | 0.0020803723 |
|    clip_fraction        | 0.0743       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.561       |
|    explained_variance   | 0.343        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.43         |
|    n_updates            | 8300         |
|    policy_gradient_loss | -0.00464     |
|    value_loss           | 2.41         |
------------------------------------------
Eval num_timesteps=332400, episode_reward=17.24 +/- 15.93
Episode length: 24.20 +/- 21.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 17.2     |
| time/              |          |
|    total_timesteps | 332400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.2     |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 831      |
|    time_elapsed    | 91998    |
|    total_timesteps | 332400   |
---------------------------------
Eval num_timesteps=332600, episode_reward=10.12 +/- 6.84
Episode length: 15.60 +/- 10.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.6         |
|    mean_reward          | 10.1         |
| time/                   |              |
|    total_timesteps      | 332600       |
| train/                  |              |
|    approx_kl            | 0.0030989216 |
|    clip_fraction        | 0.0795       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.541       |
|    explained_variance   | 0.556        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.94         |
|    n_updates            | 8310         |
|    policy_gradient_loss | -0.00629     |
|    value_loss           | 2.05         |
------------------------------------------
Eval num_timesteps=332800, episode_reward=5.35 +/- 3.07
Episode length: 8.80 +/- 4.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8.8      |
|    mean_reward     | 5.35     |
| time/              |          |
|    total_timesteps | 332800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | 16.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 832      |
|    time_elapsed    | 92081    |
|    total_timesteps | 332800   |
---------------------------------
Eval num_timesteps=333000, episode_reward=11.47 +/- 12.70
Episode length: 16.80 +/- 16.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.8        |
|    mean_reward          | 11.5        |
| time/                   |             |
|    total_timesteps      | 333000      |
| train/                  |             |
|    approx_kl            | 0.003180887 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.593      |
|    explained_variance   | 0.341       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.145       |
|    n_updates            | 8320        |
|    policy_gradient_loss | -0.00719    |
|    value_loss           | 1.07        |
-----------------------------------------
Eval num_timesteps=333200, episode_reward=22.99 +/- 14.09
Episode length: 33.60 +/- 20.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 23       |
| time/              |          |
|    total_timesteps | 333200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 833      |
|    time_elapsed    | 92187    |
|    total_timesteps | 333200   |
---------------------------------
Eval num_timesteps=333400, episode_reward=14.72 +/- 12.39
Episode length: 21.40 +/- 16.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 21.4         |
|    mean_reward          | 14.7         |
| time/                   |              |
|    total_timesteps      | 333400       |
| train/                  |              |
|    approx_kl            | 0.0047100363 |
|    clip_fraction        | 0.17         |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.579       |
|    explained_variance   | 0.336        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.489        |
|    n_updates            | 8330         |
|    policy_gradient_loss | -0.00625     |
|    value_loss           | 2.29         |
------------------------------------------
Eval num_timesteps=333600, episode_reward=27.94 +/- 12.54
Episode length: 41.00 +/- 18.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41       |
|    mean_reward     | 27.9     |
| time/              |          |
|    total_timesteps | 333600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.7     |
|    ep_rew_mean     | 17.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 834      |
|    time_elapsed    | 92321    |
|    total_timesteps | 333600   |
---------------------------------
Eval num_timesteps=333800, episode_reward=22.26 +/- 15.41
Episode length: 32.20 +/- 21.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 32.2        |
|    mean_reward          | 22.3        |
| time/                   |             |
|    total_timesteps      | 333800      |
| train/                  |             |
|    approx_kl            | 0.004162334 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.588      |
|    explained_variance   | 0.271       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.735       |
|    n_updates            | 8340        |
|    policy_gradient_loss | -0.00349    |
|    value_loss           | 0.747       |
-----------------------------------------
Eval num_timesteps=334000, episode_reward=14.85 +/- 12.41
Episode length: 21.40 +/- 17.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 14.8     |
| time/              |          |
|    total_timesteps | 334000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.4     |
|    ep_rew_mean     | 16.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 835      |
|    time_elapsed    | 92433    |
|    total_timesteps | 334000   |
---------------------------------
Eval num_timesteps=334200, episode_reward=16.63 +/- 15.08
Episode length: 24.20 +/- 21.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.2         |
|    mean_reward          | 16.6         |
| time/                   |              |
|    total_timesteps      | 334200       |
| train/                  |              |
|    approx_kl            | 0.0022964107 |
|    clip_fraction        | 0.0574       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.546       |
|    explained_variance   | 0.387        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.332        |
|    n_updates            | 8350         |
|    policy_gradient_loss | -0.00441     |
|    value_loss           | 2.19         |
------------------------------------------
Eval num_timesteps=334400, episode_reward=24.24 +/- 14.25
Episode length: 34.40 +/- 19.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 24.2     |
| time/              |          |
|    total_timesteps | 334400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 836      |
|    time_elapsed    | 92545    |
|    total_timesteps | 334400   |
---------------------------------
Eval num_timesteps=334600, episode_reward=9.58 +/- 4.54
Episode length: 14.40 +/- 6.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.4         |
|    mean_reward          | 9.58         |
| time/                   |              |
|    total_timesteps      | 334600       |
| train/                  |              |
|    approx_kl            | 0.0027159092 |
|    clip_fraction        | 0.0777       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.571       |
|    explained_variance   | 0.0475       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.847        |
|    n_updates            | 8360         |
|    policy_gradient_loss | -0.00607     |
|    value_loss           | 2.25         |
------------------------------------------
Eval num_timesteps=334800, episode_reward=15.95 +/- 15.23
Episode length: 23.60 +/- 21.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 15.9     |
| time/              |          |
|    total_timesteps | 334800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25       |
|    ep_rew_mean     | 17.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 837      |
|    time_elapsed    | 92643    |
|    total_timesteps | 334800   |
---------------------------------
Eval num_timesteps=335000, episode_reward=7.01 +/- 5.77
Episode length: 10.80 +/- 7.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 10.8        |
|    mean_reward          | 7.01        |
| time/                   |             |
|    total_timesteps      | 335000      |
| train/                  |             |
|    approx_kl            | 0.001102982 |
|    clip_fraction        | 0.0525      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.541      |
|    explained_variance   | 0.445       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.952       |
|    n_updates            | 8370        |
|    policy_gradient_loss | -0.00476    |
|    value_loss           | 1.99        |
-----------------------------------------
Eval num_timesteps=335200, episode_reward=27.69 +/- 8.96
Episode length: 39.60 +/- 12.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39.6     |
|    mean_reward     | 27.7     |
| time/              |          |
|    total_timesteps | 335200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.5     |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 838      |
|    time_elapsed    | 92732    |
|    total_timesteps | 335200   |
---------------------------------
Eval num_timesteps=335400, episode_reward=11.69 +/- 13.08
Episode length: 17.00 +/- 17.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17           |
|    mean_reward          | 11.7         |
| time/                   |              |
|    total_timesteps      | 335400       |
| train/                  |              |
|    approx_kl            | 0.0030394983 |
|    clip_fraction        | 0.0542       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.564       |
|    explained_variance   | 0.259        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.35         |
|    n_updates            | 8380         |
|    policy_gradient_loss | -0.0054      |
|    value_loss           | 1.98         |
------------------------------------------
Eval num_timesteps=335600, episode_reward=2.51 +/- 1.46
Episode length: 4.40 +/- 2.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4.4      |
|    mean_reward     | 2.51     |
| time/              |          |
|    total_timesteps | 335600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.3     |
|    ep_rew_mean     | 17.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 839      |
|    time_elapsed    | 92814    |
|    total_timesteps | 335600   |
---------------------------------
Eval num_timesteps=335800, episode_reward=17.94 +/- 15.53
Episode length: 25.20 +/- 20.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 25.2        |
|    mean_reward          | 17.9        |
| time/                   |             |
|    total_timesteps      | 335800      |
| train/                  |             |
|    approx_kl            | 0.005134546 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.568      |
|    explained_variance   | 0.486       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.396       |
|    n_updates            | 8390        |
|    policy_gradient_loss | -0.0044     |
|    value_loss           | 1.44        |
-----------------------------------------
Eval num_timesteps=336000, episode_reward=16.76 +/- 14.97
Episode length: 24.60 +/- 21.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 336000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.6     |
|    ep_rew_mean     | 15.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 840      |
|    time_elapsed    | 92951    |
|    total_timesteps | 336000   |
---------------------------------
Eval num_timesteps=336200, episode_reward=23.47 +/- 14.95
Episode length: 33.40 +/- 20.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.4         |
|    mean_reward          | 23.5         |
| time/                   |              |
|    total_timesteps      | 336200       |
| train/                  |              |
|    approx_kl            | 0.0057802913 |
|    clip_fraction        | 0.208        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.538       |
|    explained_variance   | 0.499        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.49         |
|    n_updates            | 8400         |
|    policy_gradient_loss | -0.0114      |
|    value_loss           | 1.83         |
------------------------------------------
Eval num_timesteps=336400, episode_reward=28.84 +/- 14.09
Episode length: 40.40 +/- 19.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.4     |
|    mean_reward     | 28.8     |
| time/              |          |
|    total_timesteps | 336400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | 16.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 841      |
|    time_elapsed    | 93104    |
|    total_timesteps | 336400   |
---------------------------------
Eval num_timesteps=336600, episode_reward=10.41 +/- 13.12
Episode length: 15.80 +/- 18.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | 10.4         |
| time/                   |              |
|    total_timesteps      | 336600       |
| train/                  |              |
|    approx_kl            | 0.0023697368 |
|    clip_fraction        | 0.0609       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.586       |
|    explained_variance   | -0.115       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.293        |
|    n_updates            | 8410         |
|    policy_gradient_loss | -0.00559     |
|    value_loss           | 1.79         |
------------------------------------------
Eval num_timesteps=336800, episode_reward=25.28 +/- 12.33
Episode length: 36.40 +/- 16.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 25.3     |
| time/              |          |
|    total_timesteps | 336800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.4     |
|    ep_rew_mean     | 15.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 842      |
|    time_elapsed    | 93217    |
|    total_timesteps | 336800   |
---------------------------------
Eval num_timesteps=337000, episode_reward=16.89 +/- 15.26
Episode length: 24.20 +/- 21.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 24.2        |
|    mean_reward          | 16.9        |
| time/                   |             |
|    total_timesteps      | 337000      |
| train/                  |             |
|    approx_kl            | 0.003126633 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.53       |
|    explained_variance   | 0.292       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.12        |
|    n_updates            | 8420        |
|    policy_gradient_loss | -0.00912    |
|    value_loss           | 2.61        |
-----------------------------------------
Eval num_timesteps=337200, episode_reward=18.67 +/- 13.53
Episode length: 27.40 +/- 19.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.4     |
|    mean_reward     | 18.7     |
| time/              |          |
|    total_timesteps | 337200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.9     |
|    ep_rew_mean     | 15.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 843      |
|    time_elapsed    | 93334    |
|    total_timesteps | 337200   |
---------------------------------
Eval num_timesteps=337400, episode_reward=11.74 +/- 11.20
Episode length: 18.20 +/- 16.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.2         |
|    mean_reward          | 11.7         |
| time/                   |              |
|    total_timesteps      | 337400       |
| train/                  |              |
|    approx_kl            | 0.0024691988 |
|    clip_fraction        | 0.0719       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.528       |
|    explained_variance   | 0.406        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.221        |
|    n_updates            | 8430         |
|    policy_gradient_loss | -0.00597     |
|    value_loss           | 2.09         |
------------------------------------------
Eval num_timesteps=337600, episode_reward=5.99 +/- 3.33
Episode length: 9.20 +/- 4.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.2      |
|    mean_reward     | 5.99     |
| time/              |          |
|    total_timesteps | 337600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.2     |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 844      |
|    time_elapsed    | 93420    |
|    total_timesteps | 337600   |
---------------------------------
Eval num_timesteps=337800, episode_reward=16.22 +/- 14.47
Episode length: 24.40 +/- 20.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.4         |
|    mean_reward          | 16.2         |
| time/                   |              |
|    total_timesteps      | 337800       |
| train/                  |              |
|    approx_kl            | 0.0015589353 |
|    clip_fraction        | 0.0685       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.55        |
|    explained_variance   | 0.507        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.15         |
|    n_updates            | 8440         |
|    policy_gradient_loss | -0.00511     |
|    value_loss           | 1.7          |
------------------------------------------
Eval num_timesteps=338000, episode_reward=10.49 +/- 13.11
Episode length: 15.40 +/- 17.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | 10.5     |
| time/              |          |
|    total_timesteps | 338000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.4     |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 845      |
|    time_elapsed    | 93524    |
|    total_timesteps | 338000   |
---------------------------------
Eval num_timesteps=338200, episode_reward=10.86 +/- 12.96
Episode length: 16.20 +/- 17.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.2         |
|    mean_reward          | 10.9         |
| time/                   |              |
|    total_timesteps      | 338200       |
| train/                  |              |
|    approx_kl            | 0.0018486049 |
|    clip_fraction        | 0.0498       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.549       |
|    explained_variance   | 0.459        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.694        |
|    n_updates            | 8450         |
|    policy_gradient_loss | -0.00386     |
|    value_loss           | 1.7          |
------------------------------------------
Eval num_timesteps=338400, episode_reward=18.34 +/- 14.53
Episode length: 26.20 +/- 20.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | 18.3     |
| time/              |          |
|    total_timesteps | 338400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.2     |
|    ep_rew_mean     | 15.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 846      |
|    time_elapsed    | 93662    |
|    total_timesteps | 338400   |
---------------------------------
Eval num_timesteps=338600, episode_reward=18.80 +/- 14.86
Episode length: 26.60 +/- 19.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.6         |
|    mean_reward          | 18.8         |
| time/                   |              |
|    total_timesteps      | 338600       |
| train/                  |              |
|    approx_kl            | 0.0006314275 |
|    clip_fraction        | 0.0212       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.541       |
|    explained_variance   | 0.267        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.982        |
|    n_updates            | 8460         |
|    policy_gradient_loss | -0.00308     |
|    value_loss           | 3.03         |
------------------------------------------
Eval num_timesteps=338800, episode_reward=10.11 +/- 11.76
Episode length: 15.80 +/- 17.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | 10.1     |
| time/              |          |
|    total_timesteps | 338800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.2     |
|    ep_rew_mean     | 15.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 847      |
|    time_elapsed    | 93791    |
|    total_timesteps | 338800   |
---------------------------------
Eval num_timesteps=339000, episode_reward=28.85 +/- 14.02
Episode length: 40.40 +/- 19.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 40.4         |
|    mean_reward          | 28.9         |
| time/                   |              |
|    total_timesteps      | 339000       |
| train/                  |              |
|    approx_kl            | 0.0029972936 |
|    clip_fraction        | 0.069        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.566       |
|    explained_variance   | 0.513        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.181        |
|    n_updates            | 8470         |
|    policy_gradient_loss | -0.00535     |
|    value_loss           | 1.18         |
------------------------------------------
Eval num_timesteps=339200, episode_reward=19.37 +/- 13.72
Episode length: 27.60 +/- 18.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.6     |
|    mean_reward     | 19.4     |
| time/              |          |
|    total_timesteps | 339200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.9     |
|    ep_rew_mean     | 15.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 848      |
|    time_elapsed    | 93925    |
|    total_timesteps | 339200   |
---------------------------------
Eval num_timesteps=339400, episode_reward=11.49 +/- 13.23
Episode length: 16.60 +/- 17.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.6         |
|    mean_reward          | 11.5         |
| time/                   |              |
|    total_timesteps      | 339400       |
| train/                  |              |
|    approx_kl            | 0.0021750645 |
|    clip_fraction        | 0.0763       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.555       |
|    explained_variance   | 0.47         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.7          |
|    n_updates            | 8480         |
|    policy_gradient_loss | -0.00769     |
|    value_loss           | 1.45         |
------------------------------------------
Eval num_timesteps=339600, episode_reward=13.88 +/- 15.26
Episode length: 20.00 +/- 20.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20       |
|    mean_reward     | 13.9     |
| time/              |          |
|    total_timesteps | 339600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.5     |
|    ep_rew_mean     | 16.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 849      |
|    time_elapsed    | 94029    |
|    total_timesteps | 339600   |
---------------------------------
Eval num_timesteps=339800, episode_reward=17.48 +/- 14.13
Episode length: 25.80 +/- 20.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.8         |
|    mean_reward          | 17.5         |
| time/                   |              |
|    total_timesteps      | 339800       |
| train/                  |              |
|    approx_kl            | 0.0038914275 |
|    clip_fraction        | 0.0708       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.568       |
|    explained_variance   | 0.461        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.477        |
|    n_updates            | 8490         |
|    policy_gradient_loss | -0.00462     |
|    value_loss           | 2.05         |
------------------------------------------
Eval num_timesteps=340000, episode_reward=8.17 +/- 12.67
Episode length: 12.80 +/- 18.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 12.8     |
|    mean_reward     | 8.17     |
| time/              |          |
|    total_timesteps | 340000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.2     |
|    ep_rew_mean     | 15.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 850      |
|    time_elapsed    | 94138    |
|    total_timesteps | 340000   |
---------------------------------
Eval num_timesteps=340200, episode_reward=10.31 +/- 13.17
Episode length: 15.40 +/- 18.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.4         |
|    mean_reward          | 10.3         |
| time/                   |              |
|    total_timesteps      | 340200       |
| train/                  |              |
|    approx_kl            | 0.0017048214 |
|    clip_fraction        | 0.042        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.558       |
|    explained_variance   | 0.192        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.31         |
|    n_updates            | 8500         |
|    policy_gradient_loss | -0.00395     |
|    value_loss           | 2.99         |
------------------------------------------
Eval num_timesteps=340400, episode_reward=19.17 +/- 13.25
Episode length: 27.80 +/- 18.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.8     |
|    mean_reward     | 19.2     |
| time/              |          |
|    total_timesteps | 340400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.5     |
|    ep_rew_mean     | 16.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 851      |
|    time_elapsed    | 94268    |
|    total_timesteps | 340400   |
---------------------------------
Eval num_timesteps=340600, episode_reward=17.53 +/- 13.85
Episode length: 26.00 +/- 19.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26           |
|    mean_reward          | 17.5         |
| time/                   |              |
|    total_timesteps      | 340600       |
| train/                  |              |
|    approx_kl            | 0.0024425474 |
|    clip_fraction        | 0.0962       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.584       |
|    explained_variance   | 0.422        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0849       |
|    n_updates            | 8510         |
|    policy_gradient_loss | -0.00392     |
|    value_loss           | 1.05         |
------------------------------------------
Eval num_timesteps=340800, episode_reward=17.27 +/- 14.11
Episode length: 25.40 +/- 20.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | 17.3     |
| time/              |          |
|    total_timesteps | 340800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.7     |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 852      |
|    time_elapsed    | 94374    |
|    total_timesteps | 340800   |
---------------------------------
Eval num_timesteps=341000, episode_reward=21.20 +/- 10.63
Episode length: 31.80 +/- 15.26
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 31.8       |
|    mean_reward          | 21.2       |
| time/                   |            |
|    total_timesteps      | 341000     |
| train/                  |            |
|    approx_kl            | 0.00263644 |
|    clip_fraction        | 0.117      |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.588     |
|    explained_variance   | 0.221      |
|    learning_rate        | 0.0001     |
|    loss                 | 1.44       |
|    n_updates            | 8520       |
|    policy_gradient_loss | -0.00613   |
|    value_loss           | 1.99       |
----------------------------------------
Eval num_timesteps=341200, episode_reward=8.81 +/- 12.89
Episode length: 13.60 +/- 18.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.6     |
|    mean_reward     | 8.81     |
| time/              |          |
|    total_timesteps | 341200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 853      |
|    time_elapsed    | 94477    |
|    total_timesteps | 341200   |
---------------------------------
Eval num_timesteps=341400, episode_reward=29.13 +/- 13.49
Episode length: 40.80 +/- 18.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 40.8         |
|    mean_reward          | 29.1         |
| time/                   |              |
|    total_timesteps      | 341400       |
| train/                  |              |
|    approx_kl            | 0.0027284485 |
|    clip_fraction        | 0.0786       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.576       |
|    explained_variance   | 0.411        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.75         |
|    n_updates            | 8530         |
|    policy_gradient_loss | -0.00641     |
|    value_loss           | 2.33         |
------------------------------------------
Eval num_timesteps=341600, episode_reward=12.33 +/- 11.35
Episode length: 18.40 +/- 16.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.4     |
|    mean_reward     | 12.3     |
| time/              |          |
|    total_timesteps | 341600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23       |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 854      |
|    time_elapsed    | 94582    |
|    total_timesteps | 341600   |
---------------------------------
Eval num_timesteps=341800, episode_reward=22.72 +/- 14.49
Episode length: 33.40 +/- 20.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.4         |
|    mean_reward          | 22.7         |
| time/                   |              |
|    total_timesteps      | 341800       |
| train/                  |              |
|    approx_kl            | 0.0027805236 |
|    clip_fraction        | 0.0578       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.578       |
|    explained_variance   | 0.453        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.04         |
|    n_updates            | 8540         |
|    policy_gradient_loss | -0.00336     |
|    value_loss           | 1.86         |
------------------------------------------
Eval num_timesteps=342000, episode_reward=18.65 +/- 14.62
Episode length: 27.20 +/- 19.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.2     |
|    mean_reward     | 18.6     |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.8     |
|    ep_rew_mean     | 14.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 855      |
|    time_elapsed    | 94700    |
|    total_timesteps | 342000   |
---------------------------------
Eval num_timesteps=342200, episode_reward=21.89 +/- 15.43
Episode length: 32.00 +/- 22.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32           |
|    mean_reward          | 21.9         |
| time/                   |              |
|    total_timesteps      | 342200       |
| train/                  |              |
|    approx_kl            | 0.0026135442 |
|    clip_fraction        | 0.0786       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.557       |
|    explained_variance   | 0.545        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.857        |
|    n_updates            | 8550         |
|    policy_gradient_loss | -0.00783     |
|    value_loss           | 2            |
------------------------------------------
Eval num_timesteps=342400, episode_reward=15.45 +/- 16.01
Episode length: 22.60 +/- 22.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 15.5     |
| time/              |          |
|    total_timesteps | 342400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.1     |
|    ep_rew_mean     | 14.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 856      |
|    time_elapsed    | 94805    |
|    total_timesteps | 342400   |
---------------------------------
Eval num_timesteps=342600, episode_reward=10.32 +/- 12.79
Episode length: 15.40 +/- 17.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.4        |
|    mean_reward          | 10.3        |
| time/                   |             |
|    total_timesteps      | 342600      |
| train/                  |             |
|    approx_kl            | 0.002412439 |
|    clip_fraction        | 0.0848      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.578      |
|    explained_variance   | 0.601       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.548       |
|    n_updates            | 8560        |
|    policy_gradient_loss | -0.00534    |
|    value_loss           | 1.59        |
-----------------------------------------
Eval num_timesteps=342800, episode_reward=10.89 +/- 11.32
Episode length: 17.00 +/- 16.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | 10.9     |
| time/              |          |
|    total_timesteps | 342800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.5     |
|    ep_rew_mean     | 13.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 857      |
|    time_elapsed    | 94909    |
|    total_timesteps | 342800   |
---------------------------------
Eval num_timesteps=343000, episode_reward=10.70 +/- 11.99
Episode length: 16.40 +/- 16.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | 10.7         |
| time/                   |              |
|    total_timesteps      | 343000       |
| train/                  |              |
|    approx_kl            | 0.0017944563 |
|    clip_fraction        | 0.058        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.59        |
|    explained_variance   | -0.0222      |
|    learning_rate        | 0.0001       |
|    loss                 | 1.11         |
|    n_updates            | 8570         |
|    policy_gradient_loss | -0.00519     |
|    value_loss           | 3            |
------------------------------------------
Eval num_timesteps=343200, episode_reward=9.65 +/- 13.18
Episode length: 14.40 +/- 18.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.4     |
|    mean_reward     | 9.65     |
| time/              |          |
|    total_timesteps | 343200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.3     |
|    ep_rew_mean     | 13.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 858      |
|    time_elapsed    | 95012    |
|    total_timesteps | 343200   |
---------------------------------
Eval num_timesteps=343400, episode_reward=16.31 +/- 15.42
Episode length: 24.00 +/- 21.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24           |
|    mean_reward          | 16.3         |
| time/                   |              |
|    total_timesteps      | 343400       |
| train/                  |              |
|    approx_kl            | 0.0031572147 |
|    clip_fraction        | 0.101        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.562       |
|    explained_variance   | 0.535        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.765        |
|    n_updates            | 8580         |
|    policy_gradient_loss | -0.00678     |
|    value_loss           | 2.35         |
------------------------------------------
Eval num_timesteps=343600, episode_reward=30.11 +/- 10.48
Episode length: 42.80 +/- 14.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.8     |
|    mean_reward     | 30.1     |
| time/              |          |
|    total_timesteps | 343600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.2     |
|    ep_rew_mean     | 13.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 859      |
|    time_elapsed    | 95122    |
|    total_timesteps | 343600   |
---------------------------------
Eval num_timesteps=343800, episode_reward=15.29 +/- 15.26
Episode length: 23.00 +/- 22.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23           |
|    mean_reward          | 15.3         |
| time/                   |              |
|    total_timesteps      | 343800       |
| train/                  |              |
|    approx_kl            | 0.0018099429 |
|    clip_fraction        | 0.101        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.58        |
|    explained_variance   | -0.0523      |
|    learning_rate        | 0.0001       |
|    loss                 | 0.167        |
|    n_updates            | 8590         |
|    policy_gradient_loss | -0.0068      |
|    value_loss           | 1.52         |
------------------------------------------
Eval num_timesteps=344000, episode_reward=23.68 +/- 15.69
Episode length: 33.00 +/- 21.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 23.7     |
| time/              |          |
|    total_timesteps | 344000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.2     |
|    ep_rew_mean     | 14.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 860      |
|    time_elapsed    | 95251    |
|    total_timesteps | 344000   |
---------------------------------
Eval num_timesteps=344200, episode_reward=23.89 +/- 13.60
Episode length: 34.60 +/- 19.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.6        |
|    mean_reward          | 23.9        |
| time/                   |             |
|    total_timesteps      | 344200      |
| train/                  |             |
|    approx_kl            | 0.003264126 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.607      |
|    explained_variance   | 0.284       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.459       |
|    n_updates            | 8600        |
|    policy_gradient_loss | -0.00563    |
|    value_loss           | 1.82        |
-----------------------------------------
Eval num_timesteps=344400, episode_reward=22.05 +/- 15.72
Episode length: 32.00 +/- 22.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32       |
|    mean_reward     | 22.1     |
| time/              |          |
|    total_timesteps | 344400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.7     |
|    ep_rew_mean     | 15.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 861      |
|    time_elapsed    | 95360    |
|    total_timesteps | 344400   |
---------------------------------
Eval num_timesteps=344600, episode_reward=9.88 +/- 13.55
Episode length: 14.40 +/- 18.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.4         |
|    mean_reward          | 9.88         |
| time/                   |              |
|    total_timesteps      | 344600       |
| train/                  |              |
|    approx_kl            | 0.0014368426 |
|    clip_fraction        | 0.073        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.579       |
|    explained_variance   | 0.156        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.993        |
|    n_updates            | 8610         |
|    policy_gradient_loss | -0.00331     |
|    value_loss           | 2.82         |
------------------------------------------
Eval num_timesteps=344800, episode_reward=12.85 +/- 12.40
Episode length: 18.60 +/- 16.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 12.8     |
| time/              |          |
|    total_timesteps | 344800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.2     |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 862      |
|    time_elapsed    | 95462    |
|    total_timesteps | 344800   |
---------------------------------
Eval num_timesteps=345000, episode_reward=16.86 +/- 15.33
Episode length: 24.40 +/- 20.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 24.4        |
|    mean_reward          | 16.9        |
| time/                   |             |
|    total_timesteps      | 345000      |
| train/                  |             |
|    approx_kl            | 0.005079081 |
|    clip_fraction        | 0.0962      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.601      |
|    explained_variance   | -0.058      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.407       |
|    n_updates            | 8620        |
|    policy_gradient_loss | -0.00655    |
|    value_loss           | 1.48        |
-----------------------------------------
Eval num_timesteps=345200, episode_reward=22.56 +/- 15.09
Episode length: 32.60 +/- 21.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.6     |
|    mean_reward     | 22.6     |
| time/              |          |
|    total_timesteps | 345200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22       |
|    ep_rew_mean     | 15       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 863      |
|    time_elapsed    | 95569    |
|    total_timesteps | 345200   |
---------------------------------
Eval num_timesteps=345400, episode_reward=24.54 +/- 14.01
Episode length: 34.40 +/- 19.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.4         |
|    mean_reward          | 24.5         |
| time/                   |              |
|    total_timesteps      | 345400       |
| train/                  |              |
|    approx_kl            | 0.0019773508 |
|    clip_fraction        | 0.13         |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.597       |
|    explained_variance   | 0.381        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.884        |
|    n_updates            | 8630         |
|    policy_gradient_loss | -0.00766     |
|    value_loss           | 2.65         |
------------------------------------------
Eval num_timesteps=345600, episode_reward=4.33 +/- 2.04
Episode length: 7.20 +/- 3.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7.2      |
|    mean_reward     | 4.33     |
| time/              |          |
|    total_timesteps | 345600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | 17.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 864      |
|    time_elapsed    | 95679    |
|    total_timesteps | 345600   |
---------------------------------
Eval num_timesteps=345800, episode_reward=18.58 +/- 14.45
Episode length: 26.40 +/- 19.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 26.4        |
|    mean_reward          | 18.6        |
| time/                   |             |
|    total_timesteps      | 345800      |
| train/                  |             |
|    approx_kl            | 0.005810916 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.589      |
|    explained_variance   | -0.126      |
|    learning_rate        | 0.0001      |
|    loss                 | 1.17        |
|    n_updates            | 8640        |
|    policy_gradient_loss | -0.00801    |
|    value_loss           | 2.06        |
-----------------------------------------
Eval num_timesteps=346000, episode_reward=16.22 +/- 15.39
Episode length: 23.80 +/- 21.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | 16.2     |
| time/              |          |
|    total_timesteps | 346000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | 17.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 865      |
|    time_elapsed    | 95784    |
|    total_timesteps | 346000   |
---------------------------------
Eval num_timesteps=346200, episode_reward=13.33 +/- 10.61
Episode length: 19.60 +/- 15.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 19.6         |
|    mean_reward          | 13.3         |
| time/                   |              |
|    total_timesteps      | 346200       |
| train/                  |              |
|    approx_kl            | 0.0042419853 |
|    clip_fraction        | 0.108        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.595       |
|    explained_variance   | 0.158        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.902        |
|    n_updates            | 8650         |
|    policy_gradient_loss | -0.0068      |
|    value_loss           | 1.93         |
------------------------------------------
Eval num_timesteps=346400, episode_reward=23.32 +/- 14.18
Episode length: 33.80 +/- 19.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 23.3     |
| time/              |          |
|    total_timesteps | 346400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.9     |
|    ep_rew_mean     | 17.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 866      |
|    time_elapsed    | 95890    |
|    total_timesteps | 346400   |
---------------------------------
Eval num_timesteps=346600, episode_reward=5.78 +/- 3.77
Episode length: 9.20 +/- 5.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9.2         |
|    mean_reward          | 5.78        |
| time/                   |             |
|    total_timesteps      | 346600      |
| train/                  |             |
|    approx_kl            | 0.006197394 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.569      |
|    explained_variance   | 0.235       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.694       |
|    n_updates            | 8660        |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 0.972       |
-----------------------------------------
Eval num_timesteps=346800, episode_reward=19.26 +/- 15.13
Episode length: 27.40 +/- 20.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.4     |
|    mean_reward     | 19.3     |
| time/              |          |
|    total_timesteps | 346800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.5     |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 867      |
|    time_elapsed    | 95977    |
|    total_timesteps | 346800   |
---------------------------------
Eval num_timesteps=347000, episode_reward=16.81 +/- 16.24
Episode length: 23.60 +/- 21.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.6         |
|    mean_reward          | 16.8         |
| time/                   |              |
|    total_timesteps      | 347000       |
| train/                  |              |
|    approx_kl            | 0.0014454629 |
|    clip_fraction        | 0.0319       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.543       |
|    explained_variance   | 0.278        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.62         |
|    n_updates            | 8670         |
|    policy_gradient_loss | -0.00478     |
|    value_loss           | 2.81         |
------------------------------------------
Eval num_timesteps=347200, episode_reward=18.43 +/- 13.94
Episode length: 27.00 +/- 19.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27       |
|    mean_reward     | 18.4     |
| time/              |          |
|    total_timesteps | 347200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.5     |
|    ep_rew_mean     | 16.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 868      |
|    time_elapsed    | 96115    |
|    total_timesteps | 347200   |
---------------------------------
Eval num_timesteps=347400, episode_reward=2.93 +/- 2.14
Episode length: 5.00 +/- 3.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 5            |
|    mean_reward          | 2.93         |
| time/                   |              |
|    total_timesteps      | 347400       |
| train/                  |              |
|    approx_kl            | 0.0022390613 |
|    clip_fraction        | 0.0763       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.563       |
|    explained_variance   | 0.304        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.99         |
|    n_updates            | 8680         |
|    policy_gradient_loss | -0.00575     |
|    value_loss           | 2.48         |
------------------------------------------
Eval num_timesteps=347600, episode_reward=15.65 +/- 15.44
Episode length: 23.20 +/- 21.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 15.6     |
| time/              |          |
|    total_timesteps | 347600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.8     |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 869      |
|    time_elapsed    | 96200    |
|    total_timesteps | 347600   |
---------------------------------
Eval num_timesteps=347800, episode_reward=6.88 +/- 3.52
Episode length: 10.60 +/- 5.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 10.6        |
|    mean_reward          | 6.88        |
| time/                   |             |
|    total_timesteps      | 347800      |
| train/                  |             |
|    approx_kl            | 0.002490421 |
|    clip_fraction        | 0.0837      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.546      |
|    explained_variance   | 0.272       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.416       |
|    n_updates            | 8690        |
|    policy_gradient_loss | -0.00782    |
|    value_loss           | 1.14        |
-----------------------------------------
Eval num_timesteps=348000, episode_reward=23.79 +/- 14.38
Episode length: 34.20 +/- 19.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 23.8     |
| time/              |          |
|    total_timesteps | 348000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.5     |
|    ep_rew_mean     | 18.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 870      |
|    time_elapsed    | 96288    |
|    total_timesteps | 348000   |
---------------------------------
Eval num_timesteps=348200, episode_reward=11.39 +/- 13.16
Episode length: 16.40 +/- 17.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.4        |
|    mean_reward          | 11.4        |
| time/                   |             |
|    total_timesteps      | 348200      |
| train/                  |             |
|    approx_kl            | 0.006840926 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.57       |
|    explained_variance   | 0.279       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.143       |
|    n_updates            | 8700        |
|    policy_gradient_loss | -0.00749    |
|    value_loss           | 1.36        |
-----------------------------------------
Eval num_timesteps=348400, episode_reward=10.47 +/- 11.73
Episode length: 16.20 +/- 17.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | 10.5     |
| time/              |          |
|    total_timesteps | 348400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.2     |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 871      |
|    time_elapsed    | 96391    |
|    total_timesteps | 348400   |
---------------------------------
Eval num_timesteps=348600, episode_reward=9.90 +/- 13.52
Episode length: 14.40 +/- 17.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.4        |
|    mean_reward          | 9.9         |
| time/                   |             |
|    total_timesteps      | 348600      |
| train/                  |             |
|    approx_kl            | 0.006035126 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.575      |
|    explained_variance   | 0.318       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.11        |
|    n_updates            | 8710        |
|    policy_gradient_loss | -0.00824    |
|    value_loss           | 0.684       |
-----------------------------------------
Eval num_timesteps=348800, episode_reward=5.92 +/- 2.58
Episode length: 9.40 +/- 3.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.4      |
|    mean_reward     | 5.92     |
| time/              |          |
|    total_timesteps | 348800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.3     |
|    ep_rew_mean     | 17.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 872      |
|    time_elapsed    | 96478    |
|    total_timesteps | 348800   |
---------------------------------
Eval num_timesteps=349000, episode_reward=14.50 +/- 12.05
Episode length: 22.00 +/- 17.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22           |
|    mean_reward          | 14.5         |
| time/                   |              |
|    total_timesteps      | 349000       |
| train/                  |              |
|    approx_kl            | 0.0047281724 |
|    clip_fraction        | 0.126        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.568       |
|    explained_variance   | 0.373        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.731        |
|    n_updates            | 8720         |
|    policy_gradient_loss | -0.00747     |
|    value_loss           | 1.87         |
------------------------------------------
Eval num_timesteps=349200, episode_reward=18.83 +/- 14.23
Episode length: 27.20 +/- 19.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.2     |
|    mean_reward     | 18.8     |
| time/              |          |
|    total_timesteps | 349200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.5     |
|    ep_rew_mean     | 17.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 873      |
|    time_elapsed    | 96580    |
|    total_timesteps | 349200   |
---------------------------------
Eval num_timesteps=349400, episode_reward=16.70 +/- 14.74
Episode length: 24.80 +/- 20.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.8         |
|    mean_reward          | 16.7         |
| time/                   |              |
|    total_timesteps      | 349400       |
| train/                  |              |
|    approx_kl            | 0.0023136605 |
|    clip_fraction        | 0.0917       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.566       |
|    explained_variance   | 0.313        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.45         |
|    n_updates            | 8730         |
|    policy_gradient_loss | -0.00571     |
|    value_loss           | 2.66         |
------------------------------------------
Eval num_timesteps=349600, episode_reward=15.97 +/- 15.59
Episode length: 23.40 +/- 21.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 349600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26       |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 874      |
|    time_elapsed    | 96685    |
|    total_timesteps | 349600   |
---------------------------------
Eval num_timesteps=349800, episode_reward=28.87 +/- 11.22
Episode length: 42.00 +/- 16.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42           |
|    mean_reward          | 28.9         |
| time/                   |              |
|    total_timesteps      | 349800       |
| train/                  |              |
|    approx_kl            | 0.0030755105 |
|    clip_fraction        | 0.0868       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.563       |
|    explained_variance   | 0.415        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.724        |
|    n_updates            | 8740         |
|    policy_gradient_loss | -0.0059      |
|    value_loss           | 1.63         |
------------------------------------------
Eval num_timesteps=350000, episode_reward=15.21 +/- 16.23
Episode length: 22.20 +/- 22.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 15.2     |
| time/              |          |
|    total_timesteps | 350000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28       |
|    ep_rew_mean     | 19.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 875      |
|    time_elapsed    | 96791    |
|    total_timesteps | 350000   |
---------------------------------
Eval num_timesteps=350200, episode_reward=23.07 +/- 15.37
Episode length: 32.80 +/- 21.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 32.8        |
|    mean_reward          | 23.1        |
| time/                   |             |
|    total_timesteps      | 350200      |
| train/                  |             |
|    approx_kl            | 0.004464299 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.599      |
|    explained_variance   | 0.064       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0632      |
|    n_updates            | 8750        |
|    policy_gradient_loss | -0.00837    |
|    value_loss           | 1.49        |
-----------------------------------------
Eval num_timesteps=350400, episode_reward=17.37 +/- 15.09
Episode length: 25.00 +/- 20.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 350400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.6     |
|    ep_rew_mean     | 18.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 876      |
|    time_elapsed    | 96920    |
|    total_timesteps | 350400   |
---------------------------------
Eval num_timesteps=350600, episode_reward=9.90 +/- 11.86
Episode length: 15.20 +/- 17.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.2         |
|    mean_reward          | 9.9          |
| time/                   |              |
|    total_timesteps      | 350600       |
| train/                  |              |
|    approx_kl            | 0.0039954013 |
|    clip_fraction        | 0.111        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.536       |
|    explained_variance   | 0.343        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.23         |
|    n_updates            | 8760         |
|    policy_gradient_loss | -0.00624     |
|    value_loss           | 2.25         |
------------------------------------------
Eval num_timesteps=350800, episode_reward=8.88 +/- 13.95
Episode length: 13.00 +/- 18.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13       |
|    mean_reward     | 8.88     |
| time/              |          |
|    total_timesteps | 350800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.4     |
|    ep_rew_mean     | 18       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 877      |
|    time_elapsed    | 97027    |
|    total_timesteps | 350800   |
---------------------------------
Eval num_timesteps=351000, episode_reward=24.16 +/- 13.91
Episode length: 34.80 +/- 19.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.8         |
|    mean_reward          | 24.2         |
| time/                   |              |
|    total_timesteps      | 351000       |
| train/                  |              |
|    approx_kl            | 0.0051017706 |
|    clip_fraction        | 0.208        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.594       |
|    explained_variance   | 0.25         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.224        |
|    n_updates            | 8770         |
|    policy_gradient_loss | -0.00718     |
|    value_loss           | 1.3          |
------------------------------------------
Eval num_timesteps=351200, episode_reward=8.09 +/- 13.21
Episode length: 12.40 +/- 18.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 12.4     |
|    mean_reward     | 8.09     |
| time/              |          |
|    total_timesteps | 351200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.1     |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 878      |
|    time_elapsed    | 97131    |
|    total_timesteps | 351200   |
---------------------------------
Eval num_timesteps=351400, episode_reward=9.61 +/- 12.77
Episode length: 14.60 +/- 18.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.6         |
|    mean_reward          | 9.61         |
| time/                   |              |
|    total_timesteps      | 351400       |
| train/                  |              |
|    approx_kl            | 0.0038442332 |
|    clip_fraction        | 0.132        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.591       |
|    explained_variance   | 0.367        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.07         |
|    n_updates            | 8780         |
|    policy_gradient_loss | -0.00481     |
|    value_loss           | 1.42         |
------------------------------------------
Eval num_timesteps=351600, episode_reward=15.99 +/- 16.05
Episode length: 23.20 +/- 21.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 351600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.4     |
|    ep_rew_mean     | 18.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 879      |
|    time_elapsed    | 97233    |
|    total_timesteps | 351600   |
---------------------------------
Eval num_timesteps=351800, episode_reward=5.03 +/- 3.67
Episode length: 8.00 +/- 5.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 8           |
|    mean_reward          | 5.03        |
| time/                   |             |
|    total_timesteps      | 351800      |
| train/                  |             |
|    approx_kl            | 0.002757205 |
|    clip_fraction        | 0.0719      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.552      |
|    explained_variance   | 0.134       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.754       |
|    n_updates            | 8790        |
|    policy_gradient_loss | -0.00655    |
|    value_loss           | 2.02        |
-----------------------------------------
Eval num_timesteps=352000, episode_reward=15.48 +/- 16.88
Episode length: 22.00 +/- 22.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22       |
|    mean_reward     | 15.5     |
| time/              |          |
|    total_timesteps | 352000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.7     |
|    ep_rew_mean     | 19       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 880      |
|    time_elapsed    | 97324    |
|    total_timesteps | 352000   |
---------------------------------
Eval num_timesteps=352200, episode_reward=11.91 +/- 12.53
Episode length: 17.80 +/- 17.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.8        |
|    mean_reward          | 11.9        |
| time/                   |             |
|    total_timesteps      | 352200      |
| train/                  |             |
|    approx_kl            | 0.004351838 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.56       |
|    explained_variance   | 0.456       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.615       |
|    n_updates            | 8800        |
|    policy_gradient_loss | -0.00555    |
|    value_loss           | 1.13        |
-----------------------------------------
Eval num_timesteps=352400, episode_reward=24.93 +/- 11.91
Episode length: 36.20 +/- 17.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 24.9     |
| time/              |          |
|    total_timesteps | 352400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.1     |
|    ep_rew_mean     | 18.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 881      |
|    time_elapsed    | 97438    |
|    total_timesteps | 352400   |
---------------------------------
Eval num_timesteps=352600, episode_reward=23.57 +/- 12.79
Episode length: 35.00 +/- 18.83
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 35         |
|    mean_reward          | 23.6       |
| time/                   |            |
|    total_timesteps      | 352600     |
| train/                  |            |
|    approx_kl            | 0.00219873 |
|    clip_fraction        | 0.0598     |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.57      |
|    explained_variance   | 0.442      |
|    learning_rate        | 0.0001     |
|    loss                 | 1.57       |
|    n_updates            | 8810       |
|    policy_gradient_loss | -0.0065    |
|    value_loss           | 1.55       |
----------------------------------------
Eval num_timesteps=352800, episode_reward=10.73 +/- 11.98
Episode length: 16.20 +/- 17.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | 10.7     |
| time/              |          |
|    total_timesteps | 352800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.7     |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 882      |
|    time_elapsed    | 97544    |
|    total_timesteps | 352800   |
---------------------------------
Eval num_timesteps=353000, episode_reward=17.06 +/- 15.45
Episode length: 24.60 +/- 21.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 24.6        |
|    mean_reward          | 17.1        |
| time/                   |             |
|    total_timesteps      | 353000      |
| train/                  |             |
|    approx_kl            | 0.003302647 |
|    clip_fraction        | 0.0915      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.565      |
|    explained_variance   | 0.315       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.17        |
|    n_updates            | 8820        |
|    policy_gradient_loss | -0.00629    |
|    value_loss           | 2.99        |
-----------------------------------------
Eval num_timesteps=353200, episode_reward=22.94 +/- 16.40
Episode length: 32.00 +/- 22.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32       |
|    mean_reward     | 22.9     |
| time/              |          |
|    total_timesteps | 353200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25       |
|    ep_rew_mean     | 17.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 883      |
|    time_elapsed    | 97675    |
|    total_timesteps | 353200   |
---------------------------------
Eval num_timesteps=353400, episode_reward=9.76 +/- 12.95
Episode length: 14.60 +/- 17.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.6         |
|    mean_reward          | 9.76         |
| time/                   |              |
|    total_timesteps      | 353400       |
| train/                  |              |
|    approx_kl            | 0.0035632849 |
|    clip_fraction        | 0.13         |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.573       |
|    explained_variance   | 0.285        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.922        |
|    n_updates            | 8830         |
|    policy_gradient_loss | -0.00796     |
|    value_loss           | 2.17         |
------------------------------------------
Eval num_timesteps=353600, episode_reward=17.41 +/- 14.59
Episode length: 25.20 +/- 20.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 353600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 884      |
|    time_elapsed    | 97782    |
|    total_timesteps | 353600   |
---------------------------------
Eval num_timesteps=353800, episode_reward=10.93 +/- 12.96
Episode length: 15.80 +/- 17.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | 10.9         |
| time/                   |              |
|    total_timesteps      | 353800       |
| train/                  |              |
|    approx_kl            | 0.0031199588 |
|    clip_fraction        | 0.107        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.55        |
|    explained_variance   | 0.469        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.65         |
|    n_updates            | 8840         |
|    policy_gradient_loss | -0.00593     |
|    value_loss           | 2.16         |
------------------------------------------
Eval num_timesteps=354000, episode_reward=28.73 +/- 11.53
Episode length: 41.80 +/- 16.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 28.7     |
| time/              |          |
|    total_timesteps | 354000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.1     |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 885      |
|    time_elapsed    | 97910    |
|    total_timesteps | 354000   |
---------------------------------
Eval num_timesteps=354200, episode_reward=4.59 +/- 2.09
Episode length: 7.40 +/- 3.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 7.4          |
|    mean_reward          | 4.59         |
| time/                   |              |
|    total_timesteps      | 354200       |
| train/                  |              |
|    approx_kl            | 0.0051021525 |
|    clip_fraction        | 0.189        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.602       |
|    explained_variance   | 0.428        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.776        |
|    n_updates            | 8850         |
|    policy_gradient_loss | -0.00697     |
|    value_loss           | 1.17         |
------------------------------------------
Eval num_timesteps=354400, episode_reward=4.63 +/- 2.30
Episode length: 7.60 +/- 3.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7.6      |
|    mean_reward     | 4.63     |
| time/              |          |
|    total_timesteps | 354400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | 15.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 886      |
|    time_elapsed    | 97974    |
|    total_timesteps | 354400   |
---------------------------------
Eval num_timesteps=354600, episode_reward=22.25 +/- 15.48
Episode length: 32.40 +/- 21.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.4         |
|    mean_reward          | 22.2         |
| time/                   |              |
|    total_timesteps      | 354600       |
| train/                  |              |
|    approx_kl            | 0.0044753305 |
|    clip_fraction        | 0.118        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.527       |
|    explained_variance   | 0.255        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.378        |
|    n_updates            | 8860         |
|    policy_gradient_loss | -0.00633     |
|    value_loss           | 1.73         |
------------------------------------------
Eval num_timesteps=354800, episode_reward=15.77 +/- 12.51
Episode length: 22.40 +/- 16.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 15.8     |
| time/              |          |
|    total_timesteps | 354800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.9     |
|    ep_rew_mean     | 15.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 887      |
|    time_elapsed    | 98081    |
|    total_timesteps | 354800   |
---------------------------------
Eval num_timesteps=355000, episode_reward=18.81 +/- 14.60
Episode length: 26.80 +/- 19.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.8         |
|    mean_reward          | 18.8         |
| time/                   |              |
|    total_timesteps      | 355000       |
| train/                  |              |
|    approx_kl            | 0.0030560358 |
|    clip_fraction        | 0.124        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.565       |
|    explained_variance   | 0.338        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.29         |
|    n_updates            | 8870         |
|    policy_gradient_loss | -0.00925     |
|    value_loss           | 2.4          |
------------------------------------------
Eval num_timesteps=355200, episode_reward=16.55 +/- 14.79
Episode length: 24.40 +/- 21.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 355200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 888      |
|    time_elapsed    | 98197    |
|    total_timesteps | 355200   |
---------------------------------
Eval num_timesteps=355400, episode_reward=13.22 +/- 11.83
Episode length: 19.80 +/- 17.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19.8        |
|    mean_reward          | 13.2        |
| time/                   |             |
|    total_timesteps      | 355400      |
| train/                  |             |
|    approx_kl            | 0.005419714 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.59       |
|    explained_variance   | 0.419       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.539       |
|    n_updates            | 8880        |
|    policy_gradient_loss | -0.00818    |
|    value_loss           | 1.64        |
-----------------------------------------
Eval num_timesteps=355600, episode_reward=23.89 +/- 12.98
Episode length: 34.80 +/- 18.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 23.9     |
| time/              |          |
|    total_timesteps | 355600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.7     |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 889      |
|    time_elapsed    | 98305    |
|    total_timesteps | 355600   |
---------------------------------
Eval num_timesteps=355800, episode_reward=29.72 +/- 11.80
Episode length: 42.00 +/- 16.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42           |
|    mean_reward          | 29.7         |
| time/                   |              |
|    total_timesteps      | 355800       |
| train/                  |              |
|    approx_kl            | 0.0031643766 |
|    clip_fraction        | 0.0929       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.592       |
|    explained_variance   | 0.445        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.359        |
|    n_updates            | 8890         |
|    policy_gradient_loss | -0.00548     |
|    value_loss           | 1.14         |
------------------------------------------
Eval num_timesteps=356000, episode_reward=16.09 +/- 15.50
Episode length: 23.40 +/- 21.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 16.1     |
| time/              |          |
|    total_timesteps | 356000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.3     |
|    ep_rew_mean     | 17.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 890      |
|    time_elapsed    | 98433    |
|    total_timesteps | 356000   |
---------------------------------
Eval num_timesteps=356200, episode_reward=11.09 +/- 12.39
Episode length: 16.80 +/- 17.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.8         |
|    mean_reward          | 11.1         |
| time/                   |              |
|    total_timesteps      | 356200       |
| train/                  |              |
|    approx_kl            | 0.0028666647 |
|    clip_fraction        | 0.0842       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.562       |
|    explained_variance   | 0.405        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.651        |
|    n_updates            | 8900         |
|    policy_gradient_loss | -0.00549     |
|    value_loss           | 1.76         |
------------------------------------------
Eval num_timesteps=356400, episode_reward=14.24 +/- 11.54
Episode length: 20.60 +/- 15.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 14.2     |
| time/              |          |
|    total_timesteps | 356400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.4     |
|    ep_rew_mean     | 18.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 891      |
|    time_elapsed    | 98544    |
|    total_timesteps | 356400   |
---------------------------------
Eval num_timesteps=356600, episode_reward=29.36 +/- 10.26
Episode length: 42.60 +/- 14.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.6        |
|    mean_reward          | 29.4        |
| time/                   |             |
|    total_timesteps      | 356600      |
| train/                  |             |
|    approx_kl            | 0.003106198 |
|    clip_fraction        | 0.0783      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.572      |
|    explained_variance   | 0.173       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.2         |
|    n_updates            | 8910        |
|    policy_gradient_loss | -0.0068     |
|    value_loss           | 2.32        |
-----------------------------------------
Eval num_timesteps=356800, episode_reward=9.58 +/- 11.93
Episode length: 14.80 +/- 17.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 9.58     |
| time/              |          |
|    total_timesteps | 356800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.3     |
|    ep_rew_mean     | 18.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 892      |
|    time_elapsed    | 98651    |
|    total_timesteps | 356800   |
---------------------------------
Eval num_timesteps=357000, episode_reward=17.02 +/- 14.70
Episode length: 25.40 +/- 20.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.4         |
|    mean_reward          | 17           |
| time/                   |              |
|    total_timesteps      | 357000       |
| train/                  |              |
|    approx_kl            | 0.0034467631 |
|    clip_fraction        | 0.185        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.571       |
|    explained_variance   | 0.18         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.306        |
|    n_updates            | 8920         |
|    policy_gradient_loss | -0.00872     |
|    value_loss           | 1.6          |
------------------------------------------
Eval num_timesteps=357200, episode_reward=13.50 +/- 11.88
Episode length: 20.00 +/- 16.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20       |
|    mean_reward     | 13.5     |
| time/              |          |
|    total_timesteps | 357200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.4     |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 893      |
|    time_elapsed    | 98756    |
|    total_timesteps | 357200   |
---------------------------------
Eval num_timesteps=357400, episode_reward=28.78 +/- 11.40
Episode length: 42.00 +/- 16.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42           |
|    mean_reward          | 28.8         |
| time/                   |              |
|    total_timesteps      | 357400       |
| train/                  |              |
|    approx_kl            | 0.0031263747 |
|    clip_fraction        | 0.119        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.568       |
|    explained_variance   | 0.418        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.508        |
|    n_updates            | 8930         |
|    policy_gradient_loss | -0.0103      |
|    value_loss           | 1.74         |
------------------------------------------
Eval num_timesteps=357600, episode_reward=9.86 +/- 13.07
Episode length: 14.80 +/- 17.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 9.86     |
| time/              |          |
|    total_timesteps | 357600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28       |
|    ep_rew_mean     | 19.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 894      |
|    time_elapsed    | 98885    |
|    total_timesteps | 357600   |
---------------------------------
Eval num_timesteps=357800, episode_reward=19.13 +/- 13.08
Episode length: 28.20 +/- 19.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 28.2        |
|    mean_reward          | 19.1        |
| time/                   |             |
|    total_timesteps      | 357800      |
| train/                  |             |
|    approx_kl            | 0.003106194 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.597      |
|    explained_variance   | -0.0517     |
|    learning_rate        | 0.0001      |
|    loss                 | 0.727       |
|    n_updates            | 8940        |
|    policy_gradient_loss | -0.00498    |
|    value_loss           | 0.928       |
-----------------------------------------
Eval num_timesteps=358000, episode_reward=12.81 +/- 11.46
Episode length: 19.20 +/- 16.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.2     |
|    mean_reward     | 12.8     |
| time/              |          |
|    total_timesteps | 358000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.4     |
|    ep_rew_mean     | 19.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 895      |
|    time_elapsed    | 98986    |
|    total_timesteps | 358000   |
---------------------------------
Eval num_timesteps=358200, episode_reward=17.67 +/- 14.24
Episode length: 25.80 +/- 19.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.8         |
|    mean_reward          | 17.7         |
| time/                   |              |
|    total_timesteps      | 358200       |
| train/                  |              |
|    approx_kl            | 0.0027026094 |
|    clip_fraction        | 0.075        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.599       |
|    explained_variance   | -0.0513      |
|    learning_rate        | 0.0001       |
|    loss                 | 0.194        |
|    n_updates            | 8950         |
|    policy_gradient_loss | -0.00524     |
|    value_loss           | 1.22         |
------------------------------------------
Eval num_timesteps=358400, episode_reward=15.92 +/- 15.68
Episode length: 23.20 +/- 21.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 15.9     |
| time/              |          |
|    total_timesteps | 358400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.6     |
|    ep_rew_mean     | 19       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 896      |
|    time_elapsed    | 99114    |
|    total_timesteps | 358400   |
---------------------------------
Eval num_timesteps=358600, episode_reward=22.96 +/- 14.18
Episode length: 33.60 +/- 20.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.6         |
|    mean_reward          | 23           |
| time/                   |              |
|    total_timesteps      | 358600       |
| train/                  |              |
|    approx_kl            | 0.0029283315 |
|    clip_fraction        | 0.09         |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.574       |
|    explained_variance   | 0.0802       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.98         |
|    n_updates            | 8960         |
|    policy_gradient_loss | -0.00511     |
|    value_loss           | 3.01         |
------------------------------------------
Eval num_timesteps=358800, episode_reward=16.55 +/- 15.13
Episode length: 24.20 +/- 21.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 358800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.1     |
|    ep_rew_mean     | 19.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 897      |
|    time_elapsed    | 99219    |
|    total_timesteps | 358800   |
---------------------------------
Eval num_timesteps=359000, episode_reward=7.55 +/- 3.97
Episode length: 11.60 +/- 5.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 11.6        |
|    mean_reward          | 7.55        |
| time/                   |             |
|    total_timesteps      | 359000      |
| train/                  |             |
|    approx_kl            | 0.007934006 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.585      |
|    explained_variance   | 0.177       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.552       |
|    n_updates            | 8970        |
|    policy_gradient_loss | -0.00929    |
|    value_loss           | 2.09        |
-----------------------------------------
Eval num_timesteps=359200, episode_reward=15.76 +/- 15.77
Episode length: 23.20 +/- 21.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 15.8     |
| time/              |          |
|    total_timesteps | 359200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.1     |
|    ep_rew_mean     | 19.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 898      |
|    time_elapsed    | 99316    |
|    total_timesteps | 359200   |
---------------------------------
Eval num_timesteps=359400, episode_reward=8.44 +/- 12.53
Episode length: 13.20 +/- 18.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.2        |
|    mean_reward          | 8.44        |
| time/                   |             |
|    total_timesteps      | 359400      |
| train/                  |             |
|    approx_kl            | 0.003461469 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.577      |
|    explained_variance   | 0.299       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.508       |
|    n_updates            | 8980        |
|    policy_gradient_loss | -0.00679    |
|    value_loss           | 1.88        |
-----------------------------------------
Eval num_timesteps=359600, episode_reward=10.80 +/- 11.30
Episode length: 16.80 +/- 16.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | 10.8     |
| time/              |          |
|    total_timesteps | 359600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.4     |
|    ep_rew_mean     | 19.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 899      |
|    time_elapsed    | 99417    |
|    total_timesteps | 359600   |
---------------------------------
Eval num_timesteps=359800, episode_reward=4.36 +/- 1.42
Episode length: 7.00 +/- 1.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 7            |
|    mean_reward          | 4.36         |
| time/                   |              |
|    total_timesteps      | 359800       |
| train/                  |              |
|    approx_kl            | 0.0032155435 |
|    clip_fraction        | 0.139        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.591       |
|    explained_variance   | 0.269        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00647      |
|    n_updates            | 8990         |
|    policy_gradient_loss | -0.00617     |
|    value_loss           | 0.942        |
------------------------------------------
Eval num_timesteps=360000, episode_reward=19.43 +/- 13.11
Episode length: 29.00 +/- 18.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29       |
|    mean_reward     | 19.4     |
| time/              |          |
|    total_timesteps | 360000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.7     |
|    ep_rew_mean     | 19.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 900      |
|    time_elapsed    | 99517    |
|    total_timesteps | 360000   |
---------------------------------
Eval num_timesteps=360200, episode_reward=27.63 +/- 13.14
Episode length: 40.60 +/- 18.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 40.6        |
|    mean_reward          | 27.6        |
| time/                   |             |
|    total_timesteps      | 360200      |
| train/                  |             |
|    approx_kl            | 0.002527501 |
|    clip_fraction        | 0.0888      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.557      |
|    explained_variance   | 0.242       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.669       |
|    n_updates            | 9000        |
|    policy_gradient_loss | -0.00558    |
|    value_loss           | 2.58        |
-----------------------------------------
Eval num_timesteps=360400, episode_reward=15.82 +/- 16.19
Episode length: 22.80 +/- 22.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 15.8     |
| time/              |          |
|    total_timesteps | 360400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.2     |
|    ep_rew_mean     | 17.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 901      |
|    time_elapsed    | 99648    |
|    total_timesteps | 360400   |
---------------------------------
Eval num_timesteps=360600, episode_reward=14.12 +/- 13.15
Episode length: 20.80 +/- 18.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 20.8        |
|    mean_reward          | 14.1        |
| time/                   |             |
|    total_timesteps      | 360600      |
| train/                  |             |
|    approx_kl            | 0.003699979 |
|    clip_fraction        | 0.0748      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.573      |
|    explained_variance   | 0.483       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.932       |
|    n_updates            | 9010        |
|    policy_gradient_loss | -0.0083     |
|    value_loss           | 1.58        |
-----------------------------------------
Eval num_timesteps=360800, episode_reward=11.53 +/- 11.07
Episode length: 17.60 +/- 16.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | 11.5     |
| time/              |          |
|    total_timesteps | 360800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.5     |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 902      |
|    time_elapsed    | 99751    |
|    total_timesteps | 360800   |
---------------------------------
Eval num_timesteps=361000, episode_reward=17.54 +/- 14.23
Episode length: 26.00 +/- 20.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26           |
|    mean_reward          | 17.5         |
| time/                   |              |
|    total_timesteps      | 361000       |
| train/                  |              |
|    approx_kl            | 0.0029172082 |
|    clip_fraction        | 0.065        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.577       |
|    explained_variance   | 0.366        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.13         |
|    n_updates            | 9020         |
|    policy_gradient_loss | -0.00625     |
|    value_loss           | 1.79         |
------------------------------------------
Eval num_timesteps=361200, episode_reward=15.98 +/- 15.65
Episode length: 23.40 +/- 21.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 361200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.5     |
|    ep_rew_mean     | 16.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 903      |
|    time_elapsed    | 99884    |
|    total_timesteps | 361200   |
---------------------------------
Eval num_timesteps=361400, episode_reward=27.96 +/- 13.61
Episode length: 40.40 +/- 19.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 40.4         |
|    mean_reward          | 28           |
| time/                   |              |
|    total_timesteps      | 361400       |
| train/                  |              |
|    approx_kl            | 0.0035269798 |
|    clip_fraction        | 0.108        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.613       |
|    explained_variance   | 0.408        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.326        |
|    n_updates            | 9030         |
|    policy_gradient_loss | -0.00752     |
|    value_loss           | 1.19         |
------------------------------------------
Eval num_timesteps=361600, episode_reward=24.92 +/- 12.78
Episode length: 36.40 +/- 17.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 24.9     |
| time/              |          |
|    total_timesteps | 361600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.6     |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 904      |
|    time_elapsed    | 100012   |
|    total_timesteps | 361600   |
---------------------------------
Eval num_timesteps=361800, episode_reward=16.97 +/- 14.79
Episode length: 24.80 +/- 20.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.8         |
|    mean_reward          | 17           |
| time/                   |              |
|    total_timesteps      | 361800       |
| train/                  |              |
|    approx_kl            | 0.0028745309 |
|    clip_fraction        | 0.156        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.607       |
|    explained_variance   | 0.27         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.921        |
|    n_updates            | 9040         |
|    policy_gradient_loss | -0.00775     |
|    value_loss           | 1.91         |
------------------------------------------
Eval num_timesteps=362000, episode_reward=9.41 +/- 13.65
Episode length: 13.60 +/- 18.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.6     |
|    mean_reward     | 9.41     |
| time/              |          |
|    total_timesteps | 362000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.4     |
|    ep_rew_mean     | 16.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 905      |
|    time_elapsed    | 100117   |
|    total_timesteps | 362000   |
---------------------------------
Eval num_timesteps=362200, episode_reward=2.13 +/- 1.84
Episode length: 4.00 +/- 2.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 4            |
|    mean_reward          | 2.13         |
| time/                   |              |
|    total_timesteps      | 362200       |
| train/                  |              |
|    approx_kl            | 0.0035217716 |
|    clip_fraction        | 0.0911       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.578       |
|    explained_variance   | 0.457        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.52         |
|    n_updates            | 9050         |
|    policy_gradient_loss | -0.00831     |
|    value_loss           | 1.21         |
------------------------------------------
Eval num_timesteps=362400, episode_reward=28.48 +/- 12.01
Episode length: 41.40 +/- 17.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | 28.5     |
| time/              |          |
|    total_timesteps | 362400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.3     |
|    ep_rew_mean     | 17.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 906      |
|    time_elapsed    | 100224   |
|    total_timesteps | 362400   |
---------------------------------
Eval num_timesteps=362600, episode_reward=6.28 +/- 3.57
Episode length: 10.00 +/- 5.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 10           |
|    mean_reward          | 6.28         |
| time/                   |              |
|    total_timesteps      | 362600       |
| train/                  |              |
|    approx_kl            | 0.0026435803 |
|    clip_fraction        | 0.0931       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.591       |
|    explained_variance   | 0.444        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.25         |
|    n_updates            | 9060         |
|    policy_gradient_loss | -0.00638     |
|    value_loss           | 1.41         |
------------------------------------------
Eval num_timesteps=362800, episode_reward=11.51 +/- 12.73
Episode length: 16.60 +/- 16.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | 11.5     |
| time/              |          |
|    total_timesteps | 362800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.9     |
|    ep_rew_mean     | 17.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 907      |
|    time_elapsed    | 100317   |
|    total_timesteps | 362800   |
---------------------------------
Eval num_timesteps=363000, episode_reward=18.30 +/- 14.65
Episode length: 25.80 +/- 19.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 25.8        |
|    mean_reward          | 18.3        |
| time/                   |             |
|    total_timesteps      | 363000      |
| train/                  |             |
|    approx_kl            | 0.004470295 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.582      |
|    explained_variance   | 0.119       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.82        |
|    n_updates            | 9070        |
|    policy_gradient_loss | -0.00831    |
|    value_loss           | 1.97        |
-----------------------------------------
Eval num_timesteps=363200, episode_reward=20.09 +/- 15.12
Episode length: 29.00 +/- 20.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29       |
|    mean_reward     | 20.1     |
| time/              |          |
|    total_timesteps | 363200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.6     |
|    ep_rew_mean     | 18.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 908      |
|    time_elapsed    | 100429   |
|    total_timesteps | 363200   |
---------------------------------
Eval num_timesteps=363400, episode_reward=24.56 +/- 13.54
Episode length: 35.00 +/- 18.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35          |
|    mean_reward          | 24.6        |
| time/                   |             |
|    total_timesteps      | 363400      |
| train/                  |             |
|    approx_kl            | 0.003148096 |
|    clip_fraction        | 0.0933      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.613      |
|    explained_variance   | 0.385       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.358       |
|    n_updates            | 9080        |
|    policy_gradient_loss | -0.00592    |
|    value_loss           | 2.01        |
-----------------------------------------
Eval num_timesteps=363600, episode_reward=16.43 +/- 16.57
Episode length: 23.00 +/- 22.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 16.4     |
| time/              |          |
|    total_timesteps | 363600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.4     |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 909      |
|    time_elapsed    | 100537   |
|    total_timesteps | 363600   |
---------------------------------
Eval num_timesteps=363800, episode_reward=22.31 +/- 14.96
Episode length: 32.80 +/- 21.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.8         |
|    mean_reward          | 22.3         |
| time/                   |              |
|    total_timesteps      | 363800       |
| train/                  |              |
|    approx_kl            | 0.0041113347 |
|    clip_fraction        | 0.0946       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.565       |
|    explained_variance   | 0.575        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.8          |
|    n_updates            | 9090         |
|    policy_gradient_loss | -0.00704     |
|    value_loss           | 1.6          |
------------------------------------------
Eval num_timesteps=364000, episode_reward=11.89 +/- 12.05
Episode length: 17.40 +/- 16.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 11.9     |
| time/              |          |
|    total_timesteps | 364000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | 17.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 910      |
|    time_elapsed    | 100649   |
|    total_timesteps | 364000   |
---------------------------------
Eval num_timesteps=364200, episode_reward=20.38 +/- 13.38
Episode length: 29.20 +/- 17.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29.2         |
|    mean_reward          | 20.4         |
| time/                   |              |
|    total_timesteps      | 364200       |
| train/                  |              |
|    approx_kl            | 0.0045913807 |
|    clip_fraction        | 0.135        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.597       |
|    explained_variance   | 0.559        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.421        |
|    n_updates            | 9100         |
|    policy_gradient_loss | -0.0103      |
|    value_loss           | 0.895        |
------------------------------------------
Eval num_timesteps=364400, episode_reward=17.04 +/- 14.71
Episode length: 25.00 +/- 20.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 364400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.2     |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 911      |
|    time_elapsed    | 100767   |
|    total_timesteps | 364400   |
---------------------------------
Eval num_timesteps=364600, episode_reward=23.85 +/- 14.48
Episode length: 34.00 +/- 19.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34          |
|    mean_reward          | 23.8        |
| time/                   |             |
|    total_timesteps      | 364600      |
| train/                  |             |
|    approx_kl            | 0.002214105 |
|    clip_fraction        | 0.0674      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.586      |
|    explained_variance   | 0.432       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.45        |
|    n_updates            | 9110        |
|    policy_gradient_loss | -0.00694    |
|    value_loss           | 2.48        |
-----------------------------------------
Eval num_timesteps=364800, episode_reward=10.30 +/- 11.80
Episode length: 15.80 +/- 17.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | 10.3     |
| time/              |          |
|    total_timesteps | 364800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.7     |
|    ep_rew_mean     | 16.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 912      |
|    time_elapsed    | 100873   |
|    total_timesteps | 364800   |
---------------------------------
Eval num_timesteps=365000, episode_reward=22.72 +/- 14.89
Episode length: 33.00 +/- 20.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33          |
|    mean_reward          | 22.7        |
| time/                   |             |
|    total_timesteps      | 365000      |
| train/                  |             |
|    approx_kl            | 0.004581384 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.6        |
|    explained_variance   | 0.524       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.293       |
|    n_updates            | 9120        |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 1.66        |
-----------------------------------------
Eval num_timesteps=365200, episode_reward=10.10 +/- 13.56
Episode length: 14.80 +/- 18.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 10.1     |
| time/              |          |
|    total_timesteps | 365200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23       |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 913      |
|    time_elapsed    | 100980   |
|    total_timesteps | 365200   |
---------------------------------
Eval num_timesteps=365400, episode_reward=4.92 +/- 2.12
Episode length: 7.80 +/- 2.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 7.8          |
|    mean_reward          | 4.92         |
| time/                   |              |
|    total_timesteps      | 365400       |
| train/                  |              |
|    approx_kl            | 0.0025430773 |
|    clip_fraction        | 0.0929       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.6         |
|    explained_variance   | 0.222        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.8          |
|    n_updates            | 9130         |
|    policy_gradient_loss | -0.00776     |
|    value_loss           | 2.6          |
------------------------------------------
Eval num_timesteps=365600, episode_reward=18.93 +/- 12.73
Episode length: 28.00 +/- 18.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28       |
|    mean_reward     | 18.9     |
| time/              |          |
|    total_timesteps | 365600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.8     |
|    ep_rew_mean     | 16.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 914      |
|    time_elapsed    | 101072   |
|    total_timesteps | 365600   |
---------------------------------
Eval num_timesteps=365800, episode_reward=9.16 +/- 12.23
Episode length: 14.20 +/- 18.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.2        |
|    mean_reward          | 9.16        |
| time/                   |             |
|    total_timesteps      | 365800      |
| train/                  |             |
|    approx_kl            | 0.005152614 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.593      |
|    explained_variance   | 0.441       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.893       |
|    n_updates            | 9140        |
|    policy_gradient_loss | -0.00678    |
|    value_loss           | 1.33        |
-----------------------------------------
Eval num_timesteps=366000, episode_reward=16.39 +/- 14.49
Episode length: 24.40 +/- 21.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 16.4     |
| time/              |          |
|    total_timesteps | 366000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.4     |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 915      |
|    time_elapsed    | 101180   |
|    total_timesteps | 366000   |
---------------------------------
Eval num_timesteps=366200, episode_reward=10.86 +/- 12.61
Episode length: 16.20 +/- 17.28
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.2       |
|    mean_reward          | 10.9       |
| time/                   |            |
|    total_timesteps      | 366200     |
| train/                  |            |
|    approx_kl            | 0.01182311 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.581     |
|    explained_variance   | 0.367      |
|    learning_rate        | 0.0001     |
|    loss                 | 1.09       |
|    n_updates            | 9150       |
|    policy_gradient_loss | -0.0109    |
|    value_loss           | 2.13       |
----------------------------------------
Eval num_timesteps=366400, episode_reward=10.49 +/- 11.76
Episode length: 16.20 +/- 17.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | 10.5     |
| time/              |          |
|    total_timesteps | 366400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.9     |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 916      |
|    time_elapsed    | 101288   |
|    total_timesteps | 366400   |
---------------------------------
Eval num_timesteps=366600, episode_reward=15.44 +/- 15.63
Episode length: 23.00 +/- 22.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23           |
|    mean_reward          | 15.4         |
| time/                   |              |
|    total_timesteps      | 366600       |
| train/                  |              |
|    approx_kl            | 0.0028360826 |
|    clip_fraction        | 0.0877       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.572       |
|    explained_variance   | 0.374        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.292        |
|    n_updates            | 9160         |
|    policy_gradient_loss | -0.00808     |
|    value_loss           | 2.35         |
------------------------------------------
Eval num_timesteps=366800, episode_reward=13.28 +/- 11.46
Episode length: 19.80 +/- 16.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.8     |
|    mean_reward     | 13.3     |
| time/              |          |
|    total_timesteps | 366800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.7     |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 917      |
|    time_elapsed    | 101393   |
|    total_timesteps | 366800   |
---------------------------------
Eval num_timesteps=367000, episode_reward=12.35 +/- 11.86
Episode length: 18.80 +/- 17.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.8        |
|    mean_reward          | 12.4        |
| time/                   |             |
|    total_timesteps      | 367000      |
| train/                  |             |
|    approx_kl            | 0.003363629 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.556      |
|    explained_variance   | 0.269       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.51        |
|    n_updates            | 9170        |
|    policy_gradient_loss | -0.00688    |
|    value_loss           | 2.15        |
-----------------------------------------
Eval num_timesteps=367200, episode_reward=16.79 +/- 14.97
Episode length: 24.60 +/- 20.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 367200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.1     |
|    ep_rew_mean     | 15.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 918      |
|    time_elapsed    | 101515   |
|    total_timesteps | 367200   |
---------------------------------
Eval num_timesteps=367400, episode_reward=12.20 +/- 12.34
Episode length: 17.60 +/- 16.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.6        |
|    mean_reward          | 12.2        |
| time/                   |             |
|    total_timesteps      | 367400      |
| train/                  |             |
|    approx_kl            | 0.006213438 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.557      |
|    explained_variance   | 0.501       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.364       |
|    n_updates            | 9180        |
|    policy_gradient_loss | -0.00972    |
|    value_loss           | 1.37        |
-----------------------------------------
Eval num_timesteps=367600, episode_reward=13.52 +/- 12.15
Episode length: 20.00 +/- 16.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20       |
|    mean_reward     | 13.5     |
| time/              |          |
|    total_timesteps | 367600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.7     |
|    ep_rew_mean     | 17       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 919      |
|    time_elapsed    | 101620   |
|    total_timesteps | 367600   |
---------------------------------
Eval num_timesteps=367800, episode_reward=11.08 +/- 12.93
Episode length: 16.00 +/- 17.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | 11.1         |
| time/                   |              |
|    total_timesteps      | 367800       |
| train/                  |              |
|    approx_kl            | 0.0028751458 |
|    clip_fraction        | 0.0975       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.571       |
|    explained_variance   | 0.417        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.894        |
|    n_updates            | 9190         |
|    policy_gradient_loss | -0.00682     |
|    value_loss           | 1.2          |
------------------------------------------
Eval num_timesteps=368000, episode_reward=21.10 +/- 16.39
Episode length: 31.00 +/- 23.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31       |
|    mean_reward     | 21.1     |
| time/              |          |
|    total_timesteps | 368000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.3     |
|    ep_rew_mean     | 17.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 920      |
|    time_elapsed    | 101746   |
|    total_timesteps | 368000   |
---------------------------------
Eval num_timesteps=368200, episode_reward=12.37 +/- 12.26
Episode length: 18.80 +/- 17.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.8        |
|    mean_reward          | 12.4        |
| time/                   |             |
|    total_timesteps      | 368200      |
| train/                  |             |
|    approx_kl            | 0.008789341 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.556      |
|    explained_variance   | 0.447       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.48        |
|    n_updates            | 9200        |
|    policy_gradient_loss | -0.00972    |
|    value_loss           | 0.912       |
-----------------------------------------
Eval num_timesteps=368400, episode_reward=15.48 +/- 14.68
Episode length: 23.60 +/- 21.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 15.5     |
| time/              |          |
|    total_timesteps | 368400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.9     |
|    ep_rew_mean     | 17.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 921      |
|    time_elapsed    | 101855   |
|    total_timesteps | 368400   |
---------------------------------
Eval num_timesteps=368600, episode_reward=22.78 +/- 15.38
Episode length: 32.60 +/- 21.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.6         |
|    mean_reward          | 22.8         |
| time/                   |              |
|    total_timesteps      | 368600       |
| train/                  |              |
|    approx_kl            | 0.0027167439 |
|    clip_fraction        | 0.141        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.571       |
|    explained_variance   | 0.277        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.778        |
|    n_updates            | 9210         |
|    policy_gradient_loss | -0.00672     |
|    value_loss           | 1.51         |
------------------------------------------
Eval num_timesteps=368800, episode_reward=23.52 +/- 14.36
Episode length: 33.80 +/- 19.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 23.5     |
| time/              |          |
|    total_timesteps | 368800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27       |
|    ep_rew_mean     | 18.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 922      |
|    time_elapsed    | 102009   |
|    total_timesteps | 368800   |
---------------------------------
Eval num_timesteps=369000, episode_reward=19.30 +/- 10.37
Episode length: 29.00 +/- 15.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29           |
|    mean_reward          | 19.3         |
| time/                   |              |
|    total_timesteps      | 369000       |
| train/                  |              |
|    approx_kl            | 0.0083809225 |
|    clip_fraction        | 0.188        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.593       |
|    explained_variance   | 0.0887       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.737        |
|    n_updates            | 9220         |
|    policy_gradient_loss | -0.0078      |
|    value_loss           | 1.49         |
------------------------------------------
Eval num_timesteps=369200, episode_reward=6.04 +/- 3.43
Episode length: 9.40 +/- 4.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.4      |
|    mean_reward     | 6.04     |
| time/              |          |
|    total_timesteps | 369200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.8     |
|    ep_rew_mean     | 19.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 923      |
|    time_elapsed    | 102118   |
|    total_timesteps | 369200   |
---------------------------------
Eval num_timesteps=369400, episode_reward=4.55 +/- 2.08
Episode length: 7.40 +/- 2.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 7.4          |
|    mean_reward          | 4.55         |
| time/                   |              |
|    total_timesteps      | 369400       |
| train/                  |              |
|    approx_kl            | 0.0036393125 |
|    clip_fraction        | 0.114        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.576       |
|    explained_variance   | -0.0439      |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0728       |
|    n_updates            | 9230         |
|    policy_gradient_loss | -0.00673     |
|    value_loss           | 0.612        |
------------------------------------------
Eval num_timesteps=369600, episode_reward=28.70 +/- 12.15
Episode length: 41.40 +/- 17.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | 28.7     |
| time/              |          |
|    total_timesteps | 369600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.5     |
|    ep_rew_mean     | 20.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 924      |
|    time_elapsed    | 102203   |
|    total_timesteps | 369600   |
---------------------------------
Eval num_timesteps=369800, episode_reward=16.95 +/- 14.89
Episode length: 24.60 +/- 20.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 24.6        |
|    mean_reward          | 16.9        |
| time/                   |             |
|    total_timesteps      | 369800      |
| train/                  |             |
|    approx_kl            | 0.004168748 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.587      |
|    explained_variance   | 0.142       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.511       |
|    n_updates            | 9240        |
|    policy_gradient_loss | -0.00936    |
|    value_loss           | 2.28        |
-----------------------------------------
Eval num_timesteps=370000, episode_reward=9.18 +/- 12.74
Episode length: 14.00 +/- 18.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14       |
|    mean_reward     | 9.18     |
| time/              |          |
|    total_timesteps | 370000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 20.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 925      |
|    time_elapsed    | 102313   |
|    total_timesteps | 370000   |
---------------------------------
Eval num_timesteps=370200, episode_reward=22.18 +/- 16.45
Episode length: 31.60 +/- 22.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 31.6         |
|    mean_reward          | 22.2         |
| time/                   |              |
|    total_timesteps      | 370200       |
| train/                  |              |
|    approx_kl            | 0.0048492705 |
|    clip_fraction        | 0.162        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.579       |
|    explained_variance   | 0.227        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.71         |
|    n_updates            | 9250         |
|    policy_gradient_loss | -0.0102      |
|    value_loss           | 2.35         |
------------------------------------------
Eval num_timesteps=370400, episode_reward=23.57 +/- 14.57
Episode length: 33.80 +/- 20.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 23.6     |
| time/              |          |
|    total_timesteps | 370400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 20.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 926      |
|    time_elapsed    | 102416   |
|    total_timesteps | 370400   |
---------------------------------
Eval num_timesteps=370600, episode_reward=17.19 +/- 15.57
Episode length: 24.40 +/- 21.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 24.4        |
|    mean_reward          | 17.2        |
| time/                   |             |
|    total_timesteps      | 370600      |
| train/                  |             |
|    approx_kl            | 0.002320363 |
|    clip_fraction        | 0.0714      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.564      |
|    explained_variance   | 0.219       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.32        |
|    n_updates            | 9260        |
|    policy_gradient_loss | -0.00556    |
|    value_loss           | 1.29        |
-----------------------------------------
Eval num_timesteps=370800, episode_reward=28.34 +/- 12.87
Episode length: 41.00 +/- 18.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41       |
|    mean_reward     | 28.3     |
| time/              |          |
|    total_timesteps | 370800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.7     |
|    ep_rew_mean     | 19.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 927      |
|    time_elapsed    | 102543   |
|    total_timesteps | 370800   |
---------------------------------
Eval num_timesteps=371000, episode_reward=15.54 +/- 15.11
Episode length: 23.40 +/- 21.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 23.4        |
|    mean_reward          | 15.5        |
| time/                   |             |
|    total_timesteps      | 371000      |
| train/                  |             |
|    approx_kl            | 0.004338667 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.562      |
|    explained_variance   | 0.382       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.06        |
|    n_updates            | 9270        |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 1.99        |
-----------------------------------------
Eval num_timesteps=371200, episode_reward=17.69 +/- 14.68
Episode length: 25.80 +/- 20.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | 17.7     |
| time/              |          |
|    total_timesteps | 371200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.5     |
|    ep_rew_mean     | 20.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 928      |
|    time_elapsed    | 102656   |
|    total_timesteps | 371200   |
---------------------------------
Eval num_timesteps=371400, episode_reward=18.89 +/- 14.13
Episode length: 27.20 +/- 19.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 27.2        |
|    mean_reward          | 18.9        |
| time/                   |             |
|    total_timesteps      | 371400      |
| train/                  |             |
|    approx_kl            | 0.003354102 |
|    clip_fraction        | 0.0888      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.56       |
|    explained_variance   | 0.311       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.224       |
|    n_updates            | 9280        |
|    policy_gradient_loss | -0.00748    |
|    value_loss           | 1.17        |
-----------------------------------------
Eval num_timesteps=371600, episode_reward=17.32 +/- 14.07
Episode length: 25.60 +/- 20.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 17.3     |
| time/              |          |
|    total_timesteps | 371600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.6     |
|    ep_rew_mean     | 19       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 929      |
|    time_elapsed    | 102789   |
|    total_timesteps | 371600   |
---------------------------------
Eval num_timesteps=371800, episode_reward=15.28 +/- 15.27
Episode length: 23.00 +/- 22.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23           |
|    mean_reward          | 15.3         |
| time/                   |              |
|    total_timesteps      | 371800       |
| train/                  |              |
|    approx_kl            | 0.0025909047 |
|    clip_fraction        | 0.094        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.561       |
|    explained_variance   | 0.278        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.33         |
|    n_updates            | 9290         |
|    policy_gradient_loss | -0.00893     |
|    value_loss           | 2.37         |
------------------------------------------
Eval num_timesteps=372000, episode_reward=18.60 +/- 14.27
Episode length: 27.00 +/- 19.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27       |
|    mean_reward     | 18.6     |
| time/              |          |
|    total_timesteps | 372000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.5     |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 930      |
|    time_elapsed    | 102893   |
|    total_timesteps | 372000   |
---------------------------------
Eval num_timesteps=372200, episode_reward=16.59 +/- 15.13
Episode length: 24.20 +/- 21.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.2         |
|    mean_reward          | 16.6         |
| time/                   |              |
|    total_timesteps      | 372200       |
| train/                  |              |
|    approx_kl            | 0.0041197347 |
|    clip_fraction        | 0.0828       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.547       |
|    explained_variance   | 0.226        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.571        |
|    n_updates            | 9300         |
|    policy_gradient_loss | -0.00649     |
|    value_loss           | 2.56         |
------------------------------------------
Eval num_timesteps=372400, episode_reward=14.36 +/- 10.75
Episode length: 21.40 +/- 15.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 14.4     |
| time/              |          |
|    total_timesteps | 372400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | 17       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 931      |
|    time_elapsed    | 102997   |
|    total_timesteps | 372400   |
---------------------------------
Eval num_timesteps=372600, episode_reward=23.46 +/- 14.48
Episode length: 33.60 +/- 20.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.6         |
|    mean_reward          | 23.5         |
| time/                   |              |
|    total_timesteps      | 372600       |
| train/                  |              |
|    approx_kl            | 0.0023330501 |
|    clip_fraction        | 0.0757       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.567       |
|    explained_variance   | 0.305        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.536        |
|    n_updates            | 9310         |
|    policy_gradient_loss | -0.00657     |
|    value_loss           | 1.94         |
------------------------------------------
Eval num_timesteps=372800, episode_reward=5.97 +/- 5.57
Episode length: 9.20 +/- 7.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.2      |
|    mean_reward     | 5.97     |
| time/              |          |
|    total_timesteps | 372800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24       |
|    ep_rew_mean     | 16.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 932      |
|    time_elapsed    | 103089   |
|    total_timesteps | 372800   |
---------------------------------
Eval num_timesteps=373000, episode_reward=15.42 +/- 16.07
Episode length: 22.60 +/- 22.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22.6         |
|    mean_reward          | 15.4         |
| time/                   |              |
|    total_timesteps      | 373000       |
| train/                  |              |
|    approx_kl            | 0.0029287871 |
|    clip_fraction        | 0.137        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.551       |
|    explained_variance   | 0.436        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.3          |
|    n_updates            | 9320         |
|    policy_gradient_loss | -0.009       |
|    value_loss           | 1.63         |
------------------------------------------
Eval num_timesteps=373200, episode_reward=22.20 +/- 16.44
Episode length: 31.60 +/- 22.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.6     |
|    mean_reward     | 22.2     |
| time/              |          |
|    total_timesteps | 373200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.4     |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 933      |
|    time_elapsed    | 103216   |
|    total_timesteps | 373200   |
---------------------------------
Eval num_timesteps=373400, episode_reward=23.16 +/- 16.15
Episode length: 32.40 +/- 21.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.4         |
|    mean_reward          | 23.2         |
| time/                   |              |
|    total_timesteps      | 373400       |
| train/                  |              |
|    approx_kl            | 0.0024140675 |
|    clip_fraction        | 0.0817       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.517       |
|    explained_variance   | 0.35         |
|    learning_rate        | 0.0001       |
|    loss                 | 1.21         |
|    n_updates            | 9330         |
|    policy_gradient_loss | -0.00608     |
|    value_loss           | 2.81         |
------------------------------------------
Eval num_timesteps=373600, episode_reward=34.92 +/- 1.13
Episode length: 50.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 34.9     |
| time/              |          |
|    total_timesteps | 373600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22       |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 934      |
|    time_elapsed    | 103346   |
|    total_timesteps | 373600   |
---------------------------------
Eval num_timesteps=373800, episode_reward=23.87 +/- 14.81
Episode length: 33.60 +/- 20.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.6         |
|    mean_reward          | 23.9         |
| time/                   |              |
|    total_timesteps      | 373800       |
| train/                  |              |
|    approx_kl            | 0.0031346083 |
|    clip_fraction        | 0.0844       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.558       |
|    explained_variance   | 0.439        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.683        |
|    n_updates            | 9340         |
|    policy_gradient_loss | -0.00613     |
|    value_loss           | 1.36         |
------------------------------------------
Eval num_timesteps=374000, episode_reward=15.66 +/- 14.51
Episode length: 23.80 +/- 21.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | 15.7     |
| time/              |          |
|    total_timesteps | 374000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.5     |
|    ep_rew_mean     | 15.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 935      |
|    time_elapsed    | 103479   |
|    total_timesteps | 374000   |
---------------------------------
Eval num_timesteps=374200, episode_reward=16.49 +/- 16.19
Episode length: 23.40 +/- 21.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.4         |
|    mean_reward          | 16.5         |
| time/                   |              |
|    total_timesteps      | 374200       |
| train/                  |              |
|    approx_kl            | 0.0033292098 |
|    clip_fraction        | 0.146        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.56        |
|    explained_variance   | 0.373        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.971        |
|    n_updates            | 9350         |
|    policy_gradient_loss | -0.00793     |
|    value_loss           | 1.44         |
------------------------------------------
Eval num_timesteps=374400, episode_reward=21.48 +/- 15.50
Episode length: 31.80 +/- 22.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.8     |
|    mean_reward     | 21.5     |
| time/              |          |
|    total_timesteps | 374400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.8     |
|    ep_rew_mean     | 15.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 936      |
|    time_elapsed    | 103586   |
|    total_timesteps | 374400   |
---------------------------------
Eval num_timesteps=374600, episode_reward=12.33 +/- 11.26
Episode length: 18.60 +/- 16.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.6        |
|    mean_reward          | 12.3        |
| time/                   |             |
|    total_timesteps      | 374600      |
| train/                  |             |
|    approx_kl            | 0.003148324 |
|    clip_fraction        | 0.0864      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.562      |
|    explained_variance   | 0.568       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.534       |
|    n_updates            | 9360        |
|    policy_gradient_loss | -0.00643    |
|    value_loss           | 1.08        |
-----------------------------------------
Eval num_timesteps=374800, episode_reward=12.67 +/- 12.74
Episode length: 18.40 +/- 17.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.4     |
|    mean_reward     | 12.7     |
| time/              |          |
|    total_timesteps | 374800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24       |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 937      |
|    time_elapsed    | 103697   |
|    total_timesteps | 374800   |
---------------------------------
Eval num_timesteps=375000, episode_reward=21.43 +/- 14.51
Episode length: 31.40 +/- 20.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 31.4         |
|    mean_reward          | 21.4         |
| time/                   |              |
|    total_timesteps      | 375000       |
| train/                  |              |
|    approx_kl            | 0.0038841318 |
|    clip_fraction        | 0.0739       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.571       |
|    explained_variance   | -0.107       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.03         |
|    n_updates            | 9370         |
|    policy_gradient_loss | -0.00375     |
|    value_loss           | 1.34         |
------------------------------------------
Eval num_timesteps=375200, episode_reward=9.15 +/- 12.73
Episode length: 14.00 +/- 18.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14       |
|    mean_reward     | 9.15     |
| time/              |          |
|    total_timesteps | 375200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | 17.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 938      |
|    time_elapsed    | 103799   |
|    total_timesteps | 375200   |
---------------------------------
Eval num_timesteps=375400, episode_reward=16.63 +/- 15.58
Episode length: 24.00 +/- 21.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24           |
|    mean_reward          | 16.6         |
| time/                   |              |
|    total_timesteps      | 375400       |
| train/                  |              |
|    approx_kl            | 0.0030549455 |
|    clip_fraction        | 0.0935       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.578       |
|    explained_variance   | 0.253        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.356        |
|    n_updates            | 9380         |
|    policy_gradient_loss | -0.00625     |
|    value_loss           | 1.37         |
------------------------------------------
Eval num_timesteps=375600, episode_reward=21.73 +/- 15.17
Episode length: 32.20 +/- 21.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 21.7     |
| time/              |          |
|    total_timesteps | 375600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.4     |
|    ep_rew_mean     | 18       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 939      |
|    time_elapsed    | 103907   |
|    total_timesteps | 375600   |
---------------------------------
Eval num_timesteps=375800, episode_reward=12.67 +/- 12.29
Episode length: 19.00 +/- 17.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 19         |
|    mean_reward          | 12.7       |
| time/                   |            |
|    total_timesteps      | 375800     |
| train/                  |            |
|    approx_kl            | 0.00425591 |
|    clip_fraction        | 0.09       |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.57      |
|    explained_variance   | 0.186      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.397      |
|    n_updates            | 9390       |
|    policy_gradient_loss | -0.00524   |
|    value_loss           | 1.85       |
----------------------------------------
Eval num_timesteps=376000, episode_reward=23.40 +/- 15.01
Episode length: 33.20 +/- 20.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | 23.4     |
| time/              |          |
|    total_timesteps | 376000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.6     |
|    ep_rew_mean     | 18.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 940      |
|    time_elapsed    | 104015   |
|    total_timesteps | 376000   |
---------------------------------
Eval num_timesteps=376200, episode_reward=22.99 +/- 15.03
Episode length: 33.00 +/- 20.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33           |
|    mean_reward          | 23           |
| time/                   |              |
|    total_timesteps      | 376200       |
| train/                  |              |
|    approx_kl            | 0.0026772602 |
|    clip_fraction        | 0.0982       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.575       |
|    explained_variance   | 0.268        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.131        |
|    n_updates            | 9400         |
|    policy_gradient_loss | -0.00627     |
|    value_loss           | 1.72         |
------------------------------------------
Eval num_timesteps=376400, episode_reward=22.85 +/- 16.09
Episode length: 32.20 +/- 21.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 22.9     |
| time/              |          |
|    total_timesteps | 376400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.1     |
|    ep_rew_mean     | 19.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 941      |
|    time_elapsed    | 104146   |
|    total_timesteps | 376400   |
---------------------------------
Eval num_timesteps=376600, episode_reward=10.76 +/- 12.64
Episode length: 16.00 +/- 17.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | 10.8        |
| time/                   |             |
|    total_timesteps      | 376600      |
| train/                  |             |
|    approx_kl            | 0.001856105 |
|    clip_fraction        | 0.0578      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.582      |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.334       |
|    n_updates            | 9410        |
|    policy_gradient_loss | -0.00579    |
|    value_loss           | 1.47        |
-----------------------------------------
Eval num_timesteps=376800, episode_reward=12.03 +/- 12.04
Episode length: 18.20 +/- 17.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | 12       |
| time/              |          |
|    total_timesteps | 376800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.4     |
|    ep_rew_mean     | 18       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 942      |
|    time_elapsed    | 104248   |
|    total_timesteps | 376800   |
---------------------------------
Eval num_timesteps=377000, episode_reward=10.26 +/- 12.93
Episode length: 15.40 +/- 17.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.4        |
|    mean_reward          | 10.3        |
| time/                   |             |
|    total_timesteps      | 377000      |
| train/                  |             |
|    approx_kl            | 0.006465891 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.557      |
|    explained_variance   | 0.466       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.814       |
|    n_updates            | 9420        |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 2           |
-----------------------------------------
Eval num_timesteps=377200, episode_reward=11.12 +/- 13.16
Episode length: 16.40 +/- 17.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | 11.1     |
| time/              |          |
|    total_timesteps | 377200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.1     |
|    ep_rew_mean     | 17.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 943      |
|    time_elapsed    | 104352   |
|    total_timesteps | 377200   |
---------------------------------
Eval num_timesteps=377400, episode_reward=21.37 +/- 13.08
Episode length: 30.40 +/- 17.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30.4         |
|    mean_reward          | 21.4         |
| time/                   |              |
|    total_timesteps      | 377400       |
| train/                  |              |
|    approx_kl            | 0.0014299591 |
|    clip_fraction        | 0.0705       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.57        |
|    explained_variance   | 0.265        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.32         |
|    n_updates            | 9430         |
|    policy_gradient_loss | -0.00573     |
|    value_loss           | 2.14         |
------------------------------------------
Eval num_timesteps=377600, episode_reward=23.20 +/- 14.89
Episode length: 33.40 +/- 20.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 23.2     |
| time/              |          |
|    total_timesteps | 377600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.2     |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 944      |
|    time_elapsed    | 104477   |
|    total_timesteps | 377600   |
---------------------------------
Eval num_timesteps=377800, episode_reward=18.03 +/- 13.28
Episode length: 26.80 +/- 19.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.8         |
|    mean_reward          | 18           |
| time/                   |              |
|    total_timesteps      | 377800       |
| train/                  |              |
|    approx_kl            | 0.0024639687 |
|    clip_fraction        | 0.067        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.569       |
|    explained_variance   | 0.208        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.91         |
|    n_updates            | 9440         |
|    policy_gradient_loss | -0.00361     |
|    value_loss           | 1.97         |
------------------------------------------
Eval num_timesteps=378000, episode_reward=17.90 +/- 14.83
Episode length: 26.00 +/- 20.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 378000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26       |
|    ep_rew_mean     | 17.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 945      |
|    time_elapsed    | 104583   |
|    total_timesteps | 378000   |
---------------------------------
Eval num_timesteps=378200, episode_reward=22.01 +/- 15.76
Episode length: 32.00 +/- 22.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32           |
|    mean_reward          | 22           |
| time/                   |              |
|    total_timesteps      | 378200       |
| train/                  |              |
|    approx_kl            | 0.0034829366 |
|    clip_fraction        | 0.0929       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.582       |
|    explained_variance   | 0.162        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.371        |
|    n_updates            | 9450         |
|    policy_gradient_loss | -0.00603     |
|    value_loss           | 1.59         |
------------------------------------------
Eval num_timesteps=378400, episode_reward=19.89 +/- 12.78
Episode length: 29.40 +/- 18.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.4     |
|    mean_reward     | 19.9     |
| time/              |          |
|    total_timesteps | 378400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27       |
|    ep_rew_mean     | 18.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 946      |
|    time_elapsed    | 104690   |
|    total_timesteps | 378400   |
---------------------------------
Eval num_timesteps=378600, episode_reward=10.22 +/- 12.74
Episode length: 15.00 +/- 17.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15           |
|    mean_reward          | 10.2         |
| time/                   |              |
|    total_timesteps      | 378600       |
| train/                  |              |
|    approx_kl            | 0.0050186156 |
|    clip_fraction        | 0.131        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.561       |
|    explained_variance   | 0.52         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.892        |
|    n_updates            | 9460         |
|    policy_gradient_loss | -0.00904     |
|    value_loss           | 1.36         |
------------------------------------------
Eval num_timesteps=378800, episode_reward=17.23 +/- 15.07
Episode length: 24.80 +/- 20.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 17.2     |
| time/              |          |
|    total_timesteps | 378800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26       |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 947      |
|    time_elapsed    | 104799   |
|    total_timesteps | 378800   |
---------------------------------
Eval num_timesteps=379000, episode_reward=21.21 +/- 13.51
Episode length: 30.80 +/- 19.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30.8         |
|    mean_reward          | 21.2         |
| time/                   |              |
|    total_timesteps      | 379000       |
| train/                  |              |
|    approx_kl            | 0.0030250992 |
|    clip_fraction        | 0.0556       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.562       |
|    explained_variance   | -0.00559     |
|    learning_rate        | 0.0001       |
|    loss                 | 1.04         |
|    n_updates            | 9470         |
|    policy_gradient_loss | -0.00393     |
|    value_loss           | 2.09         |
------------------------------------------
Eval num_timesteps=379200, episode_reward=22.34 +/- 14.50
Episode length: 33.00 +/- 20.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 22.3     |
| time/              |          |
|    total_timesteps | 379200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.2     |
|    ep_rew_mean     | 18.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 948      |
|    time_elapsed    | 104932   |
|    total_timesteps | 379200   |
---------------------------------
Eval num_timesteps=379400, episode_reward=15.82 +/- 15.33
Episode length: 23.40 +/- 21.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.4         |
|    mean_reward          | 15.8         |
| time/                   |              |
|    total_timesteps      | 379400       |
| train/                  |              |
|    approx_kl            | 0.0023910373 |
|    clip_fraction        | 0.0904       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.57        |
|    explained_variance   | 0.442        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.82         |
|    n_updates            | 9480         |
|    policy_gradient_loss | -0.00601     |
|    value_loss           | 1.71         |
------------------------------------------
Eval num_timesteps=379600, episode_reward=11.09 +/- 11.16
Episode length: 16.80 +/- 16.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | 11.1     |
| time/              |          |
|    total_timesteps | 379600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.7     |
|    ep_rew_mean     | 19.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 949      |
|    time_elapsed    | 105040   |
|    total_timesteps | 379600   |
---------------------------------
Eval num_timesteps=379800, episode_reward=13.01 +/- 10.96
Episode length: 19.60 +/- 16.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 19.6         |
|    mean_reward          | 13           |
| time/                   |              |
|    total_timesteps      | 379800       |
| train/                  |              |
|    approx_kl            | 0.0040473207 |
|    clip_fraction        | 0.121        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.57        |
|    explained_variance   | -0.0139      |
|    learning_rate        | 0.0001       |
|    loss                 | 0.552        |
|    n_updates            | 9490         |
|    policy_gradient_loss | -0.00845     |
|    value_loss           | 1.98         |
------------------------------------------
Eval num_timesteps=380000, episode_reward=26.39 +/- 11.47
Episode length: 37.60 +/- 15.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.6     |
|    mean_reward     | 26.4     |
| time/              |          |
|    total_timesteps | 380000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.7     |
|    ep_rew_mean     | 19.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 950      |
|    time_elapsed    | 105150   |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=380200, episode_reward=22.21 +/- 14.65
Episode length: 33.00 +/- 20.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33           |
|    mean_reward          | 22.2         |
| time/                   |              |
|    total_timesteps      | 380200       |
| train/                  |              |
|    approx_kl            | 0.0038625556 |
|    clip_fraction        | 0.144        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.564       |
|    explained_variance   | 0.263        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.457        |
|    n_updates            | 9500         |
|    policy_gradient_loss | -0.0108      |
|    value_loss           | 1.62         |
------------------------------------------
Eval num_timesteps=380400, episode_reward=11.51 +/- 10.97
Episode length: 17.60 +/- 16.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | 11.5     |
| time/              |          |
|    total_timesteps | 380400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.2     |
|    ep_rew_mean     | 19.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 951      |
|    time_elapsed    | 105278   |
|    total_timesteps | 380400   |
---------------------------------
Eval num_timesteps=380600, episode_reward=5.86 +/- 4.20
Episode length: 9.40 +/- 6.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 9.4          |
|    mean_reward          | 5.86         |
| time/                   |              |
|    total_timesteps      | 380600       |
| train/                  |              |
|    approx_kl            | 0.0029629145 |
|    clip_fraction        | 0.0813       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.545       |
|    explained_variance   | 0.325        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.676        |
|    n_updates            | 9510         |
|    policy_gradient_loss | -0.00846     |
|    value_loss           | 2.18         |
------------------------------------------
Eval num_timesteps=380800, episode_reward=17.45 +/- 14.22
Episode length: 25.80 +/- 20.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 380800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.2     |
|    ep_rew_mean     | 18.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 952      |
|    time_elapsed    | 105390   |
|    total_timesteps | 380800   |
---------------------------------
Eval num_timesteps=381000, episode_reward=15.94 +/- 15.22
Episode length: 23.60 +/- 21.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.6         |
|    mean_reward          | 15.9         |
| time/                   |              |
|    total_timesteps      | 381000       |
| train/                  |              |
|    approx_kl            | 0.0018107161 |
|    clip_fraction        | 0.0725       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.54        |
|    explained_variance   | 0.385        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.722        |
|    n_updates            | 9520         |
|    policy_gradient_loss | -0.00627     |
|    value_loss           | 2.45         |
------------------------------------------
Eval num_timesteps=381200, episode_reward=16.78 +/- 14.73
Episode length: 24.80 +/- 20.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 381200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.5     |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 953      |
|    time_elapsed    | 105504   |
|    total_timesteps | 381200   |
---------------------------------
Eval num_timesteps=381400, episode_reward=15.77 +/- 16.20
Episode length: 22.80 +/- 22.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22.8         |
|    mean_reward          | 15.8         |
| time/                   |              |
|    total_timesteps      | 381400       |
| train/                  |              |
|    approx_kl            | 0.0021615005 |
|    clip_fraction        | 0.102        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.571       |
|    explained_variance   | 0.181        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.16         |
|    n_updates            | 9530         |
|    policy_gradient_loss | -0.00593     |
|    value_loss           | 1.62         |
------------------------------------------
Eval num_timesteps=381600, episode_reward=16.82 +/- 15.89
Episode length: 24.00 +/- 21.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 381600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 954      |
|    time_elapsed    | 105609   |
|    total_timesteps | 381600   |
---------------------------------
Eval num_timesteps=381800, episode_reward=11.82 +/- 12.44
Episode length: 17.20 +/- 16.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.2         |
|    mean_reward          | 11.8         |
| time/                   |              |
|    total_timesteps      | 381800       |
| train/                  |              |
|    approx_kl            | 0.0045085857 |
|    clip_fraction        | 0.125        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.551       |
|    explained_variance   | 0.269        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.08         |
|    n_updates            | 9540         |
|    policy_gradient_loss | -0.00889     |
|    value_loss           | 2.79         |
------------------------------------------
Eval num_timesteps=382000, episode_reward=15.05 +/- 15.46
Episode length: 22.80 +/- 22.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 15       |
| time/              |          |
|    total_timesteps | 382000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | 16       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 955      |
|    time_elapsed    | 105716   |
|    total_timesteps | 382000   |
---------------------------------
Eval num_timesteps=382200, episode_reward=3.91 +/- 1.84
Episode length: 6.40 +/- 2.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 6.4          |
|    mean_reward          | 3.91         |
| time/                   |              |
|    total_timesteps      | 382200       |
| train/                  |              |
|    approx_kl            | 0.0035014877 |
|    clip_fraction        | 0.0944       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.556       |
|    explained_variance   | 0.452        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.795        |
|    n_updates            | 9550         |
|    policy_gradient_loss | -0.00674     |
|    value_loss           | 1.73         |
------------------------------------------
Eval num_timesteps=382400, episode_reward=23.33 +/- 14.27
Episode length: 33.80 +/- 20.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 23.3     |
| time/              |          |
|    total_timesteps | 382400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.5     |
|    ep_rew_mean     | 16       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 956      |
|    time_elapsed    | 105809   |
|    total_timesteps | 382400   |
---------------------------------
Eval num_timesteps=382600, episode_reward=4.68 +/- 2.88
Episode length: 7.60 +/- 4.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 7.6          |
|    mean_reward          | 4.68         |
| time/                   |              |
|    total_timesteps      | 382600       |
| train/                  |              |
|    approx_kl            | 0.0021845787 |
|    clip_fraction        | 0.0748       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.545       |
|    explained_variance   | 0.358        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.271        |
|    n_updates            | 9560         |
|    policy_gradient_loss | -0.00709     |
|    value_loss           | 1.72         |
------------------------------------------
Eval num_timesteps=382800, episode_reward=17.52 +/- 14.36
Episode length: 25.60 +/- 20.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 17.5     |
| time/              |          |
|    total_timesteps | 382800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.3     |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 957      |
|    time_elapsed    | 105897   |
|    total_timesteps | 382800   |
---------------------------------
Eval num_timesteps=383000, episode_reward=14.29 +/- 12.01
Episode length: 21.20 +/- 16.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 21.2         |
|    mean_reward          | 14.3         |
| time/                   |              |
|    total_timesteps      | 383000       |
| train/                  |              |
|    approx_kl            | 0.0037816707 |
|    clip_fraction        | 0.0862       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.522       |
|    explained_variance   | 0.499        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.534        |
|    n_updates            | 9570         |
|    policy_gradient_loss | -0.00793     |
|    value_loss           | 2.04         |
------------------------------------------
Eval num_timesteps=383200, episode_reward=17.09 +/- 13.85
Episode length: 25.60 +/- 20.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 17.1     |
| time/              |          |
|    total_timesteps | 383200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.2     |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 958      |
|    time_elapsed    | 106005   |
|    total_timesteps | 383200   |
---------------------------------
Eval num_timesteps=383400, episode_reward=21.86 +/- 16.36
Episode length: 31.40 +/- 22.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 31.4         |
|    mean_reward          | 21.9         |
| time/                   |              |
|    total_timesteps      | 383400       |
| train/                  |              |
|    approx_kl            | 0.0021765032 |
|    clip_fraction        | 0.0536       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.548       |
|    explained_variance   | 0.227        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.577        |
|    n_updates            | 9580         |
|    policy_gradient_loss | -0.00272     |
|    value_loss           | 1.87         |
------------------------------------------
Eval num_timesteps=383600, episode_reward=17.44 +/- 14.08
Episode length: 25.80 +/- 20.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 383600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.9     |
|    ep_rew_mean     | 15.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 959      |
|    time_elapsed    | 106114   |
|    total_timesteps | 383600   |
---------------------------------
Eval num_timesteps=383800, episode_reward=3.59 +/- 3.45
Episode length: 6.20 +/- 5.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 6.2          |
|    mean_reward          | 3.59         |
| time/                   |              |
|    total_timesteps      | 383800       |
| train/                  |              |
|    approx_kl            | 0.0050003785 |
|    clip_fraction        | 0.141        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.552       |
|    explained_variance   | 0.592        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.436        |
|    n_updates            | 9590         |
|    policy_gradient_loss | -0.0078      |
|    value_loss           | 1.21         |
------------------------------------------
Eval num_timesteps=384000, episode_reward=22.93 +/- 14.61
Episode length: 33.20 +/- 20.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | 22.9     |
| time/              |          |
|    total_timesteps | 384000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | 16.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 960      |
|    time_elapsed    | 106204   |
|    total_timesteps | 384000   |
---------------------------------
Eval num_timesteps=384200, episode_reward=12.33 +/- 11.95
Episode length: 18.40 +/- 16.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.4         |
|    mean_reward          | 12.3         |
| time/                   |              |
|    total_timesteps      | 384200       |
| train/                  |              |
|    approx_kl            | 0.0032209824 |
|    clip_fraction        | 0.0685       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.541       |
|    explained_variance   | 0.401        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.68         |
|    n_updates            | 9600         |
|    policy_gradient_loss | -0.006       |
|    value_loss           | 1.33         |
------------------------------------------
Eval num_timesteps=384400, episode_reward=9.59 +/- 13.64
Episode length: 14.00 +/- 18.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14       |
|    mean_reward     | 9.59     |
| time/              |          |
|    total_timesteps | 384400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | 17.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 961      |
|    time_elapsed    | 106318   |
|    total_timesteps | 384400   |
---------------------------------
Eval num_timesteps=384600, episode_reward=15.45 +/- 9.87
Episode length: 23.00 +/- 14.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23           |
|    mean_reward          | 15.5         |
| time/                   |              |
|    total_timesteps      | 384600       |
| train/                  |              |
|    approx_kl            | 0.0034432686 |
|    clip_fraction        | 0.129        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.562       |
|    explained_variance   | 0.334        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.329        |
|    n_updates            | 9610         |
|    policy_gradient_loss | -0.0103      |
|    value_loss           | 1.39         |
------------------------------------------
Eval num_timesteps=384800, episode_reward=10.32 +/- 12.16
Episode length: 15.60 +/- 17.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 10.3     |
| time/              |          |
|    total_timesteps | 384800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.3     |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 962      |
|    time_elapsed    | 106421   |
|    total_timesteps | 384800   |
---------------------------------
Eval num_timesteps=385000, episode_reward=28.55 +/- 12.40
Episode length: 41.20 +/- 17.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.2         |
|    mean_reward          | 28.6         |
| time/                   |              |
|    total_timesteps      | 385000       |
| train/                  |              |
|    approx_kl            | 0.0077038864 |
|    clip_fraction        | 0.16         |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.546       |
|    explained_variance   | 0.431        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.404        |
|    n_updates            | 9620         |
|    policy_gradient_loss | -0.0113      |
|    value_loss           | 1.66         |
------------------------------------------
Eval num_timesteps=385200, episode_reward=22.30 +/- 15.85
Episode length: 32.00 +/- 22.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32       |
|    mean_reward     | 22.3     |
| time/              |          |
|    total_timesteps | 385200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.4     |
|    ep_rew_mean     | 17.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 963      |
|    time_elapsed    | 106575   |
|    total_timesteps | 385200   |
---------------------------------
Eval num_timesteps=385400, episode_reward=10.08 +/- 12.26
Episode length: 15.40 +/- 17.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.4        |
|    mean_reward          | 10.1        |
| time/                   |             |
|    total_timesteps      | 385400      |
| train/                  |             |
|    approx_kl            | 0.003263751 |
|    clip_fraction        | 0.081       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.52       |
|    explained_variance   | 0.5         |
|    learning_rate        | 0.0001      |
|    loss                 | 1.02        |
|    n_updates            | 9630        |
|    policy_gradient_loss | -0.00634    |
|    value_loss           | 1.83        |
-----------------------------------------
Eval num_timesteps=385600, episode_reward=19.76 +/- 13.14
Episode length: 28.80 +/- 18.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.8     |
|    mean_reward     | 19.8     |
| time/              |          |
|    total_timesteps | 385600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | 17.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 964      |
|    time_elapsed    | 106681   |
|    total_timesteps | 385600   |
---------------------------------
Eval num_timesteps=385800, episode_reward=17.69 +/- 14.76
Episode length: 25.40 +/- 20.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.4         |
|    mean_reward          | 17.7         |
| time/                   |              |
|    total_timesteps      | 385800       |
| train/                  |              |
|    approx_kl            | 0.0029032773 |
|    clip_fraction        | 0.118        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.533       |
|    explained_variance   | 0.217        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.693        |
|    n_updates            | 9640         |
|    policy_gradient_loss | -0.00523     |
|    value_loss           | 1.51         |
------------------------------------------
Eval num_timesteps=386000, episode_reward=14.32 +/- 12.16
Episode length: 21.00 +/- 16.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | 14.3     |
| time/              |          |
|    total_timesteps | 386000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 965      |
|    time_elapsed    | 106790   |
|    total_timesteps | 386000   |
---------------------------------
Eval num_timesteps=386200, episode_reward=18.00 +/- 14.88
Episode length: 25.80 +/- 19.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.8         |
|    mean_reward          | 18           |
| time/                   |              |
|    total_timesteps      | 386200       |
| train/                  |              |
|    approx_kl            | 0.0031644874 |
|    clip_fraction        | 0.0989       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.502       |
|    explained_variance   | 0.461        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.781        |
|    n_updates            | 9650         |
|    policy_gradient_loss | -0.00478     |
|    value_loss           | 2.3          |
------------------------------------------
Eval num_timesteps=386400, episode_reward=15.16 +/- 16.24
Episode length: 22.20 +/- 22.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 15.2     |
| time/              |          |
|    total_timesteps | 386400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.6     |
|    ep_rew_mean     | 15.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 966      |
|    time_elapsed    | 106897   |
|    total_timesteps | 386400   |
---------------------------------
Eval num_timesteps=386600, episode_reward=11.07 +/- 12.10
Episode length: 16.80 +/- 17.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.8         |
|    mean_reward          | 11.1         |
| time/                   |              |
|    total_timesteps      | 386600       |
| train/                  |              |
|    approx_kl            | 0.0022519697 |
|    clip_fraction        | 0.0701       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.516       |
|    explained_variance   | 0.57         |
|    learning_rate        | 0.0001       |
|    loss                 | 1.38         |
|    n_updates            | 9660         |
|    policy_gradient_loss | -0.00597     |
|    value_loss           | 1.79         |
------------------------------------------
Eval num_timesteps=386800, episode_reward=27.81 +/- 11.55
Episode length: 39.60 +/- 15.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39.6     |
|    mean_reward     | 27.8     |
| time/              |          |
|    total_timesteps | 386800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23       |
|    ep_rew_mean     | 15.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 967      |
|    time_elapsed    | 107019   |
|    total_timesteps | 386800   |
---------------------------------
Eval num_timesteps=387000, episode_reward=11.81 +/- 12.04
Episode length: 18.40 +/- 17.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.4        |
|    mean_reward          | 11.8        |
| time/                   |             |
|    total_timesteps      | 387000      |
| train/                  |             |
|    approx_kl            | 0.007382791 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.519      |
|    explained_variance   | -0.0731     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.32        |
|    n_updates            | 9670        |
|    policy_gradient_loss | -0.00842    |
|    value_loss           | 1.31        |
-----------------------------------------
Eval num_timesteps=387200, episode_reward=22.87 +/- 13.80
Episode length: 34.00 +/- 19.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 22.9     |
| time/              |          |
|    total_timesteps | 387200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.7     |
|    ep_rew_mean     | 15.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 968      |
|    time_elapsed    | 107125   |
|    total_timesteps | 387200   |
---------------------------------
Eval num_timesteps=387400, episode_reward=12.26 +/- 12.18
Episode length: 18.00 +/- 16.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18           |
|    mean_reward          | 12.3         |
| time/                   |              |
|    total_timesteps      | 387400       |
| train/                  |              |
|    approx_kl            | 0.0042334674 |
|    clip_fraction        | 0.0906       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.49        |
|    explained_variance   | 0.563        |
|    learning_rate        | 0.0001       |
|    loss                 | 1            |
|    n_updates            | 9680         |
|    policy_gradient_loss | -0.00514     |
|    value_loss           | 1.38         |
------------------------------------------
Eval num_timesteps=387600, episode_reward=4.49 +/- 1.86
Episode length: 7.40 +/- 2.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7.4      |
|    mean_reward     | 4.49     |
| time/              |          |
|    total_timesteps | 387600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.4     |
|    ep_rew_mean     | 13.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 969      |
|    time_elapsed    | 107210   |
|    total_timesteps | 387600   |
---------------------------------
Eval num_timesteps=387800, episode_reward=3.37 +/- 1.67
Episode length: 5.60 +/- 2.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 5.6          |
|    mean_reward          | 3.37         |
| time/                   |              |
|    total_timesteps      | 387800       |
| train/                  |              |
|    approx_kl            | 0.0032789723 |
|    clip_fraction        | 0.114        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.502       |
|    explained_variance   | 0.37         |
|    learning_rate        | 0.0001       |
|    loss                 | 1.37         |
|    n_updates            | 9690         |
|    policy_gradient_loss | -0.00975     |
|    value_loss           | 2.86         |
------------------------------------------
Eval num_timesteps=388000, episode_reward=13.03 +/- 12.64
Episode length: 19.00 +/- 16.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 13       |
| time/              |          |
|    total_timesteps | 388000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.2     |
|    ep_rew_mean     | 14.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 970      |
|    time_elapsed    | 107296   |
|    total_timesteps | 388000   |
---------------------------------
Eval num_timesteps=388200, episode_reward=18.05 +/- 13.51
Episode length: 26.80 +/- 19.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.8         |
|    mean_reward          | 18           |
| time/                   |              |
|    total_timesteps      | 388200       |
| train/                  |              |
|    approx_kl            | 0.0023122956 |
|    clip_fraction        | 0.0783       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.492       |
|    explained_variance   | 0.289        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.306        |
|    n_updates            | 9700         |
|    policy_gradient_loss | -0.00618     |
|    value_loss           | 2.39         |
------------------------------------------
Eval num_timesteps=388400, episode_reward=19.28 +/- 12.99
Episode length: 28.00 +/- 18.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28       |
|    mean_reward     | 19.3     |
| time/              |          |
|    total_timesteps | 388400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.5     |
|    ep_rew_mean     | 13.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 971      |
|    time_elapsed    | 107414   |
|    total_timesteps | 388400   |
---------------------------------
Eval num_timesteps=388600, episode_reward=9.84 +/- 12.02
Episode length: 15.20 +/- 17.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.2         |
|    mean_reward          | 9.84         |
| time/                   |              |
|    total_timesteps      | 388600       |
| train/                  |              |
|    approx_kl            | 0.0029584528 |
|    clip_fraction        | 0.0871       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.49        |
|    explained_variance   | 0.498        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.01         |
|    n_updates            | 9710         |
|    policy_gradient_loss | -0.00685     |
|    value_loss           | 1.57         |
------------------------------------------
Eval num_timesteps=388800, episode_reward=29.69 +/- 10.79
Episode length: 42.60 +/- 14.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.6     |
|    mean_reward     | 29.7     |
| time/              |          |
|    total_timesteps | 388800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.5     |
|    ep_rew_mean     | 13.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 972      |
|    time_elapsed    | 107530   |
|    total_timesteps | 388800   |
---------------------------------
Eval num_timesteps=389000, episode_reward=11.72 +/- 12.53
Episode length: 17.20 +/- 16.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.2         |
|    mean_reward          | 11.7         |
| time/                   |              |
|    total_timesteps      | 389000       |
| train/                  |              |
|    approx_kl            | 0.0029758534 |
|    clip_fraction        | 0.0993       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.487       |
|    explained_variance   | 0.29         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.637        |
|    n_updates            | 9720         |
|    policy_gradient_loss | -0.00513     |
|    value_loss           | 2.28         |
------------------------------------------
Eval num_timesteps=389200, episode_reward=23.44 +/- 15.83
Episode length: 32.80 +/- 21.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 23.4     |
| time/              |          |
|    total_timesteps | 389200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.8     |
|    ep_rew_mean     | 12.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 973      |
|    time_elapsed    | 107635   |
|    total_timesteps | 389200   |
---------------------------------
Eval num_timesteps=389400, episode_reward=23.16 +/- 15.24
Episode length: 33.00 +/- 20.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33           |
|    mean_reward          | 23.2         |
| time/                   |              |
|    total_timesteps      | 389400       |
| train/                  |              |
|    approx_kl            | 0.0023261176 |
|    clip_fraction        | 0.0681       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.475       |
|    explained_variance   | 0.331        |
|    learning_rate        | 0.0001       |
|    loss                 | 2            |
|    n_updates            | 9730         |
|    policy_gradient_loss | -0.00575     |
|    value_loss           | 3.07         |
------------------------------------------
Eval num_timesteps=389600, episode_reward=9.41 +/- 12.61
Episode length: 14.20 +/- 18.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.2     |
|    mean_reward     | 9.41     |
| time/              |          |
|    total_timesteps | 389600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.9     |
|    ep_rew_mean     | 13.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 974      |
|    time_elapsed    | 107741   |
|    total_timesteps | 389600   |
---------------------------------
Eval num_timesteps=389800, episode_reward=15.90 +/- 16.11
Episode length: 23.00 +/- 22.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23           |
|    mean_reward          | 15.9         |
| time/                   |              |
|    total_timesteps      | 389800       |
| train/                  |              |
|    approx_kl            | 0.0038374395 |
|    clip_fraction        | 0.0873       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.5         |
|    explained_variance   | 0.467        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.695        |
|    n_updates            | 9740         |
|    policy_gradient_loss | -0.00748     |
|    value_loss           | 1.75         |
------------------------------------------
Eval num_timesteps=390000, episode_reward=4.40 +/- 2.68
Episode length: 7.40 +/- 3.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7.4      |
|    mean_reward     | 4.4      |
| time/              |          |
|    total_timesteps | 390000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.3     |
|    ep_rew_mean     | 13.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 975      |
|    time_elapsed    | 107829   |
|    total_timesteps | 390000   |
---------------------------------
Eval num_timesteps=390200, episode_reward=22.13 +/- 16.94
Episode length: 31.20 +/- 23.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 31.2         |
|    mean_reward          | 22.1         |
| time/                   |              |
|    total_timesteps      | 390200       |
| train/                  |              |
|    approx_kl            | 0.0048407963 |
|    clip_fraction        | 0.109        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.482       |
|    explained_variance   | 0.47         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.538        |
|    n_updates            | 9750         |
|    policy_gradient_loss | -0.0089      |
|    value_loss           | 1.88         |
------------------------------------------
Eval num_timesteps=390400, episode_reward=15.22 +/- 15.43
Episode length: 22.80 +/- 22.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 15.2     |
| time/              |          |
|    total_timesteps | 390400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.8     |
|    ep_rew_mean     | 13.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 976      |
|    time_elapsed    | 107938   |
|    total_timesteps | 390400   |
---------------------------------
Eval num_timesteps=390600, episode_reward=10.61 +/- 12.21
Episode length: 16.00 +/- 17.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16         |
|    mean_reward          | 10.6       |
| time/                   |            |
|    total_timesteps      | 390600     |
| train/                  |            |
|    approx_kl            | 0.00139951 |
|    clip_fraction        | 0.0578     |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.455     |
|    explained_variance   | 0.406      |
|    learning_rate        | 0.0001     |
|    loss                 | 1.43       |
|    n_updates            | 9760       |
|    policy_gradient_loss | -0.00466   |
|    value_loss           | 2.51       |
----------------------------------------
Eval num_timesteps=390800, episode_reward=22.69 +/- 15.89
Episode length: 32.20 +/- 21.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 22.7     |
| time/              |          |
|    total_timesteps | 390800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.4     |
|    ep_rew_mean     | 13.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 977      |
|    time_elapsed    | 108047   |
|    total_timesteps | 390800   |
---------------------------------
Eval num_timesteps=391000, episode_reward=17.37 +/- 15.02
Episode length: 24.80 +/- 20.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.8         |
|    mean_reward          | 17.4         |
| time/                   |              |
|    total_timesteps      | 391000       |
| train/                  |              |
|    approx_kl            | 0.0033140054 |
|    clip_fraction        | 0.0781       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.471       |
|    explained_variance   | 0.497        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.681        |
|    n_updates            | 9770         |
|    policy_gradient_loss | -0.00482     |
|    value_loss           | 1.7          |
------------------------------------------
Eval num_timesteps=391200, episode_reward=12.56 +/- 12.07
Episode length: 18.40 +/- 16.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.4     |
|    mean_reward     | 12.6     |
| time/              |          |
|    total_timesteps | 391200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.8     |
|    ep_rew_mean     | 14       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 978      |
|    time_elapsed    | 108150   |
|    total_timesteps | 391200   |
---------------------------------
Eval num_timesteps=391400, episode_reward=16.57 +/- 15.13
Episode length: 24.20 +/- 21.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.2         |
|    mean_reward          | 16.6         |
| time/                   |              |
|    total_timesteps      | 391400       |
| train/                  |              |
|    approx_kl            | 0.0039429516 |
|    clip_fraction        | 0.0888       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.452       |
|    explained_variance   | 0.265        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.748        |
|    n_updates            | 9780         |
|    policy_gradient_loss | -0.00684     |
|    value_loss           | 1.61         |
------------------------------------------
Eval num_timesteps=391600, episode_reward=10.14 +/- 11.69
Episode length: 15.80 +/- 17.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | 10.1     |
| time/              |          |
|    total_timesteps | 391600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | 13.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 979      |
|    time_elapsed    | 108255   |
|    total_timesteps | 391600   |
---------------------------------
Eval num_timesteps=391800, episode_reward=19.61 +/- 13.84
Episode length: 28.40 +/- 19.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 28.4         |
|    mean_reward          | 19.6         |
| time/                   |              |
|    total_timesteps      | 391800       |
| train/                  |              |
|    approx_kl            | 0.0064472402 |
|    clip_fraction        | 0.115        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.47        |
|    explained_variance   | 0.553        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.16         |
|    n_updates            | 9790         |
|    policy_gradient_loss | -0.00797     |
|    value_loss           | 1.65         |
------------------------------------------
Eval num_timesteps=392000, episode_reward=18.45 +/- 14.61
Episode length: 26.00 +/- 19.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | 18.4     |
| time/              |          |
|    total_timesteps | 392000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22       |
|    ep_rew_mean     | 14.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 980      |
|    time_elapsed    | 108378   |
|    total_timesteps | 392000   |
---------------------------------
Eval num_timesteps=392200, episode_reward=23.32 +/- 13.82
Episode length: 34.40 +/- 19.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.4         |
|    mean_reward          | 23.3         |
| time/                   |              |
|    total_timesteps      | 392200       |
| train/                  |              |
|    approx_kl            | 0.0024916795 |
|    clip_fraction        | 0.069        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.484       |
|    explained_variance   | 0.311        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.194        |
|    n_updates            | 9800         |
|    policy_gradient_loss | -0.00728     |
|    value_loss           | 1.89         |
------------------------------------------
Eval num_timesteps=392400, episode_reward=18.61 +/- 14.60
Episode length: 26.80 +/- 19.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.8     |
|    mean_reward     | 18.6     |
| time/              |          |
|    total_timesteps | 392400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 14.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 981      |
|    time_elapsed    | 108490   |
|    total_timesteps | 392400   |
---------------------------------
Eval num_timesteps=392600, episode_reward=17.73 +/- 14.72
Episode length: 25.60 +/- 20.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.6         |
|    mean_reward          | 17.7         |
| time/                   |              |
|    total_timesteps      | 392600       |
| train/                  |              |
|    approx_kl            | 0.0054870388 |
|    clip_fraction        | 0.105        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.502       |
|    explained_variance   | 0.531        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.18         |
|    n_updates            | 9810         |
|    policy_gradient_loss | -0.00696     |
|    value_loss           | 2            |
------------------------------------------
Eval num_timesteps=392800, episode_reward=24.94 +/- 13.91
Episode length: 35.60 +/- 18.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 24.9     |
| time/              |          |
|    total_timesteps | 392800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.9     |
|    ep_rew_mean     | 15.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 982      |
|    time_elapsed    | 108598   |
|    total_timesteps | 392800   |
---------------------------------
Eval num_timesteps=393000, episode_reward=21.63 +/- 15.75
Episode length: 31.80 +/- 22.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 31.8         |
|    mean_reward          | 21.6         |
| time/                   |              |
|    total_timesteps      | 393000       |
| train/                  |              |
|    approx_kl            | 0.0036587093 |
|    clip_fraction        | 0.0833       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.479       |
|    explained_variance   | 0.322        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.707        |
|    n_updates            | 9820         |
|    policy_gradient_loss | -0.00654     |
|    value_loss           | 2.29         |
------------------------------------------
Eval num_timesteps=393200, episode_reward=17.91 +/- 14.56
Episode length: 25.80 +/- 19.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 393200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | 16       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 983      |
|    time_elapsed    | 108706   |
|    total_timesteps | 393200   |
---------------------------------
Eval num_timesteps=393400, episode_reward=13.41 +/- 12.00
Episode length: 19.60 +/- 15.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 19.6         |
|    mean_reward          | 13.4         |
| time/                   |              |
|    total_timesteps      | 393400       |
| train/                  |              |
|    approx_kl            | 0.0033226337 |
|    clip_fraction        | 0.0926       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.515       |
|    explained_variance   | 0.356        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.103        |
|    n_updates            | 9830         |
|    policy_gradient_loss | -0.00715     |
|    value_loss           | 1.28         |
------------------------------------------
Eval num_timesteps=393600, episode_reward=24.98 +/- 12.33
Episode length: 36.40 +/- 17.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 25       |
| time/              |          |
|    total_timesteps | 393600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 984      |
|    time_elapsed    | 108809   |
|    total_timesteps | 393600   |
---------------------------------
Eval num_timesteps=393800, episode_reward=18.76 +/- 14.78
Episode length: 26.40 +/- 19.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.4         |
|    mean_reward          | 18.8         |
| time/                   |              |
|    total_timesteps      | 393800       |
| train/                  |              |
|    approx_kl            | 0.0034843772 |
|    clip_fraction        | 0.0788       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.505       |
|    explained_variance   | 0.393        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.52         |
|    n_updates            | 9840         |
|    policy_gradient_loss | -0.00594     |
|    value_loss           | 1.63         |
------------------------------------------
Eval num_timesteps=394000, episode_reward=15.32 +/- 15.68
Episode length: 22.80 +/- 22.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 15.3     |
| time/              |          |
|    total_timesteps | 394000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 985      |
|    time_elapsed    | 108915   |
|    total_timesteps | 394000   |
---------------------------------
Eval num_timesteps=394200, episode_reward=28.21 +/- 13.13
Episode length: 40.80 +/- 18.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 40.8        |
|    mean_reward          | 28.2        |
| time/                   |             |
|    total_timesteps      | 394200      |
| train/                  |             |
|    approx_kl            | 0.008943586 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.501      |
|    explained_variance   | 0.203       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.01        |
|    n_updates            | 9850        |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 2.12        |
-----------------------------------------
Eval num_timesteps=394400, episode_reward=18.76 +/- 14.05
Episode length: 27.00 +/- 19.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27       |
|    mean_reward     | 18.8     |
| time/              |          |
|    total_timesteps | 394400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | 17.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 986      |
|    time_elapsed    | 109069   |
|    total_timesteps | 394400   |
---------------------------------
Eval num_timesteps=394600, episode_reward=16.26 +/- 16.26
Episode length: 23.20 +/- 21.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 23.2        |
|    mean_reward          | 16.3        |
| time/                   |             |
|    total_timesteps      | 394600      |
| train/                  |             |
|    approx_kl            | 0.006831325 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.509      |
|    explained_variance   | 0.453       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.36        |
|    n_updates            | 9860        |
|    policy_gradient_loss | -0.0155     |
|    value_loss           | 1.37        |
-----------------------------------------
Eval num_timesteps=394800, episode_reward=13.00 +/- 13.11
Episode length: 18.60 +/- 17.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 13       |
| time/              |          |
|    total_timesteps | 394800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26       |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 987      |
|    time_elapsed    | 109174   |
|    total_timesteps | 394800   |
---------------------------------
Eval num_timesteps=395000, episode_reward=18.30 +/- 13.27
Episode length: 27.00 +/- 18.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 27           |
|    mean_reward          | 18.3         |
| time/                   |              |
|    total_timesteps      | 395000       |
| train/                  |              |
|    approx_kl            | 0.0037728238 |
|    clip_fraction        | 0.11         |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.532       |
|    explained_variance   | 0.474        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.34         |
|    n_updates            | 9870         |
|    policy_gradient_loss | -0.00968     |
|    value_loss           | 1.05         |
------------------------------------------
Eval num_timesteps=395200, episode_reward=17.24 +/- 15.00
Episode length: 25.40 +/- 20.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | 17.2     |
| time/              |          |
|    total_timesteps | 395200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.8     |
|    ep_rew_mean     | 19.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 988      |
|    time_elapsed    | 109292   |
|    total_timesteps | 395200   |
---------------------------------
Eval num_timesteps=395400, episode_reward=14.55 +/- 13.47
Episode length: 21.80 +/- 19.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 21.8       |
|    mean_reward          | 14.6       |
| time/                   |            |
|    total_timesteps      | 395400     |
| train/                  |            |
|    approx_kl            | 0.00408616 |
|    clip_fraction        | 0.0962     |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.516     |
|    explained_variance   | 0.416      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0733     |
|    n_updates            | 9880       |
|    policy_gradient_loss | -0.00579   |
|    value_loss           | 1.14       |
----------------------------------------
Eval num_timesteps=395600, episode_reward=15.40 +/- 15.18
Episode length: 23.20 +/- 21.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 15.4     |
| time/              |          |
|    total_timesteps | 395600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.1     |
|    ep_rew_mean     | 20.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 989      |
|    time_elapsed    | 109398   |
|    total_timesteps | 395600   |
---------------------------------
Eval num_timesteps=395800, episode_reward=15.49 +/- 15.99
Episode length: 22.60 +/- 22.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 22.6        |
|    mean_reward          | 15.5        |
| time/                   |             |
|    total_timesteps      | 395800      |
| train/                  |             |
|    approx_kl            | 0.006821467 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.471      |
|    explained_variance   | 0.0119      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.895       |
|    n_updates            | 9890        |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 1.8         |
-----------------------------------------
Eval num_timesteps=396000, episode_reward=15.98 +/- 15.14
Episode length: 23.60 +/- 21.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 396000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.2     |
|    ep_rew_mean     | 19.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 990      |
|    time_elapsed    | 109525   |
|    total_timesteps | 396000   |
---------------------------------
Eval num_timesteps=396200, episode_reward=13.89 +/- 10.73
Episode length: 20.80 +/- 15.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 20.8         |
|    mean_reward          | 13.9         |
| time/                   |              |
|    total_timesteps      | 396200       |
| train/                  |              |
|    approx_kl            | 0.0024917368 |
|    clip_fraction        | 0.0955       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.49        |
|    explained_variance   | 0.351        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.92         |
|    n_updates            | 9900         |
|    policy_gradient_loss | -0.00465     |
|    value_loss           | 1.69         |
------------------------------------------
Eval num_timesteps=396400, episode_reward=17.58 +/- 14.27
Episode length: 25.80 +/- 20.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 396400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.5     |
|    ep_rew_mean     | 20.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 991      |
|    time_elapsed    | 109632   |
|    total_timesteps | 396400   |
---------------------------------
Eval num_timesteps=396600, episode_reward=13.18 +/- 11.58
Episode length: 19.00 +/- 16.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 19           |
|    mean_reward          | 13.2         |
| time/                   |              |
|    total_timesteps      | 396600       |
| train/                  |              |
|    approx_kl            | 0.0048889187 |
|    clip_fraction        | 0.107        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.505       |
|    explained_variance   | 0.148        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.323        |
|    n_updates            | 9910         |
|    policy_gradient_loss | -0.00665     |
|    value_loss           | 1.46         |
------------------------------------------
Eval num_timesteps=396800, episode_reward=15.82 +/- 11.62
Episode length: 22.80 +/- 15.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 15.8     |
| time/              |          |
|    total_timesteps | 396800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29       |
|    ep_rew_mean     | 20       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 992      |
|    time_elapsed    | 109734   |
|    total_timesteps | 396800   |
---------------------------------
Eval num_timesteps=397000, episode_reward=10.71 +/- 12.10
Episode length: 16.20 +/- 17.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.2        |
|    mean_reward          | 10.7        |
| time/                   |             |
|    total_timesteps      | 397000      |
| train/                  |             |
|    approx_kl            | 0.006028506 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.477      |
|    explained_variance   | 0.457       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.804       |
|    n_updates            | 9920        |
|    policy_gradient_loss | -0.00964    |
|    value_loss           | 1.26        |
-----------------------------------------
Eval num_timesteps=397200, episode_reward=17.51 +/- 15.07
Episode length: 25.20 +/- 20.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 17.5     |
| time/              |          |
|    total_timesteps | 397200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.9     |
|    ep_rew_mean     | 19.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 993      |
|    time_elapsed    | 109839   |
|    total_timesteps | 397200   |
---------------------------------
Eval num_timesteps=397400, episode_reward=11.15 +/- 12.43
Episode length: 16.80 +/- 16.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.8        |
|    mean_reward          | 11.1        |
| time/                   |             |
|    total_timesteps      | 397400      |
| train/                  |             |
|    approx_kl            | 0.005183637 |
|    clip_fraction        | 0.098       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.504      |
|    explained_variance   | 0.233       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.22        |
|    n_updates            | 9930        |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 2.27        |
-----------------------------------------
Eval num_timesteps=397600, episode_reward=22.36 +/- 14.88
Episode length: 32.80 +/- 21.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 22.4     |
| time/              |          |
|    total_timesteps | 397600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.5     |
|    ep_rew_mean     | 18.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 994      |
|    time_elapsed    | 109968   |
|    total_timesteps | 397600   |
---------------------------------
Eval num_timesteps=397800, episode_reward=17.19 +/- 14.33
Episode length: 25.40 +/- 20.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.4         |
|    mean_reward          | 17.2         |
| time/                   |              |
|    total_timesteps      | 397800       |
| train/                  |              |
|    approx_kl            | 0.0037324429 |
|    clip_fraction        | 0.0705       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.489       |
|    explained_variance   | 0.409        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.498        |
|    n_updates            | 9940         |
|    policy_gradient_loss | -0.00805     |
|    value_loss           | 1.76         |
------------------------------------------
Eval num_timesteps=398000, episode_reward=10.33 +/- 12.22
Episode length: 15.60 +/- 17.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 10.3     |
| time/              |          |
|    total_timesteps | 398000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.9     |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 995      |
|    time_elapsed    | 110078   |
|    total_timesteps | 398000   |
---------------------------------
Eval num_timesteps=398200, episode_reward=16.67 +/- 15.48
Episode length: 24.00 +/- 21.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24           |
|    mean_reward          | 16.7         |
| time/                   |              |
|    total_timesteps      | 398200       |
| train/                  |              |
|    approx_kl            | 0.0030054466 |
|    clip_fraction        | 0.108        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.498       |
|    explained_variance   | 0.264        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.625        |
|    n_updates            | 9950         |
|    policy_gradient_loss | -0.00827     |
|    value_loss           | 2.42         |
------------------------------------------
Eval num_timesteps=398400, episode_reward=16.77 +/- 15.14
Episode length: 24.60 +/- 21.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 398400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.4     |
|    ep_rew_mean     | 18.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 996      |
|    time_elapsed    | 110183   |
|    total_timesteps | 398400   |
---------------------------------
Eval num_timesteps=398600, episode_reward=3.93 +/- 1.35
Episode length: 6.40 +/- 2.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 6.4          |
|    mean_reward          | 3.93         |
| time/                   |              |
|    total_timesteps      | 398600       |
| train/                  |              |
|    approx_kl            | 0.0036583047 |
|    clip_fraction        | 0.164        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.487       |
|    explained_variance   | 0.165        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.219        |
|    n_updates            | 9960         |
|    policy_gradient_loss | -0.00863     |
|    value_loss           | 1.99         |
------------------------------------------
Eval num_timesteps=398800, episode_reward=5.25 +/- 3.06
Episode length: 8.40 +/- 4.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8.4      |
|    mean_reward     | 5.25     |
| time/              |          |
|    total_timesteps | 398800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.8     |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 997      |
|    time_elapsed    | 110249   |
|    total_timesteps | 398800   |
---------------------------------
Eval num_timesteps=399000, episode_reward=10.45 +/- 11.55
Episode length: 16.00 +/- 17.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | 10.5        |
| time/                   |             |
|    total_timesteps      | 399000      |
| train/                  |             |
|    approx_kl            | 0.003692413 |
|    clip_fraction        | 0.0866      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.483      |
|    explained_variance   | 0.617       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.39        |
|    n_updates            | 9970        |
|    policy_gradient_loss | -0.00728    |
|    value_loss           | 1.53        |
-----------------------------------------
Eval num_timesteps=399200, episode_reward=13.02 +/- 11.79
Episode length: 19.80 +/- 16.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.8     |
|    mean_reward     | 13       |
| time/              |          |
|    total_timesteps | 399200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.2     |
|    ep_rew_mean     | 16.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 998      |
|    time_elapsed    | 110351   |
|    total_timesteps | 399200   |
---------------------------------
Eval num_timesteps=399400, episode_reward=17.04 +/- 15.37
Episode length: 24.40 +/- 21.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.4         |
|    mean_reward          | 17           |
| time/                   |              |
|    total_timesteps      | 399400       |
| train/                  |              |
|    approx_kl            | 0.0035749387 |
|    clip_fraction        | 0.11         |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.478       |
|    explained_variance   | 0.0418       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.556        |
|    n_updates            | 9980         |
|    policy_gradient_loss | -0.00626     |
|    value_loss           | 1.16         |
------------------------------------------
Eval num_timesteps=399600, episode_reward=22.20 +/- 14.59
Episode length: 32.80 +/- 21.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 22.2     |
| time/              |          |
|    total_timesteps | 399600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26       |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 999      |
|    time_elapsed    | 110464   |
|    total_timesteps | 399600   |
---------------------------------
Eval num_timesteps=399800, episode_reward=22.29 +/- 15.46
Episode length: 32.40 +/- 21.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 32.4        |
|    mean_reward          | 22.3        |
| time/                   |             |
|    total_timesteps      | 399800      |
| train/                  |             |
|    approx_kl            | 0.002983354 |
|    clip_fraction        | 0.0694      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.417      |
|    explained_variance   | 0.0608      |
|    learning_rate        | 0.0001      |
|    loss                 | 1.43        |
|    n_updates            | 9990        |
|    policy_gradient_loss | -0.00634    |
|    value_loss           | 2.12        |
-----------------------------------------
Eval num_timesteps=400000, episode_reward=28.61 +/- 12.32
Episode length: 41.40 +/- 17.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | 28.6     |
| time/              |          |
|    total_timesteps | 400000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.3     |
|    ep_rew_mean     | 18.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1000     |
|    time_elapsed    | 110592   |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=400200, episode_reward=18.05 +/- 14.59
Episode length: 26.20 +/- 20.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 26.2        |
|    mean_reward          | 18.1        |
| time/                   |             |
|    total_timesteps      | 400200      |
| train/                  |             |
|    approx_kl            | 0.002759378 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.476      |
|    explained_variance   | 0.292       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.303       |
|    n_updates            | 10000       |
|    policy_gradient_loss | -0.00544    |
|    value_loss           | 1.78        |
-----------------------------------------
Eval num_timesteps=400400, episode_reward=11.56 +/- 11.73
Episode length: 17.20 +/- 16.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | 11.6     |
| time/              |          |
|    total_timesteps | 400400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.5     |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1001     |
|    time_elapsed    | 110713   |
|    total_timesteps | 400400   |
---------------------------------
Eval num_timesteps=400600, episode_reward=18.17 +/- 14.13
Episode length: 26.40 +/- 19.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.4         |
|    mean_reward          | 18.2         |
| time/                   |              |
|    total_timesteps      | 400600       |
| train/                  |              |
|    approx_kl            | 0.0038139846 |
|    clip_fraction        | 0.0576       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.456       |
|    explained_variance   | 0.394        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.762        |
|    n_updates            | 10010        |
|    policy_gradient_loss | -0.00654     |
|    value_loss           | 1.87         |
------------------------------------------
Eval num_timesteps=400800, episode_reward=21.96 +/- 16.73
Episode length: 31.20 +/- 23.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.2     |
|    mean_reward     | 22       |
| time/              |          |
|    total_timesteps | 400800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.4     |
|    ep_rew_mean     | 18.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1002     |
|    time_elapsed    | 110821   |
|    total_timesteps | 400800   |
---------------------------------
Eval num_timesteps=401000, episode_reward=29.66 +/- 10.23
Episode length: 42.80 +/- 14.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.8        |
|    mean_reward          | 29.7        |
| time/                   |             |
|    total_timesteps      | 401000      |
| train/                  |             |
|    approx_kl            | 0.002570328 |
|    clip_fraction        | 0.0772      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.469      |
|    explained_variance   | 0.374       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.798       |
|    n_updates            | 10020       |
|    policy_gradient_loss | -0.00607    |
|    value_loss           | 1.17        |
-----------------------------------------
Eval num_timesteps=401200, episode_reward=18.74 +/- 13.53
Episode length: 27.40 +/- 18.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.4     |
|    mean_reward     | 18.7     |
| time/              |          |
|    total_timesteps | 401200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.1     |
|    ep_rew_mean     | 18.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1003     |
|    time_elapsed    | 110974   |
|    total_timesteps | 401200   |
---------------------------------
Eval num_timesteps=401400, episode_reward=20.78 +/- 12.99
Episode length: 30.20 +/- 17.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.2        |
|    mean_reward          | 20.8        |
| time/                   |             |
|    total_timesteps      | 401400      |
| train/                  |             |
|    approx_kl            | 0.007941312 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.514      |
|    explained_variance   | 0.434       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.932       |
|    n_updates            | 10030       |
|    policy_gradient_loss | -0.00758    |
|    value_loss           | 1.76        |
-----------------------------------------
Eval num_timesteps=401600, episode_reward=1.86 +/- 1.14
Episode length: 3.60 +/- 1.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3.6      |
|    mean_reward     | 1.86     |
| time/              |          |
|    total_timesteps | 401600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.2     |
|    ep_rew_mean     | 17.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1004     |
|    time_elapsed    | 111060   |
|    total_timesteps | 401600   |
---------------------------------
Eval num_timesteps=401800, episode_reward=7.37 +/- 2.37
Episode length: 11.40 +/- 3.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 11.4        |
|    mean_reward          | 7.37        |
| time/                   |             |
|    total_timesteps      | 401800      |
| train/                  |             |
|    approx_kl            | 0.002423227 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.493      |
|    explained_variance   | 0.299       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.95        |
|    n_updates            | 10040       |
|    policy_gradient_loss | -0.00997    |
|    value_loss           | 3.09        |
-----------------------------------------
Eval num_timesteps=402000, episode_reward=17.49 +/- 14.76
Episode length: 25.60 +/- 20.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 17.5     |
| time/              |          |
|    total_timesteps | 402000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.7     |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1005     |
|    time_elapsed    | 111159   |
|    total_timesteps | 402000   |
---------------------------------
Eval num_timesteps=402200, episode_reward=7.40 +/- 5.76
Episode length: 11.60 +/- 8.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 11.6         |
|    mean_reward          | 7.4          |
| time/                   |              |
|    total_timesteps      | 402200       |
| train/                  |              |
|    approx_kl            | 0.0034577805 |
|    clip_fraction        | 0.081        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.455       |
|    explained_variance   | 0.588        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.01         |
|    n_updates            | 10050        |
|    policy_gradient_loss | -0.00683     |
|    value_loss           | 1.16         |
------------------------------------------
Eval num_timesteps=402400, episode_reward=27.33 +/- 9.74
Episode length: 38.80 +/- 13.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 38.8     |
|    mean_reward     | 27.3     |
| time/              |          |
|    total_timesteps | 402400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.8     |
|    ep_rew_mean     | 15.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1006     |
|    time_elapsed    | 111274   |
|    total_timesteps | 402400   |
---------------------------------
Eval num_timesteps=402600, episode_reward=17.07 +/- 14.24
Episode length: 25.00 +/- 20.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25           |
|    mean_reward          | 17.1         |
| time/                   |              |
|    total_timesteps      | 402600       |
| train/                  |              |
|    approx_kl            | 0.0027847346 |
|    clip_fraction        | 0.0748       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.446       |
|    explained_variance   | 0.483        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.142        |
|    n_updates            | 10060        |
|    policy_gradient_loss | -0.00753     |
|    value_loss           | 1.92         |
------------------------------------------
Eval num_timesteps=402800, episode_reward=16.27 +/- 14.04
Episode length: 24.80 +/- 20.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 16.3     |
| time/              |          |
|    total_timesteps | 402800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.4     |
|    ep_rew_mean     | 14.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1007     |
|    time_elapsed    | 111380   |
|    total_timesteps | 402800   |
---------------------------------
Eval num_timesteps=403000, episode_reward=17.19 +/- 14.25
Episode length: 25.40 +/- 20.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.4         |
|    mean_reward          | 17.2         |
| time/                   |              |
|    total_timesteps      | 403000       |
| train/                  |              |
|    approx_kl            | 0.0022937334 |
|    clip_fraction        | 0.0761       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.452       |
|    explained_variance   | 0.295        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.821        |
|    n_updates            | 10070        |
|    policy_gradient_loss | -0.00796     |
|    value_loss           | 2.47         |
------------------------------------------
Eval num_timesteps=403200, episode_reward=19.39 +/- 14.65
Episode length: 27.80 +/- 20.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.8     |
|    mean_reward     | 19.4     |
| time/              |          |
|    total_timesteps | 403200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.8     |
|    ep_rew_mean     | 14.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1008     |
|    time_elapsed    | 111488   |
|    total_timesteps | 403200   |
---------------------------------
Eval num_timesteps=403400, episode_reward=17.53 +/- 13.64
Episode length: 26.20 +/- 19.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.2         |
|    mean_reward          | 17.5         |
| time/                   |              |
|    total_timesteps      | 403400       |
| train/                  |              |
|    approx_kl            | 0.0025002253 |
|    clip_fraction        | 0.0478       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.392       |
|    explained_variance   | 0.15         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.875        |
|    n_updates            | 10080        |
|    policy_gradient_loss | -0.00672     |
|    value_loss           | 2.17         |
------------------------------------------
Eval num_timesteps=403600, episode_reward=23.90 +/- 13.89
Episode length: 34.20 +/- 19.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 23.9     |
| time/              |          |
|    total_timesteps | 403600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.6     |
|    ep_rew_mean     | 15.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1009     |
|    time_elapsed    | 111624   |
|    total_timesteps | 403600   |
---------------------------------
Eval num_timesteps=403800, episode_reward=11.06 +/- 11.91
Episode length: 16.80 +/- 16.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.8         |
|    mean_reward          | 11.1         |
| time/                   |              |
|    total_timesteps      | 403800       |
| train/                  |              |
|    approx_kl            | 0.0061926357 |
|    clip_fraction        | 0.0933       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.407       |
|    explained_variance   | 0.323        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0157       |
|    n_updates            | 10090        |
|    policy_gradient_loss | -0.0105      |
|    value_loss           | 1.27         |
------------------------------------------
Eval num_timesteps=404000, episode_reward=23.53 +/- 14.78
Episode length: 33.60 +/- 20.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 23.5     |
| time/              |          |
|    total_timesteps | 404000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.5     |
|    ep_rew_mean     | 16.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1010     |
|    time_elapsed    | 111727   |
|    total_timesteps | 404000   |
---------------------------------
Eval num_timesteps=404200, episode_reward=16.71 +/- 14.98
Episode length: 24.40 +/- 20.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 24.4      |
|    mean_reward          | 16.7      |
| time/                   |           |
|    total_timesteps      | 404200    |
| train/                  |           |
|    approx_kl            | 0.0020462 |
|    clip_fraction        | 0.0786    |
|    clip_range           | 0.1       |
|    entropy_loss         | -0.445    |
|    explained_variance   | 0.389     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.753     |
|    n_updates            | 10100     |
|    policy_gradient_loss | -0.00635  |
|    value_loss           | 1.63      |
---------------------------------------
Eval num_timesteps=404400, episode_reward=35.58 +/- 0.70
Episode length: 50.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 35.6     |
| time/              |          |
|    total_timesteps | 404400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.6     |
|    ep_rew_mean     | 16.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1011     |
|    time_elapsed    | 111854   |
|    total_timesteps | 404400   |
---------------------------------
Eval num_timesteps=404600, episode_reward=15.73 +/- 14.90
Episode length: 23.60 +/- 21.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.6         |
|    mean_reward          | 15.7         |
| time/                   |              |
|    total_timesteps      | 404600       |
| train/                  |              |
|    approx_kl            | 0.0036408033 |
|    clip_fraction        | 0.09         |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.454       |
|    explained_variance   | 0.359        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.606        |
|    n_updates            | 10110        |
|    policy_gradient_loss | -0.00955     |
|    value_loss           | 1.88         |
------------------------------------------
Eval num_timesteps=404800, episode_reward=17.67 +/- 15.10
Episode length: 25.00 +/- 20.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | 17.7     |
| time/              |          |
|    total_timesteps | 404800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.4     |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1012     |
|    time_elapsed    | 112008   |
|    total_timesteps | 404800   |
---------------------------------
Eval num_timesteps=405000, episode_reward=4.80 +/- 3.40
Episode length: 7.60 +/- 4.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 7.6         |
|    mean_reward          | 4.8         |
| time/                   |             |
|    total_timesteps      | 405000      |
| train/                  |             |
|    approx_kl            | 0.003453424 |
|    clip_fraction        | 0.0987      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.472      |
|    explained_variance   | 0.136       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.16        |
|    n_updates            | 10120       |
|    policy_gradient_loss | -0.00883    |
|    value_loss           | 3.13        |
-----------------------------------------
Eval num_timesteps=405200, episode_reward=22.02 +/- 14.80
Episode length: 32.60 +/- 21.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.6     |
|    mean_reward     | 22       |
| time/              |          |
|    total_timesteps | 405200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.3     |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1013     |
|    time_elapsed    | 112098   |
|    total_timesteps | 405200   |
---------------------------------
Eval num_timesteps=405400, episode_reward=23.45 +/- 14.15
Episode length: 34.00 +/- 19.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34           |
|    mean_reward          | 23.4         |
| time/                   |              |
|    total_timesteps      | 405400       |
| train/                  |              |
|    approx_kl            | 0.0046524936 |
|    clip_fraction        | 0.127        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.437       |
|    explained_variance   | 0.32         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.721        |
|    n_updates            | 10130        |
|    policy_gradient_loss | -0.00812     |
|    value_loss           | 2.65         |
------------------------------------------
Eval num_timesteps=405600, episode_reward=20.34 +/- 12.79
Episode length: 30.20 +/- 18.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 20.3     |
| time/              |          |
|    total_timesteps | 405600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.5     |
|    ep_rew_mean     | 16       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1014     |
|    time_elapsed    | 112203   |
|    total_timesteps | 405600   |
---------------------------------
Eval num_timesteps=405800, episode_reward=15.05 +/- 15.46
Episode length: 22.80 +/- 22.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22.8         |
|    mean_reward          | 15           |
| time/                   |              |
|    total_timesteps      | 405800       |
| train/                  |              |
|    approx_kl            | 0.0034376346 |
|    clip_fraction        | 0.0797       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.471       |
|    explained_variance   | 0.377        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.34         |
|    n_updates            | 10140        |
|    policy_gradient_loss | -0.00724     |
|    value_loss           | 2.22         |
------------------------------------------
Eval num_timesteps=406000, episode_reward=17.04 +/- 14.55
Episode length: 25.40 +/- 20.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 406000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.4     |
|    ep_rew_mean     | 14.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1015     |
|    time_elapsed    | 112312   |
|    total_timesteps | 406000   |
---------------------------------
Eval num_timesteps=406200, episode_reward=30.68 +/- 9.88
Episode length: 43.40 +/- 13.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 43.4        |
|    mean_reward          | 30.7        |
| time/                   |             |
|    total_timesteps      | 406200      |
| train/                  |             |
|    approx_kl            | 0.001449989 |
|    clip_fraction        | 0.0587      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.402      |
|    explained_variance   | 0.428       |
|    learning_rate        | 0.0001      |
|    loss                 | 2           |
|    n_updates            | 10150       |
|    policy_gradient_loss | -0.00724    |
|    value_loss           | 1.86        |
-----------------------------------------
Eval num_timesteps=406400, episode_reward=17.27 +/- 13.63
Episode length: 25.80 +/- 19.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | 17.3     |
| time/              |          |
|    total_timesteps | 406400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.4     |
|    ep_rew_mean     | 14.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1016     |
|    time_elapsed    | 112446   |
|    total_timesteps | 406400   |
---------------------------------
Eval num_timesteps=406600, episode_reward=19.90 +/- 14.54
Episode length: 28.20 +/- 19.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 28.2         |
|    mean_reward          | 19.9         |
| time/                   |              |
|    total_timesteps      | 406600       |
| train/                  |              |
|    approx_kl            | 0.0019179119 |
|    clip_fraction        | 0.0696       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.442       |
|    explained_variance   | 0.608        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.321        |
|    n_updates            | 10160        |
|    policy_gradient_loss | -0.0063      |
|    value_loss           | 1.27         |
------------------------------------------
Eval num_timesteps=406800, episode_reward=10.98 +/- 12.06
Episode length: 16.60 +/- 17.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | 11       |
| time/              |          |
|    total_timesteps | 406800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22       |
|    ep_rew_mean     | 15       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1017     |
|    time_elapsed    | 112561   |
|    total_timesteps | 406800   |
---------------------------------
Eval num_timesteps=407000, episode_reward=8.49 +/- 6.29
Episode length: 12.80 +/- 8.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 12.8         |
|    mean_reward          | 8.49         |
| time/                   |              |
|    total_timesteps      | 407000       |
| train/                  |              |
|    approx_kl            | 0.0016210217 |
|    clip_fraction        | 0.0451       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.439       |
|    explained_variance   | 0.491        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.04         |
|    n_updates            | 10170        |
|    policy_gradient_loss | -0.00429     |
|    value_loss           | 1.57         |
------------------------------------------
Eval num_timesteps=407200, episode_reward=15.81 +/- 15.26
Episode length: 23.40 +/- 21.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 15.8     |
| time/              |          |
|    total_timesteps | 407200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.4     |
|    ep_rew_mean     | 13.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1018     |
|    time_elapsed    | 112652   |
|    total_timesteps | 407200   |
---------------------------------
Eval num_timesteps=407400, episode_reward=25.30 +/- 13.09
Episode length: 35.60 +/- 17.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.6         |
|    mean_reward          | 25.3         |
| time/                   |              |
|    total_timesteps      | 407400       |
| train/                  |              |
|    approx_kl            | 0.0012675493 |
|    clip_fraction        | 0.0507       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.377       |
|    explained_variance   | 0.488        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.35         |
|    n_updates            | 10180        |
|    policy_gradient_loss | -0.0057      |
|    value_loss           | 2.82         |
------------------------------------------
Eval num_timesteps=407600, episode_reward=12.36 +/- 10.93
Episode length: 18.60 +/- 16.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 12.4     |
| time/              |          |
|    total_timesteps | 407600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.4     |
|    ep_rew_mean     | 13.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1019     |
|    time_elapsed    | 112762   |
|    total_timesteps | 407600   |
---------------------------------
Eval num_timesteps=407800, episode_reward=15.82 +/- 15.74
Episode length: 23.20 +/- 21.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.2         |
|    mean_reward          | 15.8         |
| time/                   |              |
|    total_timesteps      | 407800       |
| train/                  |              |
|    approx_kl            | 0.0025129286 |
|    clip_fraction        | 0.0942       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.393       |
|    explained_variance   | 0.477        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.421        |
|    n_updates            | 10190        |
|    policy_gradient_loss | -0.00688     |
|    value_loss           | 1.55         |
------------------------------------------
Eval num_timesteps=408000, episode_reward=11.68 +/- 11.16
Episode length: 17.80 +/- 16.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | 11.7     |
| time/              |          |
|    total_timesteps | 408000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.1     |
|    ep_rew_mean     | 13.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1020     |
|    time_elapsed    | 112867   |
|    total_timesteps | 408000   |
---------------------------------
Eval num_timesteps=408200, episode_reward=16.09 +/- 15.46
Episode length: 23.40 +/- 21.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.4         |
|    mean_reward          | 16.1         |
| time/                   |              |
|    total_timesteps      | 408200       |
| train/                  |              |
|    approx_kl            | 0.0075523327 |
|    clip_fraction        | 0.176        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.447       |
|    explained_variance   | 0.277        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.97         |
|    n_updates            | 10200        |
|    policy_gradient_loss | -0.0124      |
|    value_loss           | 2.32         |
------------------------------------------
Eval num_timesteps=408400, episode_reward=28.92 +/- 12.26
Episode length: 41.40 +/- 17.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | 28.9     |
| time/              |          |
|    total_timesteps | 408400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.5     |
|    ep_rew_mean     | 14.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1021     |
|    time_elapsed    | 112997   |
|    total_timesteps | 408400   |
---------------------------------
Eval num_timesteps=408600, episode_reward=17.55 +/- 15.74
Episode length: 24.60 +/- 20.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.6         |
|    mean_reward          | 17.5         |
| time/                   |              |
|    total_timesteps      | 408600       |
| train/                  |              |
|    approx_kl            | 0.0022042769 |
|    clip_fraction        | 0.0455       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.421       |
|    explained_variance   | 0.326        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.03         |
|    n_updates            | 10210        |
|    policy_gradient_loss | -0.00577     |
|    value_loss           | 1.52         |
------------------------------------------
Eval num_timesteps=408800, episode_reward=15.25 +/- 14.88
Episode length: 23.40 +/- 21.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 15.3     |
| time/              |          |
|    total_timesteps | 408800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.5     |
|    ep_rew_mean     | 15.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1022     |
|    time_elapsed    | 113101   |
|    total_timesteps | 408800   |
---------------------------------
Eval num_timesteps=409000, episode_reward=18.04 +/- 14.47
Episode length: 26.00 +/- 19.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26           |
|    mean_reward          | 18           |
| time/                   |              |
|    total_timesteps      | 409000       |
| train/                  |              |
|    approx_kl            | 0.0073725428 |
|    clip_fraction        | 0.116        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.431       |
|    explained_variance   | 0.146        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0936       |
|    n_updates            | 10220        |
|    policy_gradient_loss | -0.00624     |
|    value_loss           | 1.7          |
------------------------------------------
Eval num_timesteps=409200, episode_reward=10.00 +/- 13.40
Episode length: 14.60 +/- 17.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 10       |
| time/              |          |
|    total_timesteps | 409200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.1     |
|    ep_rew_mean     | 15       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1023     |
|    time_elapsed    | 113210   |
|    total_timesteps | 409200   |
---------------------------------
Eval num_timesteps=409400, episode_reward=19.03 +/- 13.87
Episode length: 27.40 +/- 18.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 27.4         |
|    mean_reward          | 19           |
| time/                   |              |
|    total_timesteps      | 409400       |
| train/                  |              |
|    approx_kl            | 0.0012574886 |
|    clip_fraction        | 0.0578       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.377       |
|    explained_variance   | 0.327        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.647        |
|    n_updates            | 10230        |
|    policy_gradient_loss | -0.00588     |
|    value_loss           | 2.17         |
------------------------------------------
Eval num_timesteps=409600, episode_reward=7.45 +/- 5.00
Episode length: 11.80 +/- 7.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 11.8     |
|    mean_reward     | 7.45     |
| time/              |          |
|    total_timesteps | 409600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.8     |
|    ep_rew_mean     | 17       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1024     |
|    time_elapsed    | 113301   |
|    total_timesteps | 409600   |
---------------------------------
Eval num_timesteps=409800, episode_reward=18.17 +/- 12.97
Episode length: 27.20 +/- 18.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 27.2         |
|    mean_reward          | 18.2         |
| time/                   |              |
|    total_timesteps      | 409800       |
| train/                  |              |
|    approx_kl            | 0.0032369983 |
|    clip_fraction        | 0.104        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.43        |
|    explained_variance   | 0.196        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.356        |
|    n_updates            | 10240        |
|    policy_gradient_loss | -0.00684     |
|    value_loss           | 0.899        |
------------------------------------------
Eval num_timesteps=410000, episode_reward=20.85 +/- 12.93
Episode length: 30.00 +/- 18.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 20.8     |
| time/              |          |
|    total_timesteps | 410000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.6     |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1025     |
|    time_elapsed    | 113412   |
|    total_timesteps | 410000   |
---------------------------------
Eval num_timesteps=410200, episode_reward=15.73 +/- 15.84
Episode length: 23.00 +/- 22.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23           |
|    mean_reward          | 15.7         |
| time/                   |              |
|    total_timesteps      | 410200       |
| train/                  |              |
|    approx_kl            | 0.0044812933 |
|    clip_fraction        | 0.116        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.41        |
|    explained_variance   | 0.352        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.419        |
|    n_updates            | 10250        |
|    policy_gradient_loss | -0.00745     |
|    value_loss           | 1.8          |
------------------------------------------
Eval num_timesteps=410400, episode_reward=11.08 +/- 11.88
Episode length: 16.80 +/- 16.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | 11.1     |
| time/              |          |
|    total_timesteps | 410400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.1     |
|    ep_rew_mean     | 18.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1026     |
|    time_elapsed    | 113519   |
|    total_timesteps | 410400   |
---------------------------------
Eval num_timesteps=410600, episode_reward=25.31 +/- 12.85
Episode length: 36.00 +/- 17.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 36         |
|    mean_reward          | 25.3       |
| time/                   |            |
|    total_timesteps      | 410600     |
| train/                  |            |
|    approx_kl            | 0.00296952 |
|    clip_fraction        | 0.0772     |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.405     |
|    explained_variance   | 0.433      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.331      |
|    n_updates            | 10260      |
|    policy_gradient_loss | -0.00748   |
|    value_loss           | 1.34       |
----------------------------------------
Eval num_timesteps=410800, episode_reward=29.54 +/- 11.61
Episode length: 42.00 +/- 16.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 29.5     |
| time/              |          |
|    total_timesteps | 410800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.4     |
|    ep_rew_mean     | 19.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1027     |
|    time_elapsed    | 113672   |
|    total_timesteps | 410800   |
---------------------------------
Eval num_timesteps=411000, episode_reward=18.30 +/- 13.91
Episode length: 26.60 +/- 19.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.6         |
|    mean_reward          | 18.3         |
| time/                   |              |
|    total_timesteps      | 411000       |
| train/                  |              |
|    approx_kl            | 0.0022232258 |
|    clip_fraction        | 0.0658       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.412       |
|    explained_variance   | 0.161        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.844        |
|    n_updates            | 10270        |
|    policy_gradient_loss | -0.00593     |
|    value_loss           | 1.06         |
------------------------------------------
Eval num_timesteps=411200, episode_reward=4.87 +/- 2.69
Episode length: 8.00 +/- 3.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8        |
|    mean_reward     | 4.87     |
| time/              |          |
|    total_timesteps | 411200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.6     |
|    ep_rew_mean     | 18.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1028     |
|    time_elapsed    | 113760   |
|    total_timesteps | 411200   |
---------------------------------
Eval num_timesteps=411400, episode_reward=7.42 +/- 3.90
Episode length: 11.60 +/- 5.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 11.6         |
|    mean_reward          | 7.42         |
| time/                   |              |
|    total_timesteps      | 411400       |
| train/                  |              |
|    approx_kl            | 0.0034090837 |
|    clip_fraction        | 0.0837       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.409       |
|    explained_variance   | 0.47         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.977        |
|    n_updates            | 10280        |
|    policy_gradient_loss | -0.00757     |
|    value_loss           | 1.62         |
------------------------------------------
Eval num_timesteps=411600, episode_reward=18.70 +/- 13.80
Episode length: 27.00 +/- 18.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27       |
|    mean_reward     | 18.7     |
| time/              |          |
|    total_timesteps | 411600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.5     |
|    ep_rew_mean     | 18.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1029     |
|    time_elapsed    | 113854   |
|    total_timesteps | 411600   |
---------------------------------
Eval num_timesteps=411800, episode_reward=16.17 +/- 15.41
Episode length: 23.60 +/- 21.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 23.6        |
|    mean_reward          | 16.2        |
| time/                   |             |
|    total_timesteps      | 411800      |
| train/                  |             |
|    approx_kl            | 0.002215366 |
|    clip_fraction        | 0.0661      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.415      |
|    explained_variance   | 0.12        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.245       |
|    n_updates            | 10290       |
|    policy_gradient_loss | -0.0064     |
|    value_loss           | 2.7         |
-----------------------------------------
Eval num_timesteps=412000, episode_reward=22.51 +/- 12.29
Episode length: 33.00 +/- 17.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 22.5     |
| time/              |          |
|    total_timesteps | 412000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.6     |
|    ep_rew_mean     | 18.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1030     |
|    time_elapsed    | 113956   |
|    total_timesteps | 412000   |
---------------------------------
Eval num_timesteps=412200, episode_reward=12.39 +/- 11.88
Episode length: 18.80 +/- 16.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.8         |
|    mean_reward          | 12.4         |
| time/                   |              |
|    total_timesteps      | 412200       |
| train/                  |              |
|    approx_kl            | 0.0014631845 |
|    clip_fraction        | 0.0578       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.438       |
|    explained_variance   | 0.261        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.32         |
|    n_updates            | 10300        |
|    policy_gradient_loss | -0.00468     |
|    value_loss           | 2.32         |
------------------------------------------
Eval num_timesteps=412400, episode_reward=9.93 +/- 11.83
Episode length: 15.40 +/- 17.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | 9.93     |
| time/              |          |
|    total_timesteps | 412400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25       |
|    ep_rew_mean     | 17.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1031     |
|    time_elapsed    | 114060   |
|    total_timesteps | 412400   |
---------------------------------
Eval num_timesteps=412600, episode_reward=24.45 +/- 13.93
Episode length: 34.80 +/- 19.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.8         |
|    mean_reward          | 24.5         |
| time/                   |              |
|    total_timesteps      | 412600       |
| train/                  |              |
|    approx_kl            | 0.0017434807 |
|    clip_fraction        | 0.0489       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.399       |
|    explained_variance   | 0.51         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.437        |
|    n_updates            | 10310        |
|    policy_gradient_loss | -0.00491     |
|    value_loss           | 1.72         |
------------------------------------------
Eval num_timesteps=412800, episode_reward=22.86 +/- 13.95
Episode length: 33.80 +/- 20.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 22.9     |
| time/              |          |
|    total_timesteps | 412800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | 17.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1032     |
|    time_elapsed    | 114187   |
|    total_timesteps | 412800   |
---------------------------------
Eval num_timesteps=413000, episode_reward=21.23 +/- 16.24
Episode length: 31.20 +/- 23.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 31.2         |
|    mean_reward          | 21.2         |
| time/                   |              |
|    total_timesteps      | 413000       |
| train/                  |              |
|    approx_kl            | 0.0027252133 |
|    clip_fraction        | 0.0967       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.497       |
|    explained_variance   | 0.189        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.534        |
|    n_updates            | 10320        |
|    policy_gradient_loss | -0.00595     |
|    value_loss           | 2.52         |
------------------------------------------
Eval num_timesteps=413200, episode_reward=22.32 +/- 15.80
Episode length: 32.00 +/- 22.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32       |
|    mean_reward     | 22.3     |
| time/              |          |
|    total_timesteps | 413200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1033     |
|    time_elapsed    | 114316   |
|    total_timesteps | 413200   |
---------------------------------
Eval num_timesteps=413400, episode_reward=25.44 +/- 13.17
Episode length: 36.80 +/- 18.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.8         |
|    mean_reward          | 25.4         |
| time/                   |              |
|    total_timesteps      | 413400       |
| train/                  |              |
|    approx_kl            | 0.0037464094 |
|    clip_fraction        | 0.0862       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.414       |
|    explained_variance   | 0.533        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.677        |
|    n_updates            | 10330        |
|    policy_gradient_loss | -0.00506     |
|    value_loss           | 1.06         |
------------------------------------------
Eval num_timesteps=413600, episode_reward=30.43 +/- 9.24
Episode length: 43.80 +/- 12.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.8     |
|    mean_reward     | 30.4     |
| time/              |          |
|    total_timesteps | 413600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.9     |
|    ep_rew_mean     | 15.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1034     |
|    time_elapsed    | 114443   |
|    total_timesteps | 413600   |
---------------------------------
Eval num_timesteps=413800, episode_reward=16.52 +/- 15.21
Episode length: 24.00 +/- 21.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24           |
|    mean_reward          | 16.5         |
| time/                   |              |
|    total_timesteps      | 413800       |
| train/                  |              |
|    approx_kl            | 0.0026390005 |
|    clip_fraction        | 0.0688       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.422       |
|    explained_variance   | 0.502        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.61         |
|    n_updates            | 10340        |
|    policy_gradient_loss | -0.00895     |
|    value_loss           | 1.76         |
------------------------------------------
Eval num_timesteps=414000, episode_reward=23.10 +/- 13.62
Episode length: 34.20 +/- 19.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 23.1     |
| time/              |          |
|    total_timesteps | 414000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.7     |
|    ep_rew_mean     | 15.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1035     |
|    time_elapsed    | 114576   |
|    total_timesteps | 414000   |
---------------------------------
Eval num_timesteps=414200, episode_reward=17.62 +/- 14.25
Episode length: 25.80 +/- 19.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.8         |
|    mean_reward          | 17.6         |
| time/                   |              |
|    total_timesteps      | 414200       |
| train/                  |              |
|    approx_kl            | 0.0024644923 |
|    clip_fraction        | 0.104        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.43        |
|    explained_variance   | 0.216        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.3          |
|    n_updates            | 10350        |
|    policy_gradient_loss | -0.0064      |
|    value_loss           | 2.4          |
------------------------------------------
Eval num_timesteps=414400, episode_reward=16.53 +/- 14.34
Episode length: 24.80 +/- 20.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 414400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | 16.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1036     |
|    time_elapsed    | 114683   |
|    total_timesteps | 414400   |
---------------------------------
Eval num_timesteps=414600, episode_reward=10.70 +/- 13.32
Episode length: 15.80 +/- 18.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | 10.7         |
| time/                   |              |
|    total_timesteps      | 414600       |
| train/                  |              |
|    approx_kl            | 0.0070047653 |
|    clip_fraction        | 0.103        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.437       |
|    explained_variance   | 0.0648       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.4          |
|    n_updates            | 10360        |
|    policy_gradient_loss | -0.0112      |
|    value_loss           | 2.44         |
------------------------------------------
Eval num_timesteps=414800, episode_reward=15.39 +/- 15.20
Episode length: 23.20 +/- 21.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 15.4     |
| time/              |          |
|    total_timesteps | 414800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.6     |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1037     |
|    time_elapsed    | 114800   |
|    total_timesteps | 414800   |
---------------------------------
Eval num_timesteps=415000, episode_reward=18.45 +/- 13.64
Episode length: 27.00 +/- 18.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 27           |
|    mean_reward          | 18.4         |
| time/                   |              |
|    total_timesteps      | 415000       |
| train/                  |              |
|    approx_kl            | 0.0029801142 |
|    clip_fraction        | 0.119        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.433       |
|    explained_variance   | 0.292        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.525        |
|    n_updates            | 10370        |
|    policy_gradient_loss | -0.00895     |
|    value_loss           | 1.84         |
------------------------------------------
Eval num_timesteps=415200, episode_reward=11.71 +/- 11.26
Episode length: 18.00 +/- 16.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | 11.7     |
| time/              |          |
|    total_timesteps | 415200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25       |
|    ep_rew_mean     | 17.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1038     |
|    time_elapsed    | 114928   |
|    total_timesteps | 415200   |
---------------------------------
Eval num_timesteps=415400, episode_reward=14.57 +/- 11.60
Episode length: 21.00 +/- 15.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 21          |
|    mean_reward          | 14.6        |
| time/                   |             |
|    total_timesteps      | 415400      |
| train/                  |             |
|    approx_kl            | 0.003439013 |
|    clip_fraction        | 0.0973      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.479      |
|    explained_variance   | 0.266       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.17        |
|    n_updates            | 10380       |
|    policy_gradient_loss | -0.00839    |
|    value_loss           | 2.36        |
-----------------------------------------
Eval num_timesteps=415600, episode_reward=5.54 +/- 2.68
Episode length: 8.80 +/- 3.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8.8      |
|    mean_reward     | 5.54     |
| time/              |          |
|    total_timesteps | 415600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.2     |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1039     |
|    time_elapsed    | 115015   |
|    total_timesteps | 415600   |
---------------------------------
Eval num_timesteps=415800, episode_reward=22.94 +/- 14.26
Episode length: 33.60 +/- 20.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.6        |
|    mean_reward          | 22.9        |
| time/                   |             |
|    total_timesteps      | 415800      |
| train/                  |             |
|    approx_kl            | 0.005066083 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.427      |
|    explained_variance   | 0.359       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.373       |
|    n_updates            | 10390       |
|    policy_gradient_loss | -0.00923    |
|    value_loss           | 1.87        |
-----------------------------------------
Eval num_timesteps=416000, episode_reward=18.74 +/- 12.57
Episode length: 28.00 +/- 18.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28       |
|    mean_reward     | 18.7     |
| time/              |          |
|    total_timesteps | 416000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.1     |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1040     |
|    time_elapsed    | 115124   |
|    total_timesteps | 416000   |
---------------------------------
Eval num_timesteps=416200, episode_reward=10.73 +/- 12.02
Episode length: 16.40 +/- 17.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | 10.7         |
| time/                   |              |
|    total_timesteps      | 416200       |
| train/                  |              |
|    approx_kl            | 0.0025343727 |
|    clip_fraction        | 0.0835       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.498       |
|    explained_variance   | 0.16         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.886        |
|    n_updates            | 10400        |
|    policy_gradient_loss | -0.00458     |
|    value_loss           | 1.19         |
------------------------------------------
Eval num_timesteps=416400, episode_reward=11.85 +/- 12.64
Episode length: 17.60 +/- 17.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | 11.8     |
| time/              |          |
|    total_timesteps | 416400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.2     |
|    ep_rew_mean     | 18       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1041     |
|    time_elapsed    | 115229   |
|    total_timesteps | 416400   |
---------------------------------
Eval num_timesteps=416600, episode_reward=3.37 +/- 1.97
Episode length: 5.60 +/- 2.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 5.6          |
|    mean_reward          | 3.37         |
| time/                   |              |
|    total_timesteps      | 416600       |
| train/                  |              |
|    approx_kl            | 0.0028303596 |
|    clip_fraction        | 0.0804       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.443       |
|    explained_variance   | 0.459        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.207        |
|    n_updates            | 10410        |
|    policy_gradient_loss | -0.00802     |
|    value_loss           | 1.56         |
------------------------------------------
Eval num_timesteps=416800, episode_reward=12.41 +/- 12.88
Episode length: 18.20 +/- 17.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | 12.4     |
| time/              |          |
|    total_timesteps | 416800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.7     |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1042     |
|    time_elapsed    | 115311   |
|    total_timesteps | 416800   |
---------------------------------
Eval num_timesteps=417000, episode_reward=8.84 +/- 12.28
Episode length: 14.00 +/- 18.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14          |
|    mean_reward          | 8.84        |
| time/                   |             |
|    total_timesteps      | 417000      |
| train/                  |             |
|    approx_kl            | 0.002574595 |
|    clip_fraction        | 0.0917      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.456      |
|    explained_variance   | 0.448       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.313       |
|    n_updates            | 10420       |
|    policy_gradient_loss | -0.00474    |
|    value_loss           | 1.75        |
-----------------------------------------
Eval num_timesteps=417200, episode_reward=6.82 +/- 4.32
Episode length: 10.80 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10.8     |
|    mean_reward     | 6.82     |
| time/              |          |
|    total_timesteps | 417200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.2     |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1043     |
|    time_elapsed    | 115398   |
|    total_timesteps | 417200   |
---------------------------------
Eval num_timesteps=417400, episode_reward=15.96 +/- 15.60
Episode length: 23.40 +/- 21.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 23.4        |
|    mean_reward          | 16          |
| time/                   |             |
|    total_timesteps      | 417400      |
| train/                  |             |
|    approx_kl            | 0.005822698 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.485      |
|    explained_variance   | 0.532       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.616       |
|    n_updates            | 10430       |
|    policy_gradient_loss | -0.00712    |
|    value_loss           | 0.874       |
-----------------------------------------
Eval num_timesteps=417600, episode_reward=23.86 +/- 13.49
Episode length: 34.60 +/- 18.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 23.9     |
| time/              |          |
|    total_timesteps | 417600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.8     |
|    ep_rew_mean     | 18.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1044     |
|    time_elapsed    | 115506   |
|    total_timesteps | 417600   |
---------------------------------
Eval num_timesteps=417800, episode_reward=9.92 +/- 12.86
Episode length: 14.60 +/- 17.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.6        |
|    mean_reward          | 9.92        |
| time/                   |             |
|    total_timesteps      | 417800      |
| train/                  |             |
|    approx_kl            | 0.004205401 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.485      |
|    explained_variance   | 0.289       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.76        |
|    n_updates            | 10440       |
|    policy_gradient_loss | -0.00536    |
|    value_loss           | 1.53        |
-----------------------------------------
Eval num_timesteps=418000, episode_reward=16.78 +/- 15.42
Episode length: 24.20 +/- 21.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 418000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.6     |
|    ep_rew_mean     | 18.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1045     |
|    time_elapsed    | 115614   |
|    total_timesteps | 418000   |
---------------------------------
Eval num_timesteps=418200, episode_reward=27.94 +/- 13.65
Episode length: 40.40 +/- 19.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 40.4        |
|    mean_reward          | 27.9        |
| time/                   |             |
|    total_timesteps      | 418200      |
| train/                  |             |
|    approx_kl            | 0.002556529 |
|    clip_fraction        | 0.0944      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.472      |
|    explained_variance   | 0.121       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.66        |
|    n_updates            | 10450       |
|    policy_gradient_loss | -0.00543    |
|    value_loss           | 1.99        |
-----------------------------------------
Eval num_timesteps=418400, episode_reward=10.68 +/- 13.27
Episode length: 15.60 +/- 17.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 10.7     |
| time/              |          |
|    total_timesteps | 418400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28       |
|    ep_rew_mean     | 19.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1046     |
|    time_elapsed    | 115720   |
|    total_timesteps | 418400   |
---------------------------------
Eval num_timesteps=418600, episode_reward=23.35 +/- 14.12
Episode length: 33.80 +/- 19.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.8         |
|    mean_reward          | 23.4         |
| time/                   |              |
|    total_timesteps      | 418600       |
| train/                  |              |
|    approx_kl            | 0.0027301009 |
|    clip_fraction        | 0.0848       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.436       |
|    explained_variance   | 0.155        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.349        |
|    n_updates            | 10460        |
|    policy_gradient_loss | -0.00564     |
|    value_loss           | 1.87         |
------------------------------------------
Eval num_timesteps=418800, episode_reward=22.72 +/- 14.87
Episode length: 33.00 +/- 20.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 22.7     |
| time/              |          |
|    total_timesteps | 418800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.6     |
|    ep_rew_mean     | 18.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1047     |
|    time_elapsed    | 115831   |
|    total_timesteps | 418800   |
---------------------------------
Eval num_timesteps=419000, episode_reward=15.71 +/- 11.79
Episode length: 22.80 +/- 15.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22.8         |
|    mean_reward          | 15.7         |
| time/                   |              |
|    total_timesteps      | 419000       |
| train/                  |              |
|    approx_kl            | 0.0024164089 |
|    clip_fraction        | 0.0866       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.467       |
|    explained_variance   | 0.286        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.29         |
|    n_updates            | 10470        |
|    policy_gradient_loss | -0.00724     |
|    value_loss           | 2.74         |
------------------------------------------
Eval num_timesteps=419200, episode_reward=10.09 +/- 13.49
Episode length: 14.60 +/- 17.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 10.1     |
| time/              |          |
|    total_timesteps | 419200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.5     |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1048     |
|    time_elapsed    | 115940   |
|    total_timesteps | 419200   |
---------------------------------
Eval num_timesteps=419400, episode_reward=28.77 +/- 12.00
Episode length: 41.60 +/- 16.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.6         |
|    mean_reward          | 28.8         |
| time/                   |              |
|    total_timesteps      | 419400       |
| train/                  |              |
|    approx_kl            | 0.0053331116 |
|    clip_fraction        | 0.0946       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.48        |
|    explained_variance   | 0.35         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.827        |
|    n_updates            | 10480        |
|    policy_gradient_loss | -0.00897     |
|    value_loss           | 2.85         |
------------------------------------------
Eval num_timesteps=419600, episode_reward=22.67 +/- 15.41
Episode length: 32.60 +/- 21.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.6     |
|    mean_reward     | 22.7     |
| time/              |          |
|    total_timesteps | 419600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | 16       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1049     |
|    time_elapsed    | 116046   |
|    total_timesteps | 419600   |
---------------------------------
Eval num_timesteps=419800, episode_reward=20.26 +/- 13.59
Episode length: 29.40 +/- 18.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29.4         |
|    mean_reward          | 20.3         |
| time/                   |              |
|    total_timesteps      | 419800       |
| train/                  |              |
|    approx_kl            | 0.0016499662 |
|    clip_fraction        | 0.0725       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.461       |
|    explained_variance   | 0.412        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.26         |
|    n_updates            | 10490        |
|    policy_gradient_loss | -0.00672     |
|    value_loss           | 2.29         |
------------------------------------------
Eval num_timesteps=420000, episode_reward=12.94 +/- 11.55
Episode length: 19.00 +/- 15.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 12.9     |
| time/              |          |
|    total_timesteps | 420000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.9     |
|    ep_rew_mean     | 15.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1050     |
|    time_elapsed    | 116154   |
|    total_timesteps | 420000   |
---------------------------------
Eval num_timesteps=420200, episode_reward=29.06 +/- 10.84
Episode length: 42.20 +/- 15.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.2         |
|    mean_reward          | 29.1         |
| time/                   |              |
|    total_timesteps      | 420200       |
| train/                  |              |
|    approx_kl            | 0.0028922695 |
|    clip_fraction        | 0.0763       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.487       |
|    explained_variance   | 0.474        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.656        |
|    n_updates            | 10500        |
|    policy_gradient_loss | -0.0077      |
|    value_loss           | 1.57         |
------------------------------------------
Eval num_timesteps=420400, episode_reward=10.80 +/- 12.08
Episode length: 16.20 +/- 17.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | 10.8     |
| time/              |          |
|    total_timesteps | 420400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.1     |
|    ep_rew_mean     | 14.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1051     |
|    time_elapsed    | 116262   |
|    total_timesteps | 420400   |
---------------------------------
Eval num_timesteps=420600, episode_reward=12.08 +/- 11.55
Episode length: 18.20 +/- 17.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.2         |
|    mean_reward          | 12.1         |
| time/                   |              |
|    total_timesteps      | 420600       |
| train/                  |              |
|    approx_kl            | 0.0055430154 |
|    clip_fraction        | 0.146        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.43        |
|    explained_variance   | 0.627        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.7          |
|    n_updates            | 10510        |
|    policy_gradient_loss | -0.00877     |
|    value_loss           | 1.34         |
------------------------------------------
Eval num_timesteps=420800, episode_reward=5.14 +/- 3.92
Episode length: 8.20 +/- 5.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8.2      |
|    mean_reward     | 5.14     |
| time/              |          |
|    total_timesteps | 420800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | 14       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1052     |
|    time_elapsed    | 116349   |
|    total_timesteps | 420800   |
---------------------------------
Eval num_timesteps=421000, episode_reward=17.41 +/- 14.91
Episode length: 25.20 +/- 20.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.2         |
|    mean_reward          | 17.4         |
| time/                   |              |
|    total_timesteps      | 421000       |
| train/                  |              |
|    approx_kl            | 0.0055309804 |
|    clip_fraction        | 0.128        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.448       |
|    explained_variance   | 0.377        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.46         |
|    n_updates            | 10520        |
|    policy_gradient_loss | -0.00845     |
|    value_loss           | 2.36         |
------------------------------------------
Eval num_timesteps=421200, episode_reward=12.97 +/- 11.96
Episode length: 18.80 +/- 15.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.8     |
|    mean_reward     | 13       |
| time/              |          |
|    total_timesteps | 421200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22       |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1053     |
|    time_elapsed    | 116457   |
|    total_timesteps | 421200   |
---------------------------------
Eval num_timesteps=421400, episode_reward=10.31 +/- 13.26
Episode length: 15.00 +/- 17.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15           |
|    mean_reward          | 10.3         |
| time/                   |              |
|    total_timesteps      | 421400       |
| train/                  |              |
|    approx_kl            | 0.0024627496 |
|    clip_fraction        | 0.0661       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.444       |
|    explained_variance   | -0.0905      |
|    learning_rate        | 0.0001       |
|    loss                 | 0.503        |
|    n_updates            | 10530        |
|    policy_gradient_loss | -0.00617     |
|    value_loss           | 1.82         |
------------------------------------------
Eval num_timesteps=421600, episode_reward=15.07 +/- 15.01
Episode length: 23.00 +/- 22.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 15.1     |
| time/              |          |
|    total_timesteps | 421600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.7     |
|    ep_rew_mean     | 16.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1054     |
|    time_elapsed    | 116562   |
|    total_timesteps | 421600   |
---------------------------------
Eval num_timesteps=421800, episode_reward=13.51 +/- 10.74
Episode length: 20.40 +/- 15.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 20.4         |
|    mean_reward          | 13.5         |
| time/                   |              |
|    total_timesteps      | 421800       |
| train/                  |              |
|    approx_kl            | 0.0031572953 |
|    clip_fraction        | 0.102        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.403       |
|    explained_variance   | 0.414        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.567        |
|    n_updates            | 10540        |
|    policy_gradient_loss | -0.00613     |
|    value_loss           | 1.49         |
------------------------------------------
Eval num_timesteps=422000, episode_reward=12.02 +/- 11.73
Episode length: 18.40 +/- 16.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.4     |
|    mean_reward     | 12       |
| time/              |          |
|    total_timesteps | 422000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.7     |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1055     |
|    time_elapsed    | 116666   |
|    total_timesteps | 422000   |
---------------------------------
Eval num_timesteps=422200, episode_reward=28.79 +/- 10.81
Episode length: 42.20 +/- 15.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.2        |
|    mean_reward          | 28.8        |
| time/                   |             |
|    total_timesteps      | 422200      |
| train/                  |             |
|    approx_kl            | 0.001983843 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.433      |
|    explained_variance   | 0.408       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.06        |
|    n_updates            | 10550       |
|    policy_gradient_loss | -0.00828    |
|    value_loss           | 1.84        |
-----------------------------------------
Eval num_timesteps=422400, episode_reward=17.79 +/- 14.11
Episode length: 26.00 +/- 19.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | 17.8     |
| time/              |          |
|    total_timesteps | 422400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.3     |
|    ep_rew_mean     | 16.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1056     |
|    time_elapsed    | 116780   |
|    total_timesteps | 422400   |
---------------------------------
Eval num_timesteps=422600, episode_reward=23.09 +/- 15.33
Episode length: 32.80 +/- 21.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.8         |
|    mean_reward          | 23.1         |
| time/                   |              |
|    total_timesteps      | 422600       |
| train/                  |              |
|    approx_kl            | 0.0027856692 |
|    clip_fraction        | 0.0953       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.42        |
|    explained_variance   | 0.259        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.12         |
|    n_updates            | 10560        |
|    policy_gradient_loss | -0.00565     |
|    value_loss           | 2.76         |
------------------------------------------
Eval num_timesteps=422800, episode_reward=25.33 +/- 12.29
Episode length: 36.80 +/- 17.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 25.3     |
| time/              |          |
|    total_timesteps | 422800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.4     |
|    ep_rew_mean     | 16.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1057     |
|    time_elapsed    | 116886   |
|    total_timesteps | 422800   |
---------------------------------
Eval num_timesteps=423000, episode_reward=12.76 +/- 12.03
Episode length: 18.60 +/- 16.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.6         |
|    mean_reward          | 12.8         |
| time/                   |              |
|    total_timesteps      | 423000       |
| train/                  |              |
|    approx_kl            | 0.0061839707 |
|    clip_fraction        | 0.175        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.444       |
|    explained_variance   | 0.535        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.954        |
|    n_updates            | 10570        |
|    policy_gradient_loss | -0.00777     |
|    value_loss           | 1.49         |
------------------------------------------
Eval num_timesteps=423200, episode_reward=9.15 +/- 13.84
Episode length: 13.40 +/- 18.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.4     |
|    mean_reward     | 9.15     |
| time/              |          |
|    total_timesteps | 423200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.2     |
|    ep_rew_mean     | 17.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1058     |
|    time_elapsed    | 116989   |
|    total_timesteps | 423200   |
---------------------------------
Eval num_timesteps=423400, episode_reward=10.09 +/- 11.91
Episode length: 15.60 +/- 17.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15.6       |
|    mean_reward          | 10.1       |
| time/                   |            |
|    total_timesteps      | 423400     |
| train/                  |            |
|    approx_kl            | 0.00516735 |
|    clip_fraction        | 0.108      |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.5       |
|    explained_variance   | 0.346      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.568      |
|    n_updates            | 10580      |
|    policy_gradient_loss | -0.00632   |
|    value_loss           | 1.76       |
----------------------------------------
Eval num_timesteps=423600, episode_reward=16.24 +/- 15.85
Episode length: 23.40 +/- 21.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 16.2     |
| time/              |          |
|    total_timesteps | 423600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.6     |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1059     |
|    time_elapsed    | 117116   |
|    total_timesteps | 423600   |
---------------------------------
Eval num_timesteps=423800, episode_reward=23.07 +/- 16.24
Episode length: 32.20 +/- 21.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 32.2        |
|    mean_reward          | 23.1        |
| time/                   |             |
|    total_timesteps      | 423800      |
| train/                  |             |
|    approx_kl            | 0.004061376 |
|    clip_fraction        | 0.0737      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.432      |
|    explained_variance   | 0.304       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.192       |
|    n_updates            | 10590       |
|    policy_gradient_loss | -0.0064     |
|    value_loss           | 1.9         |
-----------------------------------------
Eval num_timesteps=424000, episode_reward=16.11 +/- 16.45
Episode length: 22.80 +/- 22.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 16.1     |
| time/              |          |
|    total_timesteps | 424000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.7     |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1060     |
|    time_elapsed    | 117218   |
|    total_timesteps | 424000   |
---------------------------------
Eval num_timesteps=424200, episode_reward=4.95 +/- 2.42
Episode length: 8.00 +/- 3.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 8            |
|    mean_reward          | 4.95         |
| time/                   |              |
|    total_timesteps      | 424200       |
| train/                  |              |
|    approx_kl            | 0.0067306324 |
|    clip_fraction        | 0.153        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.451       |
|    explained_variance   | 0.187        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.802        |
|    n_updates            | 10600        |
|    policy_gradient_loss | -0.00853     |
|    value_loss           | 0.993        |
------------------------------------------
Eval num_timesteps=424400, episode_reward=18.12 +/- 14.37
Episode length: 26.00 +/- 19.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | 18.1     |
| time/              |          |
|    total_timesteps | 424400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26       |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1061     |
|    time_elapsed    | 117306   |
|    total_timesteps | 424400   |
---------------------------------
Eval num_timesteps=424600, episode_reward=29.54 +/- 8.77
Episode length: 43.80 +/- 12.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 43.8         |
|    mean_reward          | 29.5         |
| time/                   |              |
|    total_timesteps      | 424600       |
| train/                  |              |
|    approx_kl            | 0.0040702545 |
|    clip_fraction        | 0.103        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.46        |
|    explained_variance   | 0.326        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.78         |
|    n_updates            | 10610        |
|    policy_gradient_loss | -0.00796     |
|    value_loss           | 2.13         |
------------------------------------------
Eval num_timesteps=424800, episode_reward=20.50 +/- 13.16
Episode length: 29.40 +/- 18.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.4     |
|    mean_reward     | 20.5     |
| time/              |          |
|    total_timesteps | 424800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.9     |
|    ep_rew_mean     | 17.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1062     |
|    time_elapsed    | 117425   |
|    total_timesteps | 424800   |
---------------------------------
Eval num_timesteps=425000, episode_reward=18.22 +/- 14.20
Episode length: 26.20 +/- 19.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.2         |
|    mean_reward          | 18.2         |
| time/                   |              |
|    total_timesteps      | 425000       |
| train/                  |              |
|    approx_kl            | 0.0035946476 |
|    clip_fraction        | 0.098        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.447       |
|    explained_variance   | 0.168        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.28         |
|    n_updates            | 10620        |
|    policy_gradient_loss | -0.00838     |
|    value_loss           | 2.51         |
------------------------------------------
Eval num_timesteps=425200, episode_reward=17.54 +/- 14.30
Episode length: 25.80 +/- 19.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | 17.5     |
| time/              |          |
|    total_timesteps | 425200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.9     |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1063     |
|    time_elapsed    | 117531   |
|    total_timesteps | 425200   |
---------------------------------
Eval num_timesteps=425400, episode_reward=15.69 +/- 9.51
Episode length: 23.00 +/- 13.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 23          |
|    mean_reward          | 15.7        |
| time/                   |             |
|    total_timesteps      | 425400      |
| train/                  |             |
|    approx_kl            | 0.005336526 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.464      |
|    explained_variance   | 0.248       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.875       |
|    n_updates            | 10630       |
|    policy_gradient_loss | -0.00802    |
|    value_loss           | 2.46        |
-----------------------------------------
Eval num_timesteps=425600, episode_reward=22.57 +/- 16.02
Episode length: 32.20 +/- 21.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 22.6     |
| time/              |          |
|    total_timesteps | 425600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.2     |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1064     |
|    time_elapsed    | 117669   |
|    total_timesteps | 425600   |
---------------------------------
Eval num_timesteps=425800, episode_reward=9.95 +/- 13.40
Episode length: 14.60 +/- 17.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.6         |
|    mean_reward          | 9.95         |
| time/                   |              |
|    total_timesteps      | 425800       |
| train/                  |              |
|    approx_kl            | 0.0032841552 |
|    clip_fraction        | 0.12         |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.471       |
|    explained_variance   | 0.316        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.289        |
|    n_updates            | 10640        |
|    policy_gradient_loss | -0.00678     |
|    value_loss           | 1.48         |
------------------------------------------
Eval num_timesteps=426000, episode_reward=16.88 +/- 15.26
Episode length: 24.80 +/- 21.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 426000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.7     |
|    ep_rew_mean     | 17.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1065     |
|    time_elapsed    | 117775   |
|    total_timesteps | 426000   |
---------------------------------
Eval num_timesteps=426200, episode_reward=25.00 +/- 12.57
Episode length: 35.80 +/- 17.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.8         |
|    mean_reward          | 25           |
| time/                   |              |
|    total_timesteps      | 426200       |
| train/                  |              |
|    approx_kl            | 0.0034012885 |
|    clip_fraction        | 0.0996       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.449       |
|    explained_variance   | 0.326        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.56         |
|    n_updates            | 10650        |
|    policy_gradient_loss | -0.00999     |
|    value_loss           | 2.92         |
------------------------------------------
Eval num_timesteps=426400, episode_reward=16.08 +/- 15.48
Episode length: 23.60 +/- 21.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 16.1     |
| time/              |          |
|    total_timesteps | 426400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1066     |
|    time_elapsed    | 117886   |
|    total_timesteps | 426400   |
---------------------------------
Eval num_timesteps=426600, episode_reward=17.52 +/- 15.39
Episode length: 25.00 +/- 20.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 25          |
|    mean_reward          | 17.5        |
| time/                   |             |
|    total_timesteps      | 426600      |
| train/                  |             |
|    approx_kl            | 0.004111255 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.451      |
|    explained_variance   | 0.356       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.377       |
|    n_updates            | 10660       |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 1.54        |
-----------------------------------------
Eval num_timesteps=426800, episode_reward=11.21 +/- 11.95
Episode length: 17.00 +/- 16.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | 11.2     |
| time/              |          |
|    total_timesteps | 426800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.7     |
|    ep_rew_mean     | 16.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1067     |
|    time_elapsed    | 117991   |
|    total_timesteps | 426800   |
---------------------------------
Eval num_timesteps=427000, episode_reward=10.42 +/- 12.29
Episode length: 15.80 +/- 17.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | 10.4         |
| time/                   |              |
|    total_timesteps      | 427000       |
| train/                  |              |
|    approx_kl            | 0.0015490385 |
|    clip_fraction        | 0.0493       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.412       |
|    explained_variance   | 0.329        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.708        |
|    n_updates            | 10670        |
|    policy_gradient_loss | -0.00632     |
|    value_loss           | 2.64         |
------------------------------------------
Eval num_timesteps=427200, episode_reward=10.71 +/- 13.00
Episode length: 15.60 +/- 17.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 10.7     |
| time/              |          |
|    total_timesteps | 427200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.1     |
|    ep_rew_mean     | 15.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1068     |
|    time_elapsed    | 118094   |
|    total_timesteps | 427200   |
---------------------------------
Eval num_timesteps=427400, episode_reward=21.77 +/- 14.69
Episode length: 32.60 +/- 21.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.6         |
|    mean_reward          | 21.8         |
| time/                   |              |
|    total_timesteps      | 427400       |
| train/                  |              |
|    approx_kl            | 0.0030811545 |
|    clip_fraction        | 0.121        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.43        |
|    explained_variance   | 0.424        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.698        |
|    n_updates            | 10680        |
|    policy_gradient_loss | -0.00896     |
|    value_loss           | 2.29         |
------------------------------------------
Eval num_timesteps=427600, episode_reward=10.45 +/- 12.51
Episode length: 15.80 +/- 17.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | 10.5     |
| time/              |          |
|    total_timesteps | 427600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.2     |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1069     |
|    time_elapsed    | 118197   |
|    total_timesteps | 427600   |
---------------------------------
Eval num_timesteps=427800, episode_reward=17.89 +/- 15.36
Episode length: 25.00 +/- 20.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 25          |
|    mean_reward          | 17.9        |
| time/                   |             |
|    total_timesteps      | 427800      |
| train/                  |             |
|    approx_kl            | 0.005283218 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.444      |
|    explained_variance   | 0.422       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.14        |
|    n_updates            | 10690       |
|    policy_gradient_loss | -0.00951    |
|    value_loss           | 1.33        |
-----------------------------------------
Eval num_timesteps=428000, episode_reward=22.28 +/- 15.43
Episode length: 32.40 +/- 21.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.4     |
|    mean_reward     | 22.3     |
| time/              |          |
|    total_timesteps | 428000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.4     |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1070     |
|    time_elapsed    | 118351   |
|    total_timesteps | 428000   |
---------------------------------
Eval num_timesteps=428200, episode_reward=24.33 +/- 12.92
Episode length: 35.20 +/- 18.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.2         |
|    mean_reward          | 24.3         |
| time/                   |              |
|    total_timesteps      | 428200       |
| train/                  |              |
|    approx_kl            | 0.0035550103 |
|    clip_fraction        | 0.117        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.489       |
|    explained_variance   | 0.477        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.159        |
|    n_updates            | 10700        |
|    policy_gradient_loss | -0.0086      |
|    value_loss           | 2            |
------------------------------------------
Eval num_timesteps=428400, episode_reward=17.79 +/- 14.63
Episode length: 26.00 +/- 20.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | 17.8     |
| time/              |          |
|    total_timesteps | 428400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.2     |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1071     |
|    time_elapsed    | 118489   |
|    total_timesteps | 428400   |
---------------------------------
Eval num_timesteps=428600, episode_reward=21.85 +/- 16.41
Episode length: 31.40 +/- 22.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 31.4         |
|    mean_reward          | 21.8         |
| time/                   |              |
|    total_timesteps      | 428600       |
| train/                  |              |
|    approx_kl            | 0.0039679413 |
|    clip_fraction        | 0.0926       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.477       |
|    explained_variance   | 0.373        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.247        |
|    n_updates            | 10710        |
|    policy_gradient_loss | -0.00643     |
|    value_loss           | 1.56         |
------------------------------------------
Eval num_timesteps=428800, episode_reward=13.33 +/- 11.74
Episode length: 19.80 +/- 16.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.8     |
|    mean_reward     | 13.3     |
| time/              |          |
|    total_timesteps | 428800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.6     |
|    ep_rew_mean     | 15.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1072     |
|    time_elapsed    | 118623   |
|    total_timesteps | 428800   |
---------------------------------
Eval num_timesteps=429000, episode_reward=24.94 +/- 11.80
Episode length: 36.40 +/- 16.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.4         |
|    mean_reward          | 24.9         |
| time/                   |              |
|    total_timesteps      | 429000       |
| train/                  |              |
|    approx_kl            | 0.0032995397 |
|    clip_fraction        | 0.11         |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.43        |
|    explained_variance   | 0.542        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.102        |
|    n_updates            | 10720        |
|    policy_gradient_loss | -0.00856     |
|    value_loss           | 1.07         |
------------------------------------------
Eval num_timesteps=429200, episode_reward=28.78 +/- 13.61
Episode length: 40.60 +/- 18.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.6     |
|    mean_reward     | 28.8     |
| time/              |          |
|    total_timesteps | 429200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.2     |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1073     |
|    time_elapsed    | 118757   |
|    total_timesteps | 429200   |
---------------------------------
Eval num_timesteps=429400, episode_reward=9.58 +/- 6.82
Episode length: 14.20 +/- 9.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.2        |
|    mean_reward          | 9.58        |
| time/                   |             |
|    total_timesteps      | 429400      |
| train/                  |             |
|    approx_kl            | 0.002606909 |
|    clip_fraction        | 0.0902      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.456      |
|    explained_variance   | 0.485       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.886       |
|    n_updates            | 10730       |
|    policy_gradient_loss | -0.00704    |
|    value_loss           | 1.92        |
-----------------------------------------
Eval num_timesteps=429600, episode_reward=10.51 +/- 12.65
Episode length: 15.60 +/- 17.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 10.5     |
| time/              |          |
|    total_timesteps | 429600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.4     |
|    ep_rew_mean     | 14.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1074     |
|    time_elapsed    | 118853   |
|    total_timesteps | 429600   |
---------------------------------
Eval num_timesteps=429800, episode_reward=22.21 +/- 15.94
Episode length: 32.00 +/- 22.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32           |
|    mean_reward          | 22.2         |
| time/                   |              |
|    total_timesteps      | 429800       |
| train/                  |              |
|    approx_kl            | 0.0027395678 |
|    clip_fraction        | 0.101        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.49        |
|    explained_variance   | 0.282        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.96         |
|    n_updates            | 10740        |
|    policy_gradient_loss | -0.00725     |
|    value_loss           | 3.07         |
------------------------------------------
Eval num_timesteps=430000, episode_reward=16.65 +/- 15.04
Episode length: 24.40 +/- 20.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 430000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.5     |
|    ep_rew_mean     | 14.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1075     |
|    time_elapsed    | 118981   |
|    total_timesteps | 430000   |
---------------------------------
Eval num_timesteps=430200, episode_reward=12.93 +/- 12.17
Episode length: 18.80 +/- 16.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.8         |
|    mean_reward          | 12.9         |
| time/                   |              |
|    total_timesteps      | 430200       |
| train/                  |              |
|    approx_kl            | 0.0038844405 |
|    clip_fraction        | 0.102        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.41        |
|    explained_variance   | 0.523        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.58         |
|    n_updates            | 10750        |
|    policy_gradient_loss | -0.00706     |
|    value_loss           | 1.38         |
------------------------------------------
Eval num_timesteps=430400, episode_reward=23.43 +/- 13.61
Episode length: 34.40 +/- 19.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 23.4     |
| time/              |          |
|    total_timesteps | 430400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.2     |
|    ep_rew_mean     | 14.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1076     |
|    time_elapsed    | 119094   |
|    total_timesteps | 430400   |
---------------------------------
Eval num_timesteps=430600, episode_reward=14.77 +/- 16.11
Episode length: 22.00 +/- 22.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22           |
|    mean_reward          | 14.8         |
| time/                   |              |
|    total_timesteps      | 430600       |
| train/                  |              |
|    approx_kl            | 0.0077023776 |
|    clip_fraction        | 0.14         |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.488       |
|    explained_variance   | 0.609        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.166        |
|    n_updates            | 10760        |
|    policy_gradient_loss | -0.00689     |
|    value_loss           | 1.32         |
------------------------------------------
Eval num_timesteps=430800, episode_reward=21.41 +/- 11.96
Episode length: 31.00 +/- 16.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31       |
|    mean_reward     | 21.4     |
| time/              |          |
|    total_timesteps | 430800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.7     |
|    ep_rew_mean     | 13.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1077     |
|    time_elapsed    | 119205   |
|    total_timesteps | 430800   |
---------------------------------
Eval num_timesteps=431000, episode_reward=16.47 +/- 16.54
Episode length: 23.00 +/- 22.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23           |
|    mean_reward          | 16.5         |
| time/                   |              |
|    total_timesteps      | 431000       |
| train/                  |              |
|    approx_kl            | 0.0044308216 |
|    clip_fraction        | 0.124        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.477       |
|    explained_variance   | 0.517        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.209        |
|    n_updates            | 10770        |
|    policy_gradient_loss | -0.00881     |
|    value_loss           | 2.31         |
------------------------------------------
Eval num_timesteps=431200, episode_reward=11.46 +/- 12.34
Episode length: 16.80 +/- 16.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | 11.5     |
| time/              |          |
|    total_timesteps | 431200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.8     |
|    ep_rew_mean     | 13.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1078     |
|    time_elapsed    | 119307   |
|    total_timesteps | 431200   |
---------------------------------
Eval num_timesteps=431400, episode_reward=8.73 +/- 12.95
Episode length: 13.40 +/- 18.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.4        |
|    mean_reward          | 8.73        |
| time/                   |             |
|    total_timesteps      | 431400      |
| train/                  |             |
|    approx_kl            | 0.003664311 |
|    clip_fraction        | 0.0826      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.472      |
|    explained_variance   | 0.269       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.31        |
|    n_updates            | 10780       |
|    policy_gradient_loss | -0.00412    |
|    value_loss           | 2.51        |
-----------------------------------------
Eval num_timesteps=431600, episode_reward=16.11 +/- 10.02
Episode length: 24.40 +/- 14.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 16.1     |
| time/              |          |
|    total_timesteps | 431600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | 13.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1079     |
|    time_elapsed    | 119410   |
|    total_timesteps | 431600   |
---------------------------------
Eval num_timesteps=431800, episode_reward=16.03 +/- 16.48
Episode length: 22.80 +/- 22.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22.8         |
|    mean_reward          | 16           |
| time/                   |              |
|    total_timesteps      | 431800       |
| train/                  |              |
|    approx_kl            | 0.0023667873 |
|    clip_fraction        | 0.115        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.468       |
|    explained_variance   | 0.322        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.367        |
|    n_updates            | 10790        |
|    policy_gradient_loss | -0.0062      |
|    value_loss           | 2.96         |
------------------------------------------
Eval num_timesteps=432000, episode_reward=7.31 +/- 5.60
Episode length: 11.40 +/- 8.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 11.4     |
|    mean_reward     | 7.31     |
| time/              |          |
|    total_timesteps | 432000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.8     |
|    ep_rew_mean     | 12       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1080     |
|    time_elapsed    | 119503   |
|    total_timesteps | 432000   |
---------------------------------
Eval num_timesteps=432200, episode_reward=28.24 +/- 13.08
Episode length: 40.80 +/- 18.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 40.8         |
|    mean_reward          | 28.2         |
| time/                   |              |
|    total_timesteps      | 432200       |
| train/                  |              |
|    approx_kl            | 0.0035787933 |
|    clip_fraction        | 0.105        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.514       |
|    explained_variance   | 0.608        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.7          |
|    n_updates            | 10800        |
|    policy_gradient_loss | -0.00719     |
|    value_loss           | 1.85         |
------------------------------------------
Eval num_timesteps=432400, episode_reward=11.79 +/- 11.36
Episode length: 17.80 +/- 16.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | 11.8     |
| time/              |          |
|    total_timesteps | 432400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.2     |
|    ep_rew_mean     | 12.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1081     |
|    time_elapsed    | 119631   |
|    total_timesteps | 432400   |
---------------------------------
Eval num_timesteps=432600, episode_reward=29.90 +/- 11.38
Episode length: 42.20 +/- 15.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.2        |
|    mean_reward          | 29.9        |
| time/                   |             |
|    total_timesteps      | 432600      |
| train/                  |             |
|    approx_kl            | 0.003808688 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.506      |
|    explained_variance   | 0.0288      |
|    learning_rate        | 0.0001      |
|    loss                 | 1.9         |
|    n_updates            | 10810       |
|    policy_gradient_loss | -0.00759    |
|    value_loss           | 2.71        |
-----------------------------------------
Eval num_timesteps=432800, episode_reward=11.07 +/- 11.63
Episode length: 17.00 +/- 17.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | 11.1     |
| time/              |          |
|    total_timesteps | 432800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | 13.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1082     |
|    time_elapsed    | 119739   |
|    total_timesteps | 432800   |
---------------------------------
Eval num_timesteps=433000, episode_reward=24.25 +/- 13.57
Episode length: 34.80 +/- 18.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.8         |
|    mean_reward          | 24.2         |
| time/                   |              |
|    total_timesteps      | 433000       |
| train/                  |              |
|    approx_kl            | 0.0031943028 |
|    clip_fraction        | 0.0853       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.508       |
|    explained_variance   | 0.142        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0115       |
|    n_updates            | 10820        |
|    policy_gradient_loss | -0.00695     |
|    value_loss           | 1.32         |
------------------------------------------
Eval num_timesteps=433200, episode_reward=6.95 +/- 5.26
Episode length: 10.80 +/- 7.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10.8     |
|    mean_reward     | 6.95     |
| time/              |          |
|    total_timesteps | 433200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.7     |
|    ep_rew_mean     | 14.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1083     |
|    time_elapsed    | 119830   |
|    total_timesteps | 433200   |
---------------------------------
Eval num_timesteps=433400, episode_reward=12.21 +/- 12.59
Episode length: 17.80 +/- 16.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.8         |
|    mean_reward          | 12.2         |
| time/                   |              |
|    total_timesteps      | 433400       |
| train/                  |              |
|    approx_kl            | 0.0045899427 |
|    clip_fraction        | 0.221        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.531       |
|    explained_variance   | 0.263        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.19         |
|    n_updates            | 10830        |
|    policy_gradient_loss | -0.0114      |
|    value_loss           | 1.87         |
------------------------------------------
Eval num_timesteps=433600, episode_reward=10.91 +/- 11.26
Episode length: 16.80 +/- 16.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | 10.9     |
| time/              |          |
|    total_timesteps | 433600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.7     |
|    ep_rew_mean     | 14.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1084     |
|    time_elapsed    | 119933   |
|    total_timesteps | 433600   |
---------------------------------
Eval num_timesteps=433800, episode_reward=28.60 +/- 14.01
Episode length: 40.40 +/- 19.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 40.4         |
|    mean_reward          | 28.6         |
| time/                   |              |
|    total_timesteps      | 433800       |
| train/                  |              |
|    approx_kl            | 0.0023076262 |
|    clip_fraction        | 0.0725       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.476       |
|    explained_variance   | 0.339        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.57         |
|    n_updates            | 10840        |
|    policy_gradient_loss | -0.00514     |
|    value_loss           | 2.22         |
------------------------------------------
Eval num_timesteps=434000, episode_reward=16.65 +/- 15.59
Episode length: 24.00 +/- 21.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 434000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | 16       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1085     |
|    time_elapsed    | 120062   |
|    total_timesteps | 434000   |
---------------------------------
Eval num_timesteps=434200, episode_reward=17.13 +/- 14.17
Episode length: 25.40 +/- 20.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.4         |
|    mean_reward          | 17.1         |
| time/                   |              |
|    total_timesteps      | 434200       |
| train/                  |              |
|    approx_kl            | 0.0029879583 |
|    clip_fraction        | 0.0783       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.517       |
|    explained_variance   | 0.107        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.678        |
|    n_updates            | 10850        |
|    policy_gradient_loss | -0.00703     |
|    value_loss           | 1.9          |
------------------------------------------
Eval num_timesteps=434400, episode_reward=20.17 +/- 13.31
Episode length: 29.20 +/- 18.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.2     |
|    mean_reward     | 20.2     |
| time/              |          |
|    total_timesteps | 434400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.6     |
|    ep_rew_mean     | 16.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1086     |
|    time_elapsed    | 120196   |
|    total_timesteps | 434400   |
---------------------------------
Eval num_timesteps=434600, episode_reward=23.91 +/- 13.46
Episode length: 35.00 +/- 19.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35           |
|    mean_reward          | 23.9         |
| time/                   |              |
|    total_timesteps      | 434600       |
| train/                  |              |
|    approx_kl            | 0.0033866167 |
|    clip_fraction        | 0.117        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.487       |
|    explained_variance   | 0.0663       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.279        |
|    n_updates            | 10860        |
|    policy_gradient_loss | -0.00573     |
|    value_loss           | 2.51         |
------------------------------------------
Eval num_timesteps=434800, episode_reward=23.48 +/- 14.68
Episode length: 33.80 +/- 20.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 23.5     |
| time/              |          |
|    total_timesteps | 434800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.1     |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1087     |
|    time_elapsed    | 120311   |
|    total_timesteps | 434800   |
---------------------------------
Eval num_timesteps=435000, episode_reward=12.37 +/- 11.63
Episode length: 18.60 +/- 16.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.6         |
|    mean_reward          | 12.4         |
| time/                   |              |
|    total_timesteps      | 435000       |
| train/                  |              |
|    approx_kl            | 0.0054682316 |
|    clip_fraction        | 0.103        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.468       |
|    explained_variance   | 0.298        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.536        |
|    n_updates            | 10870        |
|    policy_gradient_loss | -0.0107      |
|    value_loss           | 2.19         |
------------------------------------------
Eval num_timesteps=435200, episode_reward=18.85 +/- 13.35
Episode length: 27.40 +/- 18.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.4     |
|    mean_reward     | 18.8     |
| time/              |          |
|    total_timesteps | 435200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.6     |
|    ep_rew_mean     | 16.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1088     |
|    time_elapsed    | 120413   |
|    total_timesteps | 435200   |
---------------------------------
Eval num_timesteps=435400, episode_reward=8.05 +/- 7.97
Episode length: 12.40 +/- 11.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 12.4        |
|    mean_reward          | 8.05        |
| time/                   |             |
|    total_timesteps      | 435400      |
| train/                  |             |
|    approx_kl            | 0.005407154 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.494      |
|    explained_variance   | 0.351       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.51        |
|    n_updates            | 10880       |
|    policy_gradient_loss | -0.00887    |
|    value_loss           | 2.23        |
-----------------------------------------
Eval num_timesteps=435600, episode_reward=19.89 +/- 11.70
Episode length: 29.80 +/- 17.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.8     |
|    mean_reward     | 19.9     |
| time/              |          |
|    total_timesteps | 435600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.8     |
|    ep_rew_mean     | 16.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1089     |
|    time_elapsed    | 120534   |
|    total_timesteps | 435600   |
---------------------------------
Eval num_timesteps=435800, episode_reward=15.20 +/- 16.22
Episode length: 22.20 +/- 22.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22.2         |
|    mean_reward          | 15.2         |
| time/                   |              |
|    total_timesteps      | 435800       |
| train/                  |              |
|    approx_kl            | 0.0046081576 |
|    clip_fraction        | 0.147        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.473       |
|    explained_variance   | 0.262        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.469        |
|    n_updates            | 10890        |
|    policy_gradient_loss | -0.01        |
|    value_loss           | 2.18         |
------------------------------------------
Eval num_timesteps=436000, episode_reward=19.05 +/- 14.03
Episode length: 27.40 +/- 18.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.4     |
|    mean_reward     | 19       |
| time/              |          |
|    total_timesteps | 436000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.4     |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1090     |
|    time_elapsed    | 120637   |
|    total_timesteps | 436000   |
---------------------------------
Eval num_timesteps=436200, episode_reward=17.81 +/- 14.91
Episode length: 26.20 +/- 20.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.2         |
|    mean_reward          | 17.8         |
| time/                   |              |
|    total_timesteps      | 436200       |
| train/                  |              |
|    approx_kl            | 0.0056500924 |
|    clip_fraction        | 0.159        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.512       |
|    explained_variance   | 0.527        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.149        |
|    n_updates            | 10900        |
|    policy_gradient_loss | -0.00735     |
|    value_loss           | 1.1          |
------------------------------------------
Eval num_timesteps=436400, episode_reward=3.19 +/- 1.55
Episode length: 5.40 +/- 2.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5.4      |
|    mean_reward     | 3.19     |
| time/              |          |
|    total_timesteps | 436400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.2     |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1091     |
|    time_elapsed    | 120730   |
|    total_timesteps | 436400   |
---------------------------------
Eval num_timesteps=436600, episode_reward=23.17 +/- 15.68
Episode length: 32.60 +/- 21.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.6         |
|    mean_reward          | 23.2         |
| time/                   |              |
|    total_timesteps      | 436600       |
| train/                  |              |
|    approx_kl            | 0.0040494846 |
|    clip_fraction        | 0.105        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.512       |
|    explained_variance   | 0.175        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.107        |
|    n_updates            | 10910        |
|    policy_gradient_loss | -0.00788     |
|    value_loss           | 0.732        |
------------------------------------------
Eval num_timesteps=436800, episode_reward=24.49 +/- 12.61
Episode length: 36.00 +/- 18.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 24.5     |
| time/              |          |
|    total_timesteps | 436800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.2     |
|    ep_rew_mean     | 17.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1092     |
|    time_elapsed    | 120849   |
|    total_timesteps | 436800   |
---------------------------------
Eval num_timesteps=437000, episode_reward=19.31 +/- 13.93
Episode length: 28.00 +/- 19.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 28           |
|    mean_reward          | 19.3         |
| time/                   |              |
|    total_timesteps      | 437000       |
| train/                  |              |
|    approx_kl            | 0.0057402547 |
|    clip_fraction        | 0.161        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.471       |
|    explained_variance   | 0.363        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.53         |
|    n_updates            | 10920        |
|    policy_gradient_loss | -0.0125      |
|    value_loss           | 1.99         |
------------------------------------------
Eval num_timesteps=437200, episode_reward=16.66 +/- 15.60
Episode length: 24.00 +/- 21.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 437200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1093     |
|    time_elapsed    | 120953   |
|    total_timesteps | 437200   |
---------------------------------
Eval num_timesteps=437400, episode_reward=16.01 +/- 15.23
Episode length: 23.60 +/- 21.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.6         |
|    mean_reward          | 16           |
| time/                   |              |
|    total_timesteps      | 437400       |
| train/                  |              |
|    approx_kl            | 0.0042895707 |
|    clip_fraction        | 0.142        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.471       |
|    explained_variance   | 0.432        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.637        |
|    n_updates            | 10930        |
|    policy_gradient_loss | -0.0105      |
|    value_loss           | 1.35         |
------------------------------------------
Eval num_timesteps=437600, episode_reward=32.29 +/- 5.58
Episode length: 46.00 +/- 8.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46       |
|    mean_reward     | 32.3     |
| time/              |          |
|    total_timesteps | 437600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.7     |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1094     |
|    time_elapsed    | 121082   |
|    total_timesteps | 437600   |
---------------------------------
Eval num_timesteps=437800, episode_reward=23.19 +/- 13.61
Episode length: 34.20 +/- 19.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.2        |
|    mean_reward          | 23.2        |
| time/                   |             |
|    total_timesteps      | 437800      |
| train/                  |             |
|    approx_kl            | 0.003474449 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.439      |
|    explained_variance   | 0.462       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.666       |
|    n_updates            | 10940       |
|    policy_gradient_loss | -0.00877    |
|    value_loss           | 1.84        |
-----------------------------------------
Eval num_timesteps=438000, episode_reward=18.13 +/- 15.39
Episode length: 25.40 +/- 20.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | 18.1     |
| time/              |          |
|    total_timesteps | 438000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.7     |
|    ep_rew_mean     | 19.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1095     |
|    time_elapsed    | 121194   |
|    total_timesteps | 438000   |
---------------------------------
Eval num_timesteps=438200, episode_reward=17.48 +/- 14.39
Episode length: 25.40 +/- 20.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.4         |
|    mean_reward          | 17.5         |
| time/                   |              |
|    total_timesteps      | 438200       |
| train/                  |              |
|    approx_kl            | 0.0038633626 |
|    clip_fraction        | 0.11         |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.5         |
|    explained_variance   | 0.35         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.41         |
|    n_updates            | 10950        |
|    policy_gradient_loss | -0.00646     |
|    value_loss           | 0.803        |
------------------------------------------
Eval num_timesteps=438400, episode_reward=23.51 +/- 12.38
Episode length: 35.40 +/- 18.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 23.5     |
| time/              |          |
|    total_timesteps | 438400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.7     |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1096     |
|    time_elapsed    | 121299   |
|    total_timesteps | 438400   |
---------------------------------
Eval num_timesteps=438600, episode_reward=22.16 +/- 16.48
Episode length: 31.60 +/- 22.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 31.6        |
|    mean_reward          | 22.2        |
| time/                   |             |
|    total_timesteps      | 438600      |
| train/                  |             |
|    approx_kl            | 0.002469564 |
|    clip_fraction        | 0.048       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.439      |
|    explained_variance   | 0.426       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.737       |
|    n_updates            | 10960       |
|    policy_gradient_loss | -0.00584    |
|    value_loss           | 2.26        |
-----------------------------------------
Eval num_timesteps=438800, episode_reward=9.25 +/- 12.71
Episode length: 14.00 +/- 18.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14       |
|    mean_reward     | 9.25     |
| time/              |          |
|    total_timesteps | 438800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.2     |
|    ep_rew_mean     | 17.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1097     |
|    time_elapsed    | 121402   |
|    total_timesteps | 438800   |
---------------------------------
Eval num_timesteps=439000, episode_reward=11.44 +/- 12.62
Episode length: 17.00 +/- 17.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17           |
|    mean_reward          | 11.4         |
| time/                   |              |
|    total_timesteps      | 439000       |
| train/                  |              |
|    approx_kl            | 0.0051388037 |
|    clip_fraction        | 0.126        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.509       |
|    explained_variance   | 0.433        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0461       |
|    n_updates            | 10970        |
|    policy_gradient_loss | -0.00638     |
|    value_loss           | 0.96         |
------------------------------------------
Eval num_timesteps=439200, episode_reward=19.19 +/- 13.92
Episode length: 28.40 +/- 19.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.4     |
|    mean_reward     | 19.2     |
| time/              |          |
|    total_timesteps | 439200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.5     |
|    ep_rew_mean     | 18.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1098     |
|    time_elapsed    | 121531   |
|    total_timesteps | 439200   |
---------------------------------
Eval num_timesteps=439400, episode_reward=11.58 +/- 11.69
Episode length: 17.60 +/- 16.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.6        |
|    mean_reward          | 11.6        |
| time/                   |             |
|    total_timesteps      | 439400      |
| train/                  |             |
|    approx_kl            | 0.012242658 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.471      |
|    explained_variance   | 0.53        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.595       |
|    n_updates            | 10980       |
|    policy_gradient_loss | -0.00516    |
|    value_loss           | 1.14        |
-----------------------------------------
Eval num_timesteps=439600, episode_reward=14.32 +/- 11.39
Episode length: 21.60 +/- 16.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 14.3     |
| time/              |          |
|    total_timesteps | 439600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.7     |
|    ep_rew_mean     | 18.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1099     |
|    time_elapsed    | 121634   |
|    total_timesteps | 439600   |
---------------------------------
Eval num_timesteps=439800, episode_reward=16.31 +/- 14.78
Episode length: 24.40 +/- 21.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.4         |
|    mean_reward          | 16.3         |
| time/                   |              |
|    total_timesteps      | 439800       |
| train/                  |              |
|    approx_kl            | 0.0056211767 |
|    clip_fraction        | 0.211        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.47        |
|    explained_variance   | 0.321        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.149        |
|    n_updates            | 10990        |
|    policy_gradient_loss | -0.00948     |
|    value_loss           | 1.16         |
------------------------------------------
Eval num_timesteps=440000, episode_reward=4.93 +/- 0.90
Episode length: 7.80 +/- 1.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 7.8      |
|    mean_reward     | 4.93     |
| time/              |          |
|    total_timesteps | 440000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26       |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1100     |
|    time_elapsed    | 121745   |
|    total_timesteps | 440000   |
---------------------------------
Eval num_timesteps=440200, episode_reward=13.92 +/- 12.61
Episode length: 20.40 +/- 17.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 20.4         |
|    mean_reward          | 13.9         |
| time/                   |              |
|    total_timesteps      | 440200       |
| train/                  |              |
|    approx_kl            | 0.0050366363 |
|    clip_fraction        | 0.118        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.465       |
|    explained_variance   | 0.429        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.117        |
|    n_updates            | 11000        |
|    policy_gradient_loss | -0.0074      |
|    value_loss           | 1.9          |
------------------------------------------
Eval num_timesteps=440400, episode_reward=3.67 +/- 1.06
Episode length: 6.20 +/- 1.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 6.2      |
|    mean_reward     | 3.67     |
| time/              |          |
|    total_timesteps | 440400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.2     |
|    ep_rew_mean     | 17.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1101     |
|    time_elapsed    | 121835   |
|    total_timesteps | 440400   |
---------------------------------
Eval num_timesteps=440600, episode_reward=16.02 +/- 16.45
Episode length: 22.80 +/- 22.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 22.8        |
|    mean_reward          | 16          |
| time/                   |             |
|    total_timesteps      | 440600      |
| train/                  |             |
|    approx_kl            | 0.003577228 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.467      |
|    explained_variance   | 0.48        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.768       |
|    n_updates            | 11010       |
|    policy_gradient_loss | -0.00978    |
|    value_loss           | 1.61        |
-----------------------------------------
Eval num_timesteps=440800, episode_reward=17.59 +/- 14.45
Episode length: 25.80 +/- 20.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 440800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.2     |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1102     |
|    time_elapsed    | 121938   |
|    total_timesteps | 440800   |
---------------------------------
Eval num_timesteps=441000, episode_reward=6.04 +/- 2.17
Episode length: 9.60 +/- 3.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9.6         |
|    mean_reward          | 6.04        |
| time/                   |             |
|    total_timesteps      | 441000      |
| train/                  |             |
|    approx_kl            | 0.002672113 |
|    clip_fraction        | 0.0971      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.418      |
|    explained_variance   | 0.535       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.674       |
|    n_updates            | 11020       |
|    policy_gradient_loss | -0.00811    |
|    value_loss           | 2           |
-----------------------------------------
Eval num_timesteps=441200, episode_reward=12.84 +/- 11.42
Episode length: 18.80 +/- 15.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.8     |
|    mean_reward     | 12.8     |
| time/              |          |
|    total_timesteps | 441200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.5     |
|    ep_rew_mean     | 16.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1103     |
|    time_elapsed    | 122024   |
|    total_timesteps | 441200   |
---------------------------------
Eval num_timesteps=441400, episode_reward=9.49 +/- 11.99
Episode length: 14.80 +/- 17.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.8         |
|    mean_reward          | 9.49         |
| time/                   |              |
|    total_timesteps      | 441400       |
| train/                  |              |
|    approx_kl            | 0.0038239025 |
|    clip_fraction        | 0.104        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.461       |
|    explained_variance   | 0.352        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.919        |
|    n_updates            | 11030        |
|    policy_gradient_loss | -0.0068      |
|    value_loss           | 2.24         |
------------------------------------------
Eval num_timesteps=441600, episode_reward=10.26 +/- 12.90
Episode length: 15.20 +/- 17.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | 10.3     |
| time/              |          |
|    total_timesteps | 441600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 15       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1104     |
|    time_elapsed    | 122126   |
|    total_timesteps | 441600   |
---------------------------------
Eval num_timesteps=441800, episode_reward=22.37 +/- 15.30
Episode length: 32.40 +/- 21.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.4         |
|    mean_reward          | 22.4         |
| time/                   |              |
|    total_timesteps      | 441800       |
| train/                  |              |
|    approx_kl            | 0.0047219885 |
|    clip_fraction        | 0.146        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.424       |
|    explained_variance   | 0.641        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.581        |
|    n_updates            | 11040        |
|    policy_gradient_loss | -0.00787     |
|    value_loss           | 1.67         |
------------------------------------------
Eval num_timesteps=442000, episode_reward=18.20 +/- 13.32
Episode length: 26.80 +/- 18.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.8     |
|    mean_reward     | 18.2     |
| time/              |          |
|    total_timesteps | 442000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.5     |
|    ep_rew_mean     | 14.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1105     |
|    time_elapsed    | 122254   |
|    total_timesteps | 442000   |
---------------------------------
Eval num_timesteps=442200, episode_reward=14.88 +/- 16.49
Episode length: 21.80 +/- 23.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 21.8         |
|    mean_reward          | 14.9         |
| time/                   |              |
|    total_timesteps      | 442200       |
| train/                  |              |
|    approx_kl            | 0.0033855035 |
|    clip_fraction        | 0.071        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.451       |
|    explained_variance   | 0.371        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.125        |
|    n_updates            | 11050        |
|    policy_gradient_loss | -0.0052      |
|    value_loss           | 1.67         |
------------------------------------------
Eval num_timesteps=442400, episode_reward=9.75 +/- 13.67
Episode length: 14.20 +/- 18.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.2     |
|    mean_reward     | 9.75     |
| time/              |          |
|    total_timesteps | 442400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.3     |
|    ep_rew_mean     | 13.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1106     |
|    time_elapsed    | 122359   |
|    total_timesteps | 442400   |
---------------------------------
Eval num_timesteps=442600, episode_reward=23.71 +/- 12.30
Episode length: 33.80 +/- 16.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.8        |
|    mean_reward          | 23.7        |
| time/                   |             |
|    total_timesteps      | 442600      |
| train/                  |             |
|    approx_kl            | 0.003878206 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.458      |
|    explained_variance   | 0.388       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.455       |
|    n_updates            | 11060       |
|    policy_gradient_loss | -0.00839    |
|    value_loss           | 2.69        |
-----------------------------------------
Eval num_timesteps=442800, episode_reward=22.04 +/- 16.64
Episode length: 31.40 +/- 22.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.4     |
|    mean_reward     | 22       |
| time/              |          |
|    total_timesteps | 442800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.5     |
|    ep_rew_mean     | 13.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1107     |
|    time_elapsed    | 122485   |
|    total_timesteps | 442800   |
---------------------------------
Eval num_timesteps=443000, episode_reward=17.18 +/- 14.71
Episode length: 25.20 +/- 20.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.2         |
|    mean_reward          | 17.2         |
| time/                   |              |
|    total_timesteps      | 443000       |
| train/                  |              |
|    approx_kl            | 0.0026604417 |
|    clip_fraction        | 0.0741       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.446       |
|    explained_variance   | 0.628        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.609        |
|    n_updates            | 11070        |
|    policy_gradient_loss | -0.00645     |
|    value_loss           | 1.48         |
------------------------------------------
Eval num_timesteps=443200, episode_reward=12.45 +/- 12.37
Episode length: 18.00 +/- 16.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | 12.5     |
| time/              |          |
|    total_timesteps | 443200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.3     |
|    ep_rew_mean     | 12.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1108     |
|    time_elapsed    | 122593   |
|    total_timesteps | 443200   |
---------------------------------
Eval num_timesteps=443400, episode_reward=16.27 +/- 16.26
Episode length: 23.20 +/- 21.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.2         |
|    mean_reward          | 16.3         |
| time/                   |              |
|    total_timesteps      | 443400       |
| train/                  |              |
|    approx_kl            | 0.0036018977 |
|    clip_fraction        | 0.083        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.469       |
|    explained_variance   | 0.191        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.9          |
|    n_updates            | 11080        |
|    policy_gradient_loss | -0.00767     |
|    value_loss           | 3.14         |
------------------------------------------
Eval num_timesteps=443600, episode_reward=24.34 +/- 14.07
Episode length: 34.80 +/- 19.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 24.3     |
| time/              |          |
|    total_timesteps | 443600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.8     |
|    ep_rew_mean     | 14       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1109     |
|    time_elapsed    | 122705   |
|    total_timesteps | 443600   |
---------------------------------
Eval num_timesteps=443800, episode_reward=22.24 +/- 15.04
Episode length: 32.60 +/- 21.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.6         |
|    mean_reward          | 22.2         |
| time/                   |              |
|    total_timesteps      | 443800       |
| train/                  |              |
|    approx_kl            | 0.0029258206 |
|    clip_fraction        | 0.0891       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.437       |
|    explained_variance   | 0.183        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.45         |
|    n_updates            | 11090        |
|    policy_gradient_loss | -0.00616     |
|    value_loss           | 2.21         |
------------------------------------------
Eval num_timesteps=444000, episode_reward=11.01 +/- 12.94
Episode length: 16.00 +/- 17.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | 11       |
| time/              |          |
|    total_timesteps | 444000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.2     |
|    ep_rew_mean     | 14.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1110     |
|    time_elapsed    | 122833   |
|    total_timesteps | 444000   |
---------------------------------
Eval num_timesteps=444200, episode_reward=18.29 +/- 14.67
Episode length: 26.00 +/- 19.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 26          |
|    mean_reward          | 18.3        |
| time/                   |             |
|    total_timesteps      | 444200      |
| train/                  |             |
|    approx_kl            | 0.005514075 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.493      |
|    explained_variance   | -0.0648     |
|    learning_rate        | 0.0001      |
|    loss                 | 0.484       |
|    n_updates            | 11100       |
|    policy_gradient_loss | -0.00614    |
|    value_loss           | 2.02        |
-----------------------------------------
Eval num_timesteps=444400, episode_reward=18.02 +/- 15.42
Episode length: 25.40 +/- 20.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | 18       |
| time/              |          |
|    total_timesteps | 444400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.8     |
|    ep_rew_mean     | 14.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1111     |
|    time_elapsed    | 122946   |
|    total_timesteps | 444400   |
---------------------------------
Eval num_timesteps=444600, episode_reward=9.11 +/- 12.75
Episode length: 14.00 +/- 18.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14           |
|    mean_reward          | 9.11         |
| time/                   |              |
|    total_timesteps      | 444600       |
| train/                  |              |
|    approx_kl            | 0.0024479607 |
|    clip_fraction        | 0.0984       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.456       |
|    explained_variance   | 0.255        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.279        |
|    n_updates            | 11110        |
|    policy_gradient_loss | -0.0071      |
|    value_loss           | 1.52         |
------------------------------------------
Eval num_timesteps=444800, episode_reward=17.77 +/- 13.31
Episode length: 26.40 +/- 19.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.4     |
|    mean_reward     | 17.8     |
| time/              |          |
|    total_timesteps | 444800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24       |
|    ep_rew_mean     | 16.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1112     |
|    time_elapsed    | 123049   |
|    total_timesteps | 444800   |
---------------------------------
Eval num_timesteps=445000, episode_reward=24.29 +/- 11.93
Episode length: 36.40 +/- 17.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.4        |
|    mean_reward          | 24.3        |
| time/                   |             |
|    total_timesteps      | 445000      |
| train/                  |             |
|    approx_kl            | 0.010505824 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.46       |
|    explained_variance   | 0.231       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.23        |
|    n_updates            | 11120       |
|    policy_gradient_loss | -0.00954    |
|    value_loss           | 1.72        |
-----------------------------------------
Eval num_timesteps=445200, episode_reward=16.54 +/- 15.75
Episode length: 23.80 +/- 21.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 445200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.5     |
|    ep_rew_mean     | 16.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1113     |
|    time_elapsed    | 123163   |
|    total_timesteps | 445200   |
---------------------------------
Eval num_timesteps=445400, episode_reward=16.11 +/- 15.98
Episode length: 23.20 +/- 21.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.2         |
|    mean_reward          | 16.1         |
| time/                   |              |
|    total_timesteps      | 445400       |
| train/                  |              |
|    approx_kl            | 0.0015437581 |
|    clip_fraction        | 0.0862       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.448       |
|    explained_variance   | 0.374        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.245        |
|    n_updates            | 11130        |
|    policy_gradient_loss | -0.00333     |
|    value_loss           | 1.34         |
------------------------------------------
Eval num_timesteps=445600, episode_reward=17.32 +/- 13.62
Episode length: 26.20 +/- 19.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | 17.3     |
| time/              |          |
|    total_timesteps | 445600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.3     |
|    ep_rew_mean     | 18       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1114     |
|    time_elapsed    | 123273   |
|    total_timesteps | 445600   |
---------------------------------
Eval num_timesteps=445800, episode_reward=17.02 +/- 14.41
Episode length: 25.20 +/- 20.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 25.2        |
|    mean_reward          | 17          |
| time/                   |             |
|    total_timesteps      | 445800      |
| train/                  |             |
|    approx_kl            | 0.005633925 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.466      |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.99        |
|    n_updates            | 11140       |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 1.86        |
-----------------------------------------
Eval num_timesteps=446000, episode_reward=12.04 +/- 12.61
Episode length: 17.60 +/- 16.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | 12       |
| time/              |          |
|    total_timesteps | 446000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.2     |
|    ep_rew_mean     | 18.6     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1115     |
|    time_elapsed    | 123384   |
|    total_timesteps | 446000   |
---------------------------------
Eval num_timesteps=446200, episode_reward=35.36 +/- 0.83
Episode length: 50.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 35.4        |
| time/                   |             |
|    total_timesteps      | 446200      |
| train/                  |             |
|    approx_kl            | 0.004476011 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.449      |
|    explained_variance   | 0.278       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.12        |
|    n_updates            | 11150       |
|    policy_gradient_loss | -0.0088     |
|    value_loss           | 1.92        |
-----------------------------------------
Eval num_timesteps=446400, episode_reward=11.67 +/- 12.04
Episode length: 17.60 +/- 17.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | 11.7     |
| time/              |          |
|    total_timesteps | 446400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.7     |
|    ep_rew_mean     | 19.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1116     |
|    time_elapsed    | 123520   |
|    total_timesteps | 446400   |
---------------------------------
Eval num_timesteps=446600, episode_reward=22.60 +/- 15.48
Episode length: 32.40 +/- 21.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.4         |
|    mean_reward          | 22.6         |
| time/                   |              |
|    total_timesteps      | 446600       |
| train/                  |              |
|    approx_kl            | 0.0028654002 |
|    clip_fraction        | 0.125        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.456       |
|    explained_variance   | 0.362        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.319        |
|    n_updates            | 11160        |
|    policy_gradient_loss | -0.005       |
|    value_loss           | 1.28         |
------------------------------------------
Eval num_timesteps=446800, episode_reward=23.13 +/- 14.36
Episode length: 33.60 +/- 20.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 23.1     |
| time/              |          |
|    total_timesteps | 446800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.9     |
|    ep_rew_mean     | 19.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1117     |
|    time_elapsed    | 123650   |
|    total_timesteps | 446800   |
---------------------------------
Eval num_timesteps=447000, episode_reward=11.45 +/- 12.12
Episode length: 17.00 +/- 16.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17          |
|    mean_reward          | 11.4        |
| time/                   |             |
|    total_timesteps      | 447000      |
| train/                  |             |
|    approx_kl            | 0.003713241 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.534      |
|    explained_variance   | 0.275       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.404       |
|    n_updates            | 11170       |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 2           |
-----------------------------------------
Eval num_timesteps=447200, episode_reward=16.89 +/- 13.93
Episode length: 25.40 +/- 20.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 447200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.7     |
|    ep_rew_mean     | 18.4     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1118     |
|    time_elapsed    | 123760   |
|    total_timesteps | 447200   |
---------------------------------
Eval num_timesteps=447400, episode_reward=11.95 +/- 12.09
Episode length: 17.80 +/- 16.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.8        |
|    mean_reward          | 12          |
| time/                   |             |
|    total_timesteps      | 447400      |
| train/                  |             |
|    approx_kl            | 0.006667177 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.463      |
|    explained_variance   | 0.197       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.301       |
|    n_updates            | 11180       |
|    policy_gradient_loss | -0.00662    |
|    value_loss           | 2.12        |
-----------------------------------------
Eval num_timesteps=447600, episode_reward=22.45 +/- 16.14
Episode length: 32.00 +/- 22.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32       |
|    mean_reward     | 22.5     |
| time/              |          |
|    total_timesteps | 447600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.4     |
|    ep_rew_mean     | 18.2     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1119     |
|    time_elapsed    | 123872   |
|    total_timesteps | 447600   |
---------------------------------
Eval num_timesteps=447800, episode_reward=10.04 +/- 12.92
Episode length: 14.80 +/- 17.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.8         |
|    mean_reward          | 10           |
| time/                   |              |
|    total_timesteps      | 447800       |
| train/                  |              |
|    approx_kl            | 0.0049799597 |
|    clip_fraction        | 0.111        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.466       |
|    explained_variance   | 0.524        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.353        |
|    n_updates            | 11190        |
|    policy_gradient_loss | -0.00785     |
|    value_loss           | 1.43         |
------------------------------------------
Eval num_timesteps=448000, episode_reward=18.27 +/- 13.99
Episode length: 26.60 +/- 19.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.6     |
|    mean_reward     | 18.3     |
| time/              |          |
|    total_timesteps | 448000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25       |
|    ep_rew_mean     | 17       |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1120     |
|    time_elapsed    | 123974   |
|    total_timesteps | 448000   |
---------------------------------
Eval num_timesteps=448200, episode_reward=11.04 +/- 12.70
Episode length: 16.60 +/- 17.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.6        |
|    mean_reward          | 11          |
| time/                   |             |
|    total_timesteps      | 448200      |
| train/                  |             |
|    approx_kl            | 0.005682272 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.495      |
|    explained_variance   | 0.272       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.736       |
|    n_updates            | 11200       |
|    policy_gradient_loss | -0.00806    |
|    value_loss           | 2.4         |
-----------------------------------------
Eval num_timesteps=448400, episode_reward=16.20 +/- 15.92
Episode length: 23.40 +/- 21.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 16.2     |
| time/              |          |
|    total_timesteps | 448400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.1     |
|    ep_rew_mean     | 17.8     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1121     |
|    time_elapsed    | 124080   |
|    total_timesteps | 448400   |
---------------------------------
Eval num_timesteps=448600, episode_reward=22.02 +/- 16.19
Episode length: 31.60 +/- 22.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 31.6        |
|    mean_reward          | 22          |
| time/                   |             |
|    total_timesteps      | 448600      |
| train/                  |             |
|    approx_kl            | 0.004640556 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.444      |
|    explained_variance   | 0.201       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.595       |
|    n_updates            | 11210       |
|    policy_gradient_loss | -0.00867    |
|    value_loss           | 1.99        |
-----------------------------------------
Eval num_timesteps=448800, episode_reward=26.20 +/- 11.84
Episode length: 37.20 +/- 15.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.2     |
|    mean_reward     | 26.2     |
| time/              |          |
|    total_timesteps | 448800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.3     |
|    ep_rew_mean     | 16.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1122     |
|    time_elapsed    | 124212   |
|    total_timesteps | 448800   |
---------------------------------
Eval num_timesteps=449000, episode_reward=17.82 +/- 14.55
Episode length: 25.80 +/- 19.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.8         |
|    mean_reward          | 17.8         |
| time/                   |              |
|    total_timesteps      | 449000       |
| train/                  |              |
|    approx_kl            | 0.0025994952 |
|    clip_fraction        | 0.0902       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.425       |
|    explained_variance   | 0.436        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.113        |
|    n_updates            | 11220        |
|    policy_gradient_loss | -0.00933     |
|    value_loss           | 1.67         |
------------------------------------------
Eval num_timesteps=449200, episode_reward=13.95 +/- 11.68
Episode length: 20.60 +/- 16.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 14       |
| time/              |          |
|    total_timesteps | 449200   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.5     |
|    ep_rew_mean     | 15.3     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1123     |
|    time_elapsed    | 124318   |
|    total_timesteps | 449200   |
---------------------------------
Eval num_timesteps=449400, episode_reward=16.57 +/- 14.81
Episode length: 24.40 +/- 21.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.4         |
|    mean_reward          | 16.6         |
| time/                   |              |
|    total_timesteps      | 449400       |
| train/                  |              |
|    approx_kl            | 0.0026199054 |
|    clip_fraction        | 0.0772       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.443       |
|    explained_variance   | 0.457        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.28         |
|    n_updates            | 11230        |
|    policy_gradient_loss | -0.00774     |
|    value_loss           | 2.09         |
------------------------------------------
Eval num_timesteps=449600, episode_reward=11.03 +/- 13.16
Episode length: 16.20 +/- 17.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | 11       |
| time/              |          |
|    total_timesteps | 449600   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.1     |
|    ep_rew_mean     | 15.1     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1124     |
|    time_elapsed    | 124428   |
|    total_timesteps | 449600   |
---------------------------------
Eval num_timesteps=449800, episode_reward=4.65 +/- 2.41
Episode length: 7.60 +/- 3.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 7.6          |
|    mean_reward          | 4.65         |
| time/                   |              |
|    total_timesteps      | 449800       |
| train/                  |              |
|    approx_kl            | 0.0053410926 |
|    clip_fraction        | 0.094        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.483       |
|    explained_variance   | 0.248        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.58         |
|    n_updates            | 11240        |
|    policy_gradient_loss | -0.00535     |
|    value_loss           | 2.64         |
------------------------------------------
Eval num_timesteps=450000, episode_reward=5.53 +/- 2.30
Episode length: 8.80 +/- 3.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8.8      |
|    mean_reward     | 5.53     |
| time/              |          |
|    total_timesteps | 450000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.9     |
|    ep_rew_mean     | 15.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1125     |
|    time_elapsed    | 124497   |
|    total_timesteps | 450000   |
---------------------------------
Eval num_timesteps=450200, episode_reward=19.39 +/- 13.90
Episode length: 27.40 +/- 18.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 27.4        |
|    mean_reward          | 19.4        |
| time/                   |             |
|    total_timesteps      | 450200      |
| train/                  |             |
|    approx_kl            | 0.003179984 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.474      |
|    explained_variance   | 0.493       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.746       |
|    n_updates            | 11250       |
|    policy_gradient_loss | -0.00515    |
|    value_loss           | 1           |
-----------------------------------------
Eval num_timesteps=450400, episode_reward=15.41 +/- 16.05
Episode length: 22.60 +/- 22.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 15.4     |
| time/              |          |
|    total_timesteps | 450400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23       |
|    ep_rew_mean     | 15.7     |
| time/              |          |
|    fps             | 3        |
|    iterations      | 1126     |
|    time_elapsed    | 124602   |
|    total_timesteps | 450400   |
---------------------------------
